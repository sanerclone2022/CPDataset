[{"authorTime":"2015-06-23 06:22:17","codes":[{"authorDate":"2015-06-21 11:03:59","commitOrder":3,"curCode":"  public UTF8String getUTF8String(int i) {\n    assertIndexIsValid(i);\n    final long offsetToStringSize = getLong(i);\n    final int stringSizeInBytes =\n      (int) PlatformDependent.UNSAFE.getLong(baseObject, baseOffset + offsetToStringSize);\n    final byte[] strBytes = new byte[stringSizeInBytes];\n    PlatformDependent.copyMemory(\n      baseObject,\n      baseOffset + offsetToStringSize + 8,  \r\n      strBytes,\n      PlatformDependent.BYTE_ARRAY_OFFSET,\n      stringSizeInBytes\n    );\n    return UTF8String.fromBytes(strBytes);\n  }\n","date":"2015-06-21 11:03:59","endLine":327,"groupId":"2553","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"getUTF8String","params":"(inti)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/ed/04d2e50ec849e46b85d3e1466663db320a6109.src","preCode":"  public UTF8String getUTF8String(int i) {\n    assertIndexIsValid(i);\n    final long offsetToStringSize = getLong(i);\n    final int stringSizeInBytes =\n      (int) PlatformDependent.UNSAFE.getLong(baseObject, baseOffset + offsetToStringSize);\n    final byte[] strBytes = new byte[stringSizeInBytes];\n    PlatformDependent.copyMemory(\n      baseObject,\n      baseOffset + offsetToStringSize + 8,  \r\n      strBytes,\n      PlatformDependent.BYTE_ARRAY_OFFSET,\n      stringSizeInBytes\n    );\n    return UTF8String.fromBytes(strBytes);\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":313,"status":"NB"},{"authorDate":"2015-06-23 06:22:17","commitOrder":3,"curCode":"  public byte[] getBinary(int i) {\n    assertIndexIsValid(i);\n    final long offsetAndSize = getLong(i);\n    final int offset = (int)(offsetAndSize >> 32);\n    final int size = (int)(offsetAndSize & ((1L << 32) - 1));\n    final byte[] bytes = new byte[size];\n    PlatformDependent.copyMemory(\n      baseObject,\n      baseOffset + offset,\n      bytes,\n      PlatformDependent.BYTE_ARRAY_OFFSET,\n      size\n    );\n    return bytes;\n  }\n","date":"2015-06-23 06:22:17","endLine":331,"groupId":"657","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"getBinary","params":"(inti)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/bb/2f2079b40f0ac09f0fda216b1936ee8354189c.src","preCode":"  public byte[] getBinary(int i) {\n    assertIndexIsValid(i);\n    final long offsetAndSize = getLong(i);\n    final int offset = (int)(offsetAndSize >> 32);\n    final int size = (int)(offsetAndSize & ((1L << 32) - 1));\n    final byte[] bytes = new byte[size];\n    PlatformDependent.copyMemory(\n      baseObject,\n      baseOffset + offset,\n      bytes,\n      PlatformDependent.BYTE_ARRAY_OFFSET,\n      size\n    );\n    return bytes;\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":317,"status":"B"}],"commitId":"96aa01378e3b3dbb4601d31c7312a311cb65b22e","commitMessage":"@@@[SPARK-8492] [SQL] support binaryType in UnsafeRow\n\nSupport BinaryType in UnsafeRow.  just like StringType.\n\nAlso change the layout of StringType and BinaryType in UnsafeRow.  by combining offset and size together as Long.  which will limit the size of Row to under 2G (given that fact that any single buffer can not be bigger than 2G in JVM).\n\nAuthor: Davies Liu <davies@databricks.com>\n\nCloses #6911 from davies/unsafe_bin and squashes the following commits:\n\nd68706f [Davies Liu] update comment\n519f698 [Davies Liu] address comment\n98a964b [Davies Liu] Merge branch 'master' of github.com:apache/spark into unsafe_bin\n180b49d [Davies Liu] fix zero-out\n22e4c0a [Davies Liu] zero-out padding bytes\n6abfe93 [Davies Liu] fix style\n447dea0 [Davies Liu] support binaryType in UnsafeRow\n","date":"2015-06-23 06:22:17","modifiedFileCount":"2","status":"M","submitter":"Davies Liu"},{"authorTime":"2015-07-23 16:51:34","codes":[{"authorDate":"2015-06-21 11:03:59","commitOrder":4,"curCode":"  public UTF8String getUTF8String(int i) {\n    assertIndexIsValid(i);\n    final long offsetToStringSize = getLong(i);\n    final int stringSizeInBytes =\n      (int) PlatformDependent.UNSAFE.getLong(baseObject, baseOffset + offsetToStringSize);\n    final byte[] strBytes = new byte[stringSizeInBytes];\n    PlatformDependent.copyMemory(\n      baseObject,\n      baseOffset + offsetToStringSize + 8,  \r\n      strBytes,\n      PlatformDependent.BYTE_ARRAY_OFFSET,\n      stringSizeInBytes\n    );\n    return UTF8String.fromBytes(strBytes);\n  }\n","date":"2015-06-21 11:03:59","endLine":327,"groupId":"2553","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"getUTF8String","params":"(inti)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/ed/04d2e50ec849e46b85d3e1466663db320a6109.src","preCode":"  public UTF8String getUTF8String(int i) {\n    assertIndexIsValid(i);\n    final long offsetToStringSize = getLong(i);\n    final int stringSizeInBytes =\n      (int) PlatformDependent.UNSAFE.getLong(baseObject, baseOffset + offsetToStringSize);\n    final byte[] strBytes = new byte[stringSizeInBytes];\n    PlatformDependent.copyMemory(\n      baseObject,\n      baseOffset + offsetToStringSize + 8,  \r\n      strBytes,\n      PlatformDependent.BYTE_ARRAY_OFFSET,\n      stringSizeInBytes\n    );\n    return UTF8String.fromBytes(strBytes);\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":313,"status":"N"},{"authorDate":"2015-07-23 16:51:34","commitOrder":4,"curCode":"  public byte[] getBinary(int i) {\n    if (isNullAt(i)) {\n      return null;\n    } else {\n      assertIndexIsValid(i);\n      final long offsetAndSize = getLong(i);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      final byte[] bytes = new byte[size];\n      PlatformDependent.copyMemory(\n        baseObject,\n        baseOffset + offset,\n        bytes,\n        PlatformDependent.BYTE_ARRAY_OFFSET,\n        size\n      );\n      return bytes;\n    }\n  }\n","date":"2015-07-23 16:51:34","endLine":311,"groupId":"657","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"getBinary","params":"(inti)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/fa/1216b455a9e2aac9a010813c35fbb1301b4174.src","preCode":"  public byte[] getBinary(int i) {\n    assertIndexIsValid(i);\n    final long offsetAndSize = getLong(i);\n    final int offset = (int)(offsetAndSize >> 32);\n    final int size = (int)(offsetAndSize & ((1L << 32) - 1));\n    final byte[] bytes = new byte[size];\n    PlatformDependent.copyMemory(\n      baseObject,\n      baseOffset + offset,\n      bytes,\n      PlatformDependent.BYTE_ARRAY_OFFSET,\n      size\n    );\n    return bytes;\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":293,"status":"M"}],"commitId":"fb36397b3ce569d77db26df07ac339731cc07b1c","commitMessage":"@@@Revert \"[SPARK-8579] [SQL] support arbitrary object in UnsafeRow\"\n\nReverts ObjectPool. As it stands.  it has a few problems:\n\n1. ObjectPool doesn't work with spilling and memory accounting.\n2. I don't think in the long run the idea of an object pool is what we want to support.  since it essentially goes back to unmanaged memory.  and creates pressure on GC.  and is hard to account for the total in memory size.\n3. The ObjectPool patch removed the specialized getters for strings and binary.  and as a result.  actually introduced branches when reading non primitive data types.\n\nIf we do want to support arbitrary user defined types in the future.  I think we can just add an object array in UnsafeRow.  rather than relying on indirect memory addressing through a pool. We also need to pick execution strategies that are optimized for those.  rather than keeping a lot of unserialized JVM objects in memory during aggregation.\n\nThis is probably the hardest thing I had to revert in Spark.  due to recent patches that also change the same part of the code. Would be great to get a careful look.\n\nAuthor: Reynold Xin <rxin@databricks.com>\n\nCloses #7591 from rxin/revert-object-pool and squashes the following commits:\n\n01db0bc [Reynold Xin] Scala style.\neda89fc [Reynold Xin] Fixed describe.\n2967118 [Reynold Xin] Fixed accessor for JoinedRow.\ne3294eb [Reynold Xin] Merge branch 'master' into revert-object-pool\n657855f [Reynold Xin] Temp commit.\nc20f2c8 [Reynold Xin] Style fix.\nfe37079 [Reynold Xin] Revert \"[SPARK-8579] [SQL] support arbitrary object in UnsafeRow\"\n","date":"2015-07-23 16:51:34","modifiedFileCount":"3","status":"M","submitter":"Reynold Xin"},{"authorTime":"2015-07-26 09:41:51","codes":[{"authorDate":"2015-06-21 11:03:59","commitOrder":5,"curCode":"  public UTF8String getUTF8String(int i) {\n    assertIndexIsValid(i);\n    final long offsetToStringSize = getLong(i);\n    final int stringSizeInBytes =\n      (int) PlatformDependent.UNSAFE.getLong(baseObject, baseOffset + offsetToStringSize);\n    final byte[] strBytes = new byte[stringSizeInBytes];\n    PlatformDependent.copyMemory(\n      baseObject,\n      baseOffset + offsetToStringSize + 8,  \r\n      strBytes,\n      PlatformDependent.BYTE_ARRAY_OFFSET,\n      stringSizeInBytes\n    );\n    return UTF8String.fromBytes(strBytes);\n  }\n","date":"2015-06-21 11:03:59","endLine":327,"groupId":"2553","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"getUTF8String","params":"(inti)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/ed/04d2e50ec849e46b85d3e1466663db320a6109.src","preCode":"  public UTF8String getUTF8String(int i) {\n    assertIndexIsValid(i);\n    final long offsetToStringSize = getLong(i);\n    final int stringSizeInBytes =\n      (int) PlatformDependent.UNSAFE.getLong(baseObject, baseOffset + offsetToStringSize);\n    final byte[] strBytes = new byte[stringSizeInBytes];\n    PlatformDependent.copyMemory(\n      baseObject,\n      baseOffset + offsetToStringSize + 8,  \r\n      strBytes,\n      PlatformDependent.BYTE_ARRAY_OFFSET,\n      stringSizeInBytes\n    );\n    return UTF8String.fromBytes(strBytes);\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":313,"status":"N"},{"authorDate":"2015-07-26 09:41:51","commitOrder":5,"curCode":"  public byte[] getBinary(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      assertIndexIsValid(ordinal);\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      final byte[] bytes = new byte[size];\n      PlatformDependent.copyMemory(\n        baseObject,\n        baseOffset + offset,\n        bytes,\n        PlatformDependent.BYTE_ARRAY_OFFSET,\n        size\n      );\n      return bytes;\n    }\n  }\n","date":"2015-07-26 09:41:51","endLine":329,"groupId":"657","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"getBinary","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/9b/e90894933351059a97dcae9710d040ff1a2ab8.src","preCode":"  public byte[] getBinary(int i) {\n    if (isNullAt(i)) {\n      return null;\n    } else {\n      assertIndexIsValid(i);\n      final long offsetAndSize = getLong(i);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      final byte[] bytes = new byte[size];\n      PlatformDependent.copyMemory(\n        baseObject,\n        baseOffset + offset,\n        bytes,\n        PlatformDependent.BYTE_ARRAY_OFFSET,\n        size\n      );\n      return bytes;\n    }\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":311,"status":"M"}],"commitId":"b1f4b4abfd8d038c3684685b245b5fd31b927da0","commitMessage":"@@@[SPARK-9348][SQL] Remove apply method on InternalRow.\n\nAuthor: Reynold Xin <rxin@databricks.com>\n\nCloses #7665 from rxin/remove-row-apply and squashes the following commits:\n\n0b43001 [Reynold Xin] support getString in UnsafeRow.\n176d633 [Reynold Xin] apply -> get.\n2941324 [Reynold Xin] [SPARK-9348][SQL] Remove apply method on InternalRow.\n","date":"2015-07-26 09:41:51","modifiedFileCount":"1","status":"M","submitter":"Reynold Xin"},{"authorTime":"2015-08-02 12:50:42","codes":[{"authorDate":"2015-08-02 12:50:42","commitOrder":6,"curCode":"  public UTF8String getUTF8String(int ordinal) {\n    assertIndexIsValid(ordinal);\n    if (isNullAt(ordinal)) return null;\n    final long offsetAndSize = getLong(ordinal);\n    final int offset = (int) (offsetAndSize >> 32);\n    final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n    return UTF8String.fromAddress(baseObject, baseOffset + offset, size);\n  }\n","date":"2015-08-02 12:50:42","endLine":371,"groupId":"1673","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"getUTF8String","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/1b/475b249274775da070fef459765b43492aee59.src","preCode":"  public UTF8String getUTF8String(int i) {\n    assertIndexIsValid(i);\n    final long offsetToStringSize = getLong(i);\n    final int stringSizeInBytes =\n      (int) PlatformDependent.UNSAFE.getLong(baseObject, baseOffset + offsetToStringSize);\n    final byte[] strBytes = new byte[stringSizeInBytes];\n    PlatformDependent.copyMemory(\n      baseObject,\n      baseOffset + offsetToStringSize + 8,  \r\n      strBytes,\n      PlatformDependent.BYTE_ARRAY_OFFSET,\n      stringSizeInBytes\n    );\n    return UTF8String.fromBytes(strBytes);\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":364,"status":"M"},{"authorDate":"2015-08-02 12:50:42","commitOrder":6,"curCode":"  public byte[] getBinary(int ordinal) {\n    assertIndexIsValid(ordinal);\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      final byte[] bytes = new byte[size];\n      PlatformDependent.copyMemory(\n        baseObject,\n        baseOffset + offset,\n        bytes,\n        PlatformDependent.BYTE_ARRAY_OFFSET,\n        size\n      );\n      return bytes;\n    }\n  }\n","date":"2015-08-02 12:50:42","endLine":392,"groupId":"657","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"getBinary","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/1b/475b249274775da070fef459765b43492aee59.src","preCode":"  public byte[] getBinary(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      assertIndexIsValid(ordinal);\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      final byte[] bytes = new byte[size];\n      PlatformDependent.copyMemory(\n        baseObject,\n        baseOffset + offset,\n        bytes,\n        PlatformDependent.BYTE_ARRAY_OFFSET,\n        size\n      );\n      return bytes;\n    }\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":374,"status":"M"}],"commitId":"57084e0c7c318912208ee31c52d61c14eeddd8f4","commitMessage":"@@@[SPARK-9459] [SQL] use generated FromUnsafeProjection to do deep copy for UTF8String and struct\n\nWhen accessing a column in UnsafeRow.  it's good to avoid the copy.  then we should do deep copy when turn the UnsafeRow into generic Row.  this PR brings generated FromUnsafeProjection to do that.\n\nThis PR also fix the expressions that cache the UTF8String.  which should also copy it.\n\nAuthor: Davies Liu <davies@databricks.com>\n\nCloses #7840 from davies/avoid_copy and squashes the following commits:\n\n230c8a1 [Davies Liu] address comment\nfd797c9 [Davies Liu] Merge branch 'master' of github.com:apache/spark into avoid_copy\ne095dd0 [Davies Liu] rollback rename\n8ef5b0b [Davies Liu] copy String in Columnar\n81360b8 [Davies Liu] fix class name\n9aecb88 [Davies Liu] use FromUnsafeProjection to do deep copy for UTF8String and struct\n","date":"2015-08-02 12:50:42","modifiedFileCount":"2","status":"M","submitter":"Davies Liu"},{"authorTime":"2015-08-03 14:41:16","codes":[{"authorDate":"2015-08-03 14:41:16","commitOrder":7,"curCode":"  public UTF8String getUTF8String(int ordinal) {\n    if (isNullAt(ordinal)) return null;\n    final long offsetAndSize = getLong(ordinal);\n    final int offset = (int) (offsetAndSize >> 32);\n    final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n    return UTF8String.fromAddress(baseObject, baseOffset + offset, size);\n  }\n","date":"2015-08-03 14:41:16","endLine":373,"groupId":"1673","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"getUTF8String","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/c5/d42d73a43a4a1758e830d614526cafa5b02e39.src","preCode":"  public UTF8String getUTF8String(int ordinal) {\n    assertIndexIsValid(ordinal);\n    if (isNullAt(ordinal)) return null;\n    final long offsetAndSize = getLong(ordinal);\n    final int offset = (int) (offsetAndSize >> 32);\n    final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n    return UTF8String.fromAddress(baseObject, baseOffset + offset, size);\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":367,"status":"M"},{"authorDate":"2015-08-03 14:41:16","commitOrder":7,"curCode":"  public byte[] getBinary(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      final byte[] bytes = new byte[size];\n      PlatformDependent.copyMemory(\n        baseObject,\n        baseOffset + offset,\n        bytes,\n        PlatformDependent.BYTE_ARRAY_OFFSET,\n        size\n      );\n      return bytes;\n    }\n  }\n","date":"2015-08-03 14:41:16","endLine":393,"groupId":"657","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"getBinary","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/c5/d42d73a43a4a1758e830d614526cafa5b02e39.src","preCode":"  public byte[] getBinary(int ordinal) {\n    assertIndexIsValid(ordinal);\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      final byte[] bytes = new byte[size];\n      PlatformDependent.copyMemory(\n        baseObject,\n        baseOffset + offset,\n        bytes,\n        PlatformDependent.BYTE_ARRAY_OFFSET,\n        size\n      );\n      return bytes;\n    }\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":376,"status":"M"}],"commitId":"608353c8e8e50461fafff91a2c885dca8af3aaa8","commitMessage":"@@@[SPARK-9404][SPARK-9542][SQL] unsafe array data and map data\n\nThis PR adds a UnsafeArrayData.  current we encode it in this way:\n\nfirst 4 bytes is the # elements\nthen each 4 byte is the start offset of the element.  unless it is negative.  in which case the element is null.\nfollowed by the elements themselves\n\nan example:  [10.  11.  12.  13.  null.  14] will be encoded as:\n5.  28.  32.  36.  40.  -44.  44.  10.  11.  12.  13.  14\n\nNote that.  when we read a UnsafeArrayData from bytes.  we can read the first 4 bytes as numElements and take the rest(first 4 bytes skipped) as value region.\n\nunsafe map data just use 2 unsafe array data.  first 4 bytes is # of elements.  second 4 bytes is numBytes of key array.  the follows key array data and value array data.\n\nAuthor: Wenchen Fan <cloud0fan@outlook.com>\n\nCloses #7752 from cloud-fan/unsafe-array and squashes the following commits:\n\n3269bd7 [Wenchen Fan] fix a bug\n6445289 [Wenchen Fan] add unit tests\n49adf26 [Wenchen Fan] add unsafe map\n20d1039 [Wenchen Fan] add comments and unsafe converter\n821b8db [Wenchen Fan] add unsafe array\n","date":"2015-08-03 14:41:16","modifiedFileCount":"3","status":"M","submitter":"Wenchen Fan"},{"authorTime":"2015-08-11 23:41:06","codes":[{"authorDate":"2015-08-03 14:41:16","commitOrder":8,"curCode":"  public UTF8String getUTF8String(int ordinal) {\n    if (isNullAt(ordinal)) return null;\n    final long offsetAndSize = getLong(ordinal);\n    final int offset = (int) (offsetAndSize >> 32);\n    final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n    return UTF8String.fromAddress(baseObject, baseOffset + offset, size);\n  }\n","date":"2015-08-03 14:41:16","endLine":373,"groupId":"1673","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"getUTF8String","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/c5/d42d73a43a4a1758e830d614526cafa5b02e39.src","preCode":"  public UTF8String getUTF8String(int ordinal) {\n    if (isNullAt(ordinal)) return null;\n    final long offsetAndSize = getLong(ordinal);\n    final int offset = (int) (offsetAndSize >> 32);\n    final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n    return UTF8String.fromAddress(baseObject, baseOffset + offset, size);\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":367,"status":"N"},{"authorDate":"2015-08-11 23:41:06","commitOrder":8,"curCode":"  public byte[] getBinary(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      final byte[] bytes = new byte[size];\n      Platform.copyMemory(\n        baseObject,\n        baseOffset + offset,\n        bytes,\n        Platform.BYTE_ARRAY_OFFSET,\n        size\n      );\n      return bytes;\n    }\n  }\n","date":"2015-08-11 23:41:06","endLine":432,"groupId":"657","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"getBinary","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/7f/d94772090dfd440003dc247071418d20da31fc.src","preCode":"  public byte[] getBinary(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      final byte[] bytes = new byte[size];\n      PlatformDependent.copyMemory(\n        baseObject,\n        baseOffset + offset,\n        bytes,\n        PlatformDependent.BYTE_ARRAY_OFFSET,\n        size\n      );\n      return bytes;\n    }\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":415,"status":"M"}],"commitId":"d378396f86f625f006738d87fe5dbc2ff8fd913d","commitMessage":"@@@[SPARK-9815] Rename PlatformDependent.UNSAFE -> Platform.\n\nPlatformDependent.UNSAFE is way too verbose.\n\nAuthor: Reynold Xin <rxin@databricks.com>\n\nCloses #8094 from rxin/SPARK-9815 and squashes the following commits:\n\n229b603 [Reynold Xin] [SPARK-9815] Rename PlatformDependent.UNSAFE -> Platform.\n","date":"2015-08-11 23:41:06","modifiedFileCount":"30","status":"M","submitter":"Reynold Xin"},{"authorTime":"2015-10-22 10:20:31","codes":[{"authorDate":"2015-10-22 10:20:31","commitOrder":9,"curCode":"  public UTF8String getUTF8String(int ordinal) {\n    if (isNullAt(ordinal)) return null;\n    final long offsetAndSize = getLong(ordinal);\n    final int offset = (int) (offsetAndSize >> 32);\n    final int size = (int) offsetAndSize;\n    return UTF8String.fromAddress(baseObject, baseOffset + offset, size);\n  }\n","date":"2015-10-22 10:20:31","endLine":407,"groupId":"2774","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"getUTF8String","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/85/0838af9be359e2c65178b0ba79a662e241e654.src","preCode":"  public UTF8String getUTF8String(int ordinal) {\n    if (isNullAt(ordinal)) return null;\n    final long offsetAndSize = getLong(ordinal);\n    final int offset = (int) (offsetAndSize >> 32);\n    final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n    return UTF8String.fromAddress(baseObject, baseOffset + offset, size);\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":401,"status":"M"},{"authorDate":"2015-10-22 10:20:31","commitOrder":9,"curCode":"  public byte[] getBinary(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) offsetAndSize;\n      final byte[] bytes = new byte[size];\n      Platform.copyMemory(\n        baseObject,\n        baseOffset + offset,\n        bytes,\n        Platform.BYTE_ARRAY_OFFSET,\n        size\n      );\n      return bytes;\n    }\n  }\n","date":"2015-10-22 10:20:31","endLine":427,"groupId":"2774","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"getBinary","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/85/0838af9be359e2c65178b0ba79a662e241e654.src","preCode":"  public byte[] getBinary(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      final byte[] bytes = new byte[size];\n      Platform.copyMemory(\n        baseObject,\n        baseOffset + offset,\n        bytes,\n        Platform.BYTE_ARRAY_OFFSET,\n        size\n      );\n      return bytes;\n    }\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":410,"status":"M"}],"commitId":"1d9733271595596683a6d956a7433fa601df1cc1","commitMessage":"@@@[SPARK-11243][SQL] output UnsafeRow from columnar cache\n\nThis PR change InMemoryTableScan to output UnsafeRow.  and optimize the unrolling and scanning by coping the bytes for var-length types between UnsafeRow and ByteBuffer directly without creating the wrapper objects. When scanning the decimals in TPC-DS store_sales table.  it's 80% faster (copy it as long without create Decimal objects).\n\nAuthor: Davies Liu <davies@databricks.com>\n\nCloses #9203 from davies/unsafe_cache.\n","date":"2015-10-22 10:20:31","modifiedFileCount":"3","status":"M","submitter":"Davies Liu"},{"authorTime":"2015-10-22 10:20:31","codes":[{"authorDate":"2018-04-06 10:13:59","commitOrder":10,"curCode":"  public UTF8String getUTF8String(int ordinal) {\n    if (isNullAt(ordinal)) return null;\n    final long offsetAndSize = getLong(ordinal);\n    final int offset = (int) (offsetAndSize >> 32);\n    final int size = (int) offsetAndSize;\n    MemoryBlock mb = MemoryBlock.allocateFromObject(baseObject, baseOffset + offset, size);\n    return new UTF8String(mb);\n  }\n","date":"2018-04-06 10:13:59","endLine":420,"groupId":"122","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"getUTF8String","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/29/a1411241cf6690c1221729c12ef33ee8e93c9f.src","preCode":"  public UTF8String getUTF8String(int ordinal) {\n    if (isNullAt(ordinal)) return null;\n    final long offsetAndSize = getLong(ordinal);\n    final int offset = (int) (offsetAndSize >> 32);\n    final int size = (int) offsetAndSize;\n    return UTF8String.fromAddress(baseObject, baseOffset + offset, size);\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":413,"status":"M"},{"authorDate":"2015-10-22 10:20:31","commitOrder":10,"curCode":"  public byte[] getBinary(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) offsetAndSize;\n      final byte[] bytes = new byte[size];\n      Platform.copyMemory(\n        baseObject,\n        baseOffset + offset,\n        bytes,\n        Platform.BYTE_ARRAY_OFFSET,\n        size\n      );\n      return bytes;\n    }\n  }\n","date":"2015-10-22 10:20:31","endLine":427,"groupId":"2774","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"getBinary","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/85/0838af9be359e2c65178b0ba79a662e241e654.src","preCode":"  public byte[] getBinary(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) offsetAndSize;\n      final byte[] bytes = new byte[size];\n      Platform.copyMemory(\n        baseObject,\n        baseOffset + offset,\n        bytes,\n        Platform.BYTE_ARRAY_OFFSET,\n        size\n      );\n      return bytes;\n    }\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":410,"status":"N"}],"commitId":"4807d381bb113a5c61e6dad88202f23a8b6dd141","commitMessage":"@@@[SPARK-10399][CORE][SQL] Introduce multiple MemoryBlocks to choose several types of memory block\n\n## What changes were proposed in this pull request?\n\nThis PR allows us to use one of several types of `MemoryBlock`.  such as byte array.  int array.  long array.  or `java.nio.DirectByteBuffer`. To use `java.nio.DirectByteBuffer` allows to have off heap memory which is automatically deallocated by JVM. `MemoryBlock`  class has primitive accessors like `Platform.getInt()`.  `Platform.putint()`.  or `Platform.copyMemory()`.\n\nThis PR uses `MemoryBlock` for `OffHeapColumnVector`.  `UTF8String`.  and other places. This PR can improve performance of operations involving memory accesses (e.g. `UTF8String.trim`) by 1.8x.\n\nFor now.  this PR does not use `MemoryBlock` for `BufferHolder` based on cloud-fan's [suggestion](https://github.com/apache/spark/pull/11494#issuecomment-309694290).\n\nSince this PR is a successor of #11494.  close #11494. Many codes were ported from #11494. Many efforts were put here. **I think this PR should credit to yzotov.**\n\nThis PR can achieve **1.1-1.4x performance improvements** for  operations in `UTF8String` or `Murmur3_x86_32`. Other operations are almost comparable performances.\n\nWithout this PR\n```\nOpenJDK 64-Bit Server VM 1.8.0_121-8u121-b13-0ubuntu1.16.04.2-b13 on Linux 4.4.0-22-generic\nIntel(R) Xeon(R) CPU E5-2667 v3  3.20GHz\nOpenJDK 64-Bit Server VM 1.8.0_121-8u121-b13-0ubuntu1.16.04.2-b13 on Linux 4.4.0-22-generic\nIntel(R) Xeon(R) CPU E5-2667 v3  3.20GHz\nHash byte arrays with length 268435487:  Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nMurmur3_x86_32                                 526 /  536          0.0   131399881.5       1.0X\n\nUTF8String benchmark:                    Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nhashCode                                       525 /  552       1022.6           1.0       1.0X\nsubstring                                      414 /  423       1298.0           0.8       1.3X\n```\n\nWith this PR\n```\nOpenJDK 64-Bit Server VM 1.8.0_121-8u121-b13-0ubuntu1.16.04.2-b13 on Linux 4.4.0-22-generic\nIntel(R) Xeon(R) CPU E5-2667 v3  3.20GHz\nHash byte arrays with length 268435487:  Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nMurmur3_x86_32                                 474 /  488          0.0   118552232.0       1.0X\n\nUTF8String benchmark:                    Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nhashCode                                       476 /  480       1127.3           0.9       1.0X\nsubstring                                      287 /  291       1869.9           0.5       1.7X\n```\n\nBenchmark program\n```\ntest(\"benchmark Murmur3_x86_32\") {\n  val length = 8192 * 32768 + 31\n  val seed = 42L\n  val iters = 1 << 2\n  val random = new Random(seed)\n  val arrays = Array.fill[MemoryBlock](numArrays) {\n    val bytes = new Array[Byte](length)\n    random.nextBytes(bytes)\n    new ByteArrayMemoryBlock(bytes.  Platform.BYTE_ARRAY_OFFSET.  length)\n  }\n\n  val benchmark = new Benchmark(\"Hash byte arrays with length \" + length. \n    iters * numArrays.  minNumIters = 20)\n  benchmark.addCase(\"HiveHasher\") { _: Int =>\n    var sum = 0L\n    for (_ <- 0L until iters) {\n      sum += HiveHasher.hashUnsafeBytesBlock(\n        arrays(i).  Platform.BYTE_ARRAY_OFFSET.  length)\n    }\n  }\n  benchmark.run()\n}\n\ntest(\"benchmark UTF8String\") {\n  val N = 512 * 1024 * 1024\n  val iters = 2\n  val benchmark = new Benchmark(\"UTF8String benchmark\".  N.  minNumIters = 20)\n  val str0 = new java.io.StringWriter() { { for (i <- 0 until N) { write(\" \") } } }.toString\n  val s0 = UTF8String.fromString(str0)\n  benchmark.addCase(\"hashCode\") { _: Int =>\n    var h: Int = 0\n    for (_ <- 0L until iters) { h += s0.hashCode }\n  }\n  benchmark.addCase(\"substring\") { _: Int =>\n    var s: UTF8String = null\n    for (_ <- 0L until iters) { s = s0.substring(N / 2 - 5.  N / 2 + 5) }\n  }\n  benchmark.run()\n}\n```\n\nI run [this benchmark program](https://gist.github.com/kiszk/94f75b506c93a663bbbc372ffe8f05de) using [the commit](https://github.com/apache/spark/pull/19222/commits/ee5a79861c18725fb1cd9b518cdfd2489c05b81d6). I got the following results:\n\n```\nOpenJDK 64-Bit Server VM 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12 on Linux 4.4.0-66-generic\nIntel(R) Xeon(R) CPU E5-2667 v3  3.20GHz\nMemory access benchmarks:                Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nByteArrayMemoryBlock get/putInt()              220 /  221        609.3           1.6       1.0X\nPlatform get/putInt(byte[])                    220 /  236        610.9           1.6       1.0X\nPlatform get/putInt(Object)                    492 /  494        272.8           3.7       0.4X\nOnHeapMemoryBlock get/putLong()                322 /  323        416.5           2.4       0.7X\nlong[]                                         221 /  221        608.0           1.6       1.0X\nPlatform get/putLong(long[])                   321 /  321        418.7           2.4       0.7X\nPlatform get/putLong(Object)                   561 /  563        239.2           4.2       0.4X\n```\n\nI also run [this benchmark program](https://gist.github.com/kiszk/5fdb4e03733a5d110421177e289d1fb5) for comparing performance of `Platform.copyMemory()`.\n```\nOpenJDK 64-Bit Server VM 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12 on Linux 4.4.0-66-generic\nIntel(R) Xeon(R) CPU E5-2667 v3  3.20GHz\nPlatform copyMemory:                     Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nObject to Object                              1961 / 1967          8.6         116.9       1.0X\nSystem.arraycopy Object to Object             1917 / 1921          8.8         114.3       1.0X\nbyte array to byte array                      1961 / 1968          8.6         116.9       1.0X\nSystem.arraycopy byte array to byte array      1909 / 1937          8.8         113.8       1.0X\nint array to int array                        1921 / 1990          8.7         114.5       1.0X\ndouble array to double array                  1918 / 1923          8.7         114.3       1.0X\nObject to byte array                          1961 / 1967          8.6         116.9       1.0X\nObject to short array                         1965 / 1972          8.5         117.1       1.0X\nObject to int array                           1910 / 1915          8.8         113.9       1.0X\nObject to float array                         1971 / 1978          8.5         117.5       1.0X\nObject to double array                        1919 / 1944          8.7         114.4       1.0X\nbyte array to Object                          1959 / 1967          8.6         116.8       1.0X\nint array to Object                           1961 / 1970          8.6         116.9       1.0X\ndouble array to Object                        1917 / 1924          8.8         114.3       1.0X\n```\n\nThese results show three facts:\n1. According to the second/third or sixth/seventh results in the first experiment.  if we use `Platform.get/putInt(Object)`.  we achieve more than 2x worse performance than `Platform.get/putInt(byte[])` with concrete type (i.e. `byte[]`).\n2. According to the second/third or fourth/fifth/sixth results in the first experiment.  the fastest way to access an array element on Java heap is `array[]`. **Cons of `array[]` is that it is not possible to support unaligned-8byte access.**\n3. According to the first/second/third or fourth/sixth/seventh results in the first experiment.  `getInt()/putInt() or getLong()/putLong()` in subclasses of `MemoryBlock` can achieve comparable performance to `Platform.get/putInt()` or `Platform.get/putLong()` with concrete type (second or sixth result). There is no overhead regarding virtual call.\n4. According to results in the second experiment.  for `Platform.copy()`.  to pass `Object` can achieve the same performance as to pass any type of primitive array as source or destination.\n5. According to second/fourth results in the second experiment.  `Platform.copy()` can achieve the same performance as `System.arrayCopy`. **It would be good to use `Platform.copy()` since `Platform.copy()` can take any types for src and dst.**\n\nWe are incrementally replace `Platform.get/putXXX` with `MemoryBlock.get/putXXX`. This is because we have two advantages.\n1) Achieve better performance due to having a concrete type for an array.\n2) Use simple OO design instead of passing `Object`\nIt is easy to use `MemoryBlock` in `InternalRow`.  `BufferHolder`.  `TaskMemoryManager`.  and others that are already abstracted. It is not easy to use `MemoryBlock` in utility classes related to hashing or others.\n\nOther candidates are\n- UnsafeRow.  UnsafeArrayData.  UnsafeMapData.  SpecificUnsafeRowJoiner\n- UTF8StringBuffer\n- BufferHolder\n- TaskMemoryManager\n- OnHeapColumnVector\n- BytesToBytesMap\n- CachedBatch\n- classes for hash\n- others.\n\n## How was this patch tested?\n\nAdded `UnsafeMemoryAllocator`\n\nAuthor: Kazuaki Ishizaki <ishizaki@jp.ibm.com>\n\nCloses #19222 from kiszk/SPARK-10399.\n","date":"2018-04-06 10:13:59","modifiedFileCount":"27","status":"M","submitter":"Kazuaki Ishizaki"},{"authorTime":"2015-10-22 10:20:31","codes":[{"authorDate":"2018-09-09 21:25:19","commitOrder":11,"curCode":"  public UTF8String getUTF8String(int ordinal) {\n    if (isNullAt(ordinal)) return null;\n    final long offsetAndSize = getLong(ordinal);\n    final int offset = (int) (offsetAndSize >> 32);\n    final int size = (int) offsetAndSize;\n    return UTF8String.fromAddress(baseObject, baseOffset + offset, size);\n  }\n","date":"2018-09-09 21:25:19","endLine":420,"groupId":"10352","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"getUTF8String","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/a7/6e6ef8c91c11004c3f8cf72f41f133548c6d63.src","preCode":"  public UTF8String getUTF8String(int ordinal) {\n    if (isNullAt(ordinal)) return null;\n    final long offsetAndSize = getLong(ordinal);\n    final int offset = (int) (offsetAndSize >> 32);\n    final int size = (int) offsetAndSize;\n    MemoryBlock mb = MemoryBlock.allocateFromObject(baseObject, baseOffset + offset, size);\n    return new UTF8String(mb);\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":414,"status":"M"},{"authorDate":"2015-10-22 10:20:31","commitOrder":11,"curCode":"  public byte[] getBinary(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) offsetAndSize;\n      final byte[] bytes = new byte[size];\n      Platform.copyMemory(\n        baseObject,\n        baseOffset + offset,\n        bytes,\n        Platform.BYTE_ARRAY_OFFSET,\n        size\n      );\n      return bytes;\n    }\n  }\n","date":"2015-10-22 10:20:31","endLine":427,"groupId":"10352","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"getBinary","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/85/0838af9be359e2c65178b0ba79a662e241e654.src","preCode":"  public byte[] getBinary(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) offsetAndSize;\n      final byte[] bytes = new byte[size];\n      Platform.copyMemory(\n        baseObject,\n        baseOffset + offset,\n        bytes,\n        Platform.BYTE_ARRAY_OFFSET,\n        size\n      );\n      return bytes;\n    }\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":410,"status":"N"}],"commitId":"0b9ccd55c2986957863dcad3b44ce80403eecfa1","commitMessage":"@@@Revert [SPARK-10399] [SPARK-23879] [SPARK-23762] [SPARK-25317]\n\n## What changes were proposed in this pull request?\n\nWhen running TPC-DS benchmarks on 2.4 release.  npoggi and winglungngai  saw more than 10% performance regression on the following queries: q67.  q24a and q24b. After we applying the PR https://github.com/apache/spark/pull/22338.  the performance regression still exists. If we revert the changes in https://github.com/apache/spark/pull/19222.  npoggi and winglungngai  found the performance regression was resolved. Thus.  this PR is to revert the related changes for unblocking the 2.4 release.\n\nIn the future release.  we still can continue the investigation and find out the root cause of the regression.\n\n## How was this patch tested?\n\nThe existing test cases\n\nCloses #22361 from gatorsmile/revertMemoryBlock.\n\nAuthored-by: gatorsmile <gatorsmile@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n","date":"2018-09-09 21:25:19","modifiedFileCount":"28","status":"M","submitter":"gatorsmile"}]
