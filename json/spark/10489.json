[{"authorTime":"2015-12-08 15:26:34","codes":[{"authorDate":"2015-09-24 13:49:08","commitOrder":3,"curCode":"  public void javaCompatibilityTest() {\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n    StructType schema = new StructType(new StructField[] {\n      new StructField(\"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n    DataFrame dataset = jsql.createDataFrame(data, schema);\n\n    remover.transform(dataset).collect();\n  }\n","date":"2015-09-24 13:49:08","endLine":72,"groupId":"643","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"javaCompatibilityTest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/84/8d9f8aa92881d0b1273792a1de404fd1f45200.src","preCode":"  public void javaCompatibilityTest() {\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n    StructType schema = new StructType(new StructField[] {\n      new StructField(\"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n    DataFrame dataset = jsql.createDataFrame(data, schema);\n\n    remover.transform(dataset).collect();\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/feature/JavaStopWordsRemoverSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":57,"status":"NB"},{"authorDate":"2015-12-08 15:26:34","commitOrder":3,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaStopWordsRemoverExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    JavaRDD<Row> rdd = jsc.parallelize(Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    ));\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\n        \"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n\n    DataFrame dataset = jsql.createDataFrame(rdd, schema);\n    remover.transform(dataset).show();\n    \r\n    jsc.stop();\n  }\n","date":"2015-12-08 15:26:34","endLine":64,"groupId":"2265","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/b6/b201c6b68d2d21eff344a1ef3a29bb80d3a4e3.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaStopWordsRemoverExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    JavaRDD<Row> rdd = jsc.parallelize(Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    ));\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\n        \"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n\n    DataFrame dataset = jsql.createDataFrame(rdd, schema);\n    remover.transform(dataset).show();\n    \r\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"B"}],"commitId":"78209b0ccaf3f22b5e2345dfb2b98edfdb746819","commitMessage":"@@@[SPARK-11551][DOC][EXAMPLE] Replace example code in ml-features.md using include_example\n\nMade new patch contaning only markdown examples moved to exmaple/folder.\nOny three  java code were not shfted since they were contaning compliation error . these classes are\n1)StandardScale 2)NormalizerExample 3)VectorIndexer\n\nAuthor: Xusen Yin <yinxusen@gmail.com>\nAuthor: somideshmukh <somilde@us.ibm.com>\n\nCloses #10002 from somideshmukh/SomilBranch1.33.\n","date":"2015-12-08 15:26:34","modifiedFileCount":"0","status":"M","submitter":"somideshmukh"},{"authorTime":"2016-03-11 09:00:17","codes":[{"authorDate":"2016-03-11 09:00:17","commitOrder":4,"curCode":"  public void javaCompatibilityTest() {\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n    StructType schema = new StructType(new StructField[] {\n      new StructField(\"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n    Dataset<Row> dataset = jsql.createDataFrame(data, schema);\n\n    remover.transform(dataset).collect();\n  }\n","date":"2016-03-11 09:00:17","endLine":71,"groupId":"643","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"javaCompatibilityTest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/58/12037dee90ed56fca85aed735e0c837d1f2741.src","preCode":"  public void javaCompatibilityTest() {\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n    StructType schema = new StructType(new StructField[] {\n      new StructField(\"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n    DataFrame dataset = jsql.createDataFrame(data, schema);\n\n    remover.transform(dataset).collect();\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/feature/JavaStopWordsRemoverSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":56,"status":"M"},{"authorDate":"2016-03-11 09:00:17","commitOrder":4,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaStopWordsRemoverExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    JavaRDD<Row> rdd = jsc.parallelize(Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    ));\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\n        \"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n\n    Dataset<Row> dataset = jsql.createDataFrame(rdd, schema);\n    remover.transform(dataset).show();\n    \r\n    jsc.stop();\n  }\n","date":"2016-03-11 09:00:17","endLine":64,"groupId":"2265","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/0f/f3782cb3e9093e1945b730cdc027a0fc2ad4d0.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaStopWordsRemoverExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    JavaRDD<Row> rdd = jsc.parallelize(Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    ));\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\n        \"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n\n    DataFrame dataset = jsql.createDataFrame(rdd, schema);\n    remover.transform(dataset).show();\n    \r\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"M"}],"commitId":"1d542785b9949e7f92025e6754973a779cc37c52","commitMessage":"@@@[SPARK-13244][SQL] Migrates DataFrame to Dataset\n\n## What changes were proposed in this pull request?\n\nThis PR unifies DataFrame and Dataset by migrating existing DataFrame operations to Dataset and make `DataFrame` a type alias of `Dataset[Row]`.\n\nMost Scala code changes are source compatible.  but Java API is broken as Java knows nothing about Scala type alias (mostly replacing `DataFrame` with `Dataset<Row>`).\n\nThere are several noticeable API changes related to those returning arrays:\n\n1.  `collect`/`take`\n\n    -   Old APIs in class `DataFrame`:\n\n        ```scala\n        def collect(): Array[Row]\n        def take(n: Int): Array[Row]\n        ```\n\n    -   New APIs in class `Dataset[T]`:\n\n        ```scala\n        def collect(): Array[T]\n        def take(n: Int): Array[T]\n\n        def collectRows(): Array[Row]\n        def takeRows(n: Int): Array[Row]\n        ```\n\n    Two specialized methods `collectRows` and `takeRows` are added because Java doesn't support returning generic arrays. Thus.  for example.  `DataFrame.collect(): Array[T]` actually returns `Object` instead of `Array<T>` from Java side.\n\n    Normally.  Java users may fall back to `collectAsList` and `takeAsList`.  The two new specialized versions are added to avoid performance regression in ML related code (but maybe I'm wrong and they are not necessary here).\n\n1.  `randomSplit`\n\n    -   Old APIs in class `DataFrame`:\n\n        ```scala\n        def randomSplit(weights: Array[Double].  seed: Long): Array[DataFrame]\n        def randomSplit(weights: Array[Double]): Array[DataFrame]\n        ```\n\n    -   New APIs in class `Dataset[T]`:\n\n        ```scala\n        def randomSplit(weights: Array[Double].  seed: Long): Array[Dataset[T]]\n        def randomSplit(weights: Array[Double]): Array[Dataset[T]]\n        ```\n\n    Similar problem as above.  but hasn't been addressed for Java API yet.  We can probably add `randomSplitAsList` to fix this one.\n\n1.  `groupBy`\n\n    Some original `DataFrame.groupBy` methods have conflicting signature with original `Dataset.groupBy` methods.  To distinguish these two.  typed `Dataset.groupBy` methods are renamed to `groupByKey`.\n\nOther noticeable changes:\n\n1.  Dataset always do eager analysis now\n\n    We used to support disabling DataFrame eager analysis to help reporting partially analyzed malformed logical plan on analysis failure.  However.  Dataset encoders requires eager analysi during Dataset construction.  To preserve the error reporting feature.  `AnalysisException` now takes an extra `Option[LogicalPlan]` argument to hold the partially analyzed plan.  so that we can check the plan tree when reporting test failures.  This plan is passed by `QueryExecution.assertAnalyzed`.\n\n## How was this patch tested?\n\nExisting tests do the work.\n\n## TODO\n\n- [ ] Fix all tests\n- [ ] Re-enable MiMA check\n- [ ] Update ScalaDoc (`since`.  `group`.  and example code)\n\nAuthor: Cheng Lian <lian@databricks.com>\nAuthor: Yin Huai <yhuai@databricks.com>\nAuthor: Wenchen Fan <wenchen@databricks.com>\nAuthor: Cheng Lian <liancheng@users.noreply.github.com>\n\nCloses #11443 from liancheng/ds-to-df.\n","date":"2016-03-11 09:00:17","modifiedFileCount":"87","status":"M","submitter":"Cheng Lian"},{"authorTime":"2016-05-05 05:31:36","codes":[{"authorDate":"2016-03-11 09:00:17","commitOrder":5,"curCode":"  public void javaCompatibilityTest() {\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n    StructType schema = new StructType(new StructField[] {\n      new StructField(\"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n    Dataset<Row> dataset = jsql.createDataFrame(data, schema);\n\n    remover.transform(dataset).collect();\n  }\n","date":"2016-03-11 09:00:17","endLine":71,"groupId":"643","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"javaCompatibilityTest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/58/12037dee90ed56fca85aed735e0c837d1f2741.src","preCode":"  public void javaCompatibilityTest() {\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n    StructType schema = new StructType(new StructField[] {\n      new StructField(\"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n    Dataset<Row> dataset = jsql.createDataFrame(data, schema);\n\n    remover.transform(dataset).collect();\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/feature/JavaStopWordsRemoverSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":56,"status":"N"},{"authorDate":"2016-05-05 05:31:36","commitOrder":5,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession.builder().appName(\"JavaStopWordsRemoverExample\").getOrCreate();\n\n    \r\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\n        \"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n\n    Dataset<Row> dataset = spark.createDataFrame(data, schema);\n    remover.transform(dataset).show();\n    \r\n    spark.stop();\n  }\n","date":"2016-05-05 05:31:36","endLine":60,"groupId":"643","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/23/ed071c9f6e5a3cd039d140cd2f350517cbe358.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaStopWordsRemoverExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    JavaRDD<Row> rdd = jsc.parallelize(Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    ));\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\n        \"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n\n    Dataset<Row> dataset = jsql.createDataFrame(rdd, schema);\n    remover.transform(dataset).show();\n    \r\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":38,"status":"M"}],"commitId":"cdce4e62a5674e2034e5d395578b1a60e3d8c435","commitMessage":"@@@[SPARK-15031][EXAMPLE] Use SparkSession in Scala/Python/Java example.\n\n## What changes were proposed in this pull request?\n\nThis PR aims to update Scala/Python/Java examples by replacing `SQLContext` with newly added `SparkSession`.\n\n- Use **SparkSession Builder Pattern** in 154(Scala 55.  Java 52.  Python 47) files.\n- Add `getConf` in Python SparkContext class: `python/pyspark/context.py`\n- Replace **SQLContext Singleton Pattern** with **SparkSession Singleton Pattern**:\n  - `SqlNetworkWordCount.scala`\n  - `JavaSqlNetworkWordCount.java`\n  - `sql_network_wordcount.py`\n\nNow.  `SQLContexts` are used only in R examples and the following two Python examples. The python examples are untouched in this PR since it already fails some unknown issue.\n- `simple_params_example.py`\n- `aft_survival_regression.py`\n\n## How was this patch tested?\n\nManual.\n\nAuthor: Dongjoon Hyun <dongjoon@apache.org>\n\nCloses #12809 from dongjoon-hyun/SPARK-15031.\n","date":"2016-05-05 05:31:36","modifiedFileCount":"52","status":"M","submitter":"Dongjoon Hyun"},{"authorTime":"2016-05-05 05:31:36","codes":[{"authorDate":"2016-05-11 02:17:47","commitOrder":6,"curCode":"  public void javaCompatibilityTest() {\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"raw\", DataTypes.createArrayType(DataTypes.StringType), false,\n        Metadata.empty())\n    });\n    Dataset<Row> dataset = spark.createDataFrame(data, schema);\n\n    remover.transform(dataset).collect();\n  }\n","date":"2016-05-11 02:17:47","endLine":72,"groupId":"643","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"javaCompatibilityTest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/2b/156f3bca5b1e17fce7b540d4de59f88593b051.src","preCode":"  public void javaCompatibilityTest() {\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n    StructType schema = new StructType(new StructField[] {\n      new StructField(\"raw\", DataTypes.createArrayType(DataTypes.StringType), false,\n                      Metadata.empty())\n    });\n    Dataset<Row> dataset = jsql.createDataFrame(data, schema);\n\n    remover.transform(dataset).collect();\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/feature/JavaStopWordsRemoverSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":56,"status":"M"},{"authorDate":"2016-05-05 05:31:36","commitOrder":6,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession.builder().appName(\"JavaStopWordsRemoverExample\").getOrCreate();\n\n    \r\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\n        \"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n\n    Dataset<Row> dataset = spark.createDataFrame(data, schema);\n    remover.transform(dataset).show();\n    \r\n    spark.stop();\n  }\n","date":"2016-05-05 05:31:36","endLine":60,"groupId":"643","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/23/ed071c9f6e5a3cd039d140cd2f350517cbe358.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession.builder().appName(\"JavaStopWordsRemoverExample\").getOrCreate();\n\n    \r\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\n        \"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n\n    Dataset<Row> dataset = spark.createDataFrame(data, schema);\n    remover.transform(dataset).show();\n    \r\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":38,"status":"N"}],"commitId":"ed0b4070fb50054b1ecf66ff6c32458a4967dfd3","commitMessage":"@@@[SPARK-15037][SQL][MLLIB] Use SparkSession instead of SQLContext in Scala/Java TestSuites\n\n## What changes were proposed in this pull request?\nUse SparkSession instead of SQLContext in Scala/Java TestSuites\nas this PR already very big working Python TestSuites in a diff PR.\n\n## How was this patch tested?\nExisting tests\n\nAuthor: Sandeep Singh <sandeep@techaddict.me>\n\nCloses #12907 from techaddict/SPARK-15037.\n","date":"2016-05-11 02:17:47","modifiedFileCount":"63","status":"M","submitter":"Sandeep Singh"},{"authorTime":"2016-07-14 16:12:46","codes":[{"authorDate":"2016-05-11 02:17:47","commitOrder":7,"curCode":"  public void javaCompatibilityTest() {\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"raw\", DataTypes.createArrayType(DataTypes.StringType), false,\n        Metadata.empty())\n    });\n    Dataset<Row> dataset = spark.createDataFrame(data, schema);\n\n    remover.transform(dataset).collect();\n  }\n","date":"2016-05-11 02:17:47","endLine":72,"groupId":"643","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"javaCompatibilityTest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/2b/156f3bca5b1e17fce7b540d4de59f88593b051.src","preCode":"  public void javaCompatibilityTest() {\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"raw\", DataTypes.createArrayType(DataTypes.StringType), false,\n        Metadata.empty())\n    });\n    Dataset<Row> dataset = spark.createDataFrame(data, schema);\n\n    remover.transform(dataset).collect();\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/feature/JavaStopWordsRemoverSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":56,"status":"N"},{"authorDate":"2016-07-14 16:12:46","commitOrder":7,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaStopWordsRemoverExample\")\n      .getOrCreate();\n\n    \r\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"balloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\n        \"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n\n    Dataset<Row> dataset = spark.createDataFrame(data, schema);\n    remover.transform(dataset).show();\n    \r\n    spark.stop();\n  }\n","date":"2016-07-14 16:12:46","endLine":63,"groupId":"643","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/27/8cce084218a1b7677d6ad8c9e8cef9ec7db1af.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaStopWordsRemoverExample\")\n      .getOrCreate();\n\n    \r\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\n        \"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n\n    Dataset<Row> dataset = spark.createDataFrame(data, schema);\n    remover.transform(dataset).show();\n    \r\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":38,"status":"M"}],"commitId":"e3f8a033679261aaee15bda0f970a1890411e743","commitMessage":"@@@[SPARK-16403][EXAMPLES] Cleanup to remove unused imports.  consistent style.  minor fixes\n\n## What changes were proposed in this pull request?\n\nCleanup of examples.  mostly from PySpark-ML to fix minor issues:  unused imports.  style consistency.  pipeline_example is a duplicate.  use future print funciton.  and a spelling error.\n\n* The \"Pipeline Example\" is duplicated by \"Simple Text Classification Pipeline\" in Scala.  Python.  and Java.\n\n* \"Estimator Transformer Param Example\" is duplicated by \"Simple Params Example\" in Scala.  Python and Java\n\n* Synced random_forest_classifier_example.py with Scala by adding IndexToString label converted\n\n* Synced train_validation_split.py (in Scala ModelSelectionViaTrainValidationExample) by adjusting data split.  adding grid for intercept.\n\n* RegexTokenizer was doing nothing in tokenizer_example.py and JavaTokenizerExample.java.  synced with Scala version\n\n## How was this patch tested?\nlocal tests and run modified examples\n\nAuthor: Bryan Cutler <cutlerb@gmail.com>\n\nCloses #14081 from BryanCutler/examples-cleanup-SPARK-16403.\n","date":"2016-07-14 16:12:46","modifiedFileCount":"3","status":"M","submitter":"Bryan Cutler"},{"authorTime":"2016-08-06 03:57:46","codes":[{"authorDate":"2016-05-11 02:17:47","commitOrder":8,"curCode":"  public void javaCompatibilityTest() {\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"raw\", DataTypes.createArrayType(DataTypes.StringType), false,\n        Metadata.empty())\n    });\n    Dataset<Row> dataset = spark.createDataFrame(data, schema);\n\n    remover.transform(dataset).collect();\n  }\n","date":"2016-05-11 02:17:47","endLine":72,"groupId":"643","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"javaCompatibilityTest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/2b/156f3bca5b1e17fce7b540d4de59f88593b051.src","preCode":"  public void javaCompatibilityTest() {\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"raw\", DataTypes.createArrayType(DataTypes.StringType), false,\n        Metadata.empty())\n    });\n    Dataset<Row> dataset = spark.createDataFrame(data, schema);\n\n    remover.transform(dataset).collect();\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/feature/JavaStopWordsRemoverSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":56,"status":"N"},{"authorDate":"2016-08-06 03:57:46","commitOrder":8,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaStopWordsRemoverExample\")\n      .getOrCreate();\n\n    \r\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"balloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\n        \"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n\n    Dataset<Row> dataset = spark.createDataFrame(data, schema);\n    remover.transform(dataset).show(false);\n    \r\n    spark.stop();\n  }\n","date":"2016-08-06 03:57:46","endLine":63,"groupId":"643","id":12,"instanceNumber":2,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/94/ead625b4745232436970c0ed68674f272636d9.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaStopWordsRemoverExample\")\n      .getOrCreate();\n\n    \r\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"balloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\n        \"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n\n    Dataset<Row> dataset = spark.createDataFrame(data, schema);\n    remover.transform(dataset).show();\n    \r\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":38,"status":"M"}],"commitId":"180fd3e0a3426db200c97170926afb60751dfd0e","commitMessage":"@@@[SPARK-16421][EXAMPLES][ML] Improve ML Example Outputs\n\n## What changes were proposed in this pull request?\nImprove example outputs to better reflect the functionality that is being presented.  This mostly consisted of modifying what was printed at the end of the example.  such as calling show() with truncate=False.  but sometimes required minor tweaks in the example data to get relevant output.  Explicitly set parameters when they are used as part of the example.  Fixed Java examples that failed to run because of using old-style MLlib Vectors or problem with schema.  Synced examples between different APIs.\n\n## How was this patch tested?\nRan each example for Scala.  Python.  and Java and made sure output was legible on a terminal of width 100.\n\nAuthor: Bryan Cutler <cutlerb@gmail.com>\n\nCloses #14308 from BryanCutler/ml-examples-improve-output-SPARK-16260.\n","date":"2016-08-06 03:57:46","modifiedFileCount":"27","status":"M","submitter":"Bryan Cutler"},{"authorTime":"2016-08-06 03:57:46","codes":[{"authorDate":"2020-11-30 12:59:51","commitOrder":9,"curCode":"  public void javaCompatibilityTest() {\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"balloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"raw\", DataTypes.createArrayType(DataTypes.StringType), false,\n        Metadata.empty())\n    });\n    Dataset<Row> dataset = spark.createDataFrame(data, schema);\n\n    remover.transform(dataset).collect();\n  }\n","date":"2020-11-30 12:59:51","endLine":54,"groupId":"10489","id":13,"instanceNumber":1,"isCurCommit":1,"methodName":"javaCompatibilityTest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/af/32e03854b5336b9f1b460514f5a571a6897157.src","preCode":"  public void javaCompatibilityTest() {\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"baloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"raw\", DataTypes.createArrayType(DataTypes.StringType), false,\n        Metadata.empty())\n    });\n    Dataset<Row> dataset = spark.createDataFrame(data, schema);\n\n    remover.transform(dataset).collect();\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/feature/JavaStopWordsRemoverSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":38,"status":"M"},{"authorDate":"2016-08-06 03:57:46","commitOrder":9,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaStopWordsRemoverExample\")\n      .getOrCreate();\n\n    \r\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"balloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\n        \"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n\n    Dataset<Row> dataset = spark.createDataFrame(data, schema);\n    remover.transform(dataset).show(false);\n    \r\n    spark.stop();\n  }\n","date":"2016-08-06 03:57:46","endLine":63,"groupId":"10489","id":14,"instanceNumber":2,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/94/ead625b4745232436970c0ed68674f272636d9.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaStopWordsRemoverExample\")\n      .getOrCreate();\n\n    \r\n    StopWordsRemover remover = new StopWordsRemover()\n      .setInputCol(\"raw\")\n      .setOutputCol(\"filtered\");\n\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Arrays.asList(\"I\", \"saw\", \"the\", \"red\", \"balloon\")),\n      RowFactory.create(Arrays.asList(\"Mary\", \"had\", \"a\", \"little\", \"lamb\"))\n    );\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\n        \"raw\", DataTypes.createArrayType(DataTypes.StringType), false, Metadata.empty())\n    });\n\n    Dataset<Row> dataset = spark.createDataFrame(data, schema);\n    remover.transform(dataset).show(false);\n    \r\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":38,"status":"N"}],"commitId":"485145326a9c97ede260b0e267ee116f182cfd56","commitMessage":"@@@[MINOR] Spelling bin core docs external mllib repl\n\n\n What changes were proposed in this pull request?\n\nThis PR intends to fix typos in the sub-modules:\n* `bin`\n* `core`\n* `docs`\n* `external`\n* `mllib`\n* `repl`\n* `pom.xml`\n\nSplit per srowen https://github.com/apache/spark/pull/30323#issuecomment-728981618\n\nNOTE: The misspellings have been reported at https://github.com/jsoref/spark/commit/706a726f87a0bbf5e31467fae9015218773db85b#commitcomment-44064356\n\n\n Why are the changes needed?\n\nMisspelled words make it harder to read / understand content.\n\n\n Does this PR introduce _any_ user-facing change?\n\nThere are various fixes to documentation.  etc...\n\n\n How was this patch tested?\n\nNo testing was performed\n\nCloses #30530 from jsoref/spelling-bin-core-docs-external-mllib-repl.\n\nAuthored-by: Josh Soref <jsoref@users.noreply.github.com>\nSigned-off-by: Takeshi Yamamuro <yamamuro@apache.org>\n","date":"2020-11-30 12:59:51","modifiedFileCount":"3","status":"M","submitter":"Josh Soref"}]
