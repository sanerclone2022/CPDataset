[{"authorTime":"2015-07-11 07:44:51","codes":[{"authorDate":"2015-08-01 10:19:27","commitOrder":2,"curCode":"  public void setup() {\n    shuffleMemoryManager = new ShuffleMemoryManager(Long.MAX_VALUE);\n    taskMemoryManager = new TaskMemoryManager(new ExecutorMemoryManager(getMemoryAllocator()));\n    \r\n    \r\n    sizeLimitedTaskMemoryManager = mock(TaskMemoryManager.class);\n    when(sizeLimitedTaskMemoryManager.allocate(geq(1L << 20))).thenAnswer(\n      new Answer<MemoryBlock>() {\n        @Override\n        public MemoryBlock answer(InvocationOnMock invocation) throws Throwable {\n          if (((Long) invocation.getArguments()[0] / 8) > Integer.MAX_VALUE) {\n            throw new OutOfMemoryError(\"Requested array size exceeds VM limit\");\n          }\n          return new MemoryBlock(null, 0, (Long) invocation.getArguments()[0]);\n        }\n      }\n    );\n  }\n","date":"2015-08-01 10:19:27","endLine":66,"groupId":"1504","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/60/f483acbcb80581d60fbfb0cdd2154f05400ba6.src","preCode":"  public void setup() {\n    shuffleMemoryManager = new ShuffleMemoryManager(Long.MAX_VALUE);\n    taskMemoryManager = new TaskMemoryManager(new ExecutorMemoryManager(getMemoryAllocator()));\n    \r\n    \r\n    sizeLimitedTaskMemoryManager = mock(TaskMemoryManager.class);\n    when(sizeLimitedTaskMemoryManager.allocate(geq(1L << 20))).thenAnswer(\n      new Answer<MemoryBlock>() {\n        @Override\n        public MemoryBlock answer(InvocationOnMock invocation) throws Throwable {\n          if (((Long) invocation.getArguments()[0] / 8) > Integer.MAX_VALUE) {\n            throw new OutOfMemoryError(\"Requested array size exceeds VM limit\");\n          }\n          return new MemoryBlock(null, 0, (Long) invocation.getArguments()[0]);\n        }\n      }\n    );\n  }\n","realPath":"core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":49,"status":"B"},{"authorDate":"2015-07-11 07:44:51","commitOrder":2,"curCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    tempDir = new File(Utils.createTempDir$default$1());\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(shuffleMemoryManager.tryToAcquire(anyLong())).then(returnsFirstArg());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (BlockId) args[0],\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","date":"2015-07-11 07:44:51","endLine":131,"groupId":"2691","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/ea/8755e21eb68f300f262f1465c5f9823103a9f9.src","preCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    tempDir = new File(Utils.createTempDir$default$1());\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(shuffleMemoryManager.tryToAcquire(anyLong())).then(returnsFirstArg());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (BlockId) args[0],\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":93,"status":"NB"}],"commitId":"8cb415a4b9bc1f82127ccce4a5579d433f4e8f83","commitMessage":"@@@[SPARK-9451] [SQL] Support entries larger than default page size in BytesToBytesMap & integrate with ShuffleMemoryManager\n\nThis patch adds support for entries larger than the default page size in BytesToBytesMap.  These large rows are handled by allocating special overflow pages to hold individual entries.\n\nIn addition.  this patch integrates BytesToBytesMap with the ShuffleMemoryManager:\n\n- Move BytesToBytesMap from `unsafe` to `core` so that it can import `ShuffleMemoryManager`.\n- Before allocating new data pages.  ask the ShuffleMemoryManager to reserve the memory:\n  - `putNewKey()` now returns a boolean to indicate whether the insert succeeded or failed due to a lack of memory.  The caller can use this value to respond to the memory pressure (e.g. by spilling).\n- `UnsafeFixedWidthAggregationMap. getAggregationBuffer()` now returns `null` to signal failure due to a lack of memory.\n- Updated all uses of these classes to handle these error conditions.\n- Added new tests for allocating large records and for allocations which fail due to memory pressure.\n- Extended the `afterAll()` test teardown methods to detect ShuffleMemoryManager leaks.\n\nAuthor: Josh Rosen <joshrosen@databricks.com>\n\nCloses #7762 from JoshRosen/large-rows and squashes the following commits:\n\nae7bc56 [Josh Rosen] Fix compilation\n82fc657 [Josh Rosen] Merge remote-tracking branch 'origin/master' into large-rows\n34ab943 [Josh Rosen] Remove semi\n31a525a [Josh Rosen] Integrate BytesToBytesMap with ShuffleMemoryManager.\n626b33c [Josh Rosen] Move code to sql/core and spark/core packages so that ShuffleMemoryManager can be integrated\nec4484c [Josh Rosen] Move BytesToBytesMap from unsafe package to core.\n642ed69 [Josh Rosen] Rename size to numElements\nbea1152 [Josh Rosen] Add basic test.\n2cd3570 [Josh Rosen] Remove accidental duplicated code\n07ff9ef [Josh Rosen] Basic support for large rows in BytesToBytesMap.\n","date":"2015-08-01 10:19:27","modifiedFileCount":"2","status":"M","submitter":"Josh Rosen"},{"authorTime":"2015-08-03 03:32:14","codes":[{"authorDate":"2015-08-01 10:19:27","commitOrder":3,"curCode":"  public void setup() {\n    shuffleMemoryManager = new ShuffleMemoryManager(Long.MAX_VALUE);\n    taskMemoryManager = new TaskMemoryManager(new ExecutorMemoryManager(getMemoryAllocator()));\n    \r\n    \r\n    sizeLimitedTaskMemoryManager = mock(TaskMemoryManager.class);\n    when(sizeLimitedTaskMemoryManager.allocate(geq(1L << 20))).thenAnswer(\n      new Answer<MemoryBlock>() {\n        @Override\n        public MemoryBlock answer(InvocationOnMock invocation) throws Throwable {\n          if (((Long) invocation.getArguments()[0] / 8) > Integer.MAX_VALUE) {\n            throw new OutOfMemoryError(\"Requested array size exceeds VM limit\");\n          }\n          return new MemoryBlock(null, 0, (Long) invocation.getArguments()[0]);\n        }\n      }\n    );\n  }\n","date":"2015-08-01 10:19:27","endLine":66,"groupId":"1504","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/60/f483acbcb80581d60fbfb0cdd2154f05400ba6.src","preCode":"  public void setup() {\n    shuffleMemoryManager = new ShuffleMemoryManager(Long.MAX_VALUE);\n    taskMemoryManager = new TaskMemoryManager(new ExecutorMemoryManager(getMemoryAllocator()));\n    \r\n    \r\n    sizeLimitedTaskMemoryManager = mock(TaskMemoryManager.class);\n    when(sizeLimitedTaskMemoryManager.allocate(geq(1L << 20))).thenAnswer(\n      new Answer<MemoryBlock>() {\n        @Override\n        public MemoryBlock answer(InvocationOnMock invocation) throws Throwable {\n          if (((Long) invocation.getArguments()[0] / 8) > Integer.MAX_VALUE) {\n            throw new OutOfMemoryError(\"Requested array size exceeds VM limit\");\n          }\n          return new MemoryBlock(null, 0, (Long) invocation.getArguments()[0]);\n        }\n      }\n    );\n  }\n","realPath":"core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":49,"status":"N"},{"authorDate":"2015-08-03 03:32:14","commitOrder":3,"curCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    tempDir = new File(Utils.createTempDir$default$1());\n    shuffleMemoryManager = new ShuffleMemoryManager(Long.MAX_VALUE);\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (BlockId) args[0],\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","date":"2015-08-03 03:32:14","endLine":137,"groupId":"10","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/52/fa8bcd57e79701aa65b84cab9a72b263bcc6f5.src","preCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    tempDir = new File(Utils.createTempDir$default$1());\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(shuffleMemoryManager.tryToAcquire(anyLong())).then(returnsFirstArg());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (BlockId) args[0],\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":97,"status":"M"}],"commitId":"2e981b7bfa9dec93fdcf25f3e7220cd6aaba744f","commitMessage":"@@@[SPARK-9531] [SQL] UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter\n\nThis pull request adds a destructAndCreateExternalSorter method to UnsafeFixedWidthAggregationMap. The new method does the following:\n\n1. Creates a new external sorter UnsafeKVExternalSorter\n2. Adds all the data into an in-memory sorter.  sorts them\n3. Spills the sorted in-memory data to disk\n\nThis method can be used to fallback to sort-based aggregation when under memory pressure.\n\nThe pull request also includes accounting fixes from JoshRosen.\n\nTODOs (that can be done in follow-up PRs)\n- [x] Address Josh's feedbacks from #7849\n- [x] More documentation and test cases\n- [x] Make sure we are doing memory accounting correctly with test cases (e.g. did we release the memory in BytesToBytesMap twice?)\n- [ ] Look harder at possible memory leaks and exception handling\n- [ ] Randomized tester for the KV sorter as well as the aggregation map\n\nAuthor: Reynold Xin <rxin@databricks.com>\nAuthor: Josh Rosen <joshrosen@databricks.com>\n\nCloses #7860 from rxin/kvsorter and squashes the following commits:\n\n986a58c [Reynold Xin] Bug fix.\n599317c [Reynold Xin] Style fix and slightly more compact code.\nfe7bd4e [Reynold Xin] Bug fixes.\nfd71bef [Reynold Xin] Merge remote-tracking branch 'josh/large-records-in-sql-sorter' into kvsorter-with-josh-fix\n3efae38 [Reynold Xin] More fixes and documentation.\n45f1b09 [Josh Rosen] Ensure that spill files are cleaned up\nf6a9bd3 [Reynold Xin] Josh feedback.\n9be8139 [Reynold Xin] Remove testSpillFrequency.\n7cbe759 [Reynold Xin] [SPARK-9531][SQL] UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter.\nae4a8af [Josh Rosen] Detect leaked unsafe memory in UnsafeExternalSorterSuite.\n52f9b06 [Josh Rosen] Detect ShuffleMemoryManager leaks in UnsafeExternalSorter.\n","date":"2015-08-03 03:32:14","modifiedFileCount":"10","status":"M","submitter":"Reynold Xin"},{"authorTime":"2015-08-05 05:42:11","codes":[{"authorDate":"2015-08-01 10:19:27","commitOrder":4,"curCode":"  public void setup() {\n    shuffleMemoryManager = new ShuffleMemoryManager(Long.MAX_VALUE);\n    taskMemoryManager = new TaskMemoryManager(new ExecutorMemoryManager(getMemoryAllocator()));\n    \r\n    \r\n    sizeLimitedTaskMemoryManager = mock(TaskMemoryManager.class);\n    when(sizeLimitedTaskMemoryManager.allocate(geq(1L << 20))).thenAnswer(\n      new Answer<MemoryBlock>() {\n        @Override\n        public MemoryBlock answer(InvocationOnMock invocation) throws Throwable {\n          if (((Long) invocation.getArguments()[0] / 8) > Integer.MAX_VALUE) {\n            throw new OutOfMemoryError(\"Requested array size exceeds VM limit\");\n          }\n          return new MemoryBlock(null, 0, (Long) invocation.getArguments()[0]);\n        }\n      }\n    );\n  }\n","date":"2015-08-01 10:19:27","endLine":66,"groupId":"1504","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/60/f483acbcb80581d60fbfb0cdd2154f05400ba6.src","preCode":"  public void setup() {\n    shuffleMemoryManager = new ShuffleMemoryManager(Long.MAX_VALUE);\n    taskMemoryManager = new TaskMemoryManager(new ExecutorMemoryManager(getMemoryAllocator()));\n    \r\n    \r\n    sizeLimitedTaskMemoryManager = mock(TaskMemoryManager.class);\n    when(sizeLimitedTaskMemoryManager.allocate(geq(1L << 20))).thenAnswer(\n      new Answer<MemoryBlock>() {\n        @Override\n        public MemoryBlock answer(InvocationOnMock invocation) throws Throwable {\n          if (((Long) invocation.getArguments()[0] / 8) > Integer.MAX_VALUE) {\n            throw new OutOfMemoryError(\"Requested array size exceeds VM limit\");\n          }\n          return new MemoryBlock(null, 0, (Long) invocation.getArguments()[0]);\n        }\n      }\n    );\n  }\n","realPath":"core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":49,"status":"N"},{"authorDate":"2015-08-05 05:42:11","commitOrder":4,"curCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    sparkConf = new SparkConf();\n    tempDir = new File(Utils.createTempDir$default$1());\n    shuffleMemoryManager = new ShuffleMemoryManager(Long.MAX_VALUE);\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (BlockId) args[0],\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","date":"2015-08-05 05:42:11","endLine":142,"groupId":"10","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/96/8185bde78abfa33cc20c79a8599acfb8b47134.src","preCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    tempDir = new File(Utils.createTempDir$default$1());\n    shuffleMemoryManager = new ShuffleMemoryManager(Long.MAX_VALUE);\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (BlockId) args[0],\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":101,"status":"M"}],"commitId":"ab8ee1a3b93286a62949569615086ef5030e9fae","commitMessage":"@@@[SPARK-9452] [SQL] Support records larger than page size in UnsafeExternalSorter\n\nThis patch extends UnsafeExternalSorter to support records larger than the page size. The basic strategy is the same as in #7762: store large records in their own overflow pages.\n\nAuthor: Josh Rosen <joshrosen@databricks.com>\n\nCloses #7891 from JoshRosen/large-records-in-sql-sorter and squashes the following commits:\n\n967580b [Josh Rosen] Merge remote-tracking branch 'origin/master' into large-records-in-sql-sorter\n948c344 [Josh Rosen] Add large records tests for KV sorter.\n3c17288 [Josh Rosen] Combine memory and disk cleanup into general cleanupResources() method\n380f217 [Josh Rosen] Merge remote-tracking branch 'origin/master' into large-records-in-sql-sorter\n27eafa0 [Josh Rosen] Fix page size in PackedRecordPointerSuite\na49baef [Josh Rosen] Address initial round of review comments\n3edb931 [Josh Rosen] Remove accidentally-committed debug statements.\n2b164e2 [Josh Rosen] Support large records in UnsafeExternalSorter.\n","date":"2015-08-05 05:42:11","modifiedFileCount":"7","status":"M","submitter":"Josh Rosen"},{"authorTime":"2015-08-06 08:58:36","codes":[{"authorDate":"2015-08-01 10:19:27","commitOrder":5,"curCode":"  public void setup() {\n    shuffleMemoryManager = new ShuffleMemoryManager(Long.MAX_VALUE);\n    taskMemoryManager = new TaskMemoryManager(new ExecutorMemoryManager(getMemoryAllocator()));\n    \r\n    \r\n    sizeLimitedTaskMemoryManager = mock(TaskMemoryManager.class);\n    when(sizeLimitedTaskMemoryManager.allocate(geq(1L << 20))).thenAnswer(\n      new Answer<MemoryBlock>() {\n        @Override\n        public MemoryBlock answer(InvocationOnMock invocation) throws Throwable {\n          if (((Long) invocation.getArguments()[0] / 8) > Integer.MAX_VALUE) {\n            throw new OutOfMemoryError(\"Requested array size exceeds VM limit\");\n          }\n          return new MemoryBlock(null, 0, (Long) invocation.getArguments()[0]);\n        }\n      }\n    );\n  }\n","date":"2015-08-01 10:19:27","endLine":66,"groupId":"1504","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/60/f483acbcb80581d60fbfb0cdd2154f05400ba6.src","preCode":"  public void setup() {\n    shuffleMemoryManager = new ShuffleMemoryManager(Long.MAX_VALUE);\n    taskMemoryManager = new TaskMemoryManager(new ExecutorMemoryManager(getMemoryAllocator()));\n    \r\n    \r\n    sizeLimitedTaskMemoryManager = mock(TaskMemoryManager.class);\n    when(sizeLimitedTaskMemoryManager.allocate(geq(1L << 20))).thenAnswer(\n      new Answer<MemoryBlock>() {\n        @Override\n        public MemoryBlock answer(InvocationOnMock invocation) throws Throwable {\n          if (((Long) invocation.getArguments()[0] / 8) > Integer.MAX_VALUE) {\n            throw new OutOfMemoryError(\"Requested array size exceeds VM limit\");\n          }\n          return new MemoryBlock(null, 0, (Long) invocation.getArguments()[0]);\n        }\n      }\n    );\n  }\n","realPath":"core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":49,"status":"N"},{"authorDate":"2015-08-06 08:58:36","commitOrder":5,"curCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    sparkConf = new SparkConf();\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    shuffleMemoryManager = new ShuffleMemoryManager(Long.MAX_VALUE);\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (BlockId) args[0],\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","date":"2015-08-06 08:58:36","endLine":142,"groupId":"10","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/11/7745f9a9c00f63d31764f11b77eecdfad227d9.src","preCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    sparkConf = new SparkConf();\n    tempDir = new File(Utils.createTempDir$default$1());\n    shuffleMemoryManager = new ShuffleMemoryManager(Long.MAX_VALUE);\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (BlockId) args[0],\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":101,"status":"M"}],"commitId":"4399b7b0903d830313ab7e69731c11d587ae567c","commitMessage":"@@@[SPARK-9651] Fix UnsafeExternalSorterSuite.\n\nFirst.  it's probably a bad idea to call generated Scala methods\nfrom Java. In this case.  the method being called wasn't actually\n\"Utils.createTempDir()\".  but actually the method that returns the\nfirst default argument to the actual createTempDir method.  which\nis just the location of java.io.tmpdir; meaning that all tests in\nthe class were using the same temp dir.  and thus affecting each\nother.\n\nSecond.  spillingOccursInResponseToMemoryPressure was not writing\nenough records to actually cause a spill.\n\nAuthor: Marcelo Vanzin <vanzin@cloudera.com>\n\nCloses #7970 from vanzin/SPARK-9651 and squashes the following commits:\n\n74d357f [Marcelo Vanzin] Clean up temp dir on test tear down.\na64f36a [Marcelo Vanzin] [SPARK-9651] Fix UnsafeExternalSorterSuite.\n","date":"2015-08-06 08:58:36","modifiedFileCount":"1","status":"M","submitter":"Marcelo Vanzin"},{"authorTime":"2015-08-07 14:18:29","codes":[{"authorDate":"2015-08-07 14:18:29","commitOrder":6,"curCode":"  public void setup() {\n    shuffleMemoryManager = ShuffleMemoryManager.create(Long.MAX_VALUE, PAGE_SIZE_BYTES);\n    taskMemoryManager = new TaskMemoryManager(new ExecutorMemoryManager(getMemoryAllocator()));\n    \r\n    \r\n    sizeLimitedTaskMemoryManager = mock(TaskMemoryManager.class);\n    when(sizeLimitedTaskMemoryManager.allocate(geq(1L << 20))).thenAnswer(\n      new Answer<MemoryBlock>() {\n        @Override\n        public MemoryBlock answer(InvocationOnMock invocation) throws Throwable {\n          if (((Long) invocation.getArguments()[0] / 8) > Integer.MAX_VALUE) {\n            throw new OutOfMemoryError(\"Requested array size exceeds VM limit\");\n          }\n          return new MemoryBlock(null, 0, (Long) invocation.getArguments()[0]);\n        }\n      }\n    );\n  }\n","date":"2015-08-07 14:18:29","endLine":67,"groupId":"1504","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/0b/11562980b8e2754f0696748de498aa87c2a7ee.src","preCode":"  public void setup() {\n    shuffleMemoryManager = new ShuffleMemoryManager(Long.MAX_VALUE);\n    taskMemoryManager = new TaskMemoryManager(new ExecutorMemoryManager(getMemoryAllocator()));\n    \r\n    \r\n    sizeLimitedTaskMemoryManager = mock(TaskMemoryManager.class);\n    when(sizeLimitedTaskMemoryManager.allocate(geq(1L << 20))).thenAnswer(\n      new Answer<MemoryBlock>() {\n        @Override\n        public MemoryBlock answer(InvocationOnMock invocation) throws Throwable {\n          if (((Long) invocation.getArguments()[0] / 8) > Integer.MAX_VALUE) {\n            throw new OutOfMemoryError(\"Requested array size exceeds VM limit\");\n          }\n          return new MemoryBlock(null, 0, (Long) invocation.getArguments()[0]);\n        }\n      }\n    );\n  }\n","realPath":"core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":50,"status":"M"},{"authorDate":"2015-08-07 14:18:29","commitOrder":6,"curCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    sparkConf = new SparkConf();\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    shuffleMemoryManager = ShuffleMemoryManager.create(Long.MAX_VALUE, pageSizeBytes);\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (BlockId) args[0],\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","date":"2015-08-07 14:18:29","endLine":142,"groupId":"10","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/83/049b8a21fcf41dcd92b151b0663522a1ed78c2.src","preCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    sparkConf = new SparkConf();\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    shuffleMemoryManager = new ShuffleMemoryManager(Long.MAX_VALUE);\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (BlockId) args[0],\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":101,"status":"M"}],"commitId":"4309262ec9146d7158ee9957a128bb152289d557","commitMessage":"@@@[SPARK-9700] Pick default page size more intelligently.\n\nPreviously.  we use 64MB as the default page size.  which was way too big for a lot of Spark applications (especially for single node).\n\nThis patch changes it so that the default page size.  if unset by the user.  is determined by the number of cores available and the total execution memory available.\n\nAuthor: Reynold Xin <rxin@databricks.com>\n\nCloses #8012 from rxin/pagesize and squashes the following commits:\n\n16f4756 [Reynold Xin] Fixed failing test.\n5afd570 [Reynold Xin] private...\n0d5fb98 [Reynold Xin] Update default value.\n674a6cd [Reynold Xin] Address review feedback.\ndc00e05 [Reynold Xin] Merge with master.\n73ebdb6 [Reynold Xin] [SPARK-9700] Pick default page size more intelligently.\n","date":"2015-08-07 14:18:29","modifiedFileCount":"7","status":"M","submitter":"Reynold Xin"},{"authorTime":"2015-09-25 05:18:33","codes":[{"authorDate":"2015-08-07 14:18:29","commitOrder":7,"curCode":"  public void setup() {\n    shuffleMemoryManager = ShuffleMemoryManager.create(Long.MAX_VALUE, PAGE_SIZE_BYTES);\n    taskMemoryManager = new TaskMemoryManager(new ExecutorMemoryManager(getMemoryAllocator()));\n    \r\n    \r\n    sizeLimitedTaskMemoryManager = mock(TaskMemoryManager.class);\n    when(sizeLimitedTaskMemoryManager.allocate(geq(1L << 20))).thenAnswer(\n      new Answer<MemoryBlock>() {\n        @Override\n        public MemoryBlock answer(InvocationOnMock invocation) throws Throwable {\n          if (((Long) invocation.getArguments()[0] / 8) > Integer.MAX_VALUE) {\n            throw new OutOfMemoryError(\"Requested array size exceeds VM limit\");\n          }\n          return new MemoryBlock(null, 0, (Long) invocation.getArguments()[0]);\n        }\n      }\n    );\n  }\n","date":"2015-08-07 14:18:29","endLine":67,"groupId":"1504","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/0b/11562980b8e2754f0696748de498aa87c2a7ee.src","preCode":"  public void setup() {\n    shuffleMemoryManager = ShuffleMemoryManager.create(Long.MAX_VALUE, PAGE_SIZE_BYTES);\n    taskMemoryManager = new TaskMemoryManager(new ExecutorMemoryManager(getMemoryAllocator()));\n    \r\n    \r\n    sizeLimitedTaskMemoryManager = mock(TaskMemoryManager.class);\n    when(sizeLimitedTaskMemoryManager.allocate(geq(1L << 20))).thenAnswer(\n      new Answer<MemoryBlock>() {\n        @Override\n        public MemoryBlock answer(InvocationOnMock invocation) throws Throwable {\n          if (((Long) invocation.getArguments()[0] / 8) > Integer.MAX_VALUE) {\n            throw new OutOfMemoryError(\"Requested array size exceeds VM limit\");\n          }\n          return new MemoryBlock(null, 0, (Long) invocation.getArguments()[0]);\n        }\n      }\n    );\n  }\n","realPath":"core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":50,"status":"N"},{"authorDate":"2015-09-25 05:18:33","commitOrder":7,"curCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    sparkConf = new SparkConf();\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    shuffleMemoryManager = ShuffleMemoryManager.create(Long.MAX_VALUE, pageSizeBytes);\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","date":"2015-09-25 05:18:33","endLine":141,"groupId":"10","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/a5/bbaa95fa456981429f53a03825d92243473245.src","preCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    sparkConf = new SparkConf();\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    shuffleMemoryManager = ShuffleMemoryManager.create(Long.MAX_VALUE, pageSizeBytes);\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (BlockId) args[0],\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":101,"status":"M"}],"commitId":"8023242e77e4a799de6edc490078285684549b6d","commitMessage":"@@@[SPARK-10761] Refactor DiskBlockObjectWriter to not require BlockId\n\nThe DiskBlockObjectWriter constructor took a BlockId parameter but never used it. As part of some general cleanup in these interfaces.  this patch refactors its constructor to eliminate this parameter.\n\nAuthor: Josh Rosen <joshrosen@databricks.com>\n\nCloses #8871 from JoshRosen/disk-block-object-writer-blockid-cleanup.\n","date":"2015-09-25 05:18:33","modifiedFileCount":"3","status":"M","submitter":"Josh Rosen"},{"authorTime":"2015-10-26 12:19:52","codes":[{"authorDate":"2015-08-07 14:18:29","commitOrder":8,"curCode":"  public void setup() {\n    shuffleMemoryManager = ShuffleMemoryManager.create(Long.MAX_VALUE, PAGE_SIZE_BYTES);\n    taskMemoryManager = new TaskMemoryManager(new ExecutorMemoryManager(getMemoryAllocator()));\n    \r\n    \r\n    sizeLimitedTaskMemoryManager = mock(TaskMemoryManager.class);\n    when(sizeLimitedTaskMemoryManager.allocate(geq(1L << 20))).thenAnswer(\n      new Answer<MemoryBlock>() {\n        @Override\n        public MemoryBlock answer(InvocationOnMock invocation) throws Throwable {\n          if (((Long) invocation.getArguments()[0] / 8) > Integer.MAX_VALUE) {\n            throw new OutOfMemoryError(\"Requested array size exceeds VM limit\");\n          }\n          return new MemoryBlock(null, 0, (Long) invocation.getArguments()[0]);\n        }\n      }\n    );\n  }\n","date":"2015-08-07 14:18:29","endLine":67,"groupId":"1504","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/0b/11562980b8e2754f0696748de498aa87c2a7ee.src","preCode":"  public void setup() {\n    shuffleMemoryManager = ShuffleMemoryManager.create(Long.MAX_VALUE, PAGE_SIZE_BYTES);\n    taskMemoryManager = new TaskMemoryManager(new ExecutorMemoryManager(getMemoryAllocator()));\n    \r\n    \r\n    sizeLimitedTaskMemoryManager = mock(TaskMemoryManager.class);\n    when(sizeLimitedTaskMemoryManager.allocate(geq(1L << 20))).thenAnswer(\n      new Answer<MemoryBlock>() {\n        @Override\n        public MemoryBlock answer(InvocationOnMock invocation) throws Throwable {\n          if (((Long) invocation.getArguments()[0] / 8) > Integer.MAX_VALUE) {\n            throw new OutOfMemoryError(\"Requested array size exceeds VM limit\");\n          }\n          return new MemoryBlock(null, 0, (Long) invocation.getArguments()[0]);\n        }\n      }\n    );\n  }\n","realPath":"core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":50,"status":"N"},{"authorDate":"2015-10-26 12:19:52","commitOrder":8,"curCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    sparkConf = new SparkConf();\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","date":"2015-10-26 12:19:52","endLine":138,"groupId":"10","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/94/d50b94fde3f9d771cf13fa4e25265e0272d604.src","preCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    sparkConf = new SparkConf();\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    shuffleMemoryManager = ShuffleMemoryManager.create(Long.MAX_VALUE, pageSizeBytes);\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":99,"status":"M"}],"commitId":"85e654c5ec87e666a8845bfd77185c1ea57b268a","commitMessage":"@@@[SPARK-10984] Simplify *MemoryManager class structure\n\nThis patch refactors the MemoryManager class structure. After #9000.  Spark had the following classes:\n\n- MemoryManager\n- StaticMemoryManager\n- ExecutorMemoryManager\n- TaskMemoryManager\n- ShuffleMemoryManager\n\nThis is fairly confusing. To simplify things.  this patch consolidates several of these classes:\n\n- ShuffleMemoryManager and ExecutorMemoryManager were merged into MemoryManager.\n- TaskMemoryManager is moved into Spark Core.\n\n**Key changes and tasks**:\n\n- [x] Merge ExecutorMemoryManager into MemoryManager.\n  - [x] Move pooling logic into Allocator.\n- [x] Move TaskMemoryManager from `spark-unsafe` to `spark-core`.\n- [x] Refactor the existing Tungsten TaskMemoryManager interactions so Tungsten code use only this and not both this and ShuffleMemoryManager.\n- [x] Refactor non-Tungsten code to use the TaskMemoryManager instead of ShuffleMemoryManager.\n- [x] Merge ShuffleMemoryManager into MemoryManager.\n  - [x] Move code\n  - [x] ~~Simplify 1/n calculation.~~ **Will defer to followup.  since this needs more work.**\n- [x] Port ShuffleMemoryManagerSuite tests.\n- [x] Move classes from `unsafe` package to `memory` package.\n- [ ] Figure out how to handle the hacky use of the memory managers in HashedRelation's broadcast variable construction.\n- [x] Test porting and cleanup: several tests relied on mock functionality (such as `TestShuffleMemoryManager.markAsOutOfMemory`) which has been changed or broken during the memory manager consolidation\n  - [x] AbstractBytesToBytesMapSuite\n  - [x] UnsafeExternalSorterSuite\n  - [x] UnsafeFixedWidthAggregationMapSuite\n  - [x] UnsafeKVExternalSorterSuite\n\n**Compatiblity notes**:\n\n- This patch introduces breaking changes in `ExternalAppendOnlyMap`.  which is marked as `DevloperAPI` (likely for legacy reasons): this class now cannot be used outside of a task.\n\nAuthor: Josh Rosen <joshrosen@databricks.com>\n\nCloses #9127 from JoshRosen/SPARK-10984.\n","date":"2015-10-26 12:19:52","modifiedFileCount":"20","status":"M","submitter":"Josh Rosen"},{"authorTime":"2015-10-26 12:19:52","codes":[{"authorDate":"2015-10-30 14:38:06","commitOrder":9,"curCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf().set(\"spark.unsafe.offHeap\", \"\" + useOffHeapMemoryAllocator()));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","date":"2015-10-30 14:38:06","endLine":124,"groupId":"10","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/92/bd45e5fa2416ba8a6908cc83563614c160cbd8.src","preCode":"  public void setup() {\n    shuffleMemoryManager = ShuffleMemoryManager.create(Long.MAX_VALUE, PAGE_SIZE_BYTES);\n    taskMemoryManager = new TaskMemoryManager(new ExecutorMemoryManager(getMemoryAllocator()));\n    \r\n    \r\n    sizeLimitedTaskMemoryManager = mock(TaskMemoryManager.class);\n    when(sizeLimitedTaskMemoryManager.allocate(geq(1L << 20))).thenAnswer(\n      new Answer<MemoryBlock>() {\n        @Override\n        public MemoryBlock answer(InvocationOnMock invocation) throws Throwable {\n          if (((Long) invocation.getArguments()[0] / 8) > Integer.MAX_VALUE) {\n            throw new OutOfMemoryError(\"Requested array size exceeds VM limit\");\n          }\n          return new MemoryBlock(null, 0, (Long) invocation.getArguments()[0]);\n        }\n      }\n    );\n  }\n","realPath":"core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":83,"status":"M"},{"authorDate":"2015-10-26 12:19:52","commitOrder":9,"curCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    sparkConf = new SparkConf();\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","date":"2015-10-26 12:19:52","endLine":138,"groupId":"10","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/94/d50b94fde3f9d771cf13fa4e25265e0272d604.src","preCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    sparkConf = new SparkConf();\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":99,"status":"N"}],"commitId":"56419cf11f769c80f391b45dc41b3c7101cc5ff4","commitMessage":"@@@[SPARK-10342] [SPARK-10309] [SPARK-10474] [SPARK-10929] [SQL] Cooperative memory management\n\nThis PR introduce a mechanism to call spill() on those SQL operators that support spilling (for example.  BytesToBytesMap.  UnsafeExternalSorter and ShuffleExternalSorter) if there is not enough memory for execution. The preserved first page is needed anymore.  so removed.\n\nOther Spillable objects in Spark core (ExternalSorter and AppendOnlyMap) are not included in this PR.  but those could benefit from this (trigger others' spilling).\n\nThe PrepareRDD may be not needed anymore.  could be removed in follow up PR.\n\nThe following script will fail with OOM before this PR.  finished in 150 seconds with 2G heap (also works in 1.5 branch.  with similar duration).\n\n```python\nsqlContext.setConf(\"spark.sql.shuffle.partitions\".  \"1\")\ndf = sqlContext.range(1<<25).selectExpr(\"id\".  \"repeat(id.  2) as s\")\ndf2 = df.select(df.id.alias('id2').  df.s.alias('s2'))\nj = df.join(df2.  df.id==df2.id2).groupBy(df.id).max(\"id\".  \"id2\")\nj.explain()\nprint j.count()\n```\n\nFor thread-safety.  here what I'm got:\n\n1) Without calling spill().  the operators should only be used by single thread.  no safety problems.\n\n2) spill() could be triggered in two cases.  triggered by itself.  or by other operators. we can check trigger == this in spill().  so it's still in the same thread.  so safety problems.\n\n3) if it's triggered by other operators (right now cache will not trigger spill()).  we only spill the data into disk when it's in scanning stage (building is finished).  so the in-memory sorter or memory pages are read-only.  we only need to synchronize the iterator and change it.\n\n4) During scanning.  the iterator will only use one record in one page.  we can't free this page.  because the downstream is currently using it (used by UnsafeRow or other objects). In BytesToBytesMap.  we just skip the current page.  and dump all others into disk. In UnsafeExternalSorter.  we keep the page that is used by current record (having the same baseObject).  free it when loading the next record. In ShuffleExternalSorter.  the spill() will not trigger during scanning.\n\n5) In order to avoid deadlock.  we didn't call acquireMemory during spill (so we reused the pointer array in InMemorySorter).\n\nAuthor: Davies Liu <davies@databricks.com>\n\nCloses #9241 from davies/force_spill.\n","date":"2015-10-30 14:38:06","modifiedFileCount":"21","status":"M","submitter":"Davies Liu"},{"authorTime":"2015-10-26 12:19:52","codes":[{"authorDate":"2015-11-07 10:17:34","commitOrder":10,"curCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(\"spark.unsafe.offHeap\", \"\" + useOffHeapMemoryAllocator())\n          .set(\"spark.memory.offHeapSize\", \"256mb\"));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","date":"2015-11-07 10:17:34","endLine":126,"groupId":"10","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/3b/ca790f3087072c00b43d9afbcb9bb5be9f0678.src","preCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf().set(\"spark.unsafe.offHeap\", \"\" + useOffHeapMemoryAllocator()));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","realPath":"core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":83,"status":"M"},{"authorDate":"2015-10-26 12:19:52","commitOrder":10,"curCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    sparkConf = new SparkConf();\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","date":"2015-10-26 12:19:52","endLine":138,"groupId":"10","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/94/d50b94fde3f9d771cf13fa4e25265e0272d604.src","preCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    sparkConf = new SparkConf();\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":99,"status":"N"}],"commitId":"30b706b7b36482921ec04145a0121ca147984fa8","commitMessage":"@@@[SPARK-11389][CORE] Add support for off-heap memory to MemoryManager\n\nIn order to lay the groundwork for proper off-heap memory support in SQL / Tungsten.  we need to extend our MemoryManager to perform bookkeeping for off-heap memory.\n\n## User-facing changes\n\nThis PR introduces a new configuration.  `spark.memory.offHeapSize` (name subject to change).  which specifies the absolute amount of off-heap memory that Spark and Spark SQL can use. If Tungsten is configured to use off-heap execution memory for allocating data pages.  then all data page allocations must fit within this size limit.\n\n## Internals changes\n\nThis PR contains a lot of internal refactoring of the MemoryManager. The key change at the heart of this patch is the introduction of a `MemoryPool` class (name subject to change) to manage the bookkeeping for a particular category of memory (storage.  on-heap execution.  and off-heap execution). These MemoryPools are not fixed-size; they can be dynamically grown and shrunk according to the MemoryManager's policies. In StaticMemoryManager.  these pools have fixed sizes.  proportional to the legacy `[storage|shuffle].memoryFraction`. In the new UnifiedMemoryManager.  the sizes of these pools are dynamically adjusted according to its policies.\n\nThere are two subclasses of `MemoryPool`: `StorageMemoryPool` manages storage memory and `ExecutionMemoryPool` manages execution memory. The MemoryManager creates two execution pools.  one for on-heap memory and one for off-heap. Instances of `ExecutionMemoryPool` manage the logic for fair sharing of their pooled memory across running tasks (in other words.  the ShuffleMemoryManager-like logic has been moved out of MemoryManager and pushed into these ExecutionMemoryPool instances).\n\nI think that this design is substantially easier to understand and reason about than the previous design.  where most of these responsibilities were handled by MemoryManager and its subclasses. To see this.  take at look at how simple the logic in `UnifiedMemoryManager` has become: it's now very easy to see when memory is dynamically shifted between storage and execution.\n\n## TODOs\n\n- [x] Fix handful of test failures in the MemoryManagerSuites.\n- [x] Fix remaining TODO comments in code.\n- [ ] Document new configuration.\n- [x] Fix commented-out tests / asserts:\n  - [x] UnifiedMemoryManagerSuite.\n- [x] Write tests that exercise the new off-heap memory management policies.\n\nAuthor: Josh Rosen <joshrosen@databricks.com>\n\nCloses #9344 from JoshRosen/offheap-memory-accounting.\n","date":"2015-11-07 10:17:34","modifiedFileCount":"6","status":"M","submitter":"Josh Rosen"},{"authorTime":"2015-11-13 14:44:57","codes":[{"authorDate":"2015-11-13 14:44:57","commitOrder":11,"curCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(\"spark.unsafe.offHeap\", \"\" + useOffHeapMemoryAllocator())\n          .set(\"spark.memory.offHeapSize\", \"256mb\"));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","date":"2015-11-13 14:44:57","endLine":127,"groupId":"10","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/d8/7a1d2a56d99f0db60bbd7e04bfce28a3f4b118.src","preCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(\"spark.unsafe.offHeap\", \"\" + useOffHeapMemoryAllocator())\n          .set(\"spark.memory.offHeapSize\", \"256mb\"));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","realPath":"core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":83,"status":"M"},{"authorDate":"2015-11-13 14:44:57","commitOrder":11,"curCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    sparkConf = new SparkConf();\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","date":"2015-11-13 14:44:57","endLine":140,"groupId":"10","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/a1/c9f6fab8e6533b4ae50c89ae45c999484449c2.src","preCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    sparkConf = new SparkConf();\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":100,"status":"M"}],"commitId":"ad960885bfee7850c18eb5338546cecf2b2e9876","commitMessage":"@@@[SPARK-8029] Robust shuffle writer\n\nCurrently.  all the shuffle writer will write to target path directly.  the file could be corrupted by other attempt of the same partition on the same executor. They should write to temporary file then rename to target path.  as what we do in output committer. In order to make the rename atomic.  the temporary file should be created in the same local directory (FileSystem).\n\nThis PR is based on #9214 .  thanks to squito . Closes #9214\n\nAuthor: Davies Liu <davies@databricks.com>\n\nCloses #9610 from davies/safe_shuffle.\n","date":"2015-11-13 14:44:57","modifiedFileCount":"5","status":"M","submitter":"Davies Liu"},{"authorTime":"2015-11-13 14:44:57","codes":[{"authorDate":"2015-12-11 07:29:04","commitOrder":12,"curCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(\"spark.memory.offHeap.enabled\", \"\" + useOffHeapMemoryAllocator())\n          .set(\"spark.memory.offHeap.size\", \"256mb\"));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","date":"2015-12-11 07:29:04","endLine":128,"groupId":"10","id":21,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/70/2ba5469b8b4863f6ff12b39db5e5622b8ff276.src","preCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(\"spark.unsafe.offHeap\", \"\" + useOffHeapMemoryAllocator())\n          .set(\"spark.memory.offHeapSize\", \"256mb\"));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","realPath":"core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":84,"status":"M"},{"authorDate":"2015-11-13 14:44:57","commitOrder":12,"curCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    sparkConf = new SparkConf();\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","date":"2015-11-13 14:44:57","endLine":140,"groupId":"10","id":22,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/a1/c9f6fab8e6533b4ae50c89ae45c999484449c2.src","preCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    sparkConf = new SparkConf();\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":100,"status":"N"}],"commitId":"23a9e62bad9669e9ff5dc4bd714f58d12f9be0b5","commitMessage":"@@@[SPARK-12251] Document and improve off-heap memory configurations\n\nThis patch adds documentation for Spark configurations that affect off-heap memory and makes some naming and validation improvements for those configs.\n\n- Change `spark.memory.offHeapSize` to `spark.memory.offHeap.size`. This is fine because this configuration has not shipped in any Spark release yet (it's new in Spark 1.6).\n- Deprecated `spark.unsafe.offHeap` in favor of a new `spark.memory.offHeap.enabled` configuration. The motivation behind this change is to gather all memory-related configurations under the same prefix.\n- Add a check which prevents users from setting `spark.memory.offHeap.enabled=true` when `spark.memory.offHeap.size == 0`. After SPARK-11389 (#9344).  which was committed in Spark 1.6.  Spark enforces a hard limit on the amount of off-heap memory that it will allocate to tasks. As a result.  enabling off-heap execution memory without setting `spark.memory.offHeap.size` will lead to immediate OOMs. The new configuration validation makes this scenario easier to diagnose.  helping to avoid user confusion.\n- Document these configurations on the configuration page.\n\nAuthor: Josh Rosen <joshrosen@databricks.com>\n\nCloses #10237 from JoshRosen/SPARK-12251.\n","date":"2015-12-11 07:29:04","modifiedFileCount":"7","status":"M","submitter":"Josh Rosen"},{"authorTime":"2016-03-14 12:03:49","codes":[{"authorDate":"2015-12-11 07:29:04","commitOrder":13,"curCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(\"spark.memory.offHeap.enabled\", \"\" + useOffHeapMemoryAllocator())\n          .set(\"spark.memory.offHeap.size\", \"256mb\"));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","date":"2015-12-11 07:29:04","endLine":128,"groupId":"10","id":23,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/70/2ba5469b8b4863f6ff12b39db5e5622b8ff276.src","preCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(\"spark.memory.offHeap.enabled\", \"\" + useOffHeapMemoryAllocator())\n          .set(\"spark.memory.offHeap.size\", \"256mb\"));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","realPath":"core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":84,"status":"N"},{"authorDate":"2016-03-14 12:03:49","commitOrder":13,"curCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","date":"2016-03-14 12:03:49","endLine":138,"groupId":"10","id":24,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/a7/9ed58133f1b993bfc41e6937471ca6eab7803f.src","preCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    sparkConf = new SparkConf();\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock) throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":99,"status":"M"}],"commitId":"184085284185011d7cc6d054b54d2d38eaf1dd77","commitMessage":"@@@[SPARK-13823][CORE][STREAMING][SQL] Always specify Charset in String <-> byte[] conversions (and remaining Coverity items)\n\n## What changes were proposed in this pull request?\n\n- Fixes calls to `new String(byte[])` or `String.getBytes()` that rely on platform default encoding.  to use UTF-8\n- Same for `InputStreamReader` and `OutputStreamWriter` constructors\n- Standardizes on UTF-8 everywhere\n- Standardizes specifying the encoding with `StandardCharsets.UTF-8`.  not the Guava constant or \"UTF-8\" (which means handling `UnuspportedEncodingException`)\n- (also addresses the other remaining Coverity scan issues.  which are pretty trivial; these are separated into commit https://github.com/srowen/spark/commit/1deecd8d9ca986d8adb1a42d315890ce5349d29c )\n\n## How was this patch tested?\n\nJenkins tests\n\nAuthor: Sean Owen <sowen@cloudera.com>\n\nCloses #11657 from srowen/SPARK-13823.\n","date":"2016-03-14 12:03:49","modifiedFileCount":"24","status":"M","submitter":"Sean Owen"},{"authorTime":"2016-03-24 01:15:23","codes":[{"authorDate":"2016-03-24 01:15:23","commitOrder":14,"curCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(\"spark.memory.offHeap.enabled\", \"\" + useOffHeapMemoryAllocator())\n          .set(\"spark.memory.offHeap.size\", \"256mb\")\n          .set(\"spark.shuffle.spill.compress\", \"false\")\n          .set(\"spark.shuffle.compress\", \"false\"));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(\n        new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock)\n          throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n  }\n","date":"2016-03-24 01:15:23","endLine":132,"groupId":"10","id":25,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/44/9fb45c301e2e8d436bbda4488f45d593878bae.src","preCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(\"spark.memory.offHeap.enabled\", \"\" + useOffHeapMemoryAllocator())\n          .set(\"spark.memory.offHeap.size\", \"256mb\"));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(\n        new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock)\n          throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","realPath":"core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"M"},{"authorDate":"2016-03-24 01:15:23","commitOrder":14,"curCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(\n        new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock)\n          throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n  }\n","date":"2016-03-24 01:15:23","endLine":141,"groupId":"10","id":26,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/a2/253d855964023b37a4c3eec933da10b856f71d.src","preCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(\n        new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock)\n          throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n    when(blockManager.wrapForCompression(any(BlockId.class), any(InputStream.class)))\n      .then(returnsSecondArg());\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":102,"status":"M"}],"commitId":"3de24ae2ed6c58fc96a7e50832afe42fe7af34fb","commitMessage":"@@@[SPARK-14075] Refactor MemoryStore to be testable independent of BlockManager\n\nThis patch refactors the `MemoryStore` so that it can be tested without needing to construct / mock an entire `BlockManager`.\n\n- The block manager's serialization- and compression-related methods have been moved from `BlockManager` to `SerializerManager`.\n- `BlockInfoManager `is now passed directly to classes that need it.  rather than being passed via the `BlockManager`.\n- The `MemoryStore` now calls `dropFromMemory` via a new `BlockEvictionHandler` interface rather than directly calling the `BlockManager`. This change helps to enforce a narrow interface between the `MemoryStore` and `BlockManager` functionality and makes this interface easier to mock in tests.\n- Several of the block unrolling tests have been moved from `BlockManagerSuite` into a new `MemoryStoreSuite`.\n\nAuthor: Josh Rosen <joshrosen@databricks.com>\n\nCloses #11899 from JoshRosen/reduce-memorystore-blockmanager-coupling.\n","date":"2016-03-24 01:15:23","modifiedFileCount":"10","status":"M","submitter":"Josh Rosen"},{"authorTime":"2016-08-31 00:15:31","codes":[{"authorDate":"2016-08-31 00:15:31","commitOrder":15,"curCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(\"spark.memory.offHeap.enabled\", \"\" + useOffHeapMemoryAllocator())\n          .set(\"spark.memory.offHeap.size\", \"256mb\")\n          .set(\"spark.shuffle.spill.compress\", \"false\")\n          .set(\"spark.shuffle.compress\", \"false\"));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(\n        new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock)\n          throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new WrapStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n  }\n","date":"2016-08-31 00:15:31","endLine":132,"groupId":"10","id":27,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/33/709b454c4c98044895c213179030a6d7c781d1.src","preCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(\"spark.memory.offHeap.enabled\", \"\" + useOffHeapMemoryAllocator())\n          .set(\"spark.memory.offHeap.size\", \"256mb\")\n          .set(\"spark.shuffle.spill.compress\", \"false\")\n          .set(\"spark.shuffle.compress\", \"false\"));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(\n        new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock)\n          throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n  }\n","realPath":"core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"M"},{"authorDate":"2016-08-31 00:15:31","commitOrder":15,"curCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(\n        new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock)\n          throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new WrapStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n  }\n","date":"2016-08-31 00:15:31","endLine":138,"groupId":"10","id":28,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/a9/cf8ff520ed432bfc5033d37b0ce15aa13d1071.src","preCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(\n        new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock)\n          throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new CompressStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":99,"status":"M"}],"commitId":"4b4e329e49f8af28fa6301bd06c48d7097eaf9e6","commitMessage":"@@@[SPARK-5682][CORE] Add encrypted shuffle in spark\n\nThis patch is using Apache Commons Crypto library to enable shuffle encryption support.\n\nAuthor: Ferdinand Xu <cheng.a.xu@intel.com>\nAuthor: kellyzly <kellyzly@126.com>\n\nCloses #8880 from winningsix/SPARK-10771.\n","date":"2016-08-31 00:15:31","modifiedFileCount":"4","status":"M","submitter":"Ferdinand Xu"},{"authorTime":"2016-12-01 06:10:32","codes":[{"authorDate":"2016-12-01 06:10:32","commitOrder":16,"curCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(\"spark.memory.offHeap.enabled\", \"\" + useOffHeapMemoryAllocator())\n          .set(\"spark.memory.offHeap.size\", \"256mb\")\n          .set(\"spark.shuffle.spill.compress\", \"false\")\n          .set(\"spark.shuffle.compress\", \"false\"));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(\n        new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock)\n          throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          serializerManager,\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n  }\n","date":"2016-12-01 06:10:32","endLine":123,"groupId":"10","id":29,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/26/568146bf4d7dcf4601735a0bf5bd292a96ba1e.src","preCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(\"spark.memory.offHeap.enabled\", \"\" + useOffHeapMemoryAllocator())\n          .set(\"spark.memory.offHeap.size\", \"256mb\")\n          .set(\"spark.shuffle.spill.compress\", \"false\")\n          .set(\"spark.shuffle.compress\", \"false\"));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(\n        new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock)\n          throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new WrapStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n  }\n","realPath":"core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":77,"status":"M"},{"authorDate":"2016-12-01 06:10:32","commitOrder":16,"curCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(\n        new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock)\n          throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          serializerManager,\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n  }\n","date":"2016-12-01 06:10:32","endLine":131,"groupId":"10","id":30,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/fb/be530a132e18bc0f61742f0d4a2a401e578cec.src","preCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(\n        new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock)\n          throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          new WrapStream(),\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":92,"status":"M"}],"commitId":"93e9d880bf8a144112d74a6897af4e36fcfa5807","commitMessage":"@@@[SPARK-18546][CORE] Fix merging shuffle spills when using encryption.\n\nThe problem exists because it's not possible to just concatenate encrypted\npartition data from different spill files; currently each partition would\nhave its own initial vector to set up encryption.  and the final merged file\nshould contain a single initial vector for each merged partiton.  otherwise\niterating over each record becomes really hard.\n\nTo fix that.  UnsafeShuffleWriter now decrypts the partitions when merging. \nso that the merged file contains a single initial vector at the start of\nthe partition data.\n\nBecause it's not possible to do that using the fast transferTo path.  when\nencryption is enabled UnsafeShuffleWriter will revert back to using file\nstreams when merging. It may be possible to use a hybrid approach when\nusing encryption.  using an intermediate direct buffer when reading from\nfiles and encrypting the data.  but that's better left for a separate patch.\n\nAs part of the change I made DiskBlockObjectWriter take a SerializerManager\ninstead of a \"wrap stream\" closure.  since that makes it easier to test the\ncode without having to mock SerializerManager functionality.\n\nTested with newly added unit tests (UnsafeShuffleWriterSuite for the write\nside and ExternalAppendOnlyMapSuite for integration).  and by running some\napps that failed without the fix.\n\nAuthor: Marcelo Vanzin <vanzin@cloudera.com>\n\nCloses #15982 from vanzin/SPARK-18546.\n","date":"2016-12-01 06:10:32","modifiedFileCount":"4","status":"M","submitter":"Marcelo Vanzin"},{"authorTime":"2017-02-20 01:42:50","codes":[{"authorDate":"2017-02-20 01:42:50","commitOrder":17,"curCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(\"spark.memory.offHeap.enabled\", \"\" + useOffHeapMemoryAllocator())\n          .set(\"spark.memory.offHeap.size\", \"256mb\")\n          .set(\"spark.shuffle.spill.compress\", \"false\")\n          .set(\"spark.shuffle.compress\", \"false\"));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(invocationOnMock -> {\n      TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n      File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n      spillFilesCreated.add(file);\n      return Tuple2$.MODULE$.apply(blockId, file);\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(invocationOnMock -> {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          serializerManager,\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      });\n  }\n","date":"2017-02-20 01:42:50","endLine":112,"groupId":"1057","id":31,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/03/cec8ed81b728bfcc15c132c85af02296dea743.src","preCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(\"spark.memory.offHeap.enabled\", \"\" + useOffHeapMemoryAllocator())\n          .set(\"spark.memory.offHeap.size\", \"256mb\")\n          .set(\"spark.shuffle.spill.compress\", \"false\")\n          .set(\"spark.shuffle.compress\", \"false\"));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(\n        new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock)\n          throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          serializerManager,\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n  }\n","realPath":"core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":74,"status":"M"},{"authorDate":"2017-02-20 01:42:50","commitOrder":17,"curCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(invocationOnMock -> {\n      TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n      File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n      spillFilesCreated.add(file);\n      return Tuple2$.MODULE$.apply(blockId, file);\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(invocationOnMock -> {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          serializerManager,\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      });\n  }\n","date":"2017-02-20 01:42:50","endLine":120,"groupId":"1057","id":32,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/77/1d39016c188386ee64e0e1bbb2c884212a97a3.src","preCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(\n        new Answer<Tuple2<TempLocalBlockId, File>>() {\n      @Override\n      public Tuple2<TempLocalBlockId, File> answer(InvocationOnMock invocationOnMock)\n          throws Throwable {\n        TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n        File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n        spillFilesCreated.add(file);\n        return Tuple2$.MODULE$.apply(blockId, file);\n      }\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(new Answer<DiskBlockObjectWriter>() {\n      @Override\n      public DiskBlockObjectWriter answer(InvocationOnMock invocationOnMock) throws Throwable {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          serializerManager,\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      }\n    });\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":89,"status":"M"}],"commitId":"1487c9af20a333ead55955acf4c0aa323bea0d07","commitMessage":"@@@[SPARK-19534][TESTS] Convert Java tests to use lambdas.  Java 8 features\n\n## What changes were proposed in this pull request?\n\nConvert tests to use Java 8 lambdas.  and modest related fixes to surrounding code.\n\n## How was this patch tested?\n\nJenkins tests\n\nAuthor: Sean Owen <sowen@cloudera.com>\n\nCloses #16964 from srowen/SPARK-19534.\n","date":"2017-02-20 01:42:50","modifiedFileCount":"45","status":"M","submitter":"Sean Owen"},{"authorTime":"2017-02-20 01:42:50","codes":[{"authorDate":"2019-01-18 02:29:17","commitOrder":18,"curCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(\"spark.memory.offHeap.enabled\", \"\" + useOffHeapMemoryAllocator())\n          .set(\"spark.memory.offHeap.size\", \"256mb\")\n          .set(package$.MODULE$.SHUFFLE_SPILL_COMPRESS(), false)\n          .set(package$.MODULE$.SHUFFLE_COMPRESS(), false));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(invocationOnMock -> {\n      TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n      File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n      spillFilesCreated.add(file);\n      return Tuple2$.MODULE$.apply(blockId, file);\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(invocationOnMock -> {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          serializerManager,\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      });\n  }\n","date":"2019-01-18 02:29:17","endLine":115,"groupId":"1057","id":33,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/e4/e0d47fc1014e707f504921ebcc80363f8129df.src","preCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(\"spark.memory.offHeap.enabled\", \"\" + useOffHeapMemoryAllocator())\n          .set(\"spark.memory.offHeap.size\", \"256mb\")\n          .set(\"spark.shuffle.spill.compress\", \"false\")\n          .set(\"spark.shuffle.compress\", \"false\"));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(invocationOnMock -> {\n      TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n      File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n      spillFilesCreated.add(file);\n      return Tuple2$.MODULE$.apply(blockId, file);\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(invocationOnMock -> {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          serializerManager,\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      });\n  }\n","realPath":"core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":77,"status":"M"},{"authorDate":"2017-02-20 01:42:50","commitOrder":18,"curCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(invocationOnMock -> {\n      TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n      File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n      spillFilesCreated.add(file);\n      return Tuple2$.MODULE$.apply(blockId, file);\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(invocationOnMock -> {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          serializerManager,\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      });\n  }\n","date":"2017-02-20 01:42:50","endLine":120,"groupId":"1057","id":34,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/77/1d39016c188386ee64e0e1bbb2c884212a97a3.src","preCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(invocationOnMock -> {\n      TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n      File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n      spillFilesCreated.add(file);\n      return Tuple2$.MODULE$.apply(blockId, file);\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(invocationOnMock -> {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          serializerManager,\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      });\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":89,"status":"N"}],"commitId":"1b575ef5d1b8e3e672b2fca5c354d6678bd78bd1","commitMessage":"@@@[SPARK-26621][CORE] Use ConfigEntry for hardcoded configs for shuffle categories.\n\n## What changes were proposed in this pull request?\n\nThe PR makes hardcoded `spark.shuffle` configs to use ConfigEntry and put them in the config package.\n\n## How was this patch tested?\nExisting unit tests\n\nCloses #23550 from 10110346/ConfigEntry_shuffle.\n\nAuthored-by: liuxian <liu.xian3@zte.com.cn>\nSigned-off-by: Sean Owen <sean.owen@databricks.com>\n","date":"2019-01-18 02:29:17","modifiedFileCount":"9","status":"M","submitter":"liuxian"},{"authorTime":"2017-02-20 01:42:50","codes":[{"authorDate":"2019-01-26 12:28:12","commitOrder":19,"curCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(package$.MODULE$.MEMORY_OFFHEAP_ENABLED(), useOffHeapMemoryAllocator())\n          .set(package$.MODULE$.MEMORY_OFFHEAP_SIZE(), 256 * 1024 * 1024L)\n          .set(package$.MODULE$.SHUFFLE_SPILL_COMPRESS(), false)\n          .set(package$.MODULE$.SHUFFLE_COMPRESS(), false));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(invocationOnMock -> {\n      TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n      File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n      spillFilesCreated.add(file);\n      return Tuple2$.MODULE$.apply(blockId, file);\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(invocationOnMock -> {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          serializerManager,\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      });\n  }\n","date":"2019-01-26 12:28:12","endLine":115,"groupId":"1057","id":35,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/8d/03c6778e18b9666dcbf216b0dd82d9fd366132.src","preCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(\"spark.memory.offHeap.enabled\", \"\" + useOffHeapMemoryAllocator())\n          .set(\"spark.memory.offHeap.size\", \"256mb\")\n          .set(package$.MODULE$.SHUFFLE_SPILL_COMPRESS(), false)\n          .set(package$.MODULE$.SHUFFLE_COMPRESS(), false));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(invocationOnMock -> {\n      TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n      File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n      spillFilesCreated.add(file);\n      return Tuple2$.MODULE$.apply(blockId, file);\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(invocationOnMock -> {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          serializerManager,\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      });\n  }\n","realPath":"core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":77,"status":"M"},{"authorDate":"2017-02-20 01:42:50","commitOrder":19,"curCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(invocationOnMock -> {\n      TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n      File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n      spillFilesCreated.add(file);\n      return Tuple2$.MODULE$.apply(blockId, file);\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(invocationOnMock -> {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          serializerManager,\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      });\n  }\n","date":"2017-02-20 01:42:50","endLine":120,"groupId":"1057","id":36,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/77/1d39016c188386ee64e0e1bbb2c884212a97a3.src","preCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(invocationOnMock -> {\n      TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n      File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n      spillFilesCreated.add(file);\n      return Tuple2$.MODULE$.apply(blockId, file);\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(invocationOnMock -> {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          serializerManager,\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      });\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":89,"status":"N"}],"commitId":"aa3d16d68b7ebd9210c330905f01590ef93d875c","commitMessage":"@@@[SPARK-26698][CORE] Use ConfigEntry for hardcoded configs for memory and storage categories\n\n## What changes were proposed in this pull request?\n\nThis PR makes hardcoded configs about spark memory and storage to use `ConfigEntry` and put them in the config package.\n\n## How was this patch tested?\n\nExisting unit tests.\n\nCloses #23623 from SongYadong/configEntry_for_mem_storage.\n\nAuthored-by: SongYadong <song.yadong1@zte.com.cn>\nSigned-off-by: Sean Owen <sean.owen@databricks.com>\n","date":"2019-01-26 12:28:12","modifiedFileCount":"9","status":"M","submitter":"SongYadong"},{"authorTime":"2021-02-08 14:13:00","codes":[{"authorDate":"2021-02-08 14:13:00","commitOrder":20,"curCode":"  public void setup() throws Exception {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(package$.MODULE$.MEMORY_OFFHEAP_ENABLED(), useOffHeapMemoryAllocator())\n          .set(package$.MODULE$.MEMORY_OFFHEAP_SIZE(), 256 * 1024 * 1024L)\n          .set(package$.MODULE$.SHUFFLE_SPILL_COMPRESS(), false)\n          .set(package$.MODULE$.SHUFFLE_COMPRESS(), false));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.openMocks(this).close();\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(invocationOnMock -> {\n      TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n      File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n      spillFilesCreated.add(file);\n      return Tuple2$.MODULE$.apply(blockId, file);\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(invocationOnMock -> {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          serializerManager,\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      });\n  }\n","date":"2021-02-08 14:13:00","endLine":116,"groupId":"10555","id":37,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/91/fffbb291467eae56156926a9c6952658ceba0b.src","preCode":"  public void setup() {\n    memoryManager =\n      new TestMemoryManager(\n        new SparkConf()\n          .set(package$.MODULE$.MEMORY_OFFHEAP_ENABLED(), useOffHeapMemoryAllocator())\n          .set(package$.MODULE$.MEMORY_OFFHEAP_SIZE(), 256 * 1024 * 1024L)\n          .set(package$.MODULE$.SHUFFLE_SPILL_COMPRESS(), false)\n          .set(package$.MODULE$.SHUFFLE_COMPRESS(), false));\n    taskMemoryManager = new TaskMemoryManager(memoryManager, 0);\n\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    MockitoAnnotations.initMocks(this);\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(invocationOnMock -> {\n      TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n      File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n      spillFilesCreated.add(file);\n      return Tuple2$.MODULE$.apply(blockId, file);\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(invocationOnMock -> {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          serializerManager,\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      });\n  }\n","realPath":"core/src/test/java/org/apache/spark/unsafe/map/AbstractBytesToBytesMapSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":78,"status":"M"},{"authorDate":"2021-02-08 14:13:00","commitOrder":20,"curCode":"  public void setUp() throws Exception {\n    MockitoAnnotations.openMocks(this).close();\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(invocationOnMock -> {\n      TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n      File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n      spillFilesCreated.add(file);\n      return Tuple2$.MODULE$.apply(blockId, file);\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(invocationOnMock -> {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          serializerManager,\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      });\n  }\n","date":"2021-02-08 14:13:00","endLine":127,"groupId":"10555","id":38,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/1c/c23fd95a78d9a2ec66f02f28e0275e7bc97e37.src","preCode":"  public void setUp() {\n    MockitoAnnotations.initMocks(this);\n    tempDir = Utils.createTempDir(System.getProperty(\"java.io.tmpdir\"), \"unsafe-test\");\n    spillFilesCreated.clear();\n    taskContext = mock(TaskContext.class);\n    when(taskContext.taskMetrics()).thenReturn(new TaskMetrics());\n    when(blockManager.diskBlockManager()).thenReturn(diskBlockManager);\n    when(diskBlockManager.createTempLocalBlock()).thenAnswer(invocationOnMock -> {\n      TempLocalBlockId blockId = new TempLocalBlockId(UUID.randomUUID());\n      File file = File.createTempFile(\"spillFile\", \".spill\", tempDir);\n      spillFilesCreated.add(file);\n      return Tuple2$.MODULE$.apply(blockId, file);\n    });\n    when(blockManager.getDiskWriter(\n      any(BlockId.class),\n      any(File.class),\n      any(SerializerInstance.class),\n      anyInt(),\n      any(ShuffleWriteMetrics.class))).thenAnswer(invocationOnMock -> {\n        Object[] args = invocationOnMock.getArguments();\n\n        return new DiskBlockObjectWriter(\n          (File) args[1],\n          serializerManager,\n          (SerializerInstance) args[2],\n          (Integer) args[3],\n          false,\n          (ShuffleWriteMetrics) args[4],\n          (BlockId) args[0]\n        );\n      });\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":96,"status":"M"}],"commitId":"b344e91368ffdb22152d0450875e0549fb902200","commitMessage":"@@@[SPARK-34375][CORE][K8S][TEST] Replaces 'Mockito.initMocks' with 'Mockito.openMocks'\n\n\n What changes were proposed in this pull request?\n`Mockito.initMocks(Object)` is a deprecated api.  should use `Mockito.openMocks(Object).close()` instead.\n\n\n Why are the changes needed?\nCleanup deprecation api usage.\n\n\n Does this PR introduce _any_ user-facing change?\nNo\n\n\n How was this patch tested?\nPass the Jenkins or GitHub Action\n\nCloses #31487 from LuciferYang/mockito-api.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: HyukjinKwon <gurwls223@apache.org>\n","date":"2021-02-08 14:13:00","modifiedFileCount":"3","status":"M","submitter":"yangjie01"}]
