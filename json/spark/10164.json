[{"authorTime":"2019-08-05 14:54:45","codes":[{"authorDate":"2019-08-05 14:54:45","commitOrder":1,"curCode":"  public void testCompatibilityWithOldVersion() {\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 0)).thenReturn(blockMarkers[0]);\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 1)).thenReturn(blockMarkers[1]);\n\n    OpenBlocks openBlocks = new OpenBlocks(\n      \"app0\", \"exec1\", new String[] { \"shuffle_0_0_0\", \"shuffle_0_0_1\" });\n    checkOpenBlocksReceive(openBlocks, blockMarkers);\n\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 0);\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 1);\n    verifyOpenBlockLatencyMetrics();\n  }\n","date":"2019-08-05 14:54:45","endLine":96,"groupId":"1035","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompatibilityWithOldVersion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/9c/623a70424b6906b68d2e382e16cccac0ed71b0.src","preCode":"  public void testCompatibilityWithOldVersion() {\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 0)).thenReturn(blockMarkers[0]);\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 1)).thenReturn(blockMarkers[1]);\n\n    OpenBlocks openBlocks = new OpenBlocks(\n      \"app0\", \"exec1\", new String[] { \"shuffle_0_0_0\", \"shuffle_0_0_1\" });\n    checkOpenBlocksReceive(openBlocks, blockMarkers);\n\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 0);\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 1);\n    verifyOpenBlockLatencyMetrics();\n  }\n","realPath":"common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalBlockHandlerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":85,"status":"B"},{"authorDate":"2019-08-05 14:54:45","commitOrder":1,"curCode":"  public void testFetchShuffleBlocks() {\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 0)).thenReturn(blockMarkers[0]);\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 1)).thenReturn(blockMarkers[1]);\n\n    FetchShuffleBlocks fetchShuffleBlocks = new FetchShuffleBlocks(\n      \"app0\", \"exec1\", 0, new int[] { 0 }, new int[][] {{ 0, 1 }});\n    checkOpenBlocksReceive(fetchShuffleBlocks, blockMarkers);\n\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 0);\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 1);\n    verifyOpenBlockLatencyMetrics();\n  }\n","date":"2019-08-05 14:54:45","endLine":110,"groupId":"501","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testFetchShuffleBlocks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/9c/623a70424b6906b68d2e382e16cccac0ed71b0.src","preCode":"  public void testFetchShuffleBlocks() {\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 0)).thenReturn(blockMarkers[0]);\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 1)).thenReturn(blockMarkers[1]);\n\n    FetchShuffleBlocks fetchShuffleBlocks = new FetchShuffleBlocks(\n      \"app0\", \"exec1\", 0, new int[] { 0 }, new int[][] {{ 0, 1 }});\n    checkOpenBlocksReceive(fetchShuffleBlocks, blockMarkers);\n\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 0);\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 1);\n    verifyOpenBlockLatencyMetrics();\n  }\n","realPath":"common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalBlockHandlerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":99,"status":"B"}],"commitId":"db39f45bafc44271db8d8ec7cdb09734e5dedb37","commitMessage":"@@@[SPARK-28593][CORE] Rename ShuffleClient to BlockStoreClient which more close to its usage\n\n## What changes were proposed in this pull request?\n\nAfter SPARK-27677.  the shuffle client not only handles the shuffle block but also responsible for local persist RDD blocks. For better code scalability and precise semantics(as the [discussion](https://github.com/apache/spark/pull/24892#discussion_r300173331)).  here we did several changes:\n\n- Rename ShuffleClient to BlockStoreClient.\n- Correspondingly rename the ExternalShuffleClient to ExternalBlockStoreClient.  also change the server-side class from ExternalShuffleBlockHandler to ExternalBlockHandler.\n- Move MesosExternalBlockStoreClient to Mesos package.\n\nNote.  we still keep the name of BlockTransferService.  because the `Service` contains both client and server.  also the name of BlockTransferService is not referencing shuffle client only.\n\n## How was this patch tested?\n\nExisting UT.\n\nCloses #25327 from xuanyuanking/SPARK-28593.\n\nLead-authored-by: Yuanjian Li <xyliyuanjian@gmail.com>\nCo-authored-by: Yuanjian Li <yuanjian.li@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n","date":"2019-08-05 14:54:45","modifiedFileCount":"6","status":"B","submitter":"Yuanjian Li"},{"authorTime":"2019-09-23 16:16:52","codes":[{"authorDate":"2019-08-05 14:54:45","commitOrder":2,"curCode":"  public void testCompatibilityWithOldVersion() {\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 0)).thenReturn(blockMarkers[0]);\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 1)).thenReturn(blockMarkers[1]);\n\n    OpenBlocks openBlocks = new OpenBlocks(\n      \"app0\", \"exec1\", new String[] { \"shuffle_0_0_0\", \"shuffle_0_0_1\" });\n    checkOpenBlocksReceive(openBlocks, blockMarkers);\n\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 0);\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 1);\n    verifyOpenBlockLatencyMetrics();\n  }\n","date":"2019-08-05 14:54:45","endLine":96,"groupId":"1035","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompatibilityWithOldVersion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/9c/623a70424b6906b68d2e382e16cccac0ed71b0.src","preCode":"  public void testCompatibilityWithOldVersion() {\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 0)).thenReturn(blockMarkers[0]);\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 1)).thenReturn(blockMarkers[1]);\n\n    OpenBlocks openBlocks = new OpenBlocks(\n      \"app0\", \"exec1\", new String[] { \"shuffle_0_0_0\", \"shuffle_0_0_1\" });\n    checkOpenBlocksReceive(openBlocks, blockMarkers);\n\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 0);\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 1);\n    verifyOpenBlockLatencyMetrics();\n  }\n","realPath":"common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalBlockHandlerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":85,"status":"N"},{"authorDate":"2019-09-23 16:16:52","commitOrder":2,"curCode":"  public void testFetchShuffleBlocks() {\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 0)).thenReturn(blockMarkers[0]);\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 1)).thenReturn(blockMarkers[1]);\n\n    FetchShuffleBlocks fetchShuffleBlocks = new FetchShuffleBlocks(\n      \"app0\", \"exec1\", 0, new long[] { 0 }, new int[][] {{ 0, 1 }});\n    checkOpenBlocksReceive(fetchShuffleBlocks, blockMarkers);\n\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 0);\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 1);\n    verifyOpenBlockLatencyMetrics();\n  }\n","date":"2019-09-23 16:16:52","endLine":110,"groupId":"0","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testFetchShuffleBlocks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/6a/5d04b6f417b13f349759615aff027ce13c6f58.src","preCode":"  public void testFetchShuffleBlocks() {\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 0)).thenReturn(blockMarkers[0]);\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 1)).thenReturn(blockMarkers[1]);\n\n    FetchShuffleBlocks fetchShuffleBlocks = new FetchShuffleBlocks(\n      \"app0\", \"exec1\", 0, new int[] { 0 }, new int[][] {{ 0, 1 }});\n    checkOpenBlocksReceive(fetchShuffleBlocks, blockMarkers);\n\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 0);\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 1);\n    verifyOpenBlockLatencyMetrics();\n  }\n","realPath":"common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalBlockHandlerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":99,"status":"M"}],"commitId":"f725d472f51fb80c6ce1882ec283ff69bafb0de4","commitMessage":"@@@[SPARK-25341][CORE] Support rolling back a shuffle map stage and re-generate the shuffle files\n\nAfter the newly added shuffle block fetching protocol in #24565.  we can keep this work by extending the FetchShuffleBlocks message.\n\n\n What changes were proposed in this pull request?\nIn this patch.  we achieve the indeterminate shuffle rerun by reusing the task attempt id(unique id within an application) in shuffle id.  so that each shuffle write attempt has a different file name. For the indeterministic stage.  when the stage resubmits.  we'll clear all existing map status and rerun all partitions.\n\nAll changes are summarized as follows:\n- Change the mapId to mapTaskAttemptId in shuffle related id.\n- Record the mapTaskAttemptId in MapStatus.\n- Still keep mapId in ShuffleFetcherIterator for fetch failed scenario.\n- Add the determinate flag in Stage and use it in DAGScheduler and the cleaning work for the intermediate stage.\n\n\n Why are the changes needed?\nThis is a follow-up work for #22112's future improvment[1]: `Currently we can't rollback and rerun a shuffle map stage.  and just fail.`\n\nSpark will rerun a finished shuffle write stage while meeting fetch failures.  currently.  the rerun shuffle map stage will only resubmit the task for missing partitions and reuse the output of other partitions. This logic is fine in most scenarios.  but for indeterministic operations(like repartition).  multiple shuffle write attempts may write different data.  only rerun the missing partition will lead a correctness bug. So for the shuffle map stage of indeterministic operations.  we need to support rolling back the shuffle map stage and re-generate the shuffle files.\n\n\n Does this PR introduce any user-facing change?\nYes.  after this PR.  the indeterminate stage rerun will be accepted by rerunning the whole stage. The original behavior is aborting the stage and fail the job.\n\n\n How was this patch tested?\n- UT: Add UT for all changing code and newly added function.\n- Manual Test: Also providing a manual test to verify the effect.\n```\nimport scala.sys.process._\nimport org.apache.spark.TaskContext\n\nval determinateStage0 = sc.parallelize(0 until 1000 * 1000 * 100.  10)\nval indeterminateStage1 = determinateStage0.repartition(200)\nval indeterminateStage2 = indeterminateStage1.repartition(200)\nval indeterminateStage3 = indeterminateStage2.repartition(100)\nval indeterminateStage4 = indeterminateStage3.repartition(300)\nval fetchFailIndeterminateStage4 = indeterminateStage4.map { x =>\nif (TaskContext.get.attemptNumber == 0 && TaskContext.get.partitionId == 190 &&\n  TaskContext.get.stageAttemptNumber == 0) {\n  throw new Exception(\"pkill -f -n java\".!!)\n  }\n  x\n}\nval indeterminateStage5 = fetchFailIndeterminateStage4.repartition(200)\nval finalStage6 = indeterminateStage5.repartition(100).collect().distinct.length\n```\nIt's a simple job with multi indeterminate stage.  it will get a wrong answer while using old Spark version like 2.2/2.3.  and will be killed after #22112. With this fix.  the job can retry all indeterminate stage as below screenshot and get the right result.\n![image](https://user-images.githubusercontent.com/4833765/63948434-3477de00-caab-11e9-9ed1-75abfe6d16bd.png)\n\nCloses #25620 from xuanyuanking/SPARK-25341-8.27.\n\nAuthored-by: Yuanjian Li <xyliyuanjian@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n","date":"2019-09-23 16:16:52","modifiedFileCount":"16","status":"M","submitter":"Yuanjian Li"},{"authorTime":"2019-10-17 14:47:56","codes":[{"authorDate":"2019-08-05 14:54:45","commitOrder":3,"curCode":"  public void testCompatibilityWithOldVersion() {\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 0)).thenReturn(blockMarkers[0]);\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 1)).thenReturn(blockMarkers[1]);\n\n    OpenBlocks openBlocks = new OpenBlocks(\n      \"app0\", \"exec1\", new String[] { \"shuffle_0_0_0\", \"shuffle_0_0_1\" });\n    checkOpenBlocksReceive(openBlocks, blockMarkers);\n\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 0);\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 1);\n    verifyOpenBlockLatencyMetrics();\n  }\n","date":"2019-08-05 14:54:45","endLine":96,"groupId":"1035","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompatibilityWithOldVersion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/9c/623a70424b6906b68d2e382e16cccac0ed71b0.src","preCode":"  public void testCompatibilityWithOldVersion() {\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 0)).thenReturn(blockMarkers[0]);\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 1)).thenReturn(blockMarkers[1]);\n\n    OpenBlocks openBlocks = new OpenBlocks(\n      \"app0\", \"exec1\", new String[] { \"shuffle_0_0_0\", \"shuffle_0_0_1\" });\n    checkOpenBlocksReceive(openBlocks, blockMarkers);\n\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 0);\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 1);\n    verifyOpenBlockLatencyMetrics();\n  }\n","realPath":"common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalBlockHandlerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":85,"status":"N"},{"authorDate":"2019-10-17 14:47:56","commitOrder":3,"curCode":"  public void testFetchShuffleBlocks() {\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 0)).thenReturn(blockMarkers[0]);\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 1)).thenReturn(blockMarkers[1]);\n\n    FetchShuffleBlocks fetchShuffleBlocks = new FetchShuffleBlocks(\n      \"app0\", \"exec1\", 0, new long[] { 0 }, new int[][] {{ 0, 1 }}, false);\n    checkOpenBlocksReceive(fetchShuffleBlocks, blockMarkers);\n\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 0);\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 1);\n    verifyOpenBlockLatencyMetrics();\n  }\n","date":"2019-10-17 14:47:56","endLine":110,"groupId":"3336","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testFetchShuffleBlocks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/45/5351fcf767c867254355817da02c2faee2f29d.src","preCode":"  public void testFetchShuffleBlocks() {\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 0)).thenReturn(blockMarkers[0]);\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 1)).thenReturn(blockMarkers[1]);\n\n    FetchShuffleBlocks fetchShuffleBlocks = new FetchShuffleBlocks(\n      \"app0\", \"exec1\", 0, new long[] { 0 }, new int[][] {{ 0, 1 }});\n    checkOpenBlocksReceive(fetchShuffleBlocks, blockMarkers);\n\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 0);\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 1);\n    verifyOpenBlockLatencyMetrics();\n  }\n","realPath":"common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalBlockHandlerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":99,"status":"M"}],"commitId":"239ee3f5611889683e4a432a544bd790633680f1","commitMessage":"@@@[SPARK-9853][CORE] Optimize shuffle fetch of continuous partition IDs\n\nThis PR takes over #19788. After we split the shuffle fetch protocol from `OpenBlock` in #24565.  this optimization can be extended in the new shuffle protocol. Credit to yucai.  closes #19788.\n\n\n What changes were proposed in this pull request?\nThis PR adds the support for continuous shuffle block fetching in batch:\n\n- Shuffle client changes:\n    - Add new feature tag `spark.shuffle.fetchContinuousBlocksInBatch`.  implement the decision logic in `BlockStoreShuffleReader`.\n    - Merge the continuous shuffle block ids in batch if needed in ShuffleBlockFetcherIterator.\n- Shuffle server changes:\n    - Add support in `ExternalBlockHandler` for the external shuffle service side.\n    - Make `ShuffleBlockResolver.getBlockData` accept getting block data by range.\n- Protocol changes:\n    - Add new block id type `ShuffleBlockBatchId` represent continuous shuffle block ids.\n    - Extend `FetchShuffleBlocks` and `OneForOneBlockFetcher`.\n    - After the new shuffle fetch protocol completed in #24565.  the backward compatibility for external shuffle service can be controlled by `spark.shuffle.useOldFetchProtocol`.\n\n\n Why are the changes needed?\nIn adaptive execution.  one reducer may fetch multiple continuous shuffle blocks from one map output file. However.  as the original approach.  each reducer needs to fetch those 10 reducer blocks one by one. This way needs many IO and impacts performance. This PR is to support fetching those continuous shuffle blocks in one IO (batch way). See below example:\n\nThe shuffle block is stored like below:\n![image](https://user-images.githubusercontent.com/2989575/51654634-c37fbd80-1fd3-11e9-935e-5652863676c3.png)\nThe ShuffleId format is s\"shuffle_$shuffleId_$mapId_$reduceId\".  referring to BlockId.scala.\n\nIn adaptive execution.  one reducer may want to read output for reducer 5 to 14.  whose block Ids are from shuffle_0_x_5 to shuffle_0_x_14.\nBefore this PR.  Spark needs 10 disk IOs + 10 network IOs for each output file.\nAfter this PR.  Spark only needs 1 disk IO and 1 network IO. This way can reduce IO dramatically.\n\n\n Does this PR introduce any user-facing change?\nNo.\n\n\n How was this patch tested?\nAdd new UT.\nIntegrate test with setting `spark.sql.adaptive.enabled=true`.\n\nCloses #26040 from xuanyuanking/SPARK-9853.\n\nLead-authored-by: Yuanjian Li <xyliyuanjian@gmail.com>\nCo-authored-by: yucai <yyu1@ebay.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n","date":"2019-10-17 14:47:56","modifiedFileCount":"9","status":"M","submitter":"Yuanjian Li"},{"authorTime":"2021-06-28 15:36:17","codes":[{"authorDate":"2021-06-28 15:36:17","commitOrder":4,"curCode":"  public void testCompatibilityWithOldVersion() {\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 0)).thenReturn(blockMarkers[0]);\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 1)).thenReturn(blockMarkers[1]);\n\n    OpenBlocks openBlocks = new OpenBlocks(\n      \"app0\", \"exec1\", new String[] { \"shuffle_0_0_0\", \"shuffle_0_0_1\" });\n    checkOpenBlocksReceive(openBlocks, blockMarkers);\n\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 0);\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 1);\n    verifyOpenBlockLatencyMetrics(2, 2);\n  }\n","date":"2021-06-28 15:36:17","endLine":109,"groupId":"10164","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompatibilityWithOldVersion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/dc/41e957f0fcdb1e95e0f4a02b88308c0d0d969a.src","preCode":"  public void testCompatibilityWithOldVersion() {\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 0)).thenReturn(blockMarkers[0]);\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 1)).thenReturn(blockMarkers[1]);\n\n    OpenBlocks openBlocks = new OpenBlocks(\n      \"app0\", \"exec1\", new String[] { \"shuffle_0_0_0\", \"shuffle_0_0_1\" });\n    checkOpenBlocksReceive(openBlocks, blockMarkers);\n\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 0);\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 1);\n    verifyOpenBlockLatencyMetrics();\n  }\n","realPath":"common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalBlockHandlerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":98,"status":"M"},{"authorDate":"2021-06-28 15:36:17","commitOrder":4,"curCode":"  public void testFetchShuffleBlocks() {\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 0)).thenReturn(blockMarkers[0]);\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 1)).thenReturn(blockMarkers[1]);\n\n    FetchShuffleBlocks fetchShuffleBlocks = new FetchShuffleBlocks(\n      \"app0\", \"exec1\", 0, new long[] { 0 }, new int[][] {{ 0, 1 }}, false);\n    checkOpenBlocksReceive(fetchShuffleBlocks, blockMarkers);\n\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 0);\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 1);\n    verifyOpenBlockLatencyMetrics(2, 2);\n  }\n","date":"2021-06-28 15:36:17","endLine":123,"groupId":"10164","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testFetchShuffleBlocks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/dc/41e957f0fcdb1e95e0f4a02b88308c0d0d969a.src","preCode":"  public void testFetchShuffleBlocks() {\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 0)).thenReturn(blockMarkers[0]);\n    when(blockResolver.getBlockData(\"app0\", \"exec1\", 0, 0, 1)).thenReturn(blockMarkers[1]);\n\n    FetchShuffleBlocks fetchShuffleBlocks = new FetchShuffleBlocks(\n      \"app0\", \"exec1\", 0, new long[] { 0 }, new int[][] {{ 0, 1 }}, false);\n    checkOpenBlocksReceive(fetchShuffleBlocks, blockMarkers);\n\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 0);\n    verify(blockResolver, times(1)).getBlockData(\"app0\", \"exec1\", 0, 0, 1);\n    verifyOpenBlockLatencyMetrics();\n  }\n","realPath":"common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalBlockHandlerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":112,"status":"M"}],"commitId":"3255511d52f0c9652b34de4f499ee5081f59e0a5","commitMessage":"@@@[SPARK-35258][SHUFFLE][YARN] Add new metrics to ExternalShuffleService for better monitoring\n\n\n What changes were proposed in this pull request?\nThis adds two new additional metrics to `ExternalBlockHandler`:\n- `blockTransferRate` -- for indicating the rate of transferring blocks.  vs. the data within them\n- `blockTransferAvgSize_1min` -- a 1-minute trailing average of block sizes transferred by the ESS\n\nAdditionally.  this enhances `YarnShuffleServiceMetrics` to expose the histogram/`Snapshot` information from `Timer` metrics within `ExternalBlockHandler`.\n\n\n Why are the changes needed?\nCurrently `ExternalBlockHandler` exposes some useful metrics.  but is lacking around metrics for the rate of block transfers. We have `blockTransferRateBytes` to tell us the rate of _bytes_.  but no metric to tell us the rate of _blocks_.  which is especially relevant when running the ESS on HDDs that are sensitive to random reads. Many small block transfers can have a negative impact on performance.  but won't show up as a spike in `blockTransferRateBytes` since the sizes are small. Thus the new metrics to show information around average block size and block transfer rate are very useful to monitor the health/performance of the ESS.  especially when running on HDDs.\n\nFor the `YarnShuffleServiceMetrics`.  currently the three `Timer` metrics exposed by `ExternalBlockHandler` are being underutilized in a YARN-based environment -- they are basically treated as a `Meter`.  only exposing rate-based information.  when the metrics themselves are collected detailed histograms of timing information. We should expose this information for better observability.\n\n\n Does this PR introduce _any_ user-facing change?\nYes.  there are two entirely new metrics for the ESS.  as documented in `monitoring.md`. Additionally in a YARN environment.  `Timer` metrics exposed by the ESS will include more rich timing information.\n\n\n How was this patch tested?\nNew unit tests are added to verify that new metrics are showing up as expected.\n\nWe have been running this patch internally for approx. 1 year and have found it to be useful for monitoring the health of ESS and diagnosing performance issues.\n\nCloses #32388 from xkrogen/xkrogen-SPARK-35258-ess-new-metrics.\n\nAuthored-by: Erik Krogen <xkrogen@apache.org>\nSigned-off-by: Mridul Muralidharan <mridul<at>gmail.com>\n","date":"2021-06-28 15:36:17","modifiedFileCount":"3","status":"M","submitter":"Erik Krogen"}]
