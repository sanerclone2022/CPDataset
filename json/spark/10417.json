[{"authorTime":"2016-05-03 04:16:46","codes":[{"authorDate":"2016-05-03 04:16:46","commitOrder":1,"curCode":"  public void putIntsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    if (!bigEndianPlatform) {\n      Platform.copyMemory(src, srcIndex + Platform.BYTE_ARRAY_OFFSET,\n          null, data + 4 * rowId, count * 4);\n    } else {\n      int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n      long offset = data + 4 * rowId;\n      for (int i = 0; i < count; ++i, offset += 4, srcOffset += 4) {\n        Platform.putInt(null, offset, java.lang.Integer.reverseBytes(Platform.getInt(src, srcOffset)));\n      }\n    }\n  }\n","date":"2016-05-03 04:16:46","endLine":236,"groupId":"1094","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"putIntsLittleEndian","params":"(introwId@intcount@byte[]src@intsrcIndex)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/b8/dd16227ec179439bb8e655e514ce4748bd5fcd.src","preCode":"  public void putIntsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    if (!bigEndianPlatform) {\n      Platform.copyMemory(src, srcIndex + Platform.BYTE_ARRAY_OFFSET,\n          null, data + 4 * rowId, count * 4);\n    } else {\n      int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n      long offset = data + 4 * rowId;\n      for (int i = 0; i < count; ++i, offset += 4, srcOffset += 4) {\n        Platform.putInt(null, offset, java.lang.Integer.reverseBytes(Platform.getInt(src, srcOffset)));\n      }\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":225,"status":"B"},{"authorDate":"2016-05-03 04:16:46","commitOrder":1,"curCode":"  public void putLongsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    if (!bigEndianPlatform) {\n      Platform.copyMemory(src, srcIndex + Platform.BYTE_ARRAY_OFFSET,\n          null, data + 8 * rowId, count * 8);\n    } else {\n      int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n      long offset = data + 8 * rowId;\n      for (int i = 0; i < count; ++i, offset += 8, srcOffset += 8) {\n        Platform.putLong(null, offset, java.lang.Long.reverseBytes(Platform.getLong(src, srcOffset)));\n      }\n    }\n  }\n","date":"2016-05-03 04:16:46","endLine":282,"groupId":"1094","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"putLongsLittleEndian","params":"(introwId@intcount@byte[]src@intsrcIndex)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/b8/dd16227ec179439bb8e655e514ce4748bd5fcd.src","preCode":"  public void putLongsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    if (!bigEndianPlatform) {\n      Platform.copyMemory(src, srcIndex + Platform.BYTE_ARRAY_OFFSET,\n          null, data + 8 * rowId, count * 8);\n    } else {\n      int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n      long offset = data + 8 * rowId;\n      for (int i = 0; i < count; ++i, offset += 8, srcOffset += 8) {\n        Platform.putLong(null, offset, java.lang.Long.reverseBytes(Platform.getLong(src, srcOffset)));\n      }\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":271,"status":"B"}],"commitId":"8a1ce4899fb9f751dedaaa34ea654dfbc8330852","commitMessage":"@@@[SPARK-13745] [SQL] Support columnar in memory representation on Big Endian platforms\n\n## What changes were proposed in this pull request?\n\nparquet datasource and ColumnarBatch tests fail on big-endian platforms This patch adds support for the little-endian byte arrays being correctly interpreted on a big-endian platform\n\n## How was this patch tested?\n\nSpark test builds ran on big endian z/Linux and regression build on little endian amd64\n\nAuthor: Pete Robbins <robbinspg@gmail.com>\n\nCloses #12397 from robbinspg/master.\n","date":"2016-05-03 04:16:46","modifiedFileCount":"3","status":"B","submitter":"Pete Robbins"},{"authorTime":"2018-06-16 04:47:48","codes":[{"authorDate":"2018-06-16 04:47:48","commitOrder":2,"curCode":"  public void putIntsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    if (!bigEndianPlatform) {\n      Platform.copyMemory(src, srcIndex + Platform.BYTE_ARRAY_OFFSET,\n          null, data + 4L * rowId, count * 4L);\n    } else {\n      int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n      long offset = data + 4L * rowId;\n      for (int i = 0; i < count; ++i, offset += 4, srcOffset += 4) {\n        Platform.putInt(null, offset,\n            java.lang.Integer.reverseBytes(Platform.getInt(src, srcOffset)));\n      }\n    }\n  }\n","date":"2018-06-16 04:47:48","endLine":301,"groupId":"10417","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"putIntsLittleEndian","params":"(introwId@intcount@byte[]src@intsrcIndex)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/6f/dadde62855127b0596bbbab8c661a4f6f87460.src","preCode":"  public void putIntsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    if (!bigEndianPlatform) {\n      Platform.copyMemory(src, srcIndex + Platform.BYTE_ARRAY_OFFSET,\n          null, data + 4 * rowId, count * 4);\n    } else {\n      int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n      long offset = data + 4 * rowId;\n      for (int i = 0; i < count; ++i, offset += 4, srcOffset += 4) {\n        Platform.putInt(null, offset,\n            java.lang.Integer.reverseBytes(Platform.getInt(src, srcOffset)));\n      }\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":289,"status":"M"},{"authorDate":"2018-06-16 04:47:48","commitOrder":2,"curCode":"  public void putLongsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    if (!bigEndianPlatform) {\n      Platform.copyMemory(src, srcIndex + Platform.BYTE_ARRAY_OFFSET,\n          null, data + 8L * rowId, count * 8L);\n    } else {\n      int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n      long offset = data + 8L * rowId;\n      for (int i = 0; i < count; ++i, offset += 8, srcOffset += 8) {\n        Platform.putLong(null, offset,\n            java.lang.Long.reverseBytes(Platform.getLong(src, srcOffset)));\n      }\n    }\n  }\n","date":"2018-06-16 04:47:48","endLine":373,"groupId":"10417","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"putLongsLittleEndian","params":"(introwId@intcount@byte[]src@intsrcIndex)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/6f/dadde62855127b0596bbbab8c661a4f6f87460.src","preCode":"  public void putLongsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    if (!bigEndianPlatform) {\n      Platform.copyMemory(src, srcIndex + Platform.BYTE_ARRAY_OFFSET,\n          null, data + 8 * rowId, count * 8);\n    } else {\n      int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n      long offset = data + 8 * rowId;\n      for (int i = 0; i < count; ++i, offset += 8, srcOffset += 8) {\n        Platform.putLong(null, offset,\n            java.lang.Long.reverseBytes(Platform.getLong(src, srcOffset)));\n      }\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OffHeapColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":361,"status":"M"}],"commitId":"90da7dc241f8eec2348c0434312c97c116330bc4","commitMessage":"@@@[SPARK-24452][SQL][CORE] Avoid possible overflow in int add or multiple\n\n## What changes were proposed in this pull request?\n\nThis PR fixes possible overflow in int add or multiply. In particular.  their overflows in multiply are detected by [Spotbugs](https://spotbugs.github.io/)\n\nThe following assignments may cause overflow in right hand side. As a result.  the result may be negative.\n```\nlong = int * int\nlong = int + int\n```\n\nTo avoid this problem.  this PR performs cast from int to long in right hand side.\n\n## How was this patch tested?\n\nExisting UTs.\n\nAuthor: Kazuaki Ishizaki <ishizaki@jp.ibm.com>\n\nCloses #21481 from kiszk/SPARK-24452.\n","date":"2018-06-16 04:47:48","modifiedFileCount":"5","status":"M","submitter":"Kazuaki Ishizaki"}]
