[{"authorTime":"2015-10-23 00:46:30","codes":[{"authorDate":"2015-10-23 00:46:30","commitOrder":1,"curCode":"  public void heap() {\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new ExecutorMemoryManager(MemoryAllocator.HEAP));\n    final MemoryBlock page0 = memoryManager.allocatePage(128);\n    final MemoryBlock page1 = memoryManager.allocatePage(128);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","date":"2015-10-23 00:46:30","endLine":48,"groupId":"2782","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"heap","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/23/2ae4d926bcd7e1b5c0a0c2673bfcd096b1b806.src","preCode":"  public void heap() {\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new ExecutorMemoryManager(MemoryAllocator.HEAP));\n    final MemoryBlock page0 = memoryManager.allocatePage(128);\n    final MemoryBlock page1 = memoryManager.allocatePage(128);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","realPath":"core/src/test/java/org/apache/spark/shuffle/sort/PackedRecordPointerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":33,"status":"B"},{"authorDate":"2015-10-23 00:46:30","commitOrder":1,"curCode":"  public void offHeap() {\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new ExecutorMemoryManager(MemoryAllocator.UNSAFE));\n    final MemoryBlock page0 = memoryManager.allocatePage(128);\n    final MemoryBlock page1 = memoryManager.allocatePage(128);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","date":"2015-10-23 00:46:30","endLine":66,"groupId":"2782","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"offHeap","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/23/2ae4d926bcd7e1b5c0a0c2673bfcd096b1b806.src","preCode":"  public void offHeap() {\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new ExecutorMemoryManager(MemoryAllocator.UNSAFE));\n    final MemoryBlock page0 = memoryManager.allocatePage(128);\n    final MemoryBlock page1 = memoryManager.allocatePage(128);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","realPath":"core/src/test/java/org/apache/spark/shuffle/sort/PackedRecordPointerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":51,"status":"B"}],"commitId":"f6d06adf05afa9c5386dc2396c94e7a98730289f","commitMessage":"@@@[SPARK-10708] Consolidate sort shuffle implementations\n\nThere's a lot of duplication between SortShuffleManager and UnsafeShuffleManager. Given that these now provide the same set of functionality.  now that UnsafeShuffleManager supports large records.  I think that we should replace SortShuffleManager's serialized shuffle implementation with UnsafeShuffleManager's and should merge the two managers together.\n\nAuthor: Josh Rosen <joshrosen@databricks.com>\n\nCloses #8829 from JoshRosen/consolidate-sort-shuffle-implementations.\n","date":"2015-10-23 00:46:30","modifiedFileCount":"1","status":"B","submitter":"Josh Rosen"},{"authorTime":"2015-10-26 12:19:52","codes":[{"authorDate":"2015-10-26 12:19:52","commitOrder":2,"curCode":"  public void heap() {\n    final SparkConf conf = new SparkConf().set(\"spark.unsafe.offHeap\", \"false\");\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new GrantEverythingMemoryManager(conf), 0);\n    final MemoryBlock page0 = memoryManager.allocatePage(128);\n    final MemoryBlock page1 = memoryManager.allocatePage(128);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","date":"2015-10-26 12:19:52","endLine":49,"groupId":"2782","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"heap","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/7f/b2f92ca80e89eda920e9eb16066dc08ee17ba9.src","preCode":"  public void heap() {\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new ExecutorMemoryManager(MemoryAllocator.HEAP));\n    final MemoryBlock page0 = memoryManager.allocatePage(128);\n    final MemoryBlock page1 = memoryManager.allocatePage(128);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","realPath":"core/src/test/java/org/apache/spark/shuffle/sort/PackedRecordPointerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":33,"status":"M"},{"authorDate":"2015-10-26 12:19:52","commitOrder":2,"curCode":"  public void offHeap() {\n    final SparkConf conf = new SparkConf().set(\"spark.unsafe.offHeap\", \"true\");\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new GrantEverythingMemoryManager(conf), 0);\n    final MemoryBlock page0 = memoryManager.allocatePage(128);\n    final MemoryBlock page1 = memoryManager.allocatePage(128);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","date":"2015-10-26 12:19:52","endLine":68,"groupId":"2782","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"offHeap","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/7f/b2f92ca80e89eda920e9eb16066dc08ee17ba9.src","preCode":"  public void offHeap() {\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new ExecutorMemoryManager(MemoryAllocator.UNSAFE));\n    final MemoryBlock page0 = memoryManager.allocatePage(128);\n    final MemoryBlock page1 = memoryManager.allocatePage(128);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","realPath":"core/src/test/java/org/apache/spark/shuffle/sort/PackedRecordPointerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":52,"status":"M"}],"commitId":"85e654c5ec87e666a8845bfd77185c1ea57b268a","commitMessage":"@@@[SPARK-10984] Simplify *MemoryManager class structure\n\nThis patch refactors the MemoryManager class structure. After #9000.  Spark had the following classes:\n\n- MemoryManager\n- StaticMemoryManager\n- ExecutorMemoryManager\n- TaskMemoryManager\n- ShuffleMemoryManager\n\nThis is fairly confusing. To simplify things.  this patch consolidates several of these classes:\n\n- ShuffleMemoryManager and ExecutorMemoryManager were merged into MemoryManager.\n- TaskMemoryManager is moved into Spark Core.\n\n**Key changes and tasks**:\n\n- [x] Merge ExecutorMemoryManager into MemoryManager.\n  - [x] Move pooling logic into Allocator.\n- [x] Move TaskMemoryManager from `spark-unsafe` to `spark-core`.\n- [x] Refactor the existing Tungsten TaskMemoryManager interactions so Tungsten code use only this and not both this and ShuffleMemoryManager.\n- [x] Refactor non-Tungsten code to use the TaskMemoryManager instead of ShuffleMemoryManager.\n- [x] Merge ShuffleMemoryManager into MemoryManager.\n  - [x] Move code\n  - [x] ~~Simplify 1/n calculation.~~ **Will defer to followup.  since this needs more work.**\n- [x] Port ShuffleMemoryManagerSuite tests.\n- [x] Move classes from `unsafe` package to `memory` package.\n- [ ] Figure out how to handle the hacky use of the memory managers in HashedRelation's broadcast variable construction.\n- [x] Test porting and cleanup: several tests relied on mock functionality (such as `TestShuffleMemoryManager.markAsOutOfMemory`) which has been changed or broken during the memory manager consolidation\n  - [x] AbstractBytesToBytesMapSuite\n  - [x] UnsafeExternalSorterSuite\n  - [x] UnsafeFixedWidthAggregationMapSuite\n  - [x] UnsafeKVExternalSorterSuite\n\n**Compatiblity notes**:\n\n- This patch introduces breaking changes in `ExternalAppendOnlyMap`.  which is marked as `DevloperAPI` (likely for legacy reasons): this class now cannot be used outside of a task.\n\nAuthor: Josh Rosen <joshrosen@databricks.com>\n\nCloses #9127 from JoshRosen/SPARK-10984.\n","date":"2015-10-26 12:19:52","modifiedFileCount":"20","status":"M","submitter":"Josh Rosen"},{"authorTime":"2015-10-30 14:38:06","codes":[{"authorDate":"2015-10-30 14:38:06","commitOrder":3,"curCode":"  public void heap() throws IOException {\n    final SparkConf conf = new SparkConf().set(\"spark.unsafe.offHeap\", \"false\");\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new TestMemoryManager(conf), 0);\n    final MemoryBlock page0 = memoryManager.allocatePage(128, null);\n    final MemoryBlock page1 = memoryManager.allocatePage(128, null);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","date":"2015-10-30 14:38:06","endLine":53,"groupId":"2782","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"heap","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/9a/43f1f3a9235bb4a1134902f5e0bb117a88eb88.src","preCode":"  public void heap() {\n    final SparkConf conf = new SparkConf().set(\"spark.unsafe.offHeap\", \"false\");\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new GrantEverythingMemoryManager(conf), 0);\n    final MemoryBlock page0 = memoryManager.allocatePage(128);\n    final MemoryBlock page1 = memoryManager.allocatePage(128);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","realPath":"core/src/test/java/org/apache/spark/shuffle/sort/PackedRecordPointerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":37,"status":"M"},{"authorDate":"2015-10-30 14:38:06","commitOrder":3,"curCode":"  public void offHeap() throws IOException {\n    final SparkConf conf = new SparkConf().set(\"spark.unsafe.offHeap\", \"true\");\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new TestMemoryManager(conf), 0);\n    final MemoryBlock page0 = memoryManager.allocatePage(128, null);\n    final MemoryBlock page1 = memoryManager.allocatePage(128, null);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","date":"2015-10-30 14:38:06","endLine":72,"groupId":"2782","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"offHeap","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/9a/43f1f3a9235bb4a1134902f5e0bb117a88eb88.src","preCode":"  public void offHeap() {\n    final SparkConf conf = new SparkConf().set(\"spark.unsafe.offHeap\", \"true\");\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new GrantEverythingMemoryManager(conf), 0);\n    final MemoryBlock page0 = memoryManager.allocatePage(128);\n    final MemoryBlock page1 = memoryManager.allocatePage(128);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","realPath":"core/src/test/java/org/apache/spark/shuffle/sort/PackedRecordPointerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":56,"status":"M"}],"commitId":"56419cf11f769c80f391b45dc41b3c7101cc5ff4","commitMessage":"@@@[SPARK-10342] [SPARK-10309] [SPARK-10474] [SPARK-10929] [SQL] Cooperative memory management\n\nThis PR introduce a mechanism to call spill() on those SQL operators that support spilling (for example.  BytesToBytesMap.  UnsafeExternalSorter and ShuffleExternalSorter) if there is not enough memory for execution. The preserved first page is needed anymore.  so removed.\n\nOther Spillable objects in Spark core (ExternalSorter and AppendOnlyMap) are not included in this PR.  but those could benefit from this (trigger others' spilling).\n\nThe PrepareRDD may be not needed anymore.  could be removed in follow up PR.\n\nThe following script will fail with OOM before this PR.  finished in 150 seconds with 2G heap (also works in 1.5 branch.  with similar duration).\n\n```python\nsqlContext.setConf(\"spark.sql.shuffle.partitions\".  \"1\")\ndf = sqlContext.range(1<<25).selectExpr(\"id\".  \"repeat(id.  2) as s\")\ndf2 = df.select(df.id.alias('id2').  df.s.alias('s2'))\nj = df.join(df2.  df.id==df2.id2).groupBy(df.id).max(\"id\".  \"id2\")\nj.explain()\nprint j.count()\n```\n\nFor thread-safety.  here what I'm got:\n\n1) Without calling spill().  the operators should only be used by single thread.  no safety problems.\n\n2) spill() could be triggered in two cases.  triggered by itself.  or by other operators. we can check trigger == this in spill().  so it's still in the same thread.  so safety problems.\n\n3) if it's triggered by other operators (right now cache will not trigger spill()).  we only spill the data into disk when it's in scanning stage (building is finished).  so the in-memory sorter or memory pages are read-only.  we only need to synchronize the iterator and change it.\n\n4) During scanning.  the iterator will only use one record in one page.  we can't free this page.  because the downstream is currently using it (used by UnsafeRow or other objects). In BytesToBytesMap.  we just skip the current page.  and dump all others into disk. In UnsafeExternalSorter.  we keep the page that is used by current record (having the same baseObject).  free it when loading the next record. In ShuffleExternalSorter.  the spill() will not trigger during scanning.\n\n5) In order to avoid deadlock.  we didn't call acquireMemory during spill (so we reused the pointer array in InMemorySorter).\n\nAuthor: Davies Liu <davies@databricks.com>\n\nCloses #9241 from davies/force_spill.\n","date":"2015-10-30 14:38:06","modifiedFileCount":"21","status":"M","submitter":"Davies Liu"},{"authorTime":"2015-12-11 07:29:04","codes":[{"authorDate":"2015-12-11 07:29:04","commitOrder":4,"curCode":"  public void heap() throws IOException {\n    final SparkConf conf = new SparkConf().set(\"spark.memory.offHeap.enabled\", \"false\");\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new TestMemoryManager(conf), 0);\n    final MemoryBlock page0 = memoryManager.allocatePage(128, null);\n    final MemoryBlock page1 = memoryManager.allocatePage(128, null);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","date":"2015-12-11 07:29:04","endLine":53,"groupId":"2782","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"heap","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/fe/5abc5c230491b8217da6c9868052ed94fade97.src","preCode":"  public void heap() throws IOException {\n    final SparkConf conf = new SparkConf().set(\"spark.unsafe.offHeap\", \"false\");\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new TestMemoryManager(conf), 0);\n    final MemoryBlock page0 = memoryManager.allocatePage(128, null);\n    final MemoryBlock page1 = memoryManager.allocatePage(128, null);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","realPath":"core/src/test/java/org/apache/spark/shuffle/sort/PackedRecordPointerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":37,"status":"M"},{"authorDate":"2015-12-11 07:29:04","commitOrder":4,"curCode":"  public void offHeap() throws IOException {\n    final SparkConf conf = new SparkConf()\n      .set(\"spark.memory.offHeap.enabled\", \"true\")\n      .set(\"spark.memory.offHeap.size\", \"10000\");\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new TestMemoryManager(conf), 0);\n    final MemoryBlock page0 = memoryManager.allocatePage(128, null);\n    final MemoryBlock page1 = memoryManager.allocatePage(128, null);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","date":"2015-12-11 07:29:04","endLine":74,"groupId":"2782","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"offHeap","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/fe/5abc5c230491b8217da6c9868052ed94fade97.src","preCode":"  public void offHeap() throws IOException {\n    final SparkConf conf = new SparkConf().set(\"spark.unsafe.offHeap\", \"true\");\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new TestMemoryManager(conf), 0);\n    final MemoryBlock page0 = memoryManager.allocatePage(128, null);\n    final MemoryBlock page1 = memoryManager.allocatePage(128, null);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","realPath":"core/src/test/java/org/apache/spark/shuffle/sort/PackedRecordPointerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":56,"status":"M"}],"commitId":"23a9e62bad9669e9ff5dc4bd714f58d12f9be0b5","commitMessage":"@@@[SPARK-12251] Document and improve off-heap memory configurations\n\nThis patch adds documentation for Spark configurations that affect off-heap memory and makes some naming and validation improvements for those configs.\n\n- Change `spark.memory.offHeapSize` to `spark.memory.offHeap.size`. This is fine because this configuration has not shipped in any Spark release yet (it's new in Spark 1.6).\n- Deprecated `spark.unsafe.offHeap` in favor of a new `spark.memory.offHeap.enabled` configuration. The motivation behind this change is to gather all memory-related configurations under the same prefix.\n- Add a check which prevents users from setting `spark.memory.offHeap.enabled=true` when `spark.memory.offHeap.size == 0`. After SPARK-11389 (#9344).  which was committed in Spark 1.6.  Spark enforces a hard limit on the amount of off-heap memory that it will allocate to tasks. As a result.  enabling off-heap execution memory without setting `spark.memory.offHeap.size` will lead to immediate OOMs. The new configuration validation makes this scenario easier to diagnose.  helping to avoid user confusion.\n- Document these configurations on the configuration page.\n\nAuthor: Josh Rosen <joshrosen@databricks.com>\n\nCloses #10237 from JoshRosen/SPARK-12251.\n","date":"2015-12-11 07:29:04","modifiedFileCount":"7","status":"M","submitter":"Josh Rosen"},{"authorTime":"2016-05-19 00:44:21","codes":[{"authorDate":"2016-05-19 00:44:21","commitOrder":5,"curCode":"  public void heap() throws IOException {\n    final SparkConf conf = new SparkConf().set(\"spark.memory.offHeap.enabled\", \"false\");\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new TestMemoryManager(conf), 0);\n    final MemoryConsumer c = new TestMemoryConsumer(memoryManager, MemoryMode.ON_HEAP);\n    final MemoryBlock page0 = memoryManager.allocatePage(128, c);\n    final MemoryBlock page1 = memoryManager.allocatePage(128, c);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","date":"2016-05-19 00:44:21","endLine":53,"groupId":"2782","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"heap","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/35/4efe18dbde733e45f3a8d7c24283d07a98aff9.src","preCode":"  public void heap() throws IOException {\n    final SparkConf conf = new SparkConf().set(\"spark.memory.offHeap.enabled\", \"false\");\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new TestMemoryManager(conf), 0);\n    final MemoryBlock page0 = memoryManager.allocatePage(128, null);\n    final MemoryBlock page1 = memoryManager.allocatePage(128, null);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","realPath":"core/src/test/java/org/apache/spark/shuffle/sort/PackedRecordPointerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":36,"status":"M"},{"authorDate":"2016-05-19 00:44:21","commitOrder":5,"curCode":"  public void offHeap() throws IOException {\n    final SparkConf conf = new SparkConf()\n      .set(\"spark.memory.offHeap.enabled\", \"true\")\n      .set(\"spark.memory.offHeap.size\", \"10000\");\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new TestMemoryManager(conf), 0);\n    final MemoryConsumer c = new TestMemoryConsumer(memoryManager, MemoryMode.OFF_HEAP);\n    final MemoryBlock page0 = memoryManager.allocatePage(128, c);\n    final MemoryBlock page1 = memoryManager.allocatePage(128, c);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","date":"2016-05-19 00:44:21","endLine":75,"groupId":"2782","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"offHeap","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/35/4efe18dbde733e45f3a8d7c24283d07a98aff9.src","preCode":"  public void offHeap() throws IOException {\n    final SparkConf conf = new SparkConf()\n      .set(\"spark.memory.offHeap.enabled\", \"true\")\n      .set(\"spark.memory.offHeap.size\", \"10000\");\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new TestMemoryManager(conf), 0);\n    final MemoryBlock page0 = memoryManager.allocatePage(128, null);\n    final MemoryBlock page1 = memoryManager.allocatePage(128, null);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","realPath":"core/src/test/java/org/apache/spark/shuffle/sort/PackedRecordPointerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":56,"status":"M"}],"commitId":"8fb1d1c7f3ed1b62625052a532b7388ebec71bbf","commitMessage":"@@@[SPARK-15357] Cooperative spilling should check consumer memory mode\n\n## What changes were proposed in this pull request?\n\nSince we support forced spilling for Spillable.  which only works in OnHeap mode.  different from other SQL operators (could be OnHeap or OffHeap).  we should considering the mode of consumer before calling trigger forced spilling.\n\n## How was this patch tested?\n\nAdd new test.\n\nAuthor: Davies Liu <davies@databricks.com>\n\nCloses #13151 from davies/fix_mode.\n","date":"2016-05-19 00:44:21","modifiedFileCount":"10","status":"M","submitter":"Davies Liu"},{"authorTime":"2019-01-26 12:28:12","codes":[{"authorDate":"2019-01-26 12:28:12","commitOrder":6,"curCode":"  public void heap() throws IOException {\n    final SparkConf conf = new SparkConf().set(package$.MODULE$.MEMORY_OFFHEAP_ENABLED(), false);\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new TestMemoryManager(conf), 0);\n    final MemoryConsumer c = new TestMemoryConsumer(memoryManager, MemoryMode.ON_HEAP);\n    final MemoryBlock page0 = memoryManager.allocatePage(128, c);\n    final MemoryBlock page1 = memoryManager.allocatePage(128, c);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","date":"2019-01-26 12:28:12","endLine":54,"groupId":"10554","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"heap","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/cc/f47a4de558f50931d786441ec20977484c9361.src","preCode":"  public void heap() throws IOException {\n    final SparkConf conf = new SparkConf().set(\"spark.memory.offHeap.enabled\", \"false\");\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new TestMemoryManager(conf), 0);\n    final MemoryConsumer c = new TestMemoryConsumer(memoryManager, MemoryMode.ON_HEAP);\n    final MemoryBlock page0 = memoryManager.allocatePage(128, c);\n    final MemoryBlock page1 = memoryManager.allocatePage(128, c);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","realPath":"core/src/test/java/org/apache/spark/shuffle/sort/PackedRecordPointerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":37,"status":"M"},{"authorDate":"2019-01-26 12:28:12","commitOrder":6,"curCode":"  public void offHeap() throws IOException {\n    final SparkConf conf = new SparkConf()\n      .set(package$.MODULE$.MEMORY_OFFHEAP_ENABLED(), true)\n      .set(package$.MODULE$.MEMORY_OFFHEAP_SIZE(), 10000L);\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new TestMemoryManager(conf), 0);\n    final MemoryConsumer c = new TestMemoryConsumer(memoryManager, MemoryMode.OFF_HEAP);\n    final MemoryBlock page0 = memoryManager.allocatePage(128, c);\n    final MemoryBlock page1 = memoryManager.allocatePage(128, c);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","date":"2019-01-26 12:28:12","endLine":76,"groupId":"10554","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"offHeap","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/cc/f47a4de558f50931d786441ec20977484c9361.src","preCode":"  public void offHeap() throws IOException {\n    final SparkConf conf = new SparkConf()\n      .set(\"spark.memory.offHeap.enabled\", \"true\")\n      .set(\"spark.memory.offHeap.size\", \"10000\");\n    final TaskMemoryManager memoryManager =\n      new TaskMemoryManager(new TestMemoryManager(conf), 0);\n    final MemoryConsumer c = new TestMemoryConsumer(memoryManager, MemoryMode.OFF_HEAP);\n    final MemoryBlock page0 = memoryManager.allocatePage(128, c);\n    final MemoryBlock page1 = memoryManager.allocatePage(128, c);\n    final long addressInPage1 = memoryManager.encodePageNumberAndOffset(page1,\n      page1.getBaseOffset() + 42);\n    PackedRecordPointer packedPointer = new PackedRecordPointer();\n    packedPointer.set(PackedRecordPointer.packPointer(addressInPage1, 360));\n    assertEquals(360, packedPointer.getPartitionId());\n    final long recordPointer = packedPointer.getRecordPointer();\n    assertEquals(1, TaskMemoryManager.decodePageNumber(recordPointer));\n    assertEquals(page1.getBaseOffset() + 42, memoryManager.getOffsetInPage(recordPointer));\n    assertEquals(addressInPage1, recordPointer);\n    memoryManager.cleanUpAllAllocatedMemory();\n  }\n","realPath":"core/src/test/java/org/apache/spark/shuffle/sort/PackedRecordPointerSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":57,"status":"M"}],"commitId":"aa3d16d68b7ebd9210c330905f01590ef93d875c","commitMessage":"@@@[SPARK-26698][CORE] Use ConfigEntry for hardcoded configs for memory and storage categories\n\n## What changes were proposed in this pull request?\n\nThis PR makes hardcoded configs about spark memory and storage to use `ConfigEntry` and put them in the config package.\n\n## How was this patch tested?\n\nExisting unit tests.\n\nCloses #23623 from SongYadong/configEntry_for_mem_storage.\n\nAuthored-by: SongYadong <song.yadong1@zte.com.cn>\nSigned-off-by: Sean Owen <sean.owen@databricks.com>\n","date":"2019-01-26 12:28:12","modifiedFileCount":"9","status":"M","submitter":"SongYadong"}]
