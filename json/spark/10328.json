[{"authorTime":"2015-08-11 23:41:06","codes":[{"authorDate":"2015-08-11 23:41:06","commitOrder":4,"curCode":"  public CalendarInterval getInterval(int ordinal) {\n    assertIndexIsValid(ordinal);\n    final int offset = getElementOffset(ordinal);\n    if (offset < 0) return null;\n    final int months = (int) Platform.getLong(baseObject, baseOffset + offset);\n    final long microseconds = Platform.getLong(baseObject, baseOffset + offset + 8);\n    return new CalendarInterval(months, microseconds);\n  }\n","date":"2015-08-11 23:41:06","endLine":256,"groupId":"502","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"getInterval","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/50/1dff090313cef44b0b3a2d15dda5d4a607e56e.src","preCode":"  public CalendarInterval getInterval(int ordinal) {\n    assertIndexIsValid(ordinal);\n    final int offset = getElementOffset(ordinal);\n    if (offset < 0) return null;\n    final int months = (int) Platform.getLong(baseObject, baseOffset + offset);\n    final long microseconds = Platform.getLong(baseObject, baseOffset + offset + 8);\n    return new CalendarInterval(months, microseconds);\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeArrayData.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":249,"status":"MB"},{"authorDate":"2015-08-11 23:41:06","commitOrder":4,"curCode":"  public CalendarInterval getInterval(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int months = (int) Platform.getLong(baseObject, baseOffset + offset);\n      final long microseconds = Platform.getLong(baseObject, baseOffset + offset + 8);\n      return new CalendarInterval(months, microseconds);\n    }\n  }\n","date":"2015-08-11 23:41:06","endLine":445,"groupId":"1055","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"getInterval","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/7f/d94772090dfd440003dc247071418d20da31fc.src","preCode":"  public CalendarInterval getInterval(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int months = (int) Platform.getLong(baseObject, baseOffset + offset);\n      final long microseconds = Platform.getLong(baseObject, baseOffset + offset + 8);\n      return new CalendarInterval(months, microseconds);\n    }\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":435,"status":"MB"}],"commitId":"d378396f86f625f006738d87fe5dbc2ff8fd913d","commitMessage":"@@@[SPARK-9815] Rename PlatformDependent.UNSAFE -> Platform.\n\nPlatformDependent.UNSAFE is way too verbose.\n\nAuthor: Reynold Xin <rxin@databricks.com>\n\nCloses #8094 from rxin/SPARK-9815 and squashes the following commits:\n\n229b603 [Reynold Xin] [SPARK-9815] Rename PlatformDependent.UNSAFE -> Platform.\n","date":"2015-08-11 23:41:06","modifiedFileCount":"30","status":"M","submitter":"Reynold Xin"},{"authorTime":"2015-08-11 23:41:06","codes":[{"authorDate":"2016-09-27 14:18:32","commitOrder":5,"curCode":"  public CalendarInterval getInterval(int ordinal) {\n    if (isNullAt(ordinal)) return null;\n    final long offsetAndSize = getLong(ordinal);\n    final int offset = (int) (offsetAndSize >> 32);\n    final int months = (int) Platform.getLong(baseObject, baseOffset + offset);\n    final long microseconds = Platform.getLong(baseObject, baseOffset + offset + 8);\n    return new CalendarInterval(months, microseconds);\n  }\n","date":"2016-09-27 14:18:32","endLine":254,"groupId":"1055","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"getInterval","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/86/523c14740157e32d534185bdc170733f7476b4.src","preCode":"  public CalendarInterval getInterval(int ordinal) {\n    assertIndexIsValid(ordinal);\n    final int offset = getElementOffset(ordinal);\n    if (offset < 0) return null;\n    final int months = (int) Platform.getLong(baseObject, baseOffset + offset);\n    final long microseconds = Platform.getLong(baseObject, baseOffset + offset + 8);\n    return new CalendarInterval(months, microseconds);\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeArrayData.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":247,"status":"M"},{"authorDate":"2015-08-11 23:41:06","commitOrder":5,"curCode":"  public CalendarInterval getInterval(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int months = (int) Platform.getLong(baseObject, baseOffset + offset);\n      final long microseconds = Platform.getLong(baseObject, baseOffset + offset + 8);\n      return new CalendarInterval(months, microseconds);\n    }\n  }\n","date":"2015-08-11 23:41:06","endLine":445,"groupId":"1055","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"getInterval","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/7f/d94772090dfd440003dc247071418d20da31fc.src","preCode":"  public CalendarInterval getInterval(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int months = (int) Platform.getLong(baseObject, baseOffset + offset);\n      final long microseconds = Platform.getLong(baseObject, baseOffset + offset + 8);\n      return new CalendarInterval(months, microseconds);\n    }\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":435,"status":"N"}],"commitId":"85b0a157543201895557d66306b38b3ca52f2151","commitMessage":"@@@[SPARK-15962][SQL] Introduce implementation with a dense format for UnsafeArrayData\n\n## What changes were proposed in this pull request?\n\nThis PR introduces more compact representation for ```UnsafeArrayData```.\n\n```UnsafeArrayData``` needs to accept ```null``` value in each entry of an array. In the current version.  it has three parts\n```\n[numElements] [offsets] [values]\n```\n`Offsets` has the number of `numElements`.  and represents `null` if its value is negative. It may increase memory footprint.  and introduces an indirection for accessing each of `values`.\n\nThis PR uses bitvectors to represent nullability for each element like `UnsafeRow`.  and eliminates an indirection for accessing each element. The new ```UnsafeArrayData``` has four parts.\n```\n[numElements][null bits][values or offset&length][variable length portion]\n```\nIn the `null bits` region.  we store 1 bit per element.  represents whether an element is null. Its total size is ceil(numElements / 8) bytes.  and it is aligned to 8-byte boundaries.\nIn the `values or offset&length` region.  we store the content of elements. For fields that hold fixed-length primitive types.  such as long.  double.  or int.  we store the value directly in the field. For fields with non-primitive or variable-length values.  we store a relative offset (w.r.t. the base address of the array) that points to the beginning of the variable-length field and length (they are combined into a long). Each is word-aligned. For `variable length portion`.  each is aligned to 8-byte boundaries.\n\nThe new format can reduce memory footprint and improve performance of accessing each element. An example of memory foot comparison:\n1024x1024 elements integer array\nSize of ```baseObject``` for ```UnsafeArrayData```: 8 + 1024x1024 + 1024x1024 = 2M bytes\nSize of ```baseObject``` for ```UnsafeArrayData```: 8 + 1024x1024/8 + 1024x1024 = 1.25M bytes\n\nIn summary.  we got 1.0-2.6x performance improvements over the code before applying this PR.\nHere are performance results of [benchmark programs](https://github.com/kiszk/spark/blob/04d2e4b6dbdc4eff43ce18b3c9b776e0129257c7/sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/UnsafeArrayDataBenchmark.scala):\n\n**Read UnsafeArrayData**: 1.7x and 1.6x performance improvements over the code before applying this PR\n````\nOpenJDK 64-Bit Server VM 1.8.0_91-b14 on Linux 4.4.11-200.fc22.x86_64\nIntel Xeon E3-12xx v2 (Ivy Bridge)\n\nWithout SPARK-15962\nRead UnsafeArrayData:                    Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nInt                                            430 /  436        390.0           2.6       1.0X\nDouble                                         456 /  485        367.8           2.7       0.9X\n\nWith SPARK-15962\nRead UnsafeArrayData:                    Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nInt                                            252 /  260        666.1           1.5       1.0X\nDouble                                         281 /  292        597.7           1.7       0.9X\n````\n**Write UnsafeArrayData**: 1.0x and 1.1x performance improvements over the code before applying this PR\n````\nOpenJDK 64-Bit Server VM 1.8.0_91-b14 on Linux 4.0.4-301.fc22.x86_64\nIntel Xeon E3-12xx v2 (Ivy Bridge)\n\nWithout SPARK-15962\nWrite UnsafeArrayData:                   Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nInt                                            203 /  273        103.4           9.7       1.0X\nDouble                                         239 /  356         87.9          11.4       0.8X\n\nWith SPARK-15962\nWrite UnsafeArrayData:                   Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nInt                                            196 /  249        107.0           9.3       1.0X\nDouble                                         227 /  367         92.3          10.8       0.9X\n````\n\n**Get primitive array from UnsafeArrayData**: 2.6x and 1.6x performance improvements over the code before applying this PR\n````\nOpenJDK 64-Bit Server VM 1.8.0_91-b14 on Linux 4.0.4-301.fc22.x86_64\nIntel Xeon E3-12xx v2 (Ivy Bridge)\n\nWithout SPARK-15962\nGet primitive array from UnsafeArrayData: Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nInt                                            207 /  217        304.2           3.3       1.0X\nDouble                                         257 /  363        245.2           4.1       0.8X\n\nWith SPARK-15962\nGet primitive array from UnsafeArrayData: Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nInt                                            151 /  198        415.8           2.4       1.0X\nDouble                                         214 /  394        293.6           3.4       0.7X\n````\n\n**Create UnsafeArrayData from primitive array**: 1.7x and 2.1x performance improvements over the code before applying this PR\n````\nOpenJDK 64-Bit Server VM 1.8.0_91-b14 on Linux 4.0.4-301.fc22.x86_64\nIntel Xeon E3-12xx v2 (Ivy Bridge)\n\nWithout SPARK-15962\nCreate UnsafeArrayData from primitive array: Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nInt                                            340 /  385        185.1           5.4       1.0X\nDouble                                         479 /  705        131.3           7.6       0.7X\n\nWith SPARK-15962\nCreate UnsafeArrayData from primitive array: Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nInt                                            206 /  211        306.0           3.3       1.0X\nDouble                                         232 /  406        271.6           3.7       0.9X\n````\n\n1.7x and 1.4x performance improvements in [```UDTSerializationBenchmark```](https://github.com/apache/spark/blob/master/mllib/src/test/scala/org/apache/spark/mllib/linalg/UDTSerializationBenchmark.scala)  over the code before applying this PR\n````\nOpenJDK 64-Bit Server VM 1.8.0_91-b14 on Linux 4.4.11-200.fc22.x86_64\nIntel Xeon E3-12xx v2 (Ivy Bridge)\n\nWithout SPARK-15962\nVectorUDT de/serialization:              Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nserialize                                      442 /  533          0.0      441927.1       1.0X\ndeserialize                                    217 /  274          0.0      217087.6       2.0X\n\nWith SPARK-15962\nVectorUDT de/serialization:              Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nserialize                                      265 /  318          0.0      265138.5       1.0X\ndeserialize                                    155 /  197          0.0      154611.4       1.7X\n````\n\n## How was this patch tested?\n\nAdded unit tests into ```UnsafeArraySuite```\n\nAuthor: Kazuaki Ishizaki <ishizaki@jp.ibm.com>\n\nCloses #13680 from kiszk/SPARK-15962.\n","date":"2016-09-27 14:18:32","modifiedFileCount":"4","status":"M","submitter":"Kazuaki Ishizaki"},{"authorTime":"2019-11-01 18:12:33","codes":[{"authorDate":"2019-11-01 18:12:33","commitOrder":6,"curCode":"  public CalendarInterval getInterval(int ordinal) {\n    if (isNullAt(ordinal)) return null;\n    final long offsetAndSize = getLong(ordinal);\n    final int offset = (int) (offsetAndSize >> 32);\n    final int months = Platform.getInt(baseObject, baseOffset + offset);\n    final int days = Platform.getInt(baseObject, baseOffset + offset + 4);\n    final long microseconds = Platform.getLong(baseObject, baseOffset + offset + 8);\n    return new CalendarInterval(months, days, microseconds);\n  }\n","date":"2019-11-01 18:12:33","endLine":237,"groupId":"10328","id":5,"instanceNumber":1,"isCurCommit":1,"methodName":"getInterval","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/9e/686985b0607f59f6bfa151541a728bababcc87.src","preCode":"  public CalendarInterval getInterval(int ordinal) {\n    if (isNullAt(ordinal)) return null;\n    final long offsetAndSize = getLong(ordinal);\n    final int offset = (int) (offsetAndSize >> 32);\n    final int months = (int) Platform.getLong(baseObject, baseOffset + offset);\n    final long microseconds = Platform.getLong(baseObject, baseOffset + offset + 8);\n    return new CalendarInterval(months, microseconds);\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeArrayData.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":229,"status":"M"},{"authorDate":"2019-11-01 18:12:33","commitOrder":6,"curCode":"  public CalendarInterval getInterval(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int months = Platform.getInt(baseObject, baseOffset + offset);\n      final int days = Platform.getInt(baseObject, baseOffset + offset + 4);\n      final long microseconds = Platform.getLong(baseObject, baseOffset + offset + 8);\n      return new CalendarInterval(months, days, microseconds);\n    }\n  }\n","date":"2019-11-01 18:12:33","endLine":409,"groupId":"10328","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"getInterval","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/dc/6fa30e14f4db5c2e6a67c8c15a866b6a32c4fd.src","preCode":"  public CalendarInterval getInterval(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int months = (int) Platform.getLong(baseObject, baseOffset + offset);\n      final long microseconds = Platform.getLong(baseObject, baseOffset + offset + 8);\n      return new CalendarInterval(months, microseconds);\n    }\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":398,"status":"M"}],"commitId":"a4382f7fe1c36a51c64f460c6cb91e93470e0825","commitMessage":"@@@[SPARK-29486][SQL] CalendarInterval should have 3 fields: months.  days and microseconds\n\n\n What changes were proposed in this pull request?\nCurrent CalendarInterval has 2 fields: months and microseconds. This PR try to change it\nto 3 fields: months.  days and microseconds. This is because one logical day interval may\nhave different number of microseconds (daylight saving).\n\n\n Why are the changes needed?\nOne logical day interval may have different number of microseconds (daylight saving).\nFor example.  in PST timezone.  there will be 25 hours from 2019-11-2 12:00:00 to\n2019-11-3 12:00:00\n\n\n Does this PR introduce any user-facing change?\nno\n\n\n How was this patch tested?\nunit test and new added test cases\n\nCloses #26134 from LinhongLiu/calendarinterval.\n\nAuthored-by: Liu. Linhong <liulinhong@baidu.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n","date":"2019-11-01 18:12:33","modifiedFileCount":"8","status":"M","submitter":"LiuLinhong"}]
