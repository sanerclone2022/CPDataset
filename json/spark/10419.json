[{"authorTime":"2017-08-24 21:13:44","codes":[{"authorDate":"2017-08-24 21:13:44","commitOrder":1,"curCode":"  public UTF8String getUTF8String(int rowId) {\n    if (dictionary == null) {\n      ColumnVector.Array a = getByteArray(rowId);\n      return UTF8String.fromBytes(a.byteArray, a.byteArrayOffset, a.length);\n    } else {\n      byte[] bytes = dictionary.decodeToBinary(dictionaryIds.getDictId(rowId));\n      return UTF8String.fromBytes(bytes);\n    }\n  }\n","date":"2017-08-24 21:13:44","endLine":296,"groupId":"2557","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"getUTF8String","params":"(introwId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/b4/f753c0bc2a398f7a54cc15f4471988651e3e35.src","preCode":"  public UTF8String getUTF8String(int rowId) {\n    if (dictionary == null) {\n      ColumnVector.Array a = getByteArray(rowId);\n      return UTF8String.fromBytes(a.byteArray, a.byteArrayOffset, a.length);\n    } else {\n      byte[] bytes = dictionary.decodeToBinary(dictionaryIds.getDictId(rowId));\n      return UTF8String.fromBytes(bytes);\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":288,"status":"B"},{"authorDate":"2017-08-24 21:13:44","commitOrder":1,"curCode":"  public byte[] getBinary(int rowId) {\n    if (dictionary == null) {\n      ColumnVector.Array array = getByteArray(rowId);\n      byte[] bytes = new byte[array.length];\n      System.arraycopy(array.byteArray, array.byteArrayOffset, bytes, 0, bytes.length);\n      return bytes;\n    } else {\n      return dictionary.decodeToBinary(dictionaryIds.getDictId(rowId));\n    }\n  }\n","date":"2017-08-24 21:13:44","endLine":311,"groupId":"2558","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"getBinary","params":"(introwId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/b4/f753c0bc2a398f7a54cc15f4471988651e3e35.src","preCode":"  public byte[] getBinary(int rowId) {\n    if (dictionary == null) {\n      ColumnVector.Array array = getByteArray(rowId);\n      byte[] bytes = new byte[array.length];\n      System.arraycopy(array.byteArray, array.byteArrayOffset, bytes, 0, bytes.length);\n      return bytes;\n    } else {\n      return dictionary.decodeToBinary(dictionaryIds.getDictId(rowId));\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":302,"status":"B"}],"commitId":"9e33954ddfe1148f69e523c89827feb76ba892c9","commitMessage":"@@@[SPARK-21745][SQL] Refactor ColumnVector hierarchy to make ColumnVector read-only and to introduce WritableColumnVector.\n\n## What changes were proposed in this pull request?\n\nThis is a refactoring of `ColumnVector` hierarchy and related classes.\n\n1. make `ColumnVector` read-only\n2. introduce `WritableColumnVector` with write interface\n3. remove `ReadOnlyColumnVector`\n\n## How was this patch tested?\n\nExisting tests.\n\nAuthor: Takuya UESHIN <ueshin@databricks.com>\n\nCloses #18958 from ueshin/issues/SPARK-21745.\n","date":"2017-08-24 21:13:44","modifiedFileCount":"12","status":"B","submitter":"Takuya UESHIN"},{"authorTime":"2017-11-15 21:42:37","codes":[{"authorDate":"2017-11-15 21:42:37","commitOrder":2,"curCode":"  public UTF8String getUTF8String(int rowId) {\n    if (dictionary == null) {\n      ColumnarArray a = getByteArray(rowId);\n      return UTF8String.fromBytes(a.byteArray, a.byteArrayOffset, a.length);\n    } else {\n      byte[] bytes = dictionary.decodeToBinary(dictionaryIds.getDictId(rowId));\n      return UTF8String.fromBytes(bytes);\n    }\n  }\n","date":"2017-11-15 21:42:37","endLine":333,"groupId":"2557","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"getUTF8String","params":"(introwId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/96/cfeed34f300c30016a15b928519795609ddcbc.src","preCode":"  public UTF8String getUTF8String(int rowId) {\n    if (dictionary == null) {\n      ColumnVector.Array a = getByteArray(rowId);\n      return UTF8String.fromBytes(a.byteArray, a.byteArrayOffset, a.length);\n    } else {\n      byte[] bytes = dictionary.decodeToBinary(dictionaryIds.getDictId(rowId));\n      return UTF8String.fromBytes(bytes);\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":325,"status":"M"},{"authorDate":"2017-11-15 21:42:37","commitOrder":2,"curCode":"  public byte[] getBinary(int rowId) {\n    if (dictionary == null) {\n      ColumnarArray array = getByteArray(rowId);\n      byte[] bytes = new byte[array.length];\n      System.arraycopy(array.byteArray, array.byteArrayOffset, bytes, 0, bytes.length);\n      return bytes;\n    } else {\n      return dictionary.decodeToBinary(dictionaryIds.getDictId(rowId));\n    }\n  }\n","date":"2017-11-15 21:42:37","endLine":348,"groupId":"2558","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"getBinary","params":"(introwId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/96/cfeed34f300c30016a15b928519795609ddcbc.src","preCode":"  public byte[] getBinary(int rowId) {\n    if (dictionary == null) {\n      ColumnVector.Array array = getByteArray(rowId);\n      byte[] bytes = new byte[array.length];\n      System.arraycopy(array.byteArray, array.byteArrayOffset, bytes, 0, bytes.length);\n      return bytes;\n    } else {\n      return dictionary.decodeToBinary(dictionaryIds.getDictId(rowId));\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":339,"status":"M"}],"commitId":"dce1610ae376af00712ba7f4c99bfb4c006dbaec","commitMessage":"@@@[SPARK-22514][SQL] move ColumnVector.Array and ColumnarBatch.Row to individual files\n\n## What changes were proposed in this pull request?\n\nLogically the `Array` doesn't belong to `ColumnVector`.  and `Row` doesn't belong to `ColumnarBatch`. e.g. `ColumnVector` needs to return `Array` for `getArray`.  and `Row` for `getStruct`. `Array` and `Row` can return each other with the `getArray`/`getStruct` methods.\n\nThis is also a step to make `ColumnVector` public.  it's cleaner to have `Array` and `Row` as top-level classes.\n\nThis PR is just code moving around.  with 2 renaming: `Array` -> `VectorBasedArray`.  `Row` -> `VectorBasedRow`.\n\n## How was this patch tested?\n\nexisting tests.\n\nAuthor: Wenchen Fan <wenchen@databricks.com>\n\nCloses #19740 from cloud-fan/vector.\n","date":"2017-11-15 21:42:37","modifiedFileCount":"8","status":"M","submitter":"Wenchen Fan"},{"authorTime":"2017-11-15 21:42:37","codes":[{"authorDate":"2017-11-27 13:49:09","commitOrder":3,"curCode":"  public UTF8String getUTF8String(int rowId) {\n    if (dictionary == null) {\n      return arrayData().getBytesAsUTF8String(getArrayOffset(rowId), getArrayLength(rowId));\n    } else {\n      byte[] bytes = dictionary.decodeToBinary(dictionaryIds.getDictId(rowId));\n      return UTF8String.fromBytes(bytes);\n    }\n  }\n","date":"2017-11-27 13:49:09","endLine":317,"groupId":"235","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"getUTF8String","params":"(introwId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/e7/653f0c00b9a93fab5198df659b19b042f3457f.src","preCode":"  public UTF8String getUTF8String(int rowId) {\n    if (dictionary == null) {\n      ColumnarArray a = getByteArray(rowId);\n      return UTF8String.fromBytes(a.byteArray, a.byteArrayOffset, a.length);\n    } else {\n      byte[] bytes = dictionary.decodeToBinary(dictionaryIds.getDictId(rowId));\n      return UTF8String.fromBytes(bytes);\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":310,"status":"M"},{"authorDate":"2017-11-15 21:42:37","commitOrder":3,"curCode":"  public byte[] getBinary(int rowId) {\n    if (dictionary == null) {\n      ColumnarArray array = getByteArray(rowId);\n      byte[] bytes = new byte[array.length];\n      System.arraycopy(array.byteArray, array.byteArrayOffset, bytes, 0, bytes.length);\n      return bytes;\n    } else {\n      return dictionary.decodeToBinary(dictionaryIds.getDictId(rowId));\n    }\n  }\n","date":"2017-11-15 21:42:37","endLine":348,"groupId":"2558","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"getBinary","params":"(introwId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/96/cfeed34f300c30016a15b928519795609ddcbc.src","preCode":"  public byte[] getBinary(int rowId) {\n    if (dictionary == null) {\n      ColumnarArray array = getByteArray(rowId);\n      byte[] bytes = new byte[array.length];\n      System.arraycopy(array.byteArray, array.byteArrayOffset, bytes, 0, bytes.length);\n      return bytes;\n    } else {\n      return dictionary.decodeToBinary(dictionaryIds.getDictId(rowId));\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":339,"status":"N"}],"commitId":"5a02e3a2ac8a25d92d98d3b3b0d1173dddb9cc91","commitMessage":"@@@[SPARK-22602][SQL] remove ColumnVector#loadBytes\n\n## What changes were proposed in this pull request?\n\n`ColumnVector#loadBytes` is only used as an optimization for reading UTF8String in `WritableColumnVector`.  this PR moves this optimization to `WritableColumnVector` and simplified it.\n\n## How was this patch tested?\n\nexisting test\n\nAuthor: Wenchen Fan <wenchen@databricks.com>\n\nCloses #19815 from cloud-fan/load-bytes.\n","date":"2017-11-27 13:49:09","modifiedFileCount":"6","status":"M","submitter":"Wenchen Fan"},{"authorTime":"2018-02-02 10:18:32","codes":[{"authorDate":"2018-02-02 10:18:32","commitOrder":4,"curCode":"  public UTF8String getUTF8String(int rowId) {\n    if (isNullAt(rowId)) return null;\n    if (dictionary == null) {\n      return arrayData().getBytesAsUTF8String(getArrayOffset(rowId), getArrayLength(rowId));\n    } else {\n      byte[] bytes = dictionary.decodeToBinary(dictionaryIds.getDictId(rowId));\n      return UTF8String.fromBytes(bytes);\n    }\n  }\n","date":"2018-02-02 10:18:32","endLine":378,"groupId":"10419","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"getUTF8String","params":"(introwId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/52/75e4a91eac0e58d9a15bd51a815db1ac022261.src","preCode":"  public UTF8String getUTF8String(int rowId) {\n    if (dictionary == null) {\n      return arrayData().getBytesAsUTF8String(getArrayOffset(rowId), getArrayLength(rowId));\n    } else {\n      byte[] bytes = dictionary.decodeToBinary(dictionaryIds.getDictId(rowId));\n      return UTF8String.fromBytes(bytes);\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":370,"status":"M"},{"authorDate":"2018-02-02 10:18:32","commitOrder":4,"curCode":"  public byte[] getBinary(int rowId) {\n    if (isNullAt(rowId)) return null;\n    if (dictionary == null) {\n      return arrayData().getBytes(getArrayOffset(rowId), getArrayLength(rowId));\n    } else {\n      return dictionary.decodeToBinary(dictionaryIds.getDictId(rowId));\n    }\n  }\n","date":"2018-02-02 10:18:32","endLine":395,"groupId":"10419","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"getBinary","params":"(introwId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/52/75e4a91eac0e58d9a15bd51a815db1ac022261.src","preCode":"  public byte[] getBinary(int rowId) {\n    if (dictionary == null) {\n      ColumnarArray array = getByteArray(rowId);\n      byte[] bytes = new byte[array.length];\n      System.arraycopy(array.byteArray, array.byteArrayOffset, bytes, 0, bytes.length);\n      return bytes;\n    } else {\n      return dictionary.decodeToBinary(dictionaryIds.getDictId(rowId));\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/WritableColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":388,"status":"M"}],"commitId":"90848d507457d30abb36e3ba07618dfc87c34cd6","commitMessage":"@@@[SPARK-23284][SQL] Document the behavior of several ColumnVector's get APIs when accessing null slot\n\n## What changes were proposed in this pull request?\n\nFor some ColumnVector get APIs such as getDecimal.  getBinary.  getStruct.  getArray.  getInterval.  getUTF8String.  we should clearly document their behaviors when accessing null slot. They should return null in this case. Then we can remove null checks from the places using above APIs.\n\nFor the APIs of primitive values like getInt.  getInts.  etc..  this also documents their behaviors when accessing null slots. Their returning values are undefined and can be anything.\n\n## How was this patch tested?\n\nAdded tests into `ColumnarBatchSuite`.\n\nAuthor: Liang-Chi Hsieh <viirya@gmail.com>\n\nCloses #20455 from viirya/SPARK-23272-followup.\n","date":"2018-02-02 10:18:32","modifiedFileCount":"6","status":"M","submitter":"Liang-Chi Hsieh"}]
