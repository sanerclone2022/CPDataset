[{"authorTime":"2016-07-27 09:08:07","codes":[{"authorDate":"2016-07-27 09:08:07","commitOrder":1,"curCode":"  public void appendRowUntilExceedingCapacity() throws Exception {\n    RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n            valueSchema, taskMemoryManager, 10);\n    try {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      for (int i = 0; i < 10; i++) {\n        appendRow(batch, key, value);\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), 10);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < 10; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    } finally {\n      batch.close();\n    }\n  }\n","date":"2016-07-27 09:08:07","endLine":337,"groupId":"1270","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"appendRowUntilExceedingCapacity","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/0d/d129cea7b3f96cb65e1da6f3ee72f3d9e87aa9.src","preCode":"  public void appendRowUntilExceedingCapacity() throws Exception {\n    RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n            valueSchema, taskMemoryManager, 10);\n    try {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      for (int i = 0; i < 10; i++) {\n        appendRow(batch, key, value);\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), 10);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < 10; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    } finally {\n      batch.close();\n    }\n  }\n","realPath":"sql/catalyst/src/test/java/org/apache/spark/sql/catalyst/expressions/RowBasedKeyValueBatchSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":312,"status":"B"},{"authorDate":"2016-07-27 09:08:07","commitOrder":1,"curCode":"  public void appendRowUntilExceedingPageSize() throws Exception {\n    RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n            valueSchema, taskMemoryManager, 64 * 1024 * 1024); \r\n    try {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      int recordLength = 8 + key.getSizeInBytes() + value.getSizeInBytes() + 8;\n      int totalSize = 4;\n      int numRows = 0;\n      while (totalSize + recordLength < 64 * 1024 * 1024) { \r\n        appendRow(batch, key, value);\n        totalSize += recordLength;\n        numRows++;\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), numRows);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < numRows; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    } finally {\n      batch.close();\n    }\n  }\n","date":"2016-07-27 09:08:07","endLine":370,"groupId":"3056","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"appendRowUntilExceedingPageSize","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/0d/d129cea7b3f96cb65e1da6f3ee72f3d9e87aa9.src","preCode":"  public void appendRowUntilExceedingPageSize() throws Exception {\n    RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n            valueSchema, taskMemoryManager, 64 * 1024 * 1024); \r\n    try {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      int recordLength = 8 + key.getSizeInBytes() + value.getSizeInBytes() + 8;\n      int totalSize = 4;\n      int numRows = 0;\n      while (totalSize + recordLength < 64 * 1024 * 1024) { \r\n        appendRow(batch, key, value);\n        totalSize += recordLength;\n        numRows++;\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), numRows);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < numRows; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    } finally {\n      batch.close();\n    }\n  }\n","realPath":"sql/catalyst/src/test/java/org/apache/spark/sql/catalyst/expressions/RowBasedKeyValueBatchSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":340,"status":"B"}],"commitId":"738b4cc548ca48c010b682b8bc19a2f7e1947cfe","commitMessage":"@@@[SPARK-16524][SQL] Add RowBatch and RowBasedHashMapGenerator\n\n## What changes were proposed in this pull request?\n\nThis PR is the first step for the following feature:\n\nFor hash aggregation in Spark SQL.  we use a fast aggregation hashmap to act as a \"cache\" in order to boost aggregation performance. Previously.  the hashmap is backed by a `ColumnarBatch`. This has performance issues when we have wide schema for the aggregation table (large number of key fields or value fields).\nIn this JIRA.  we support another implementation of fast hashmap.  which is backed by a `RowBasedKeyValueBatch`. We then automatically pick between the two implementations based on certain knobs.\n\nIn this first-step PR.  implementations for `RowBasedKeyValueBatch` and `RowBasedHashMapGenerator` are added.\n\n## How was this patch tested?\n\nUnit tests: `RowBasedKeyValueBatchSuite`\n\nAuthor: Qifan Pu <qifan.pu@gmail.com>\n\nCloses #14349 from ooq/SPARK-16524.\n","date":"2016-07-27 09:08:07","modifiedFileCount":"0","status":"B","submitter":"Qifan Pu"},{"authorTime":"2016-09-15 16:37:12","codes":[{"authorDate":"2016-07-27 09:08:07","commitOrder":2,"curCode":"  public void appendRowUntilExceedingCapacity() throws Exception {\n    RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n            valueSchema, taskMemoryManager, 10);\n    try {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      for (int i = 0; i < 10; i++) {\n        appendRow(batch, key, value);\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), 10);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < 10; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    } finally {\n      batch.close();\n    }\n  }\n","date":"2016-07-27 09:08:07","endLine":337,"groupId":"1270","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"appendRowUntilExceedingCapacity","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/0d/d129cea7b3f96cb65e1da6f3ee72f3d9e87aa9.src","preCode":"  public void appendRowUntilExceedingCapacity() throws Exception {\n    RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n            valueSchema, taskMemoryManager, 10);\n    try {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      for (int i = 0; i < 10; i++) {\n        appendRow(batch, key, value);\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), 10);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < 10; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    } finally {\n      batch.close();\n    }\n  }\n","realPath":"sql/catalyst/src/test/java/org/apache/spark/sql/catalyst/expressions/RowBasedKeyValueBatchSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":312,"status":"N"},{"authorDate":"2016-09-15 16:37:12","commitOrder":2,"curCode":"  public void appendRowUntilExceedingPageSize() throws Exception {\n    \r\n    int pageSizeToUse = (int) memoryManager.pageSizeBytes();\n    RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n            valueSchema, taskMemoryManager, pageSizeToUse); \r\n    try {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      int recordLength = 8 + key.getSizeInBytes() + value.getSizeInBytes() + 8;\n      int totalSize = 4;\n      int numRows = 0;\n      while (totalSize + recordLength < pageSizeToUse) {\n        appendRow(batch, key, value);\n        totalSize += recordLength;\n        numRows++;\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), numRows);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < numRows; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    } finally {\n      batch.close();\n    }\n  }\n","date":"2016-09-15 16:37:12","endLine":372,"groupId":"3056","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"appendRowUntilExceedingPageSize","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/fb/3dbe8ed19969d6fd535b8ee0f329e0a3e361f3.src","preCode":"  public void appendRowUntilExceedingPageSize() throws Exception {\n    RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n            valueSchema, taskMemoryManager, 64 * 1024 * 1024); \r\n    try {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      int recordLength = 8 + key.getSizeInBytes() + value.getSizeInBytes() + 8;\n      int totalSize = 4;\n      int numRows = 0;\n      while (totalSize + recordLength < 64 * 1024 * 1024) { \r\n        appendRow(batch, key, value);\n        totalSize += recordLength;\n        numRows++;\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), numRows);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < numRows; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    } finally {\n      batch.close();\n    }\n  }\n","realPath":"sql/catalyst/src/test/java/org/apache/spark/sql/catalyst/expressions/RowBasedKeyValueBatchSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":340,"status":"M"}],"commitId":"f893e262500e2f183de88e984300dd5b085e1f71","commitMessage":"@@@[SPARK-17524][TESTS] Use specified spark.buffer.pageSize\n\n## What changes were proposed in this pull request?\n\nThis PR has the appendRowUntilExceedingPageSize test in RowBasedKeyValueBatchSuite use whatever spark.buffer.pageSize value a user has specified to prevent a test failure for anyone testing Apache Spark on a box with a reduced page size. The test is currently hardcoded to use the default page size which is 64 MB so this minor PR is a test improvement\n\n## How was this patch tested?\nExisting unit tests with 1 MB page size and with 64 MB (the default) page size\n\nAuthor: Adam Roberts <aroberts@uk.ibm.com>\n\nCloses #15079 from a-roberts/patch-5.\n","date":"2016-09-15 16:37:12","modifiedFileCount":"1","status":"M","submitter":"Adam Roberts"},{"authorTime":"2018-10-05 09:58:25","codes":[{"authorDate":"2018-10-05 09:58:25","commitOrder":3,"curCode":"  public void appendRowUntilExceedingCapacity() throws Exception {\n    try (RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n            valueSchema, taskMemoryManager, 10)) {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      for (int i = 0; i < 10; i++) {\n        appendRow(batch, key, value);\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), 10);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < 10; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    }\n  }\n","date":"2018-10-05 09:58:25","endLine":305,"groupId":"1270","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"appendRowUntilExceedingCapacity","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/ef/02f0ae72686535ec3409ada82ff562746feed3.src","preCode":"  public void appendRowUntilExceedingCapacity() throws Exception {\n    RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n            valueSchema, taskMemoryManager, 10);\n    try {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      for (int i = 0; i < 10; i++) {\n        appendRow(batch, key, value);\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), 10);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < 10; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    } finally {\n      batch.close();\n    }\n  }\n","realPath":"sql/catalyst/src/test/java/org/apache/spark/sql/catalyst/expressions/RowBasedKeyValueBatchSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":283,"status":"M"},{"authorDate":"2018-10-05 09:58:25","commitOrder":3,"curCode":"  public void appendRowUntilExceedingPageSize() throws Exception {\n    \r\n    int pageSizeToUse = (int) memoryManager.pageSizeBytes();\n    try (RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n            valueSchema, taskMemoryManager, pageSizeToUse)) {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      int recordLength = 8 + key.getSizeInBytes() + value.getSizeInBytes() + 8;\n      int totalSize = 4;\n      int numRows = 0;\n      while (totalSize + recordLength < pageSizeToUse) {\n        appendRow(batch, key, value);\n        totalSize += recordLength;\n        numRows++;\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), numRows);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < numRows; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    }\n  }\n","date":"2018-10-05 09:58:25","endLine":337,"groupId":"3056","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"appendRowUntilExceedingPageSize","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/ef/02f0ae72686535ec3409ada82ff562746feed3.src","preCode":"  public void appendRowUntilExceedingPageSize() throws Exception {\n    \r\n    int pageSizeToUse = (int) memoryManager.pageSizeBytes();\n    RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n            valueSchema, taskMemoryManager, pageSizeToUse); \r\n    try {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      int recordLength = 8 + key.getSizeInBytes() + value.getSizeInBytes() + 8;\n      int totalSize = 4;\n      int numRows = 0;\n      while (totalSize + recordLength < pageSizeToUse) {\n        appendRow(batch, key, value);\n        totalSize += recordLength;\n        numRows++;\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), numRows);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < numRows; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    } finally {\n      batch.close();\n    }\n  }\n","realPath":"sql/catalyst/src/test/java/org/apache/spark/sql/catalyst/expressions/RowBasedKeyValueBatchSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":308,"status":"M"}],"commitId":"44c1e1ab1c26560371831b1593f96f30344c4363","commitMessage":"@@@[SPARK-25408] Move to mode ideomatic Java8\n\nWhile working on another PR.  I noticed that there is quite some legacy Java in there that can be beautified. For example the use og features from Java8.  such as:\n- Collection libraries\n- Try-with-resource blocks\n\nNo code has been changed\n\nWhat are your thoughts on this?\n\nThis makes code easier to read.  and using try-with-resource makes is less likely to forget to close something.\n\n## What changes were proposed in this pull request?\n\n(Please fill in changes proposed in this fix)\n\n## How was this patch tested?\n\n(Please explain how this patch was tested. E.g. unit tests.  integration tests.  manual tests)\n(If this patch involves UI changes.  please attach a screenshot; otherwise.  remove this)\n\nPlease review http://spark.apache.org/contributing.html before opening a pull request.\n\nCloses #22399 from Fokko/SPARK-25408.\n\nAuthored-by: Fokko Driesprong <fokkodriesprong@godatadriven.com>\nSigned-off-by: Sean Owen <sean.owen@databricks.com>\n","date":"2018-10-05 09:58:25","modifiedFileCount":"19","status":"M","submitter":"Fokko Driesprong"},{"authorTime":"2018-10-05 11:03:41","codes":[{"authorDate":"2018-10-05 11:03:41","commitOrder":4,"curCode":"  public void appendRowUntilExceedingCapacity() throws Exception {\n    RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n            valueSchema, taskMemoryManager, 10);\n    try {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      for (int i = 0; i < 10; i++) {\n        appendRow(batch, key, value);\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), 10);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < 10; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    } finally {\n      batch.close();\n    }\n  }\n","date":"2018-10-05 11:03:41","endLine":327,"groupId":"1270","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"appendRowUntilExceedingCapacity","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/2d/a87113c6229a807c41c7cae298bd8efe426034.src","preCode":"  public void appendRowUntilExceedingCapacity() throws Exception {\n    try (RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n            valueSchema, taskMemoryManager, 10)) {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      for (int i = 0; i < 10; i++) {\n        appendRow(batch, key, value);\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), 10);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < 10; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    }\n  }\n","realPath":"sql/catalyst/src/test/java/org/apache/spark/sql/catalyst/expressions/RowBasedKeyValueBatchSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":302,"status":"M"},{"authorDate":"2018-10-05 11:03:41","commitOrder":4,"curCode":"  public void appendRowUntilExceedingPageSize() throws Exception {\n    \r\n    int pageSizeToUse = (int) memoryManager.pageSizeBytes();\n    RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n            valueSchema, taskMemoryManager, pageSizeToUse); \r\n    try {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      int recordLength = 8 + key.getSizeInBytes() + value.getSizeInBytes() + 8;\n      int totalSize = 4;\n      int numRows = 0;\n      while (totalSize + recordLength < pageSizeToUse) {\n        appendRow(batch, key, value);\n        totalSize += recordLength;\n        numRows++;\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), numRows);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < numRows; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    } finally {\n      batch.close();\n    }\n  }\n","date":"2018-10-05 11:03:41","endLine":362,"groupId":"3056","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"appendRowUntilExceedingPageSize","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/2d/a87113c6229a807c41c7cae298bd8efe426034.src","preCode":"  public void appendRowUntilExceedingPageSize() throws Exception {\n    \r\n    int pageSizeToUse = (int) memoryManager.pageSizeBytes();\n    try (RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n            valueSchema, taskMemoryManager, pageSizeToUse)) {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      int recordLength = 8 + key.getSizeInBytes() + value.getSizeInBytes() + 8;\n      int totalSize = 4;\n      int numRows = 0;\n      while (totalSize + recordLength < pageSizeToUse) {\n        appendRow(batch, key, value);\n        totalSize += recordLength;\n        numRows++;\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), numRows);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < numRows; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    }\n  }\n","realPath":"sql/catalyst/src/test/java/org/apache/spark/sql/catalyst/expressions/RowBasedKeyValueBatchSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":330,"status":"M"}],"commitId":"5ae20cf1a96a33f5de4435fcfb55914d64466525","commitMessage":"@@@Revert \"[SPARK-25408] Move to mode ideomatic Java8\"\n\nThis reverts commit 44c1e1ab1c26560371831b1593f96f30344c4363.\n","date":"2018-10-05 11:03:41","modifiedFileCount":"19","status":"M","submitter":"Wenchen Fan"},{"authorTime":"2018-10-08 22:58:52","codes":[{"authorDate":"2018-10-08 22:58:52","commitOrder":5,"curCode":"  public void appendRowUntilExceedingCapacity() throws Exception {\n    try (RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n        valueSchema, taskMemoryManager, 10)) {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      for (int i = 0; i < 10; i++) {\n        appendRow(batch, key, value);\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), 10);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < 10; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    }\n  }\n","date":"2018-10-08 22:58:52","endLine":305,"groupId":"1270","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"appendRowUntilExceedingCapacity","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/8d/a778800bb9f253184321108e97a77143173c1a.src","preCode":"  public void appendRowUntilExceedingCapacity() throws Exception {\n    RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n            valueSchema, taskMemoryManager, 10);\n    try {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      for (int i = 0; i < 10; i++) {\n        appendRow(batch, key, value);\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), 10);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < 10; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    } finally {\n      batch.close();\n    }\n  }\n","realPath":"sql/catalyst/src/test/java/org/apache/spark/sql/catalyst/expressions/RowBasedKeyValueBatchSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":283,"status":"M"},{"authorDate":"2018-10-08 22:58:52","commitOrder":5,"curCode":"  public void appendRowUntilExceedingPageSize() throws Exception {\n    \r\n    int pageSizeToUse = (int) memoryManager.pageSizeBytes();\n    try (RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n        valueSchema, taskMemoryManager, pageSizeToUse)) {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      int recordLength = 8 + key.getSizeInBytes() + value.getSizeInBytes() + 8;\n      int totalSize = 4;\n      int numRows = 0;\n      while (totalSize + recordLength < pageSizeToUse) {\n        appendRow(batch, key, value);\n        totalSize += recordLength;\n        numRows++;\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), numRows);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < numRows; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    }\n  }\n","date":"2018-10-08 22:58:52","endLine":337,"groupId":"3056","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"appendRowUntilExceedingPageSize","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/8d/a778800bb9f253184321108e97a77143173c1a.src","preCode":"  public void appendRowUntilExceedingPageSize() throws Exception {\n    \r\n    int pageSizeToUse = (int) memoryManager.pageSizeBytes();\n    RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n            valueSchema, taskMemoryManager, pageSizeToUse); \r\n    try {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      int recordLength = 8 + key.getSizeInBytes() + value.getSizeInBytes() + 8;\n      int totalSize = 4;\n      int numRows = 0;\n      while (totalSize + recordLength < pageSizeToUse) {\n        appendRow(batch, key, value);\n        totalSize += recordLength;\n        numRows++;\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), numRows);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < numRows; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    } finally {\n      batch.close();\n    }\n  }\n","realPath":"sql/catalyst/src/test/java/org/apache/spark/sql/catalyst/expressions/RowBasedKeyValueBatchSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":308,"status":"M"}],"commitId":"1a28625355d75076bde4bcc95a72e9b187cda606","commitMessage":"@@@[SPARK-25408] Move to more ideomatic Java8\n\nWhile working on another PR.  I noticed that there is quite some legacy Java in there that can be beautified. For example the use of features from Java8.  such as:\n- Collection libraries\n- Try-with-resource blocks\n\nNo logic has been changed. I think it is important to have a solid codebase with examples that will inspire next PR's to follow up on the best practices.\n\nWhat are your thoughts on this?\n\nThis makes code easier to read.  and using try-with-resource makes is less likely to forget to close something.\n\n## What changes were proposed in this pull request?\n\nNo changes in the logic of Spark.  but more in the aesthetics of the code.\n\n## How was this patch tested?\n\nUsing the existing unit tests. Since no logic is changed.  the existing unit tests should pass.\n\nPlease review http://spark.apache.org/contributing.html before opening a pull request.\n\nCloses #22637 from Fokko/SPARK-25408.\n\nAuthored-by: Fokko Driesprong <fokkodriesprong@godatadriven.com>\nSigned-off-by: Sean Owen <sean.owen@databricks.com>\n","date":"2018-10-08 22:58:52","modifiedFileCount":"17","status":"M","submitter":"Fokko Driesprong"},{"authorTime":"2019-11-04 03:21:28","codes":[{"authorDate":"2019-11-04 03:21:28","commitOrder":6,"curCode":"  public void appendRowUntilExceedingCapacity() throws Exception {\n    try (RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n        valueSchema, taskMemoryManager, 10)) {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      for (int i = 0; i < 10; i++) {\n        appendRow(batch, key, value);\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(10, batch.numRows());\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < 10; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    }\n  }\n","date":"2019-11-04 03:21:28","endLine":306,"groupId":"10315","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"appendRowUntilExceedingCapacity","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/b0/2346adecf826c5d64677c1fc83e17a93ebca90.src","preCode":"  public void appendRowUntilExceedingCapacity() throws Exception {\n    try (RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n        valueSchema, taskMemoryManager, 10)) {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      for (int i = 0; i < 10; i++) {\n        appendRow(batch, key, value);\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), 10);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < 10; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    }\n  }\n","realPath":"sql/catalyst/src/test/java/org/apache/spark/sql/catalyst/expressions/RowBasedKeyValueBatchSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":284,"status":"M"},{"authorDate":"2019-11-04 03:21:28","commitOrder":6,"curCode":"  public void appendRowUntilExceedingPageSize() throws Exception {\n    \r\n    int pageSizeToUse = (int) memoryManager.pageSizeBytes();\n    try (RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n        valueSchema, taskMemoryManager, pageSizeToUse)) {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      int recordLength = 8 + key.getSizeInBytes() + value.getSizeInBytes() + 8;\n      int totalSize = 4;\n      int numRows = 0;\n      while (totalSize + recordLength < pageSizeToUse) {\n        appendRow(batch, key, value);\n        totalSize += recordLength;\n        numRows++;\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(numRows, batch.numRows());\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < numRows; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    }\n  }\n","date":"2019-11-04 03:21:28","endLine":338,"groupId":"10315","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"appendRowUntilExceedingPageSize","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/b0/2346adecf826c5d64677c1fc83e17a93ebca90.src","preCode":"  public void appendRowUntilExceedingPageSize() throws Exception {\n    \r\n    int pageSizeToUse = (int) memoryManager.pageSizeBytes();\n    try (RowBasedKeyValueBatch batch = RowBasedKeyValueBatch.allocate(keySchema,\n        valueSchema, taskMemoryManager, pageSizeToUse)) {\n      UnsafeRow key = makeKeyRow(1, \"A\");\n      UnsafeRow value = makeValueRow(1, 1);\n      int recordLength = 8 + key.getSizeInBytes() + value.getSizeInBytes() + 8;\n      int totalSize = 4;\n      int numRows = 0;\n      while (totalSize + recordLength < pageSizeToUse) {\n        appendRow(batch, key, value);\n        totalSize += recordLength;\n        numRows++;\n      }\n      UnsafeRow ret = appendRow(batch, key, value);\n      Assert.assertEquals(batch.numRows(), numRows);\n      Assert.assertNull(ret);\n      org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> iterator\n              = batch.rowIterator();\n      for (int i = 0; i < numRows; i++) {\n        Assert.assertTrue(iterator.next());\n        UnsafeRow key1 = iterator.getKey();\n        UnsafeRow value1 = iterator.getValue();\n        Assert.assertTrue(checkKey(key1, 1, \"A\"));\n        Assert.assertTrue(checkValue(value1, 1, 1));\n      }\n      Assert.assertFalse(iterator.next());\n    }\n  }\n","realPath":"sql/catalyst/src/test/java/org/apache/spark/sql/catalyst/expressions/RowBasedKeyValueBatchSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":309,"status":"M"}],"commitId":"80a89873b20aa07e2522bed5da0fc50e616246d9","commitMessage":"@@@[SPARK-29733][TESTS] Fix wrong order of parameters passed to `assertEquals`\n\n\n What changes were proposed in this pull request?\nThe `assertEquals` method of JUnit Assert requires the first parameter to be the expected value. In this PR.  I propose to change the order of parameters when the expected value is passed as the second parameter.\n\n\n Why are the changes needed?\nWrong order of assert parameters confuses when the assert fails and the parameters have special string representation. For example:\n```java\nassertEquals(input1.add(input2).  new CalendarInterval(5.  5.  367200000000L));\n```\n```\njava.lang.AssertionError:\nExpected :interval 5 months 5 days 101 hours\nActual   :interval 5 months 5 days 102 hours\n```\n\n\n Does this PR introduce any user-facing change?\nNo\n\n\n How was this patch tested?\nBy existing tests.\n\nCloses #26377 from MaxGekk/fix-order-in-assert-equals.\n\nAuthored-by: Maxim Gekk <max.gekk@gmail.com>\nSigned-off-by: Dongjoon Hyun <dhyun@apple.com>\n","date":"2019-11-04 03:21:28","modifiedFileCount":"21","status":"M","submitter":"Maxim Gekk"}]
