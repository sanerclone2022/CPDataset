[{"authorTime":"2019-08-05 14:54:45","codes":[{"authorDate":"2019-08-05 14:54:45","commitOrder":1,"curCode":"    private int[] rddAndSplitIds(String[] blockIds) {\n      final int[] rddAndSplitIds = new int[2 * blockIds.length];\n      for (int i = 0; i < blockIds.length; i++) {\n        String[] blockIdParts = blockIds[i].split(\"_\");\n        if (blockIdParts.length != 3 || !blockIdParts[0].equals(\"rdd\")) {\n          throw new IllegalArgumentException(\"Unexpected RDD block id format: \" + blockIds[i]);\n        }\n        rddAndSplitIds[2 * i] = Integer.parseInt(blockIdParts[1]);\n        rddAndSplitIds[2 * i + 1] = Integer.parseInt(blockIdParts[2]);\n      }\n      return rddAndSplitIds;\n    }\n","date":"2019-08-05 14:54:45","endLine":283,"groupId":"3410","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"rddAndSplitIds","params":"(String[]blockIds)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/03/7e5cf7e5222bfc3a2608064256c56fc3dcb4cc.src","preCode":"    private int[] rddAndSplitIds(String[] blockIds) {\n      final int[] rddAndSplitIds = new int[2 * blockIds.length];\n      for (int i = 0; i < blockIds.length; i++) {\n        String[] blockIdParts = blockIds[i].split(\"_\");\n        if (blockIdParts.length != 3 || !blockIdParts[0].equals(\"rdd\")) {\n          throw new IllegalArgumentException(\"Unexpected RDD block id format: \" + blockIds[i]);\n        }\n        rddAndSplitIds[2 * i] = Integer.parseInt(blockIdParts[1]);\n        rddAndSplitIds[2 * i + 1] = Integer.parseInt(blockIdParts[2]);\n      }\n      return rddAndSplitIds;\n    }\n","realPath":"common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalBlockHandler.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":272,"status":"B"},{"authorDate":"2019-08-05 14:54:45","commitOrder":1,"curCode":"    private int[] shuffleMapIdAndReduceIds(String[] blockIds, int shuffleId) {\n      final int[] mapIdAndReduceIds = new int[2 * blockIds.length];\n      for (int i = 0; i < blockIds.length; i++) {\n        String[] blockIdParts = blockIds[i].split(\"_\");\n        if (blockIdParts.length != 4 || !blockIdParts[0].equals(\"shuffle\")) {\n          throw new IllegalArgumentException(\"Unexpected shuffle block id format: \" + blockIds[i]);\n        }\n        if (Integer.parseInt(blockIdParts[1]) != shuffleId) {\n          throw new IllegalArgumentException(\"Expected shuffleId=\" + shuffleId +\n            \", got:\" + blockIds[i]);\n        }\n        mapIdAndReduceIds[2 * i] = Integer.parseInt(blockIdParts[2]);\n        mapIdAndReduceIds[2 * i + 1] = Integer.parseInt(blockIdParts[3]);\n      }\n      return mapIdAndReduceIds;\n    }\n","date":"2019-08-05 14:54:45","endLine":300,"groupId":"1262","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"shuffleMapIdAndReduceIds","params":"(String[]blockIds@intshuffleId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/03/7e5cf7e5222bfc3a2608064256c56fc3dcb4cc.src","preCode":"    private int[] shuffleMapIdAndReduceIds(String[] blockIds, int shuffleId) {\n      final int[] mapIdAndReduceIds = new int[2 * blockIds.length];\n      for (int i = 0; i < blockIds.length; i++) {\n        String[] blockIdParts = blockIds[i].split(\"_\");\n        if (blockIdParts.length != 4 || !blockIdParts[0].equals(\"shuffle\")) {\n          throw new IllegalArgumentException(\"Unexpected shuffle block id format: \" + blockIds[i]);\n        }\n        if (Integer.parseInt(blockIdParts[1]) != shuffleId) {\n          throw new IllegalArgumentException(\"Expected shuffleId=\" + shuffleId +\n            \", got:\" + blockIds[i]);\n        }\n        mapIdAndReduceIds[2 * i] = Integer.parseInt(blockIdParts[2]);\n        mapIdAndReduceIds[2 * i + 1] = Integer.parseInt(blockIdParts[3]);\n      }\n      return mapIdAndReduceIds;\n    }\n","realPath":"common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalBlockHandler.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":285,"status":"B"}],"commitId":"db39f45bafc44271db8d8ec7cdb09734e5dedb37","commitMessage":"@@@[SPARK-28593][CORE] Rename ShuffleClient to BlockStoreClient which more close to its usage\n\n## What changes were proposed in this pull request?\n\nAfter SPARK-27677.  the shuffle client not only handles the shuffle block but also responsible for local persist RDD blocks. For better code scalability and precise semantics(as the [discussion](https://github.com/apache/spark/pull/24892#discussion_r300173331)).  here we did several changes:\n\n- Rename ShuffleClient to BlockStoreClient.\n- Correspondingly rename the ExternalShuffleClient to ExternalBlockStoreClient.  also change the server-side class from ExternalShuffleBlockHandler to ExternalBlockHandler.\n- Move MesosExternalBlockStoreClient to Mesos package.\n\nNote.  we still keep the name of BlockTransferService.  because the `Service` contains both client and server.  also the name of BlockTransferService is not referencing shuffle client only.\n\n## How was this patch tested?\n\nExisting UT.\n\nCloses #25327 from xuanyuanking/SPARK-28593.\n\nLead-authored-by: Yuanjian Li <xyliyuanjian@gmail.com>\nCo-authored-by: Yuanjian Li <yuanjian.li@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n","date":"2019-08-05 14:54:45","modifiedFileCount":"6","status":"B","submitter":"Yuanjian Li"},{"authorTime":"2021-06-21 06:22:37","codes":[{"authorDate":"2019-08-05 14:54:45","commitOrder":2,"curCode":"    private int[] rddAndSplitIds(String[] blockIds) {\n      final int[] rddAndSplitIds = new int[2 * blockIds.length];\n      for (int i = 0; i < blockIds.length; i++) {\n        String[] blockIdParts = blockIds[i].split(\"_\");\n        if (blockIdParts.length != 3 || !blockIdParts[0].equals(\"rdd\")) {\n          throw new IllegalArgumentException(\"Unexpected RDD block id format: \" + blockIds[i]);\n        }\n        rddAndSplitIds[2 * i] = Integer.parseInt(blockIdParts[1]);\n        rddAndSplitIds[2 * i + 1] = Integer.parseInt(blockIdParts[2]);\n      }\n      return rddAndSplitIds;\n    }\n","date":"2019-08-05 14:54:45","endLine":283,"groupId":"3410","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"rddAndSplitIds","params":"(String[]blockIds)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/03/7e5cf7e5222bfc3a2608064256c56fc3dcb4cc.src","preCode":"    private int[] rddAndSplitIds(String[] blockIds) {\n      final int[] rddAndSplitIds = new int[2 * blockIds.length];\n      for (int i = 0; i < blockIds.length; i++) {\n        String[] blockIdParts = blockIds[i].split(\"_\");\n        if (blockIdParts.length != 3 || !blockIdParts[0].equals(\"rdd\")) {\n          throw new IllegalArgumentException(\"Unexpected RDD block id format: \" + blockIds[i]);\n        }\n        rddAndSplitIds[2 * i] = Integer.parseInt(blockIdParts[1]);\n        rddAndSplitIds[2 * i + 1] = Integer.parseInt(blockIdParts[2]);\n      }\n      return rddAndSplitIds;\n    }\n","realPath":"common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalBlockHandler.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":272,"status":"N"},{"authorDate":"2021-06-21 06:22:37","commitOrder":2,"curCode":"    private int[] shuffleMapIdAndReduceIds(String[] blockIds, int shuffleId) {\n      \r\n      \r\n      final int[] primaryIdAndSecondaryIds = new int[2 * blockIds.length];\n      for (int i = 0; i < blockIds.length; i++) {\n        String[] blockIdParts = blockIds[i].split(\"_\");\n        if (blockIdParts.length != 4\n          || (!requestForMergedBlockChunks && !blockIdParts[0].equals(SHUFFLE_BLOCK_ID))\n          || (requestForMergedBlockChunks && !blockIdParts[0].equals(SHUFFLE_CHUNK_ID))) {\n          throw new IllegalArgumentException(\"Unexpected shuffle block id format: \" + blockIds[i]);\n        }\n        if (Integer.parseInt(blockIdParts[1]) != shuffleId) {\n          throw new IllegalArgumentException(\"Expected shuffleId=\" + shuffleId +\n            \", got:\" + blockIds[i]);\n        }\n        \r\n        primaryIdAndSecondaryIds[2 * i] = Integer.parseInt(blockIdParts[2]);\n        \r\n        primaryIdAndSecondaryIds[2 * i + 1] = Integer.parseInt(blockIdParts[3]);\n      }\n      return primaryIdAndSecondaryIds;\n    }\n","date":"2021-06-21 06:22:37","endLine":403,"groupId":"803","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"shuffleMapIdAndReduceIds","params":"(String[]blockIds@intshuffleId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/c5/f5834c0b85f70abc956b37d3bdc15d84e57990.src","preCode":"    private int[] shuffleMapIdAndReduceIds(String[] blockIds, int shuffleId) {\n      final int[] mapIdAndReduceIds = new int[2 * blockIds.length];\n      for (int i = 0; i < blockIds.length; i++) {\n        String[] blockIdParts = blockIds[i].split(\"_\");\n        if (blockIdParts.length != 4 || !blockIdParts[0].equals(\"shuffle\")) {\n          throw new IllegalArgumentException(\"Unexpected shuffle block id format: \" + blockIds[i]);\n        }\n        if (Integer.parseInt(blockIdParts[1]) != shuffleId) {\n          throw new IllegalArgumentException(\"Expected shuffleId=\" + shuffleId +\n            \", got:\" + blockIds[i]);\n        }\n        mapIdAndReduceIds[2 * i] = Integer.parseInt(blockIdParts[2]);\n        mapIdAndReduceIds[2 * i + 1] = Integer.parseInt(blockIdParts[3]);\n      }\n      return mapIdAndReduceIds;\n    }\n","realPath":"common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalBlockHandler.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":382,"status":"M"}],"commitId":"8ce1e344e58dbfbddecd9e9fd9f0a5a6f15dbea9","commitMessage":"@@@[SPARK-35671][SHUFFLE][CORE] Add support in the ESS to serve merged shuffle block meta and data to executors\n\n\n What changes were proposed in this pull request?\nThis adds support in the ESS to serve merged shuffle block meta and data requests to executors.\nThis change is needed for fetching remote merged shuffle data from the remote shuffle services. This is part of push-based shuffle SPIP [SPARK-30602](https://issues.apache.org/jira/browse/SPARK-30602).\n\nThis change introduces new messages between clients and the external shuffle service:\n\n1. `MergedBlockMetaRequest`: The client sends this to external shuffle to get the meta information for a merged block. The response to this is one of these :\n  - `MergedBlockMetaSuccess` : contains request id.  number of chunks.  and a `ManagedBuffer` which is a `FileSegmentBuffer` backed by the merged block meta file.\n  - `RpcFailure`: this is sent back to client in case of failure. This is an existing message.\n\n2. `FetchShuffleBlockChunks`: This is similar to `FetchShuffleBlocks` message but it is to fetch merged shuffle chunks instead of blocks.\n\n\n Why are the changes needed?\nThese changes are needed for push-based shuffle. Refer to the SPIP in [SPARK-30602](https://issues.apache.org/jira/browse/SPARK-30602).\n\n\n Does this PR introduce _any_ user-facing change?\nNo.\n\n\n How was this patch tested?\nAdded unit tests.\nThe reference PR with the consolidated changes covering the complete implementation is also provided in [SPARK-30602](https://issues.apache.org/jira/browse/SPARK-30602).\nWe have already verified the functionality and the improved performance as documented in the SPIP doc.\n\nLead-authored-by: Chandni Singh chsinghlinkedin.com\nCo-authored-by: Min Shen mshenlinkedin.com\n\nCloses #32811 from otterc/SPARK-35671.\n\nLead-authored-by: Chandni Singh <singh.chandni@gmail.com>\nCo-authored-by: Min Shen <mshen@linkedin.com>\nCo-authored-by: Chandni Singh <chsingh@linkedin.com>\nSigned-off-by: Mridul Muralidharan <mridul<at>gmail.com>\n","date":"2021-06-21 06:22:37","modifiedFileCount":"20","status":"M","submitter":"Chandni Singh"},{"authorTime":"2021-08-02 12:16:33","codes":[{"authorDate":"2019-08-05 14:54:45","commitOrder":3,"curCode":"    private int[] rddAndSplitIds(String[] blockIds) {\n      final int[] rddAndSplitIds = new int[2 * blockIds.length];\n      for (int i = 0; i < blockIds.length; i++) {\n        String[] blockIdParts = blockIds[i].split(\"_\");\n        if (blockIdParts.length != 3 || !blockIdParts[0].equals(\"rdd\")) {\n          throw new IllegalArgumentException(\"Unexpected RDD block id format: \" + blockIds[i]);\n        }\n        rddAndSplitIds[2 * i] = Integer.parseInt(blockIdParts[1]);\n        rddAndSplitIds[2 * i + 1] = Integer.parseInt(blockIdParts[2]);\n      }\n      return rddAndSplitIds;\n    }\n","date":"2019-08-05 14:54:45","endLine":283,"groupId":"10175","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"rddAndSplitIds","params":"(String[]blockIds)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/03/7e5cf7e5222bfc3a2608064256c56fc3dcb4cc.src","preCode":"    private int[] rddAndSplitIds(String[] blockIds) {\n      final int[] rddAndSplitIds = new int[2 * blockIds.length];\n      for (int i = 0; i < blockIds.length; i++) {\n        String[] blockIdParts = blockIds[i].split(\"_\");\n        if (blockIdParts.length != 3 || !blockIdParts[0].equals(\"rdd\")) {\n          throw new IllegalArgumentException(\"Unexpected RDD block id format: \" + blockIds[i]);\n        }\n        rddAndSplitIds[2 * i] = Integer.parseInt(blockIdParts[1]);\n        rddAndSplitIds[2 * i + 1] = Integer.parseInt(blockIdParts[2]);\n      }\n      return rddAndSplitIds;\n    }\n","realPath":"common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalBlockHandler.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":272,"status":"N"},{"authorDate":"2021-08-02 12:16:33","commitOrder":3,"curCode":"    private int[] shuffleMapIdAndReduceIds(String[] blockIds, int shuffleId) {\n      final int[] mapIdAndReduceIds = new int[2 * blockIds.length];\n      for (int i = 0; i < blockIds.length; i++) {\n        String[] blockIdParts = blockIds[i].split(\"_\");\n        if (blockIdParts.length != 4 || !blockIdParts[0].equals(SHUFFLE_BLOCK_ID)) {\n          throw new IllegalArgumentException(\"Unexpected shuffle block id format: \" + blockIds[i]);\n        }\n        if (Integer.parseInt(blockIdParts[1]) != shuffleId) {\n          throw new IllegalArgumentException(\"Expected shuffleId=\" + shuffleId +\n            \", got:\" + blockIds[i]);\n        }\n        \r\n        mapIdAndReduceIds[2 * i] = Integer.parseInt(blockIdParts[2]);\n        \r\n        mapIdAndReduceIds[2 * i + 1] = Integer.parseInt(blockIdParts[3]);\n      }\n      return mapIdAndReduceIds;\n    }\n","date":"2021-08-02 12:16:33","endLine":435,"groupId":"10175","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"shuffleMapIdAndReduceIds","params":"(String[]blockIds@intshuffleId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/cf/abcd5ba4a287988a09d03e602346c681c04f4b.src","preCode":"    private int[] shuffleMapIdAndReduceIds(String[] blockIds, int shuffleId) {\n      \r\n      \r\n      final int[] primaryIdAndSecondaryIds = new int[2 * blockIds.length];\n      for (int i = 0; i < blockIds.length; i++) {\n        String[] blockIdParts = blockIds[i].split(\"_\");\n        if (blockIdParts.length != 4\n          || (!requestForMergedBlockChunks && !blockIdParts[0].equals(SHUFFLE_BLOCK_ID))\n          || (requestForMergedBlockChunks && !blockIdParts[0].equals(SHUFFLE_CHUNK_ID))) {\n          throw new IllegalArgumentException(\"Unexpected shuffle block id format: \" + blockIds[i]);\n        }\n        if (Integer.parseInt(blockIdParts[1]) != shuffleId) {\n          throw new IllegalArgumentException(\"Expected shuffleId=\" + shuffleId +\n            \", got:\" + blockIds[i]);\n        }\n        \r\n        primaryIdAndSecondaryIds[2 * i] = Integer.parseInt(blockIdParts[2]);\n        \r\n        primaryIdAndSecondaryIds[2 * i + 1] = Integer.parseInt(blockIdParts[3]);\n      }\n      return primaryIdAndSecondaryIds;\n    }\n","realPath":"common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalBlockHandler.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":418,"status":"M"}],"commitId":"c039d998128dd0dab27f43e7de083a71b9d1cfcf","commitMessage":"@@@[SPARK-32923][CORE][SHUFFLE] Handle indeterminate stage retries for push-based shuffle\n\n\n What changes were proposed in this pull request?\n[[SPARK-23243](https://issues.apache.org/jira/browse/SPARK-23243)] and [[SPARK-25341](https://issues.apache.org/jira/browse/SPARK-25341)] addressed cases of stage retries for indeterminate stage involving operations like repartition. This PR addresses the same issues in the context of push-based shuffle. Currently there is no way to distinguish the current execution of a stage for a shuffle ID. Therefore the changes explained below are necessary.\n\nCore changes are summarized as follows:\n\n1. Introduce a new variable `shuffleMergeId` in `ShuffleDependency` which is monotonically increasing value tracking the temporal ordering of execution of <stage-id.  stage-attempt-id> for a shuffle ID.\n2. Correspondingly make changes in the push-based shuffle protocol layer in `MergedShuffleFileManager`.  `BlockStoreClient` passing the `shuffleMergeId` in order to keep track of the shuffle output in separate files on the shuffle service side.\n3. `DAGScheduler` increments the `shuffleMergeId` tracked in `ShuffleDependency` in the cases of a indeterministic stage execution\n4. Deterministic stage will have `shuffleMergeId` set to 0 as no special handling is needed in this case and indeterminate stage will have `shuffleMergeId` starting from 1.\n\n\n Why are the changes needed?\n\nNew protocol changes are needed due to the reasons explained above.\n\n\n Does this PR introduce _any_ user-facing change?\n\nNo\n\n\n How was this patch tested?\nAdded new unit tests in `RemoteBlockPushResolverSuite.  DAGSchedulerSuite.  BlockIdSuite.  ErrorHandlerSuite`\n\nCloses #33034 from venkata91/SPARK-32923.\n\nAuthored-by: Venkata krishnan Sowrirajan <vsowrirajan@linkedin.com>\nSigned-off-by: Mridul Muralidharan <mridul<at>gmail.com>\n","date":"2021-08-02 12:16:33","modifiedFileCount":"22","status":"M","submitter":"Venkata krishnan Sowrirajan"}]
