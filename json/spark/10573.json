[{"authorTime":"2015-08-11 23:41:06","codes":[{"authorDate":"2015-10-23 00:46:30","commitOrder":7,"curCode":"  public void insertRecord(\n      Object recordBaseObject,\n      long recordBaseOffset,\n      int lengthInBytes,\n      int partitionId) throws IOException {\n\n    if (numRecordsInsertedSinceLastSpill > numElementsForSpillThreshold) {\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    \r\n    final int totalSpaceRequired = lengthInBytes + 4;\n\n    \r\n\n    final MemoryBlock dataPage;\n    long dataPagePosition;\n    boolean useOverflowPage = totalSpaceRequired > pageSizeBytes;\n    if (useOverflowPage) {\n      long overflowPageSize = ByteArrayMethods.roundNumberOfBytesToNearestWord(totalSpaceRequired);\n      \r\n      \r\n      final long memoryGranted = shuffleMemoryManager.tryToAcquire(overflowPageSize);\n      if (memoryGranted != overflowPageSize) {\n        shuffleMemoryManager.release(memoryGranted);\n        spill();\n        final long memoryGrantedAfterSpill = shuffleMemoryManager.tryToAcquire(overflowPageSize);\n        if (memoryGrantedAfterSpill != overflowPageSize) {\n          shuffleMemoryManager.release(memoryGrantedAfterSpill);\n          throw new IOException(\"Unable to acquire \" + overflowPageSize + \" bytes of memory\");\n        }\n      }\n      MemoryBlock overflowPage = taskMemoryManager.allocatePage(overflowPageSize);\n      allocatedPages.add(overflowPage);\n      dataPage = overflowPage;\n      dataPagePosition = overflowPage.getBaseOffset();\n    } else {\n      \r\n      \r\n      acquireNewPageIfNecessary(totalSpaceRequired);\n      dataPage = currentPage;\n      dataPagePosition = currentPagePosition;\n      \r\n      freeSpaceInCurrentPage -= totalSpaceRequired;\n      currentPagePosition += totalSpaceRequired;\n    }\n    final Object dataPageBaseObject = dataPage.getBaseObject();\n\n    final long recordAddress =\n      taskMemoryManager.encodePageNumberAndOffset(dataPage, dataPagePosition);\n    Platform.putInt(dataPageBaseObject, dataPagePosition, lengthInBytes);\n    dataPagePosition += 4;\n    Platform.copyMemory(\n      recordBaseObject, recordBaseOffset, dataPageBaseObject, dataPagePosition, lengthInBytes);\n    assert(inMemSorter != null);\n    inMemSorter.insertRecord(recordAddress, partitionId);\n    numRecordsInsertedSinceLastSpill += 1;\n  }\n","date":"2015-10-23 00:46:30","endLine":468,"groupId":"1772","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"insertRecord","params":"(ObjectrecordBaseObject@longrecordBaseOffset@intlengthInBytes@intpartitionId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/85/fdaa8115fa38f2c47b4ed0924727569ccfdd93.src","preCode":"  public void insertRecord(\n      Object recordBaseObject,\n      long recordBaseOffset,\n      int lengthInBytes,\n      int partitionId) throws IOException {\n\n    if (numRecordsInsertedSinceLastSpill > numElementsForSpillThreshold) {\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    \r\n    final int totalSpaceRequired = lengthInBytes + 4;\n\n    \r\n\n    final MemoryBlock dataPage;\n    long dataPagePosition;\n    boolean useOverflowPage = totalSpaceRequired > pageSizeBytes;\n    if (useOverflowPage) {\n      long overflowPageSize = ByteArrayMethods.roundNumberOfBytesToNearestWord(totalSpaceRequired);\n      \r\n      \r\n      final long memoryGranted = shuffleMemoryManager.tryToAcquire(overflowPageSize);\n      if (memoryGranted != overflowPageSize) {\n        shuffleMemoryManager.release(memoryGranted);\n        spill();\n        final long memoryGrantedAfterSpill = shuffleMemoryManager.tryToAcquire(overflowPageSize);\n        if (memoryGrantedAfterSpill != overflowPageSize) {\n          shuffleMemoryManager.release(memoryGrantedAfterSpill);\n          throw new IOException(\"Unable to acquire \" + overflowPageSize + \" bytes of memory\");\n        }\n      }\n      MemoryBlock overflowPage = taskMemoryManager.allocatePage(overflowPageSize);\n      allocatedPages.add(overflowPage);\n      dataPage = overflowPage;\n      dataPagePosition = overflowPage.getBaseOffset();\n    } else {\n      \r\n      \r\n      acquireNewPageIfNecessary(totalSpaceRequired);\n      dataPage = currentPage;\n      dataPagePosition = currentPagePosition;\n      \r\n      freeSpaceInCurrentPage -= totalSpaceRequired;\n      currentPagePosition += totalSpaceRequired;\n    }\n    final Object dataPageBaseObject = dataPage.getBaseObject();\n\n    final long recordAddress =\n      taskMemoryManager.encodePageNumberAndOffset(dataPage, dataPagePosition);\n    Platform.putInt(dataPageBaseObject, dataPagePosition, lengthInBytes);\n    dataPagePosition += 4;\n    Platform.copyMemory(\n      recordBaseObject, recordBaseOffset, dataPageBaseObject, dataPagePosition, lengthInBytes);\n    assert(inMemSorter != null);\n    inMemSorter.insertRecord(recordAddress, partitionId);\n    numRecordsInsertedSinceLastSpill += 1;\n  }\n","realPath":"core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":410,"status":"B"},{"authorDate":"2015-08-11 23:41:06","commitOrder":7,"curCode":"  public void insertRecord(\n      Object recordBaseObject,\n      long recordBaseOffset,\n      int lengthInBytes,\n      long prefix) throws IOException {\n\n    growPointerArrayIfNecessary();\n    \r\n    final int totalSpaceRequired = lengthInBytes + 4;\n\n    \r\n\n    final MemoryBlock dataPage;\n    long dataPagePosition;\n    boolean useOverflowPage = totalSpaceRequired > pageSizeBytes;\n    if (useOverflowPage) {\n      long overflowPageSize = ByteArrayMethods.roundNumberOfBytesToNearestWord(totalSpaceRequired);\n      \r\n      \r\n      final long memoryGranted = shuffleMemoryManager.tryToAcquire(overflowPageSize);\n      if (memoryGranted != overflowPageSize) {\n        shuffleMemoryManager.release(memoryGranted);\n        spill();\n        final long memoryGrantedAfterSpill = shuffleMemoryManager.tryToAcquire(overflowPageSize);\n        if (memoryGrantedAfterSpill != overflowPageSize) {\n          shuffleMemoryManager.release(memoryGrantedAfterSpill);\n          throw new IOException(\"Unable to acquire \" + overflowPageSize + \" bytes of memory\");\n        }\n      }\n      MemoryBlock overflowPage = taskMemoryManager.allocatePage(overflowPageSize);\n      allocatedPages.add(overflowPage);\n      dataPage = overflowPage;\n      dataPagePosition = overflowPage.getBaseOffset();\n    } else {\n      \r\n      \r\n      acquireNewPageIfNecessary(totalSpaceRequired);\n      dataPage = currentPage;\n      dataPagePosition = currentPagePosition;\n      \r\n      freeSpaceInCurrentPage -= totalSpaceRequired;\n      currentPagePosition += totalSpaceRequired;\n    }\n    final Object dataPageBaseObject = dataPage.getBaseObject();\n\n    \r\n\n    final long recordAddress =\n      taskMemoryManager.encodePageNumberAndOffset(dataPage, dataPagePosition);\n    Platform.putInt(dataPageBaseObject, dataPagePosition, lengthInBytes);\n    dataPagePosition += 4;\n    Platform.copyMemory(\n      recordBaseObject, recordBaseOffset, dataPageBaseObject, dataPagePosition, lengthInBytes);\n    assert(inMemSorter != null);\n    inMemSorter.insertRecord(recordAddress, prefix);\n  }\n","date":"2015-08-11 23:41:06","endLine":436,"groupId":"1772","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"insertRecord","params":"(ObjectrecordBaseObject@longrecordBaseOffset@intlengthInBytes@longprefix)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/96/01aafe554649629336eaf856c816d256e61afe.src","preCode":"  public void insertRecord(\n      Object recordBaseObject,\n      long recordBaseOffset,\n      int lengthInBytes,\n      long prefix) throws IOException {\n\n    growPointerArrayIfNecessary();\n    \r\n    final int totalSpaceRequired = lengthInBytes + 4;\n\n    \r\n\n    final MemoryBlock dataPage;\n    long dataPagePosition;\n    boolean useOverflowPage = totalSpaceRequired > pageSizeBytes;\n    if (useOverflowPage) {\n      long overflowPageSize = ByteArrayMethods.roundNumberOfBytesToNearestWord(totalSpaceRequired);\n      \r\n      \r\n      final long memoryGranted = shuffleMemoryManager.tryToAcquire(overflowPageSize);\n      if (memoryGranted != overflowPageSize) {\n        shuffleMemoryManager.release(memoryGranted);\n        spill();\n        final long memoryGrantedAfterSpill = shuffleMemoryManager.tryToAcquire(overflowPageSize);\n        if (memoryGrantedAfterSpill != overflowPageSize) {\n          shuffleMemoryManager.release(memoryGrantedAfterSpill);\n          throw new IOException(\"Unable to acquire \" + overflowPageSize + \" bytes of memory\");\n        }\n      }\n      MemoryBlock overflowPage = taskMemoryManager.allocatePage(overflowPageSize);\n      allocatedPages.add(overflowPage);\n      dataPage = overflowPage;\n      dataPagePosition = overflowPage.getBaseOffset();\n    } else {\n      \r\n      \r\n      acquireNewPageIfNecessary(totalSpaceRequired);\n      dataPage = currentPage;\n      dataPagePosition = currentPagePosition;\n      \r\n      freeSpaceInCurrentPage -= totalSpaceRequired;\n      currentPagePosition += totalSpaceRequired;\n    }\n    final Object dataPageBaseObject = dataPage.getBaseObject();\n\n    \r\n\n    final long recordAddress =\n      taskMemoryManager.encodePageNumberAndOffset(dataPage, dataPagePosition);\n    Platform.putInt(dataPageBaseObject, dataPagePosition, lengthInBytes);\n    dataPagePosition += 4;\n    Platform.copyMemory(\n      recordBaseObject, recordBaseOffset, dataPageBaseObject, dataPagePosition, lengthInBytes);\n    assert(inMemSorter != null);\n    inMemSorter.insertRecord(recordAddress, prefix);\n  }\n","realPath":"core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":381,"status":"NB"}],"commitId":"f6d06adf05afa9c5386dc2396c94e7a98730289f","commitMessage":"@@@[SPARK-10708] Consolidate sort shuffle implementations\n\nThere's a lot of duplication between SortShuffleManager and UnsafeShuffleManager. Given that these now provide the same set of functionality.  now that UnsafeShuffleManager supports large records.  I think that we should replace SortShuffleManager's serialized shuffle implementation with UnsafeShuffleManager's and should merge the two managers together.\n\nAuthor: Josh Rosen <joshrosen@databricks.com>\n\nCloses #8829 from JoshRosen/consolidate-sort-shuffle-implementations.\n","date":"2015-10-23 00:46:30","modifiedFileCount":"1","status":"M","submitter":"Josh Rosen"},{"authorTime":"2015-10-26 12:19:52","codes":[{"authorDate":"2015-10-26 12:19:52","commitOrder":8,"curCode":"  public void insertRecord(\n      Object recordBaseObject,\n      long recordBaseOffset,\n      int lengthInBytes,\n      int partitionId) throws IOException {\n\n    if (numRecordsInsertedSinceLastSpill > numElementsForSpillThreshold) {\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    \r\n    final int totalSpaceRequired = lengthInBytes + 4;\n\n    \r\n\n    final MemoryBlock dataPage;\n    long dataPagePosition;\n    boolean useOverflowPage = totalSpaceRequired > pageSizeBytes;\n    if (useOverflowPage) {\n      long overflowPageSize = ByteArrayMethods.roundNumberOfBytesToNearestWord(totalSpaceRequired);\n      \r\n      \r\n      MemoryBlock overflowPage = taskMemoryManager.allocatePage(overflowPageSize);\n      if (overflowPage == null) {\n        spill();\n        overflowPage = taskMemoryManager.allocatePage(overflowPageSize);\n        if (overflowPage == null) {\n          throw new IOException(\"Unable to acquire \" + overflowPageSize + \" bytes of memory\");\n        }\n      }\n      allocatedPages.add(overflowPage);\n      dataPage = overflowPage;\n      dataPagePosition = overflowPage.getBaseOffset();\n    } else {\n      \r\n      \r\n      acquireNewPageIfNecessary(totalSpaceRequired);\n      dataPage = currentPage;\n      dataPagePosition = currentPagePosition;\n      \r\n      freeSpaceInCurrentPage -= totalSpaceRequired;\n      currentPagePosition += totalSpaceRequired;\n    }\n    final Object dataPageBaseObject = dataPage.getBaseObject();\n\n    final long recordAddress =\n      taskMemoryManager.encodePageNumberAndOffset(dataPage, dataPagePosition);\n    Platform.putInt(dataPageBaseObject, dataPagePosition, lengthInBytes);\n    dataPagePosition += 4;\n    Platform.copyMemory(\n      recordBaseObject, recordBaseOffset, dataPageBaseObject, dataPagePosition, lengthInBytes);\n    assert(inMemSorter != null);\n    inMemSorter.insertRecord(recordAddress, partitionId);\n    numRecordsInsertedSinceLastSpill += 1;\n  }\n","date":"2015-10-26 12:19:52","endLine":463,"groupId":"1772","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"insertRecord","params":"(ObjectrecordBaseObject@longrecordBaseOffset@intlengthInBytes@intpartitionId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/f4/3236f41ae7bb1eb63a0e9e3526ebf09978500b.src","preCode":"  public void insertRecord(\n      Object recordBaseObject,\n      long recordBaseOffset,\n      int lengthInBytes,\n      int partitionId) throws IOException {\n\n    if (numRecordsInsertedSinceLastSpill > numElementsForSpillThreshold) {\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    \r\n    final int totalSpaceRequired = lengthInBytes + 4;\n\n    \r\n\n    final MemoryBlock dataPage;\n    long dataPagePosition;\n    boolean useOverflowPage = totalSpaceRequired > pageSizeBytes;\n    if (useOverflowPage) {\n      long overflowPageSize = ByteArrayMethods.roundNumberOfBytesToNearestWord(totalSpaceRequired);\n      \r\n      \r\n      final long memoryGranted = shuffleMemoryManager.tryToAcquire(overflowPageSize);\n      if (memoryGranted != overflowPageSize) {\n        shuffleMemoryManager.release(memoryGranted);\n        spill();\n        final long memoryGrantedAfterSpill = shuffleMemoryManager.tryToAcquire(overflowPageSize);\n        if (memoryGrantedAfterSpill != overflowPageSize) {\n          shuffleMemoryManager.release(memoryGrantedAfterSpill);\n          throw new IOException(\"Unable to acquire \" + overflowPageSize + \" bytes of memory\");\n        }\n      }\n      MemoryBlock overflowPage = taskMemoryManager.allocatePage(overflowPageSize);\n      allocatedPages.add(overflowPage);\n      dataPage = overflowPage;\n      dataPagePosition = overflowPage.getBaseOffset();\n    } else {\n      \r\n      \r\n      acquireNewPageIfNecessary(totalSpaceRequired);\n      dataPage = currentPage;\n      dataPagePosition = currentPagePosition;\n      \r\n      freeSpaceInCurrentPage -= totalSpaceRequired;\n      currentPagePosition += totalSpaceRequired;\n    }\n    final Object dataPageBaseObject = dataPage.getBaseObject();\n\n    final long recordAddress =\n      taskMemoryManager.encodePageNumberAndOffset(dataPage, dataPagePosition);\n    Platform.putInt(dataPageBaseObject, dataPagePosition, lengthInBytes);\n    dataPagePosition += 4;\n    Platform.copyMemory(\n      recordBaseObject, recordBaseOffset, dataPageBaseObject, dataPagePosition, lengthInBytes);\n    assert(inMemSorter != null);\n    inMemSorter.insertRecord(recordAddress, partitionId);\n    numRecordsInsertedSinceLastSpill += 1;\n  }\n","realPath":"core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":408,"status":"M"},{"authorDate":"2015-10-26 12:19:52","commitOrder":8,"curCode":"  public void insertRecord(\n      Object recordBaseObject,\n      long recordBaseOffset,\n      int lengthInBytes,\n      long prefix) throws IOException {\n\n    growPointerArrayIfNecessary();\n    \r\n    final int totalSpaceRequired = lengthInBytes + 4;\n\n    \r\n\n    final MemoryBlock dataPage;\n    long dataPagePosition;\n    boolean useOverflowPage = totalSpaceRequired > pageSizeBytes;\n    if (useOverflowPage) {\n      long overflowPageSize = ByteArrayMethods.roundNumberOfBytesToNearestWord(totalSpaceRequired);\n      \r\n      \r\n      MemoryBlock overflowPage = taskMemoryManager.allocatePage(overflowPageSize);\n      if (overflowPage == null) {\n        spill();\n        overflowPage = taskMemoryManager.allocatePage(overflowPageSize);\n        if (overflowPage == null) {\n          throw new IOException(\"Unable to acquire \" + overflowPageSize + \" bytes of memory\");\n        }\n      }\n      allocatedPages.add(overflowPage);\n      dataPage = overflowPage;\n      dataPagePosition = overflowPage.getBaseOffset();\n    } else {\n      \r\n      \r\n      acquireNewPageIfNecessary(totalSpaceRequired);\n      dataPage = currentPage;\n      dataPagePosition = currentPagePosition;\n      \r\n      freeSpaceInCurrentPage -= totalSpaceRequired;\n      currentPagePosition += totalSpaceRequired;\n    }\n    final Object dataPageBaseObject = dataPage.getBaseObject();\n\n    \r\n\n    final long recordAddress =\n      taskMemoryManager.encodePageNumberAndOffset(dataPage, dataPagePosition);\n    Platform.putInt(dataPageBaseObject, dataPagePosition, lengthInBytes);\n    dataPagePosition += 4;\n    Platform.copyMemory(\n      recordBaseObject, recordBaseOffset, dataPageBaseObject, dataPagePosition, lengthInBytes);\n    assert(inMemSorter != null);\n    inMemSorter.insertRecord(recordAddress, prefix);\n  }\n","date":"2015-10-26 12:19:52","endLine":404,"groupId":"1772","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"insertRecord","params":"(ObjectrecordBaseObject@longrecordBaseOffset@intlengthInBytes@longprefix)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/e3/17ea391c5566e8217103758b4ff11356a68f29.src","preCode":"  public void insertRecord(\n      Object recordBaseObject,\n      long recordBaseOffset,\n      int lengthInBytes,\n      long prefix) throws IOException {\n\n    growPointerArrayIfNecessary();\n    \r\n    final int totalSpaceRequired = lengthInBytes + 4;\n\n    \r\n\n    final MemoryBlock dataPage;\n    long dataPagePosition;\n    boolean useOverflowPage = totalSpaceRequired > pageSizeBytes;\n    if (useOverflowPage) {\n      long overflowPageSize = ByteArrayMethods.roundNumberOfBytesToNearestWord(totalSpaceRequired);\n      \r\n      \r\n      final long memoryGranted = shuffleMemoryManager.tryToAcquire(overflowPageSize);\n      if (memoryGranted != overflowPageSize) {\n        shuffleMemoryManager.release(memoryGranted);\n        spill();\n        final long memoryGrantedAfterSpill = shuffleMemoryManager.tryToAcquire(overflowPageSize);\n        if (memoryGrantedAfterSpill != overflowPageSize) {\n          shuffleMemoryManager.release(memoryGrantedAfterSpill);\n          throw new IOException(\"Unable to acquire \" + overflowPageSize + \" bytes of memory\");\n        }\n      }\n      MemoryBlock overflowPage = taskMemoryManager.allocatePage(overflowPageSize);\n      allocatedPages.add(overflowPage);\n      dataPage = overflowPage;\n      dataPagePosition = overflowPage.getBaseOffset();\n    } else {\n      \r\n      \r\n      acquireNewPageIfNecessary(totalSpaceRequired);\n      dataPage = currentPage;\n      dataPagePosition = currentPagePosition;\n      \r\n      freeSpaceInCurrentPage -= totalSpaceRequired;\n      currentPagePosition += totalSpaceRequired;\n    }\n    final Object dataPageBaseObject = dataPage.getBaseObject();\n\n    \r\n\n    final long recordAddress =\n      taskMemoryManager.encodePageNumberAndOffset(dataPage, dataPagePosition);\n    Platform.putInt(dataPageBaseObject, dataPagePosition, lengthInBytes);\n    dataPagePosition += 4;\n    Platform.copyMemory(\n      recordBaseObject, recordBaseOffset, dataPageBaseObject, dataPagePosition, lengthInBytes);\n    assert(inMemSorter != null);\n    inMemSorter.insertRecord(recordAddress, prefix);\n  }\n","realPath":"core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":352,"status":"M"}],"commitId":"85e654c5ec87e666a8845bfd77185c1ea57b268a","commitMessage":"@@@[SPARK-10984] Simplify *MemoryManager class structure\n\nThis patch refactors the MemoryManager class structure. After #9000.  Spark had the following classes:\n\n- MemoryManager\n- StaticMemoryManager\n- ExecutorMemoryManager\n- TaskMemoryManager\n- ShuffleMemoryManager\n\nThis is fairly confusing. To simplify things.  this patch consolidates several of these classes:\n\n- ShuffleMemoryManager and ExecutorMemoryManager were merged into MemoryManager.\n- TaskMemoryManager is moved into Spark Core.\n\n**Key changes and tasks**:\n\n- [x] Merge ExecutorMemoryManager into MemoryManager.\n  - [x] Move pooling logic into Allocator.\n- [x] Move TaskMemoryManager from `spark-unsafe` to `spark-core`.\n- [x] Refactor the existing Tungsten TaskMemoryManager interactions so Tungsten code use only this and not both this and ShuffleMemoryManager.\n- [x] Refactor non-Tungsten code to use the TaskMemoryManager instead of ShuffleMemoryManager.\n- [x] Merge ShuffleMemoryManager into MemoryManager.\n  - [x] Move code\n  - [x] ~~Simplify 1/n calculation.~~ **Will defer to followup.  since this needs more work.**\n- [x] Port ShuffleMemoryManagerSuite tests.\n- [x] Move classes from `unsafe` package to `memory` package.\n- [ ] Figure out how to handle the hacky use of the memory managers in HashedRelation's broadcast variable construction.\n- [x] Test porting and cleanup: several tests relied on mock functionality (such as `TestShuffleMemoryManager.markAsOutOfMemory`) which has been changed or broken during the memory manager consolidation\n  - [x] AbstractBytesToBytesMapSuite\n  - [x] UnsafeExternalSorterSuite\n  - [x] UnsafeFixedWidthAggregationMapSuite\n  - [x] UnsafeKVExternalSorterSuite\n\n**Compatiblity notes**:\n\n- This patch introduces breaking changes in `ExternalAppendOnlyMap`.  which is marked as `DevloperAPI` (likely for legacy reasons): this class now cannot be used outside of a task.\n\nAuthor: Josh Rosen <joshrosen@databricks.com>\n\nCloses #9127 from JoshRosen/SPARK-10984.\n","date":"2015-10-26 12:19:52","modifiedFileCount":"20","status":"M","submitter":"Josh Rosen"},{"authorTime":"2015-10-30 14:38:06","codes":[{"authorDate":"2015-10-30 14:38:06","commitOrder":9,"curCode":"  public void insertRecord(Object recordBase, long recordOffset, int length, int partitionId)\n    throws IOException {\n\n    \r\n    assert(inMemSorter != null);\n    if (inMemSorter.numRecords() > numElementsForSpillThreshold) {\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    \r\n    final int required = length + 4;\n    acquireNewPageIfNecessary(required);\n\n    assert(currentPage != null);\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    Platform.putInt(base, pageCursor, length);\n    pageCursor += 4;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    inMemSorter.insertRecord(recordAddress, partitionId);\n  }\n","date":"2015-10-30 14:38:06","endLine":392,"groupId":"645","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"insertRecord","params":"(ObjectrecordBase@longrecordOffset@intlength@intpartitionId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/40/0d8520019b9639dd4618fce0c30d1589327f23.src","preCode":"  public void insertRecord(\n      Object recordBaseObject,\n      long recordBaseOffset,\n      int lengthInBytes,\n      int partitionId) throws IOException {\n\n    if (numRecordsInsertedSinceLastSpill > numElementsForSpillThreshold) {\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    \r\n    final int totalSpaceRequired = lengthInBytes + 4;\n\n    \r\n\n    final MemoryBlock dataPage;\n    long dataPagePosition;\n    boolean useOverflowPage = totalSpaceRequired > pageSizeBytes;\n    if (useOverflowPage) {\n      long overflowPageSize = ByteArrayMethods.roundNumberOfBytesToNearestWord(totalSpaceRequired);\n      \r\n      \r\n      MemoryBlock overflowPage = taskMemoryManager.allocatePage(overflowPageSize);\n      if (overflowPage == null) {\n        spill();\n        overflowPage = taskMemoryManager.allocatePage(overflowPageSize);\n        if (overflowPage == null) {\n          throw new IOException(\"Unable to acquire \" + overflowPageSize + \" bytes of memory\");\n        }\n      }\n      allocatedPages.add(overflowPage);\n      dataPage = overflowPage;\n      dataPagePosition = overflowPage.getBaseOffset();\n    } else {\n      \r\n      \r\n      acquireNewPageIfNecessary(totalSpaceRequired);\n      dataPage = currentPage;\n      dataPagePosition = currentPagePosition;\n      \r\n      freeSpaceInCurrentPage -= totalSpaceRequired;\n      currentPagePosition += totalSpaceRequired;\n    }\n    final Object dataPageBaseObject = dataPage.getBaseObject();\n\n    final long recordAddress =\n      taskMemoryManager.encodePageNumberAndOffset(dataPage, dataPagePosition);\n    Platform.putInt(dataPageBaseObject, dataPagePosition, lengthInBytes);\n    dataPagePosition += 4;\n    Platform.copyMemory(\n      recordBaseObject, recordBaseOffset, dataPageBaseObject, dataPagePosition, lengthInBytes);\n    assert(inMemSorter != null);\n    inMemSorter.insertRecord(recordAddress, partitionId);\n    numRecordsInsertedSinceLastSpill += 1;\n  }\n","realPath":"core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":370,"status":"M"},{"authorDate":"2015-10-30 14:38:06","commitOrder":9,"curCode":"  public void insertRecord(Object recordBase, long recordOffset, int length, long prefix)\n    throws IOException {\n\n    growPointerArrayIfNecessary();\n    \r\n    final int required = length + 4;\n    acquireNewPageIfNecessary(required);\n\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    Platform.putInt(base, pageCursor, length);\n    pageCursor += 4;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    assert(inMemSorter != null);\n    inMemSorter.insertRecord(recordAddress, prefix);\n  }\n","date":"2015-10-30 14:38:06","endLine":355,"groupId":"645","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"insertRecord","params":"(ObjectrecordBase@longrecordOffset@intlength@longprefix)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/49/a5a4b13b70d24a257d0a97d2e059a735d7d4d8.src","preCode":"  public void insertRecord(\n      Object recordBaseObject,\n      long recordBaseOffset,\n      int lengthInBytes,\n      long prefix) throws IOException {\n\n    growPointerArrayIfNecessary();\n    \r\n    final int totalSpaceRequired = lengthInBytes + 4;\n\n    \r\n\n    final MemoryBlock dataPage;\n    long dataPagePosition;\n    boolean useOverflowPage = totalSpaceRequired > pageSizeBytes;\n    if (useOverflowPage) {\n      long overflowPageSize = ByteArrayMethods.roundNumberOfBytesToNearestWord(totalSpaceRequired);\n      \r\n      \r\n      MemoryBlock overflowPage = taskMemoryManager.allocatePage(overflowPageSize);\n      if (overflowPage == null) {\n        spill();\n        overflowPage = taskMemoryManager.allocatePage(overflowPageSize);\n        if (overflowPage == null) {\n          throw new IOException(\"Unable to acquire \" + overflowPageSize + \" bytes of memory\");\n        }\n      }\n      allocatedPages.add(overflowPage);\n      dataPage = overflowPage;\n      dataPagePosition = overflowPage.getBaseOffset();\n    } else {\n      \r\n      \r\n      acquireNewPageIfNecessary(totalSpaceRequired);\n      dataPage = currentPage;\n      dataPagePosition = currentPagePosition;\n      \r\n      freeSpaceInCurrentPage -= totalSpaceRequired;\n      currentPagePosition += totalSpaceRequired;\n    }\n    final Object dataPageBaseObject = dataPage.getBaseObject();\n\n    \r\n\n    final long recordAddress =\n      taskMemoryManager.encodePageNumberAndOffset(dataPage, dataPagePosition);\n    Platform.putInt(dataPageBaseObject, dataPagePosition, lengthInBytes);\n    dataPagePosition += 4;\n    Platform.copyMemory(\n      recordBaseObject, recordBaseOffset, dataPageBaseObject, dataPagePosition, lengthInBytes);\n    assert(inMemSorter != null);\n    inMemSorter.insertRecord(recordAddress, prefix);\n  }\n","realPath":"core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":339,"status":"M"}],"commitId":"56419cf11f769c80f391b45dc41b3c7101cc5ff4","commitMessage":"@@@[SPARK-10342] [SPARK-10309] [SPARK-10474] [SPARK-10929] [SQL] Cooperative memory management\n\nThis PR introduce a mechanism to call spill() on those SQL operators that support spilling (for example.  BytesToBytesMap.  UnsafeExternalSorter and ShuffleExternalSorter) if there is not enough memory for execution. The preserved first page is needed anymore.  so removed.\n\nOther Spillable objects in Spark core (ExternalSorter and AppendOnlyMap) are not included in this PR.  but those could benefit from this (trigger others' spilling).\n\nThe PrepareRDD may be not needed anymore.  could be removed in follow up PR.\n\nThe following script will fail with OOM before this PR.  finished in 150 seconds with 2G heap (also works in 1.5 branch.  with similar duration).\n\n```python\nsqlContext.setConf(\"spark.sql.shuffle.partitions\".  \"1\")\ndf = sqlContext.range(1<<25).selectExpr(\"id\".  \"repeat(id.  2) as s\")\ndf2 = df.select(df.id.alias('id2').  df.s.alias('s2'))\nj = df.join(df2.  df.id==df2.id2).groupBy(df.id).max(\"id\".  \"id2\")\nj.explain()\nprint j.count()\n```\n\nFor thread-safety.  here what I'm got:\n\n1) Without calling spill().  the operators should only be used by single thread.  no safety problems.\n\n2) spill() could be triggered in two cases.  triggered by itself.  or by other operators. we can check trigger == this in spill().  so it's still in the same thread.  so safety problems.\n\n3) if it's triggered by other operators (right now cache will not trigger spill()).  we only spill the data into disk when it's in scanning stage (building is finished).  so the in-memory sorter or memory pages are read-only.  we only need to synchronize the iterator and change it.\n\n4) During scanning.  the iterator will only use one record in one page.  we can't free this page.  because the downstream is currently using it (used by UnsafeRow or other objects). In BytesToBytesMap.  we just skip the current page.  and dump all others into disk. In UnsafeExternalSorter.  we keep the page that is used by current record (having the same baseObject).  free it when loading the next record. In ShuffleExternalSorter.  the spill() will not trigger during scanning.\n\n5) In order to avoid deadlock.  we didn't call acquireMemory during spill (so we reused the pointer array in InMemorySorter).\n\nAuthor: Davies Liu <davies@databricks.com>\n\nCloses #9241 from davies/force_spill.\n","date":"2015-10-30 14:38:06","modifiedFileCount":"21","status":"M","submitter":"Davies Liu"},{"authorTime":"2016-06-12 06:42:58","codes":[{"authorDate":"2015-10-30 14:38:06","commitOrder":10,"curCode":"  public void insertRecord(Object recordBase, long recordOffset, int length, int partitionId)\n    throws IOException {\n\n    \r\n    assert(inMemSorter != null);\n    if (inMemSorter.numRecords() > numElementsForSpillThreshold) {\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    \r\n    final int required = length + 4;\n    acquireNewPageIfNecessary(required);\n\n    assert(currentPage != null);\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    Platform.putInt(base, pageCursor, length);\n    pageCursor += 4;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    inMemSorter.insertRecord(recordAddress, partitionId);\n  }\n","date":"2015-10-30 14:38:06","endLine":392,"groupId":"645","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"insertRecord","params":"(ObjectrecordBase@longrecordOffset@intlength@intpartitionId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/40/0d8520019b9639dd4618fce0c30d1589327f23.src","preCode":"  public void insertRecord(Object recordBase, long recordOffset, int length, int partitionId)\n    throws IOException {\n\n    \r\n    assert(inMemSorter != null);\n    if (inMemSorter.numRecords() > numElementsForSpillThreshold) {\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    \r\n    final int required = length + 4;\n    acquireNewPageIfNecessary(required);\n\n    assert(currentPage != null);\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    Platform.putInt(base, pageCursor, length);\n    pageCursor += 4;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    inMemSorter.insertRecord(recordAddress, partitionId);\n  }\n","realPath":"core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":370,"status":"N"},{"authorDate":"2016-06-12 06:42:58","commitOrder":10,"curCode":"  public void insertRecord(\n      Object recordBase, long recordOffset, int length, long prefix, boolean prefixIsNull)\n    throws IOException {\n\n    growPointerArrayIfNecessary();\n    \r\n    final int required = length + 4;\n    acquireNewPageIfNecessary(required);\n\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    Platform.putInt(base, pageCursor, length);\n    pageCursor += 4;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    assert(inMemSorter != null);\n    inMemSorter.insertRecord(recordAddress, prefix, prefixIsNull);\n  }\n","date":"2016-06-12 06:42:58","endLine":389,"groupId":"645","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"insertRecord","params":"(ObjectrecordBase@longrecordOffset@intlength@longprefix@booleanprefixIsNull)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/ec/15f0b59d3dca94ec4f2d12fa07421cf415a960.src","preCode":"  public void insertRecord(Object recordBase, long recordOffset, int length, long prefix)\n    throws IOException {\n\n    growPointerArrayIfNecessary();\n    \r\n    final int required = length + 4;\n    acquireNewPageIfNecessary(required);\n\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    Platform.putInt(base, pageCursor, length);\n    pageCursor += 4;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    assert(inMemSorter != null);\n    inMemSorter.insertRecord(recordAddress, prefix);\n  }\n","realPath":"core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":372,"status":"M"}],"commitId":"c06c58bbbb2de0c22cfc70c486d23a94c3079ba4","commitMessage":"@@@[SPARK-14851][CORE] Support radix sort with nullable longs\n\n## What changes were proposed in this pull request?\n\nThis adds support for radix sort of nullable long fields. When a sort field is null and radix sort is enabled.  we keep nulls in a separate region of the sort buffer so that radix sort does not need to deal with them. This also has performance benefits when sorting smaller integer types.  since the current representation of nulls in two's complement (Long.MIN_VALUE) otherwise forces a full-width radix sort.\n\nThis strategy for nulls does mean the sort is no longer stable. cc davies\n\n## How was this patch tested?\n\nExisting randomized sort tests for correctness. I also tested some TPCDS queries and there does not seem to be any significant regression for non-null sorts.\n\nSome test queries (best of 5 runs each).\nBefore change:\nscala> val start = System.nanoTime; spark.range(5000000).selectExpr(\"if(id > 5.  cast(hash(id) as long).  NULL) as h\").coalesce(1).orderBy(\"h\").collect(); (System.nanoTime - start) / 1e6\nstart: Long = 3190437233227987\nres3: Double = 4716.471091\n\nAfter change:\nscala> val start = System.nanoTime; spark.range(5000000).selectExpr(\"if(id > 5.  cast(hash(id) as long).  NULL) as h\").coalesce(1).orderBy(\"h\").collect(); (System.nanoTime - start) / 1e6\nstart: Long = 3190367870952791\nres4: Double = 2981.143045\n\nAuthor: Eric Liang <ekl@databricks.com>\n\nCloses #13161 from ericl/sc-2998.\n","date":"2016-06-12 06:42:58","modifiedFileCount":"7","status":"M","submitter":"Eric Liang"},{"authorTime":"2016-07-01 01:53:18","codes":[{"authorDate":"2016-07-01 01:53:18","commitOrder":11,"curCode":"  public void insertRecord(Object recordBase, long recordOffset, int length, int partitionId)\n    throws IOException {\n\n    \r\n    assert(inMemSorter != null);\n    if (inMemSorter.numRecords() >= numElementsForSpillThreshold) {\n      logger.info(\"Spilling data because number of spilledRecords crossed the threshold \" + numElementsForSpillThreshold);\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    \r\n    final int required = length + 4;\n    acquireNewPageIfNecessary(required);\n\n    assert(currentPage != null);\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    Platform.putInt(base, pageCursor, length);\n    pageCursor += 4;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    inMemSorter.insertRecord(recordAddress, partitionId);\n  }\n","date":"2016-07-01 01:53:18","endLine":396,"groupId":"645","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"insertRecord","params":"(ObjectrecordBase@longrecordOffset@intlength@intpartitionId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/69/6ee73a76e02cb83758ec9ad5b538f0ba727d74.src","preCode":"  public void insertRecord(Object recordBase, long recordOffset, int length, int partitionId)\n    throws IOException {\n\n    \r\n    assert(inMemSorter != null);\n    if (inMemSorter.numRecords() > numElementsForSpillThreshold) {\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    \r\n    final int required = length + 4;\n    acquireNewPageIfNecessary(required);\n\n    assert(currentPage != null);\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    Platform.putInt(base, pageCursor, length);\n    pageCursor += 4;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    inMemSorter.insertRecord(recordAddress, partitionId);\n  }\n","realPath":"core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":373,"status":"M"},{"authorDate":"2016-07-01 01:53:18","commitOrder":11,"curCode":"  public void insertRecord(\n      Object recordBase, long recordOffset, int length, long prefix, boolean prefixIsNull)\n    throws IOException {\n\n    assert(inMemSorter != null);\n    if (inMemSorter.numRecords() >= numElementsForSpillThreshold) {\n      logger.info(\"Spilling data because number of spilledRecords crossed the threshold \" + numElementsForSpillThreshold);\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    \r\n    final int required = length + 4;\n    acquireNewPageIfNecessary(required);\n\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    Platform.putInt(base, pageCursor, length);\n    pageCursor += 4;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    inMemSorter.insertRecord(recordAddress, prefix, prefixIsNull);\n  }\n","date":"2016-07-01 01:53:18","endLine":406,"groupId":"645","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"insertRecord","params":"(ObjectrecordBase@longrecordOffset@intlength@longprefix@booleanprefixIsNull)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/d6/a255ed9df12747f11811ddb806671415d8d1c9.src","preCode":"  public void insertRecord(\n      Object recordBase, long recordOffset, int length, long prefix, boolean prefixIsNull)\n    throws IOException {\n\n    growPointerArrayIfNecessary();\n    \r\n    final int required = length + 4;\n    acquireNewPageIfNecessary(required);\n\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    Platform.putInt(base, pageCursor, length);\n    pageCursor += 4;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    assert(inMemSorter != null);\n    inMemSorter.insertRecord(recordAddress, prefix, prefixIsNull);\n  }\n","realPath":"core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":384,"status":"M"}],"commitId":"07f46afc733b1718d528a6ea5c0d774f047024fa","commitMessage":"@@@[SPARK-13850] Force the sorter to Spill when number of elements in th?\n\n## What changes were proposed in this pull request?\n\nForce the sorter to Spill when number of elements in the pointer array reach a certain size. This is to workaround the issue of timSort failing on large buffer size.\n\n## How was this patch tested?\n\nTested by running a job which was failing without this change due to TimSort bug.\n\nAuthor: Sital Kedia <skedia@fb.com>\n\nCloses #13107 from sitalkedia/fix_TimSort.\n","date":"2016-07-01 01:53:18","modifiedFileCount":"6","status":"M","submitter":"Sital Kedia"},{"authorTime":"2016-10-04 17:31:56","codes":[{"authorDate":"2016-07-01 01:53:18","commitOrder":12,"curCode":"  public void insertRecord(Object recordBase, long recordOffset, int length, int partitionId)\n    throws IOException {\n\n    \r\n    assert(inMemSorter != null);\n    if (inMemSorter.numRecords() >= numElementsForSpillThreshold) {\n      logger.info(\"Spilling data because number of spilledRecords crossed the threshold \" + numElementsForSpillThreshold);\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    \r\n    final int required = length + 4;\n    acquireNewPageIfNecessary(required);\n\n    assert(currentPage != null);\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    Platform.putInt(base, pageCursor, length);\n    pageCursor += 4;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    inMemSorter.insertRecord(recordAddress, partitionId);\n  }\n","date":"2016-07-01 01:53:18","endLine":396,"groupId":"645","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"insertRecord","params":"(ObjectrecordBase@longrecordOffset@intlength@intpartitionId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/69/6ee73a76e02cb83758ec9ad5b538f0ba727d74.src","preCode":"  public void insertRecord(Object recordBase, long recordOffset, int length, int partitionId)\n    throws IOException {\n\n    \r\n    assert(inMemSorter != null);\n    if (inMemSorter.numRecords() >= numElementsForSpillThreshold) {\n      logger.info(\"Spilling data because number of spilledRecords crossed the threshold \" + numElementsForSpillThreshold);\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    \r\n    final int required = length + 4;\n    acquireNewPageIfNecessary(required);\n\n    assert(currentPage != null);\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    Platform.putInt(base, pageCursor, length);\n    pageCursor += 4;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    inMemSorter.insertRecord(recordAddress, partitionId);\n  }\n","realPath":"core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":373,"status":"N"},{"authorDate":"2016-10-04 17:31:56","commitOrder":12,"curCode":"  public void insertRecord(\n      Object recordBase, long recordOffset, int length, long prefix, boolean prefixIsNull)\n    throws IOException {\n\n    assert(inMemSorter != null);\n    if (inMemSorter.numRecords() >= numElementsForSpillThreshold) {\n      logger.info(\"Spilling data because number of spilledRecords crossed the threshold \" +\n        numElementsForSpillThreshold);\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    int uaoSize = UnsafeAlignedOffset.getUaoSize();\n    \r\n    final int required = length + uaoSize;\n    acquireNewPageIfNecessary(required);\n\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    UnsafeAlignedOffset.putSize(base, pageCursor, length);\n    pageCursor += uaoSize;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    inMemSorter.insertRecord(recordAddress, prefix, prefixIsNull);\n  }\n","date":"2016-10-04 17:31:56","endLine":408,"groupId":"3547","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"insertRecord","params":"(ObjectrecordBase@longrecordOffset@intlength@longprefix@booleanprefixIsNull)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/42/8ff72e71a43e4993d9958577680a00101b74fc.src","preCode":"  public void insertRecord(\n      Object recordBase, long recordOffset, int length, long prefix, boolean prefixIsNull)\n    throws IOException {\n\n    assert(inMemSorter != null);\n    if (inMemSorter.numRecords() >= numElementsForSpillThreshold) {\n      logger.info(\"Spilling data because number of spilledRecords crossed the threshold \" +\n        numElementsForSpillThreshold);\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    \r\n    final int required = length + 4;\n    acquireNewPageIfNecessary(required);\n\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    Platform.putInt(base, pageCursor, length);\n    pageCursor += 4;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    inMemSorter.insertRecord(recordAddress, prefix, prefixIsNull);\n  }\n","realPath":"core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":384,"status":"M"}],"commitId":"7d5160883542f3d9dcb3babda92880985398e9af","commitMessage":"@@@[SPARK-16962][CORE][SQL] Fix misaligned record accesses for SPARC architectures\n\n## What changes were proposed in this pull request?\n\nMade changes to record length offsets to make them uniform throughout various areas of Spark core and unsafe\n\n## How was this patch tested?\n\nThis change affects only SPARC architectures and was tested on X86 architectures as well for regression.\n\nAuthor: sumansomasundar <suman.somasundar@oracle.com>\n\nCloses #14762 from sumansomasundar/master.\n","date":"2016-10-04 17:31:56","modifiedFileCount":"4","status":"M","submitter":"sumansomasundar"},{"authorTime":"2016-10-04 17:31:56","codes":[{"authorDate":"2018-08-13 09:09:25","commitOrder":13,"curCode":"  public void insertRecord(Object recordBase, long recordOffset, int length, int partitionId)\n    throws IOException {\n\n    \r\n    assert(inMemSorter != null);\n    if (inMemSorter.numRecords() >= numElementsForSpillThreshold) {\n      logger.info(\"Spilling data because number of spilledRecords crossed the threshold \" +\n        numElementsForSpillThreshold);\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    final int uaoSize = UnsafeAlignedOffset.getUaoSize();\n    \r\n    final int required = length + uaoSize;\n    acquireNewPageIfNecessary(required);\n\n    assert(currentPage != null);\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    UnsafeAlignedOffset.putSize(base, pageCursor, length);\n    pageCursor += uaoSize;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    inMemSorter.insertRecord(recordAddress, partitionId);\n  }\n","date":"2018-08-13 09:09:25","endLine":407,"groupId":"3547","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"insertRecord","params":"(ObjectrecordBase@longrecordOffset@intlength@intpartitionId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/c7/d2db4217d96e4d211a76e4c661ca3f661ab7b9.src","preCode":"  public void insertRecord(Object recordBase, long recordOffset, int length, int partitionId)\n    throws IOException {\n\n    \r\n    assert(inMemSorter != null);\n    if (inMemSorter.numRecords() >= numElementsForSpillThreshold) {\n      logger.info(\"Spilling data because number of spilledRecords crossed the threshold \" +\n        numElementsForSpillThreshold);\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    \r\n    final int required = length + 4;\n    acquireNewPageIfNecessary(required);\n\n    assert(currentPage != null);\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    Platform.putInt(base, pageCursor, length);\n    pageCursor += 4;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    inMemSorter.insertRecord(recordAddress, partitionId);\n  }\n","realPath":"core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":382,"status":"M"},{"authorDate":"2016-10-04 17:31:56","commitOrder":13,"curCode":"  public void insertRecord(\n      Object recordBase, long recordOffset, int length, long prefix, boolean prefixIsNull)\n    throws IOException {\n\n    assert(inMemSorter != null);\n    if (inMemSorter.numRecords() >= numElementsForSpillThreshold) {\n      logger.info(\"Spilling data because number of spilledRecords crossed the threshold \" +\n        numElementsForSpillThreshold);\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    int uaoSize = UnsafeAlignedOffset.getUaoSize();\n    \r\n    final int required = length + uaoSize;\n    acquireNewPageIfNecessary(required);\n\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    UnsafeAlignedOffset.putSize(base, pageCursor, length);\n    pageCursor += uaoSize;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    inMemSorter.insertRecord(recordAddress, prefix, prefixIsNull);\n  }\n","date":"2016-10-04 17:31:56","endLine":408,"groupId":"3547","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"insertRecord","params":"(ObjectrecordBase@longrecordOffset@intlength@longprefix@booleanprefixIsNull)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/42/8ff72e71a43e4993d9958577680a00101b74fc.src","preCode":"  public void insertRecord(\n      Object recordBase, long recordOffset, int length, long prefix, boolean prefixIsNull)\n    throws IOException {\n\n    assert(inMemSorter != null);\n    if (inMemSorter.numRecords() >= numElementsForSpillThreshold) {\n      logger.info(\"Spilling data because number of spilledRecords crossed the threshold \" +\n        numElementsForSpillThreshold);\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    int uaoSize = UnsafeAlignedOffset.getUaoSize();\n    \r\n    final int required = length + uaoSize;\n    acquireNewPageIfNecessary(required);\n\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    UnsafeAlignedOffset.putSize(base, pageCursor, length);\n    pageCursor += uaoSize;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    inMemSorter.insertRecord(recordAddress, prefix, prefixIsNull);\n  }\n","realPath":"core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":384,"status":"N"}],"commitId":"02d0a1ffd9adb4bf898905095318eb099ed1807f","commitMessage":"@@@[SPARK-25069][CORE] Using UnsafeAlignedOffset to make the entire record of 8 byte Items aligned like which is used in UnsafeExternalSorter\n\n## What changes were proposed in this pull request?\n\nThe class of UnsafeExternalSorter used UnsafeAlignedOffset to make the entire record of 8 byte Items aligned.  but ShuffleExternalSorter not.\nThe SPARC platform requires this because using a 4 byte Int for record lengths causes the entire record of 8 byte Items to become misaligned by 4 bytes. Using a 8 byte long for record length keeps things 8 byte aligned.\n\n## How was this patch tested?\nExisting Test.\n\nCloses #22053 from eatoncys/UnsafeAlignedOffset.\n\nAuthored-by: 10129659 <chen.yanshan@zte.com.cn>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n","date":"2018-08-13 09:09:25","modifiedFileCount":"2","status":"M","submitter":"10129659"},{"authorTime":"2020-09-29 19:05:33","codes":[{"authorDate":"2018-08-13 09:09:25","commitOrder":14,"curCode":"  public void insertRecord(Object recordBase, long recordOffset, int length, int partitionId)\n    throws IOException {\n\n    \r\n    assert(inMemSorter != null);\n    if (inMemSorter.numRecords() >= numElementsForSpillThreshold) {\n      logger.info(\"Spilling data because number of spilledRecords crossed the threshold \" +\n        numElementsForSpillThreshold);\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    final int uaoSize = UnsafeAlignedOffset.getUaoSize();\n    \r\n    final int required = length + uaoSize;\n    acquireNewPageIfNecessary(required);\n\n    assert(currentPage != null);\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    UnsafeAlignedOffset.putSize(base, pageCursor, length);\n    pageCursor += uaoSize;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    inMemSorter.insertRecord(recordAddress, partitionId);\n  }\n","date":"2018-08-13 09:09:25","endLine":407,"groupId":"10573","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"insertRecord","params":"(ObjectrecordBase@longrecordOffset@intlength@intpartitionId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/c7/d2db4217d96e4d211a76e4c661ca3f661ab7b9.src","preCode":"  public void insertRecord(Object recordBase, long recordOffset, int length, int partitionId)\n    throws IOException {\n\n    \r\n    assert(inMemSorter != null);\n    if (inMemSorter.numRecords() >= numElementsForSpillThreshold) {\n      logger.info(\"Spilling data because number of spilledRecords crossed the threshold \" +\n        numElementsForSpillThreshold);\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    final int uaoSize = UnsafeAlignedOffset.getUaoSize();\n    \r\n    final int required = length + uaoSize;\n    acquireNewPageIfNecessary(required);\n\n    assert(currentPage != null);\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    UnsafeAlignedOffset.putSize(base, pageCursor, length);\n    pageCursor += uaoSize;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    inMemSorter.insertRecord(recordAddress, partitionId);\n  }\n","realPath":"core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":382,"status":"N"},{"authorDate":"2020-09-29 19:05:33","commitOrder":14,"curCode":"  public void insertRecord(\n      Object recordBase, long recordOffset, int length, long prefix, boolean prefixIsNull)\n    throws IOException {\n\n    assert(inMemSorter != null);\n    if (inMemSorter.numRecords() >= numElementsForSpillThreshold) {\n      logger.info(\"Spilling data because number of spilledRecords crossed the threshold \" +\n        numElementsForSpillThreshold);\n      spill();\n    }\n\n    final int uaoSize = UnsafeAlignedOffset.getUaoSize();\n    \r\n    final int required = length + uaoSize;\n    allocateMemoryForRecordIfNecessary(required);\n\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    UnsafeAlignedOffset.putSize(base, pageCursor, length);\n    pageCursor += uaoSize;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    inMemSorter.insertRecord(recordAddress, prefix, prefixIsNull);\n  }\n","date":"2020-09-29 19:05:33","endLine":461,"groupId":"10573","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"insertRecord","params":"(ObjectrecordBase@longrecordOffset@intlength@longprefix@booleanprefixIsNull)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/dd/a8ed4c239ae72d9473e647fb3fa783c51fd8cb.src","preCode":"  public void insertRecord(\n      Object recordBase, long recordOffset, int length, long prefix, boolean prefixIsNull)\n    throws IOException {\n\n    assert(inMemSorter != null);\n    if (inMemSorter.numRecords() >= numElementsForSpillThreshold) {\n      logger.info(\"Spilling data because number of spilledRecords crossed the threshold \" +\n        numElementsForSpillThreshold);\n      spill();\n    }\n\n    growPointerArrayIfNecessary();\n    int uaoSize = UnsafeAlignedOffset.getUaoSize();\n    \r\n    final int required = length + uaoSize;\n    acquireNewPageIfNecessary(required);\n\n    final Object base = currentPage.getBaseObject();\n    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);\n    UnsafeAlignedOffset.putSize(base, pageCursor, length);\n    pageCursor += uaoSize;\n    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);\n    pageCursor += length;\n    inMemSorter.insertRecord(recordAddress, prefix, prefixIsNull);\n  }\n","realPath":"core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":438,"status":"M"}],"commitId":"f167002522d50eefb261c8ba2d66a23b781a38c4","commitMessage":"@@@[SPARK-32901][CORE] Do not allocate memory while spilling UnsafeExternalSorter\n\n\n What changes were proposed in this pull request?\n\nThis PR changes `UnsafeExternalSorter` to no longer allocate any memory while spilling. In particular it removes the allocation of a new pointer array in `UnsafeInMemorySorter`. Instead the new pointer array is allocated whenever the next record is inserted into the sorter.\n\n\n Why are the changes needed?\n\nWithout this change the `UnsafeExternalSorter` could throw an OOM while spilling. The following sequence of events would have triggered an OOM:\n\n1. `UnsafeExternalSorter` runs out of space in its pointer array and attempts to allocate a new large array to replace the old one.\n2. `TaskMemoryManager` tries to allocate the memory backing the new large array using `MemoryManager`.  but `MemoryManager` is only willing to return most but not all of the memory requested.\n3. `TaskMemoryManager` asks `UnsafeExternalSorter` to spill.  which causes `UnsafeExternalSorter` to spill the current run to disk.  to free its record pages and to reset its `UnsafeInMemorySorter`.\n4. `UnsafeInMemorySorter` frees the old pointer array.  and tries to allocate a new small pointer array.\n5. `TaskMemoryManager` tries to allocate the memory backing the small array using `MemoryManager`.  but `MemoryManager` is unwilling to give it any memory.  as the `TaskMemoryManager` is still holding on to the memory it got for the new large array.\n6. `TaskMemoryManager` again asks `UnsafeExternalSorter` to spill.  but this time there is nothing to spill.\n7. `UnsafeInMemorySorter` receives less memory than it requested.  and causes a `SparkOutOfMemoryError` to be thrown.  which causes the current task to fail.\n\nWith the changes in the PR the following will happen instead:\n\n1. `UnsafeExternalSorter` runs out of space in its pointer array and attempts to allocate a new large array to replace the old one.\n2. `TaskMemoryManager` tries to allocate the memory backing the new large array using `MemoryManager`.  but `MemoryManager` is only willing to return most but not all of the memory requested.\n3. `TaskMemoryManager` asks `UnsafeExternalSorter` to spill.  which causes `UnsafeExternalSorter` to spill the current run to disk.  to free its record pages and to reset its `UnsafeInMemorySorter`.\n4. `UnsafeInMemorySorter` frees the old pointer array.\n5. `TaskMemoryManager` returns control to `UnsafeExternalSorter.growPointerArrayIfNecessary` (either by returning the the new large array or by throwing a `SparkOutOfMemoryError`).\n6. `UnsafeExternalSorter` either frees the new large array or it ignores the `SparkOutOfMemoryError` depending on what happened in the previous step.\n7. `UnsafeExternalSorter` successfully allocates a new small pointer array and operation continues as normal.\n\n\n Does this PR introduce _any_ user-facing change?\n\nNo\n\n\n How was this patch tested?\n\nTests were added in `UnsafeExternalSorterSuite` and `UnsafeInMemorySorterSuite`.\n\nCloses #29785 from tomvanbussel/SPARK-32901.\n\nAuthored-by: Tom van Bussel <tom.vanbussel@databricks.com>\nSigned-off-by: herman <herman@databricks.com>\n","date":"2020-09-29 19:05:33","modifiedFileCount":"4","status":"M","submitter":"Tom van Bussel"}]
