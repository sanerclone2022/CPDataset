[{"authorTime":"2016-07-27 09:08:07","codes":[{"authorDate":"2016-07-27 09:08:07","commitOrder":1,"curCode":"  public final UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    final long recordLength = 8 + klen + vlen + 8;\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.putInt(base, offset, klen + vlen + 4);\n    Platform.putInt(base, offset + 4, klen);\n\n    offset += 8;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyOffsets[numRows] = recordOffset + 8;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset + 8, klen);\n    valueRow.pointTo(base, recordOffset + 8 + klen, vlen + 4);\n    numRows++;\n    return valueRow;\n  }\n","date":"2016-07-27 09:08:07","endLine":71,"groupId":"1127","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"appendRow","params":"(Objectkbase@longkoff@intklen@Objectvbase@longvoff@intvlen)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/f4/002ee0d50dedefc55757aee52d896c8c405c7f.src","preCode":"  public final UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    final long recordLength = 8 + klen + vlen + 8;\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.putInt(base, offset, klen + vlen + 4);\n    Platform.putInt(base, offset + 4, klen);\n\n    offset += 8;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyOffsets[numRows] = recordOffset + 8;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset + 8, klen);\n    valueRow.pointTo(base, recordOffset + 8 + klen, vlen + 4);\n    numRows++;\n    return valueRow;\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/VariableLengthRowBasedKeyValueBatch.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"B"},{"authorDate":"2016-07-27 09:08:07","commitOrder":1,"curCode":"  public final UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset, klen);\n    valueRow.pointTo(base, recordOffset + klen, vlen + 4);\n    numRows++;\n    return valueRow;\n  }\n","date":"2016-07-27 09:08:07","endLine":68,"groupId":"2904","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"appendRow","params":"(Objectkbase@longkoff@intklen@Objectvbase@longvoff@intvlen)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/b6/130d1f332b930cc1dc5d68a69d694a75c55f4b.src","preCode":"  public final UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset, klen);\n    valueRow.pointTo(base, recordOffset + klen, vlen + 4);\n    numRows++;\n    return valueRow;\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/FixedLengthRowBasedKeyValueBatch.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":46,"status":"B"}],"commitId":"738b4cc548ca48c010b682b8bc19a2f7e1947cfe","commitMessage":"@@@[SPARK-16524][SQL] Add RowBatch and RowBasedHashMapGenerator\n\n## What changes were proposed in this pull request?\n\nThis PR is the first step for the following feature:\n\nFor hash aggregation in Spark SQL.  we use a fast aggregation hashmap to act as a \"cache\" in order to boost aggregation performance. Previously.  the hashmap is backed by a `ColumnarBatch`. This has performance issues when we have wide schema for the aggregation table (large number of key fields or value fields).\nIn this JIRA.  we support another implementation of fast hashmap.  which is backed by a `RowBasedKeyValueBatch`. We then automatically pick between the two implementations based on certain knobs.\n\nIn this first-step PR.  implementations for `RowBasedKeyValueBatch` and `RowBasedHashMapGenerator` are added.\n\n## How was this patch tested?\n\nUnit tests: `RowBasedKeyValueBatchSuite`\n\nAuthor: Qifan Pu <qifan.pu@gmail.com>\n\nCloses #14349 from ooq/SPARK-16524.\n","date":"2016-07-27 09:08:07","modifiedFileCount":"0","status":"B","submitter":"Qifan Pu"},{"authorTime":"2016-08-08 16:24:37","codes":[{"authorDate":"2016-08-08 16:24:37","commitOrder":2,"curCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    final long recordLength = 8 + klen + vlen + 8;\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.putInt(base, offset, klen + vlen + 4);\n    Platform.putInt(base, offset + 4, klen);\n\n    offset += 8;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyOffsets[numRows] = recordOffset + 8;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset + 8, klen);\n    valueRow.pointTo(base, recordOffset + 8 + klen, vlen + 4);\n    numRows++;\n    return valueRow;\n  }\n","date":"2016-08-08 16:24:37","endLine":71,"groupId":"1127","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"appendRow","params":"(Objectkbase@longkoff@intklen@Objectvbase@longvoff@intvlen)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/ea/4f984be24e5ac0a7a7a53ff64ccb873378339c.src","preCode":"  public final UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    final long recordLength = 8 + klen + vlen + 8;\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.putInt(base, offset, klen + vlen + 4);\n    Platform.putInt(base, offset + 4, klen);\n\n    offset += 8;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyOffsets[numRows] = recordOffset + 8;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset + 8, klen);\n    valueRow.pointTo(base, recordOffset + 8 + klen, vlen + 4);\n    numRows++;\n    return valueRow;\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/VariableLengthRowBasedKeyValueBatch.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"M"},{"authorDate":"2016-08-08 16:24:37","commitOrder":2,"curCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset, klen);\n    valueRow.pointTo(base, recordOffset + klen, vlen + 4);\n    numRows++;\n    return valueRow;\n  }\n","date":"2016-08-08 16:24:37","endLine":68,"groupId":"2904","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"appendRow","params":"(Objectkbase@longkoff@intklen@Objectvbase@longvoff@intvlen)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/85/529f6a0aa1ec636682148e2632483663900350.src","preCode":"  public final UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset, klen);\n    valueRow.pointTo(base, recordOffset + klen, vlen + 4);\n    numRows++;\n    return valueRow;\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/FixedLengthRowBasedKeyValueBatch.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":46,"status":"M"}],"commitId":"e10ca8de49206087b336c6db0c40868fa271b989","commitMessage":"@@@[SPARK-16945] Fix Java Lint errors\n\n## What changes were proposed in this pull request?\nThis PR is to fix the minor Java linter errors as following:\n[ERROR] src/main/java/org/apache/spark/sql/catalyst/expressions/VariableLengthRowBasedKeyValueBatch.java:[42. 10] (modifier) RedundantModifier: Redundant 'final' modifier.\n[ERROR] src/main/java/org/apache/spark/sql/catalyst/expressions/VariableLengthRowBasedKeyValueBatch.java:[97. 10] (modifier) RedundantModifier: Redundant 'final' modifier.\n\n## How was this patch tested?\nManual test.\ndev/lint-java\nUsing `mvn` from path: /usr/local/bin/mvn\nCheckstyle checks passed.\n\nAuthor: Weiqing Yang <yangweiqing001@gmail.com>\n\nCloses #14532 from Sherry302/master.\n","date":"2016-08-08 16:24:37","modifiedFileCount":"4","status":"M","submitter":"Weiqing Yang"},{"authorTime":"2017-07-27 15:27:24","codes":[{"authorDate":"2017-07-27 15:27:24","commitOrder":3,"curCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    final long recordLength = 8 + klen + vlen + 8;\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.putInt(base, offset, klen + vlen + 4);\n    Platform.putInt(base, offset + 4, klen);\n\n    offset += 8;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyOffsets[numRows] = recordOffset + 8;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset + 8, klen);\n    valueRow.pointTo(base, recordOffset + 8 + klen, vlen);\n    numRows++;\n    return valueRow;\n  }\n","date":"2017-07-27 15:27:24","endLine":71,"groupId":"1127","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"appendRow","params":"(Objectkbase@longkoff@intklen@Objectvbase@longvoff@intvlen)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/90/5e6820ce6e270edd294d1902493a17082b4828.src","preCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    final long recordLength = 8 + klen + vlen + 8;\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.putInt(base, offset, klen + vlen + 4);\n    Platform.putInt(base, offset + 4, klen);\n\n    offset += 8;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyOffsets[numRows] = recordOffset + 8;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset + 8, klen);\n    valueRow.pointTo(base, recordOffset + 8 + klen, vlen + 4);\n    numRows++;\n    return valueRow;\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/VariableLengthRowBasedKeyValueBatch.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"M"},{"authorDate":"2017-07-27 15:27:24","commitOrder":3,"curCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset, klen);\n    valueRow.pointTo(base, recordOffset + klen, vlen);\n    numRows++;\n    return valueRow;\n  }\n","date":"2017-07-27 15:27:24","endLine":68,"groupId":"2904","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"appendRow","params":"(Objectkbase@longkoff@intklen@Objectvbase@longvoff@intvlen)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/df/52f9c2d54960f840bfbc8beca75af34887ed3f.src","preCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset, klen);\n    valueRow.pointTo(base, recordOffset + klen, vlen + 4);\n    numRows++;\n    return valueRow;\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/FixedLengthRowBasedKeyValueBatch.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":46,"status":"M"}],"commitId":"ebbe589d12434bc108672268bee05a7b7e571ee6","commitMessage":"@@@[SPARK-21271][SQL] Ensure Unsafe.sizeInBytes is a multiple of 8\n\n## What changes were proposed in this pull request?\n\nThis PR ensures that `Unsafe.sizeInBytes` must be a multiple of 8. It it is not satisfied. `Unsafe.hashCode` causes the assertion violation.\n\n## How was this patch tested?\n\nWill add test cases\n\nAuthor: Kazuaki Ishizaki <ishizaki@jp.ibm.com>\n\nCloses #18503 from kiszk/SPARK-21271.\n","date":"2017-07-27 15:27:24","modifiedFileCount":"5","status":"M","submitter":"Kazuaki Ishizaki"},{"authorTime":"2017-07-27 15:27:24","codes":[{"authorDate":"2018-06-16 04:47:48","commitOrder":4,"curCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    final long recordLength = 8L + klen + vlen + 8;\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.putInt(base, offset, klen + vlen + 4);\n    Platform.putInt(base, offset + 4, klen);\n\n    offset += 8;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyOffsets[numRows] = recordOffset + 8;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset + 8, klen);\n    valueRow.pointTo(base, recordOffset + 8 + klen, vlen);\n    numRows++;\n    return valueRow;\n  }\n","date":"2018-06-16 04:47:48","endLine":71,"groupId":"1127","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"appendRow","params":"(Objectkbase@longkoff@intklen@Objectvbase@longvoff@intvlen)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/c8/23de4810f2bc56a7e4faf7b09cf7a1d3cbffe8.src","preCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    final long recordLength = 8 + klen + vlen + 8;\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.putInt(base, offset, klen + vlen + 4);\n    Platform.putInt(base, offset + 4, klen);\n\n    offset += 8;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyOffsets[numRows] = recordOffset + 8;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset + 8, klen);\n    valueRow.pointTo(base, recordOffset + 8 + klen, vlen);\n    numRows++;\n    return valueRow;\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/VariableLengthRowBasedKeyValueBatch.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"M"},{"authorDate":"2017-07-27 15:27:24","commitOrder":4,"curCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset, klen);\n    valueRow.pointTo(base, recordOffset + klen, vlen);\n    numRows++;\n    return valueRow;\n  }\n","date":"2017-07-27 15:27:24","endLine":68,"groupId":"2904","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"appendRow","params":"(Objectkbase@longkoff@intklen@Objectvbase@longvoff@intvlen)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/df/52f9c2d54960f840bfbc8beca75af34887ed3f.src","preCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset, klen);\n    valueRow.pointTo(base, recordOffset + klen, vlen);\n    numRows++;\n    return valueRow;\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/FixedLengthRowBasedKeyValueBatch.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":46,"status":"N"}],"commitId":"90da7dc241f8eec2348c0434312c97c116330bc4","commitMessage":"@@@[SPARK-24452][SQL][CORE] Avoid possible overflow in int add or multiple\n\n## What changes were proposed in this pull request?\n\nThis PR fixes possible overflow in int add or multiply. In particular.  their overflows in multiply are detected by [Spotbugs](https://spotbugs.github.io/)\n\nThe following assignments may cause overflow in right hand side. As a result.  the result may be negative.\n```\nlong = int * int\nlong = int + int\n```\n\nTo avoid this problem.  this PR performs cast from int to long in right hand side.\n\n## How was this patch tested?\n\nExisting UTs.\n\nAuthor: Kazuaki Ishizaki <ishizaki@jp.ibm.com>\n\nCloses #21481 from kiszk/SPARK-24452.\n","date":"2018-06-16 04:47:48","modifiedFileCount":"5","status":"M","submitter":"Kazuaki Ishizaki"},{"authorTime":"2017-07-27 15:27:24","codes":[{"authorDate":"2020-04-17 12:48:27","commitOrder":5,"curCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    int uaoSize = UnsafeAlignedOffset.getUaoSize();\n    final long recordLength = 2 * uaoSize + klen + vlen + 8L;\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    UnsafeAlignedOffset.putSize(base, offset, klen + vlen + uaoSize);\n    UnsafeAlignedOffset.putSize(base, offset + uaoSize, klen);\n\n    offset += 2 * uaoSize;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyOffsets[numRows] = recordOffset + 2 * uaoSize;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset + 2 * uaoSize, klen);\n    valueRow.pointTo(base, recordOffset + 2 * uaoSize + klen, vlen);\n    numRows++;\n    return valueRow;\n  }\n","date":"2020-04-17 12:48:27","endLine":73,"groupId":"3104","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"appendRow","params":"(Objectkbase@longkoff@intklen@Objectvbase@longvoff@intvlen)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/4e/e913c9bf02dbd3b2655f67334e5545ca8e6560.src","preCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    final long recordLength = 8L + klen + vlen + 8;\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.putInt(base, offset, klen + vlen + 4);\n    Platform.putInt(base, offset + 4, klen);\n\n    offset += 8;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyOffsets[numRows] = recordOffset + 8;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset + 8, klen);\n    valueRow.pointTo(base, recordOffset + 8 + klen, vlen);\n    numRows++;\n    return valueRow;\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/VariableLengthRowBasedKeyValueBatch.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":43,"status":"M"},{"authorDate":"2017-07-27 15:27:24","commitOrder":5,"curCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset, klen);\n    valueRow.pointTo(base, recordOffset + klen, vlen);\n    numRows++;\n    return valueRow;\n  }\n","date":"2017-07-27 15:27:24","endLine":68,"groupId":"2904","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"appendRow","params":"(Objectkbase@longkoff@intklen@Objectvbase@longvoff@intvlen)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/df/52f9c2d54960f840bfbc8beca75af34887ed3f.src","preCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset, klen);\n    valueRow.pointTo(base, recordOffset + klen, vlen);\n    numRows++;\n    return valueRow;\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/FixedLengthRowBasedKeyValueBatch.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":46,"status":"N"}],"commitId":"40f9dbb6284ca6d5664ec0983faba723bc72d7f1","commitMessage":"@@@[SPARK-31425][SQL][CORE] UnsafeKVExternalSorter/VariableLengthRowBasedKeyValueBatch should also respect UnsafeAlignedOffset\n\n\n What changes were proposed in this pull request?\n\nMake `UnsafeKVExternalSorter` / `VariableLengthRowBasedKeyValueBatch ` also respect `UnsafeAlignedOffset` when reading the record and update some out of date comemnts.\n\n\n Why are the changes needed?\n\nSince `BytesToBytesMap` respects `UnsafeAlignedOffset` when writing the record.  `UnsafeKVExternalSorter` should also respect `UnsafeAlignedOffset` when reading the record from `BytesToBytesMap` otherwise it will causes data correctness issue.\n\nUnlike `UnsafeKVExternalSorter` may reading records from `BytesToBytesMap`.  `VariableLengthRowBasedKeyValueBatch` writes and reads records by itself. Thus.  similar to #22053 and [comment](https://github.com/apache/spark/pull/22053#issuecomment-411975239) there.  fix for `VariableLengthRowBasedKeyValueBatch` more likely an improvement for the support of SPARC platform.\n\n\n Does this PR introduce any user-facing change?\n\nNo.\n\n\n How was this patch tested?\n\nManually tested `HashAggregationQueryWithControlledFallbackSuite` with `UAO_SIZE=8`  to simulate SPARC platform. And tests only pass with this fix.\n\nCloses #28195 from Ngone51/fix_uao.\n\nAuthored-by: yi.wu <yi.wu@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n","date":"2020-04-17 12:48:27","modifiedFileCount":"5","status":"M","submitter":"yi.wu"},{"authorTime":"2021-02-08 22:46:01","codes":[{"authorDate":"2020-04-17 12:48:27","commitOrder":6,"curCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    int uaoSize = UnsafeAlignedOffset.getUaoSize();\n    final long recordLength = 2 * uaoSize + klen + vlen + 8L;\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    UnsafeAlignedOffset.putSize(base, offset, klen + vlen + uaoSize);\n    UnsafeAlignedOffset.putSize(base, offset + uaoSize, klen);\n\n    offset += 2 * uaoSize;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyOffsets[numRows] = recordOffset + 2 * uaoSize;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset + 2 * uaoSize, klen);\n    valueRow.pointTo(base, recordOffset + 2 * uaoSize + klen, vlen);\n    numRows++;\n    return valueRow;\n  }\n","date":"2020-04-17 12:48:27","endLine":73,"groupId":"3104","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"appendRow","params":"(Objectkbase@longkoff@intklen@Objectvbase@longvoff@intvlen)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/4e/e913c9bf02dbd3b2655f67334e5545ca8e6560.src","preCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    int uaoSize = UnsafeAlignedOffset.getUaoSize();\n    final long recordLength = 2 * uaoSize + klen + vlen + 8L;\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    UnsafeAlignedOffset.putSize(base, offset, klen + vlen + uaoSize);\n    UnsafeAlignedOffset.putSize(base, offset + uaoSize, klen);\n\n    offset += 2 * uaoSize;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyOffsets[numRows] = recordOffset + 2 * uaoSize;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset + 2 * uaoSize, klen);\n    valueRow.pointTo(base, recordOffset + 2 * uaoSize + klen, vlen);\n    numRows++;\n    return valueRow;\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/VariableLengthRowBasedKeyValueBatch.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":43,"status":"N"},{"authorDate":"2021-02-08 22:46:01","commitOrder":6,"curCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    \r\n    assert(vlen == this.vlen);\n    assert(klen == this.klen);\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset, klen);\n    valueRow.pointTo(base, recordOffset + klen, vlen);\n    numRows++;\n    return valueRow;\n  }\n","date":"2021-02-08 22:46:01","endLine":70,"groupId":"2904","id":12,"instanceNumber":2,"isCurCommit":1,"methodName":"appendRow","params":"(Objectkbase@longkoff@intklen@Objectvbase@longvoff@intvlen)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/25/400beeb0d7517b666b4a604f664ba8fd7c35a4.src","preCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset, klen);\n    valueRow.pointTo(base, recordOffset + klen, vlen);\n    numRows++;\n    return valueRow;\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/FixedLengthRowBasedKeyValueBatch.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":46,"status":"M"}],"commitId":"d1131bc85028ea0f78ac9ef73bba731080f1ff6a","commitMessage":"@@@[MINOR][SQL][FOLLOW-UP] Add assertion to FixedLengthRowBasedKeyValueBatch\n\n\n What changes were proposed in this pull request?\nAdds an assert to `FixedLengthRowBasedKeyValueBatch#appendRow` method to check the incoming vlen and klen by comparing them with the lengths stored as member variables as followup to https://github.com/apache/spark/pull/30788\n\n\n Why are the changes needed?\nAdd assert statement to catch similar bugs in future.\n\n\n Does this PR introduce _any_ user-facing change?\nNo\n\n\n How was this patch tested?\nRan some tests locally.  though not easy to test.\n\nCloses #31447 from yliou/SPARK-33726-Assert.\n\nAuthored-by: yliou <yliou@berkeley.edu>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n","date":"2021-02-08 22:46:01","modifiedFileCount":"1","status":"M","submitter":"yliou"},{"authorTime":"2021-02-08 22:46:01","codes":[{"authorDate":"2021-08-01 13:35:57","commitOrder":7,"curCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    int uaoSize = UnsafeAlignedOffset.getUaoSize();\n    final long recordLength = 2L * uaoSize + klen + vlen + 8L;\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    UnsafeAlignedOffset.putSize(base, offset, klen + vlen + uaoSize);\n    UnsafeAlignedOffset.putSize(base, offset + uaoSize, klen);\n\n    offset += 2L * uaoSize;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyOffsets[numRows] = recordOffset + 2L * uaoSize;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset + 2L * uaoSize, klen);\n    valueRow.pointTo(base, recordOffset + 2L * uaoSize + klen, vlen);\n    numRows++;\n    return valueRow;\n  }\n","date":"2021-08-01 13:35:57","endLine":73,"groupId":"10370","id":13,"instanceNumber":1,"isCurCommit":1,"methodName":"appendRow","params":"(Objectkbase@longkoff@intklen@Objectvbase@longvoff@intvlen)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/32/ffa1459e6126cf386d12d81ca5c59a4de71677.src","preCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    int uaoSize = UnsafeAlignedOffset.getUaoSize();\n    final long recordLength = 2 * uaoSize + klen + vlen + 8L;\n    \r\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    UnsafeAlignedOffset.putSize(base, offset, klen + vlen + uaoSize);\n    UnsafeAlignedOffset.putSize(base, offset + uaoSize, klen);\n\n    offset += 2 * uaoSize;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyOffsets[numRows] = recordOffset + 2 * uaoSize;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset + 2 * uaoSize, klen);\n    valueRow.pointTo(base, recordOffset + 2 * uaoSize + klen, vlen);\n    numRows++;\n    return valueRow;\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/VariableLengthRowBasedKeyValueBatch.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":43,"status":"M"},{"authorDate":"2021-02-08 22:46:01","commitOrder":7,"curCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    \r\n    assert(vlen == this.vlen);\n    assert(klen == this.klen);\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset, klen);\n    valueRow.pointTo(base, recordOffset + klen, vlen);\n    numRows++;\n    return valueRow;\n  }\n","date":"2021-02-08 22:46:01","endLine":70,"groupId":"10370","id":14,"instanceNumber":2,"isCurCommit":1,"methodName":"appendRow","params":"(Objectkbase@longkoff@intklen@Objectvbase@longvoff@intvlen)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/25/400beeb0d7517b666b4a604f664ba8fd7c35a4.src","preCode":"  public UnsafeRow appendRow(Object kbase, long koff, int klen,\n                             Object vbase, long voff, int vlen) {\n    \r\n    assert(vlen == this.vlen);\n    assert(klen == this.klen);\n    if (numRows >= capacity || page == null || page.size() - pageCursor < recordLength) {\n      return null;\n    }\n\n    long offset = page.getBaseOffset() + pageCursor;\n    final long recordOffset = offset;\n    Platform.copyMemory(kbase, koff, base, offset, klen);\n    offset += klen;\n    Platform.copyMemory(vbase, voff, base, offset, vlen);\n    offset += vlen;\n    Platform.putLong(base, offset, 0);\n\n    pageCursor += recordLength;\n\n    keyRowId = numRows;\n    keyRow.pointTo(base, recordOffset, klen);\n    valueRow.pointTo(base, recordOffset + klen, vlen);\n    numRows++;\n    return valueRow;\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/FixedLengthRowBasedKeyValueBatch.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":46,"status":"N"}],"commitId":"72615bc551adaa238d15a8b43a8f99aaf741c30f","commitMessage":"@@@[SPARK-36362][CORE][SQL][TESTS] Omnibus Java code static analyzer warning fixes\n\n\n What changes were proposed in this pull request?\n\nFix up some minor Java issues:\n\n- Some int*int multiplications that widen to long maybe could overflow\n- Unnecessarily non-static inner classes\n- Some tests \"catch (AssertionError)\" and do nothing\n- Manual array iteration vs very slightly faster/simpler foreach\n- Incorrect generic types that just happen to not cause a runtime error\n- Missed opportunities for try-close\n- Mutable enums\n- .. and a few other minor things\n\n\n Why are the changes needed?\n\nSome are minor but clear fixes; some may have a marginal perf impact or avoid a bug later. Also: maybe avoid future PRs to address these one by one.\n\n\n Does this PR introduce _any_ user-facing change?\n\nNo.\n\n\n How was this patch tested?\n\nExisting tests\n\nCloses #33594 from srowen/SPARK-36362.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n","date":"2021-08-01 13:35:57","modifiedFileCount":"41","status":"M","submitter":"Sean Owen"}]
