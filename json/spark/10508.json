[{"authorTime":"2016-12-04 08:58:15","codes":[{"authorDate":"2016-12-04 08:58:15","commitOrder":1,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaMinHashLSHExample\")\n      .getOrCreate();\n\n    \r\n    List<Row> data = Arrays.asList(\n      RowFactory.create(0, Vectors.sparse(6, new int[]{0, 1, 2}, new double[]{1.0, 1.0, 1.0})),\n      RowFactory.create(1, Vectors.sparse(6, new int[]{2, 3, 4}, new double[]{1.0, 1.0, 1.0})),\n      RowFactory.create(2, Vectors.sparse(6, new int[]{0, 2, 4}, new double[]{1.0, 1.0, 1.0}))\n    );\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"id\", DataTypes.IntegerType, false, Metadata.empty()),\n      new StructField(\"keys\", new VectorUDT(), false, Metadata.empty())\n    });\n    Dataset<Row> dataFrame = spark.createDataFrame(data, schema);\n\n    MinHashLSH mh = new MinHashLSH()\n      .setNumHashTables(1)\n      .setInputCol(\"keys\")\n      .setOutputCol(\"values\");\n\n    MinHashLSHModel model = mh.fit(dataFrame);\n    model.transform(dataFrame).show();\n    \r\n\n    spark.stop();\n  }\n","date":"2016-12-04 08:58:15","endLine":69,"groupId":"2359","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/9d/bbf6d117246097326eecaf6c8ea4dc2c616259.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaMinHashLSHExample\")\n      .getOrCreate();\n\n    \r\n    List<Row> data = Arrays.asList(\n      RowFactory.create(0, Vectors.sparse(6, new int[]{0, 1, 2}, new double[]{1.0, 1.0, 1.0})),\n      RowFactory.create(1, Vectors.sparse(6, new int[]{2, 3, 4}, new double[]{1.0, 1.0, 1.0})),\n      RowFactory.create(2, Vectors.sparse(6, new int[]{0, 2, 4}, new double[]{1.0, 1.0, 1.0}))\n    );\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"id\", DataTypes.IntegerType, false, Metadata.empty()),\n      new StructField(\"keys\", new VectorUDT(), false, Metadata.empty())\n    });\n    Dataset<Row> dataFrame = spark.createDataFrame(data, schema);\n\n    MinHashLSH mh = new MinHashLSH()\n      .setNumHashTables(1)\n      .setInputCol(\"keys\")\n      .setOutputCol(\"values\");\n\n    MinHashLSHModel model = mh.fit(dataFrame);\n    model.transform(dataFrame).show();\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"B"},{"authorDate":"2016-12-04 08:58:15","commitOrder":1,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaBucketedRandomProjectionLSHExample\")\n      .getOrCreate();\n\n    \r\n    List<Row> dataA = Arrays.asList(\n      RowFactory.create(0, Vectors.dense(1.0, 1.0)),\n      RowFactory.create(1, Vectors.dense(1.0, -1.0)),\n      RowFactory.create(2, Vectors.dense(-1.0, -1.0)),\n      RowFactory.create(3, Vectors.dense(-1.0, 1.0))\n    );\n\n    List<Row> dataB = Arrays.asList(\n        RowFactory.create(4, Vectors.dense(1.0, 0.0)),\n        RowFactory.create(5, Vectors.dense(-1.0, 0.0)),\n        RowFactory.create(6, Vectors.dense(0.0, 1.0)),\n        RowFactory.create(7, Vectors.dense(0.0, -1.0))\n    );\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"id\", DataTypes.IntegerType, false, Metadata.empty()),\n      new StructField(\"keys\", new VectorUDT(), false, Metadata.empty())\n    });\n    Dataset<Row> dfA = spark.createDataFrame(dataA, schema);\n    Dataset<Row> dfB = spark.createDataFrame(dataB, schema);\n\n    Vector key = Vectors.dense(1.0, 0.0);\n\n    BucketedRandomProjectionLSH mh = new BucketedRandomProjectionLSH()\n      .setBucketLength(2.0)\n      .setNumHashTables(3)\n      .setInputCol(\"keys\")\n      .setOutputCol(\"values\");\n\n    BucketedRandomProjectionLSHModel model = mh.fit(dfA);\n\n    \r\n    model.transform(dfA).show();\n    \r\n    Dataset<Row> transformedA = model.transform(dfA).cache();\n    Dataset<Row> transformedB = model.transform(dfB).cache();\n\n    \r\n    model.approxSimilarityJoin(dfA, dfB, 1.5).show();\n    model.approxSimilarityJoin(transformedA, transformedB, 1.5).show();\n    \r\n    model.approxSimilarityJoin(dfA, dfA, 2.5).filter(\"datasetA.id < datasetB.id\").show();\n\n    \r\n    model.approxNearestNeighbors(dfA, key, 2).show();\n    model.approxNearestNeighbors(transformedA, key, 2).show();\n    \r\n\n    spark.stop();\n  }\n","date":"2016-12-04 08:58:15","endLine":97,"groupId":"2409","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/ca/3ee5a285255a329f99be8856de37b3d1841763.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaBucketedRandomProjectionLSHExample\")\n      .getOrCreate();\n\n    \r\n    List<Row> dataA = Arrays.asList(\n      RowFactory.create(0, Vectors.dense(1.0, 1.0)),\n      RowFactory.create(1, Vectors.dense(1.0, -1.0)),\n      RowFactory.create(2, Vectors.dense(-1.0, -1.0)),\n      RowFactory.create(3, Vectors.dense(-1.0, 1.0))\n    );\n\n    List<Row> dataB = Arrays.asList(\n        RowFactory.create(4, Vectors.dense(1.0, 0.0)),\n        RowFactory.create(5, Vectors.dense(-1.0, 0.0)),\n        RowFactory.create(6, Vectors.dense(0.0, 1.0)),\n        RowFactory.create(7, Vectors.dense(0.0, -1.0))\n    );\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"id\", DataTypes.IntegerType, false, Metadata.empty()),\n      new StructField(\"keys\", new VectorUDT(), false, Metadata.empty())\n    });\n    Dataset<Row> dfA = spark.createDataFrame(dataA, schema);\n    Dataset<Row> dfB = spark.createDataFrame(dataB, schema);\n\n    Vector key = Vectors.dense(1.0, 0.0);\n\n    BucketedRandomProjectionLSH mh = new BucketedRandomProjectionLSH()\n      .setBucketLength(2.0)\n      .setNumHashTables(3)\n      .setInputCol(\"keys\")\n      .setOutputCol(\"values\");\n\n    BucketedRandomProjectionLSHModel model = mh.fit(dfA);\n\n    \r\n    model.transform(dfA).show();\n    \r\n    Dataset<Row> transformedA = model.transform(dfA).cache();\n    Dataset<Row> transformedB = model.transform(dfB).cache();\n\n    \r\n    model.approxSimilarityJoin(dfA, dfB, 1.5).show();\n    model.approxSimilarityJoin(transformedA, transformedB, 1.5).show();\n    \r\n    model.approxSimilarityJoin(dfA, dfA, 2.5).filter(\"datasetA.id < datasetB.id\").show();\n\n    \r\n    model.approxNearestNeighbors(dfA, key, 2).show();\n    model.approxNearestNeighbors(transformedA, key, 2).show();\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":41,"status":"B"}],"commitId":"34777184cd8cab61e1dd25d0a4d5e738880a57b2","commitMessage":"@@@[SPARK-18081][ML][DOCS] Add user guide for Locality Sensitive Hashing(LSH)\n\n## What changes were proposed in this pull request?\nThe user guide for LSH is added to ml-features.md.  with several scala/java examples in spark-examples.\n\n## How was this patch tested?\nDoc has been generated through Jekyll.  and checked through manual inspection.\n\nAuthor: Yunni <Euler57721@gmail.com>\nAuthor: Yun Ni <yunn@uber.com>\nAuthor: Joseph K. Bradley <joseph@databricks.com>\nAuthor: Yun Ni <Euler57721@gmail.com>\n\nCloses #15795 from Yunni/SPARK-18081-lsh-guide.\n","date":"2016-12-04 08:58:15","modifiedFileCount":"0","status":"B","submitter":"Yunni"},{"authorTime":"2017-02-16 08:26:05","codes":[{"authorDate":"2017-02-16 08:26:05","commitOrder":2,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaMinHashLSHExample\")\n      .getOrCreate();\n\n    \r\n    List<Row> dataA = Arrays.asList(\n      RowFactory.create(0, Vectors.sparse(6, new int[]{0, 1, 2}, new double[]{1.0, 1.0, 1.0})),\n      RowFactory.create(1, Vectors.sparse(6, new int[]{2, 3, 4}, new double[]{1.0, 1.0, 1.0})),\n      RowFactory.create(2, Vectors.sparse(6, new int[]{0, 2, 4}, new double[]{1.0, 1.0, 1.0}))\n    );\n\n    List<Row> dataB = Arrays.asList(\n      RowFactory.create(0, Vectors.sparse(6, new int[]{1, 3, 5}, new double[]{1.0, 1.0, 1.0})),\n      RowFactory.create(1, Vectors.sparse(6, new int[]{2, 3, 5}, new double[]{1.0, 1.0, 1.0})),\n      RowFactory.create(2, Vectors.sparse(6, new int[]{1, 2, 4}, new double[]{1.0, 1.0, 1.0}))\n    );\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"id\", DataTypes.IntegerType, false, Metadata.empty()),\n      new StructField(\"features\", new VectorUDT(), false, Metadata.empty())\n    });\n    Dataset<Row> dfA = spark.createDataFrame(dataA, schema);\n    Dataset<Row> dfB = spark.createDataFrame(dataB, schema);\n\n    int[] indices = {1, 3};\n    double[] values = {1.0, 1.0};\n    Vector key = Vectors.sparse(6, indices, values);\n\n    MinHashLSH mh = new MinHashLSH()\n      .setNumHashTables(5)\n      .setInputCol(\"features\")\n      .setOutputCol(\"hashes\");\n\n    MinHashLSHModel model = mh.fit(dfA);\n\n    \r\n    System.out.println(\"The hashed dataset where hashed values are stored in the column 'hashes':\");\n    model.transform(dfA).show();\n\n    \r\n    \r\n    \r\n    \r\n    System.out.println(\"Approximately joining dfA and dfB on Jaccard distance smaller than 0.6:\");\n    model.approxSimilarityJoin(dfA, dfB, 0.6, \"JaccardDistance\")\n      .select(col(\"datasetA.id\").alias(\"idA\"),\n        col(\"datasetB.id\").alias(\"idB\"),\n        col(\"JaccardDistance\")).show();\n\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    System.out.println(\"Approximately searching dfA for 2 nearest neighbors of the key:\");\n    model.approxNearestNeighbors(dfA, key, 2).show();\n    \r\n\n    spark.stop();\n  }\n","date":"2017-02-16 08:26:05","endLine":110,"groupId":"10508","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/0a/ace46939257b9b0679b30be32e5703947f00c6.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaMinHashLSHExample\")\n      .getOrCreate();\n\n    \r\n    List<Row> data = Arrays.asList(\n      RowFactory.create(0, Vectors.sparse(6, new int[]{0, 1, 2}, new double[]{1.0, 1.0, 1.0})),\n      RowFactory.create(1, Vectors.sparse(6, new int[]{2, 3, 4}, new double[]{1.0, 1.0, 1.0})),\n      RowFactory.create(2, Vectors.sparse(6, new int[]{0, 2, 4}, new double[]{1.0, 1.0, 1.0}))\n    );\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"id\", DataTypes.IntegerType, false, Metadata.empty()),\n      new StructField(\"keys\", new VectorUDT(), false, Metadata.empty())\n    });\n    Dataset<Row> dataFrame = spark.createDataFrame(data, schema);\n\n    MinHashLSH mh = new MinHashLSH()\n      .setNumHashTables(1)\n      .setInputCol(\"keys\")\n      .setOutputCol(\"values\");\n\n    MinHashLSHModel model = mh.fit(dataFrame);\n    model.transform(dataFrame).show();\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":48,"status":"M"},{"authorDate":"2017-02-16 08:26:05","commitOrder":2,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaBucketedRandomProjectionLSHExample\")\n      .getOrCreate();\n\n    \r\n    List<Row> dataA = Arrays.asList(\n      RowFactory.create(0, Vectors.dense(1.0, 1.0)),\n      RowFactory.create(1, Vectors.dense(1.0, -1.0)),\n      RowFactory.create(2, Vectors.dense(-1.0, -1.0)),\n      RowFactory.create(3, Vectors.dense(-1.0, 1.0))\n    );\n\n    List<Row> dataB = Arrays.asList(\n        RowFactory.create(4, Vectors.dense(1.0, 0.0)),\n        RowFactory.create(5, Vectors.dense(-1.0, 0.0)),\n        RowFactory.create(6, Vectors.dense(0.0, 1.0)),\n        RowFactory.create(7, Vectors.dense(0.0, -1.0))\n    );\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"id\", DataTypes.IntegerType, false, Metadata.empty()),\n      new StructField(\"features\", new VectorUDT(), false, Metadata.empty())\n    });\n    Dataset<Row> dfA = spark.createDataFrame(dataA, schema);\n    Dataset<Row> dfB = spark.createDataFrame(dataB, schema);\n\n    Vector key = Vectors.dense(1.0, 0.0);\n\n    BucketedRandomProjectionLSH mh = new BucketedRandomProjectionLSH()\n      .setBucketLength(2.0)\n      .setNumHashTables(3)\n      .setInputCol(\"features\")\n      .setOutputCol(\"hashes\");\n\n    BucketedRandomProjectionLSHModel model = mh.fit(dfA);\n\n    \r\n    System.out.println(\"The hashed dataset where hashed values are stored in the column 'hashes':\");\n    model.transform(dfA).show();\n\n    \r\n    \r\n    \r\n    \r\n    System.out.println(\"Approximately joining dfA and dfB on distance smaller than 1.5:\");\n    model.approxSimilarityJoin(dfA, dfB, 1.5, \"EuclideanDistance\")\n      .select(col(\"datasetA.id\").alias(\"idA\"),\n        col(\"datasetB.id\").alias(\"idB\"),\n        col(\"EuclideanDistance\")).show();\n\n    \r\n    \r\n    \r\n    \r\n    System.out.println(\"Approximately searching dfA for 2 nearest neighbors of the key:\");\n    model.approxNearestNeighbors(dfA, key, 2).show();\n    \r\n\n    spark.stop();\n  }\n","date":"2017-02-16 08:26:05","endLine":109,"groupId":"10508","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/45/94e3462b2a5aed578850c797de06af9309cafa.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaBucketedRandomProjectionLSHExample\")\n      .getOrCreate();\n\n    \r\n    List<Row> dataA = Arrays.asList(\n      RowFactory.create(0, Vectors.dense(1.0, 1.0)),\n      RowFactory.create(1, Vectors.dense(1.0, -1.0)),\n      RowFactory.create(2, Vectors.dense(-1.0, -1.0)),\n      RowFactory.create(3, Vectors.dense(-1.0, 1.0))\n    );\n\n    List<Row> dataB = Arrays.asList(\n        RowFactory.create(4, Vectors.dense(1.0, 0.0)),\n        RowFactory.create(5, Vectors.dense(-1.0, 0.0)),\n        RowFactory.create(6, Vectors.dense(0.0, 1.0)),\n        RowFactory.create(7, Vectors.dense(0.0, -1.0))\n    );\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"id\", DataTypes.IntegerType, false, Metadata.empty()),\n      new StructField(\"keys\", new VectorUDT(), false, Metadata.empty())\n    });\n    Dataset<Row> dfA = spark.createDataFrame(dataA, schema);\n    Dataset<Row> dfB = spark.createDataFrame(dataB, schema);\n\n    Vector key = Vectors.dense(1.0, 0.0);\n\n    BucketedRandomProjectionLSH mh = new BucketedRandomProjectionLSH()\n      .setBucketLength(2.0)\n      .setNumHashTables(3)\n      .setInputCol(\"keys\")\n      .setOutputCol(\"values\");\n\n    BucketedRandomProjectionLSHModel model = mh.fit(dfA);\n\n    \r\n    model.transform(dfA).show();\n    \r\n    Dataset<Row> transformedA = model.transform(dfA).cache();\n    Dataset<Row> transformedB = model.transform(dfB).cache();\n\n    \r\n    model.approxSimilarityJoin(dfA, dfB, 1.5).show();\n    model.approxSimilarityJoin(transformedA, transformedB, 1.5).show();\n    \r\n    model.approxSimilarityJoin(dfA, dfA, 2.5).filter(\"datasetA.id < datasetB.id\").show();\n\n    \r\n    model.approxNearestNeighbors(dfA, key, 2).show();\n    model.approxNearestNeighbors(transformedA, key, 2).show();\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":48,"status":"M"}],"commitId":"08c1972a0661d42f300520cc6e5fb31023de093b","commitMessage":"@@@[SPARK-18080][ML][PYTHON] Python API & Examples for Locality Sensitive Hashing\n\n## What changes were proposed in this pull request?\nThis pull request includes python API and examples for LSH. The API changes was based on yanboliang 's PR #15768 and resolved conflicts and API changes on the Scala API. The examples are consistent with Scala examples of MinHashLSH and BucketedRandomProjectionLSH.\n\n## How was this patch tested?\nAPI and examples are tested using spark-submit:\n`bin/spark-submit examples/src/main/python/ml/min_hash_lsh.py`\n`bin/spark-submit examples/src/main/python/ml/bucketed_random_projection_lsh.py`\n\nUser guide changes are generated and manually inspected:\n`SKIP_API=1 jekyll build`\n\nAuthor: Yun Ni <yunn@uber.com>\nAuthor: Yanbo Liang <ybliang8@gmail.com>\nAuthor: Yunni <Euler57721@gmail.com>\n\nCloses #16715 from Yunni/spark-18080.\n","date":"2017-02-16 08:26:05","modifiedFileCount":"2","status":"M","submitter":"Yun Ni"}]
