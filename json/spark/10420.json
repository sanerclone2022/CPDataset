[{"authorTime":"2016-01-27 09:34:01","codes":[{"authorDate":"2016-01-13 10:21:04","commitOrder":2,"curCode":"  public final void putIntsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n    for (int i = 0; i < count; ++i) {\n      intData[i + rowId] = Platform.getInt(src, srcOffset);;\n      srcIndex += 4;\n      srcOffset += 4;\n    }\n  }\n","date":"2016-01-13 10:21:04","endLine":139,"groupId":"3058","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"putIntsLittleEndian","params":"(introwId@intcount@byte[]src@intsrcIndex)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/a7/b3addf11b143492a0d7552540a0990abe76cc7.src","preCode":"  public final void putIntsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n    for (int i = 0; i < count; ++i) {\n      intData[i + rowId] = Platform.getInt(src, srcOffset);;\n      srcIndex += 4;\n      srcOffset += 4;\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":132,"status":"NB"},{"authorDate":"2016-01-27 09:34:01","commitOrder":2,"curCode":"  public final void putLongsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n    for (int i = 0; i < count; ++i) {\n      longData[i + rowId] = Platform.getLong(src, srcOffset);\n      srcIndex += 8;\n      srcOffset += 8;\n    }\n  }\n","date":"2016-01-27 09:34:01","endLine":198,"groupId":"3058","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"putLongsLittleEndian","params":"(introwId@intcount@byte[]src@intsrcIndex)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/81/97fa11cd4c8632a709690c7f4cd60eb1bde7b5.src","preCode":"  public final void putLongsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n    for (int i = 0; i < count; ++i) {\n      longData[i + rowId] = Platform.getLong(src, srcOffset);\n      srcIndex += 8;\n      srcOffset += 8;\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":191,"status":"B"}],"commitId":"555127387accdd7c1cf236912941822ba8af0a52","commitMessage":"@@@[SPARK-12854][SQL] Implement complex types support in ColumnarBatch\n\nThis patch adds support for complex types for ColumnarBatch. ColumnarBatch supports structs\nand arrays. There is a simple mapping between the richer catalyst types to these two. Strings\nare treated as an array of bytes.\n\nColumnarBatch will contain a column for each node of the schema. Non-complex schemas consists\nof just leaf nodes. Structs represent an internal node with one child for each field. Arrays\nare internal nodes with one child. Structs just contain nullability. Arrays contain offsets\nand lengths into the child array. This structure is able to handle arbitrary nesting. It has\nthe key property that we maintain columnar throughout and that primitive types are only stored\nin the leaf nodes and contiguous across rows. For example.  if the schema is\n```\narray<array<int>>\n```\nThere are three columns in the schema. The internal nodes each have one children. The leaf node contains all the int data stored consecutively.\n\nAs part of this.  this patch adds append APIs in addition to the Put APIs (e.g. putLong(rowid.  v)\nvs appendLong(v)). These APIs are necessary when the batch contains variable length elements.\nThe vectors are not fixed length and will grow as necessary. This should make the usage a lot\nsimpler for the writer.\n\nAuthor: Nong Li <nong@databricks.com>\n\nCloses #10820 from nongli/spark-12854.\n","date":"2016-01-27 09:34:01","modifiedFileCount":"6","status":"M","submitter":"Nong Li"},{"authorTime":"2016-03-21 15:58:57","codes":[{"authorDate":"2016-03-21 15:58:57","commitOrder":3,"curCode":"  public void putIntsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n    for (int i = 0; i < count; ++i) {\n      intData[i + rowId] = Platform.getInt(src, srcOffset);;\n      srcIndex += 4;\n      srcOffset += 4;\n    }\n  }\n","date":"2016-03-21 15:58:57","endLine":219,"groupId":"3058","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"putIntsLittleEndian","params":"(introwId@intcount@byte[]src@intsrcIndex)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/5b/671a7432c4d938f0e0eabd1667cda887a74a70.src","preCode":"  public final void putIntsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n    for (int i = 0; i < count; ++i) {\n      intData[i + rowId] = Platform.getInt(src, srcOffset);;\n      srcIndex += 4;\n      srcOffset += 4;\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":212,"status":"M"},{"authorDate":"2016-03-21 15:58:57","commitOrder":3,"curCode":"  public void putLongsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n    for (int i = 0; i < count; ++i) {\n      longData[i + rowId] = Platform.getLong(src, srcOffset);\n      srcIndex += 8;\n      srcOffset += 8;\n    }\n  }\n","date":"2016-03-21 15:58:57","endLine":259,"groupId":"3058","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"putLongsLittleEndian","params":"(introwId@intcount@byte[]src@intsrcIndex)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/5b/671a7432c4d938f0e0eabd1667cda887a74a70.src","preCode":"  public final void putLongsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n    for (int i = 0; i < count; ++i) {\n      longData[i + rowId] = Platform.getLong(src, srcOffset);\n      srcIndex += 8;\n      srcOffset += 8;\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":252,"status":"M"}],"commitId":"20fd254101553cb5a4c932c8d03064899112bee6","commitMessage":"@@@[SPARK-14011][CORE][SQL] Enable `LineLength` Java checkstyle rule\n\n## What changes were proposed in this pull request?\n\n[Spark Coding Style Guide](https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide) has 100-character limit on lines.  but it's disabled for Java since 11/09/15. This PR enables **LineLength** checkstyle again. To help that.  this also introduces **RedundantImport** and **RedundantModifier**.  too. The following is the diff on `checkstyle.xml`.\n\n```xml\n-        <!-- TODO: 11/09/15 disabled - the lengths are currently > 100 in many places -->\n-        <!--\n         <module name=\"LineLength\">\n             <property name=\"max\" value=\"100\"/>\n             <property name=\"ignorePattern\" value=\"^package.*|^import.*|a href|href|http://|https://|ftp://\"/>\n         </module>\n-        -->\n         <module name=\"NoLineWrap\"/>\n         <module name=\"EmptyBlock\">\n             <property name=\"option\" value=\"TEXT\"/>\n -167. 5 +164. 7\n         </module>\n         <module name=\"CommentsIndentation\"/>\n         <module name=\"UnusedImports\"/>\n+        <module name=\"RedundantImport\"/>\n+        <module name=\"RedundantModifier\"/>\n```\n\n## How was this patch tested?\n\nCurrently.  `lint-java` is disabled in Jenkins. It needs a manual test.\nAfter passing the Jenkins tests.  `dev/lint-java` should passes locally.\n\nAuthor: Dongjoon Hyun <dongjoon@apache.org>\n\nCloses #11831 from dongjoon-hyun/SPARK-14011.\n","date":"2016-03-21 15:58:57","modifiedFileCount":"72","status":"M","submitter":"Dongjoon Hyun"},{"authorTime":"2016-03-21 15:58:57","codes":[{"authorDate":"2016-04-04 09:14:16","commitOrder":4,"curCode":"  public void putIntsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n    for (int i = 0; i < count; ++i) {\n      intData[i + rowId] = Platform.getInt(src, srcOffset);\n      srcIndex += 4;\n      srcOffset += 4;\n    }\n  }\n","date":"2016-04-04 09:14:16","endLine":219,"groupId":"3058","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"putIntsLittleEndian","params":"(introwId@intcount@byte[]src@intsrcIndex)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/70/8a00953abde6e0de129730647e6af61bf256e2.src","preCode":"  public void putIntsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n    for (int i = 0; i < count; ++i) {\n      intData[i + rowId] = Platform.getInt(src, srcOffset);;\n      srcIndex += 4;\n      srcOffset += 4;\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":212,"status":"M"},{"authorDate":"2016-03-21 15:58:57","commitOrder":4,"curCode":"  public void putLongsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n    for (int i = 0; i < count; ++i) {\n      longData[i + rowId] = Platform.getLong(src, srcOffset);\n      srcIndex += 8;\n      srcOffset += 8;\n    }\n  }\n","date":"2016-03-21 15:58:57","endLine":259,"groupId":"3058","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"putLongsLittleEndian","params":"(introwId@intcount@byte[]src@intsrcIndex)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/5b/671a7432c4d938f0e0eabd1667cda887a74a70.src","preCode":"  public void putLongsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n    for (int i = 0; i < count; ++i) {\n      longData[i + rowId] = Platform.getLong(src, srcOffset);\n      srcIndex += 8;\n      srcOffset += 8;\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":252,"status":"N"}],"commitId":"3f749f7ed443899d667c9e2b2a11bc595d6fc7f6","commitMessage":"@@@[SPARK-14355][BUILD] Fix typos in Exception/Testcase/Comments and static analysis results\n\n## What changes were proposed in this pull request?\n\nThis PR contains the following 5 types of maintenance fix over 59 files (+94 lines.  -93 lines).\n- Fix typos(exception/log strings.  testcase name.  comments) in 44 lines.\n- Fix lint-java errors (MaxLineLength) in 6 lines. (New codes after SPARK-14011)\n- Use diamond operators in 40 lines. (New codes after SPARK-13702)\n- Fix redundant semicolon in 5 lines.\n- Rename class `InferSchemaSuite` to `CSVInferSchemaSuite` in CSVInferSchemaSuite.scala.\n\n## How was this patch tested?\n\nManual and pass the Jenkins tests.\n\nAuthor: Dongjoon Hyun <dongjoon@apache.org>\n\nCloses #12139 from dongjoon-hyun/SPARK-14355.\n","date":"2016-04-04 09:14:16","modifiedFileCount":"26","status":"M","submitter":"Dongjoon Hyun"},{"authorTime":"2016-05-03 04:16:46","codes":[{"authorDate":"2016-05-03 04:16:46","commitOrder":5,"curCode":"  public void putIntsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n    for (int i = 0; i < count; ++i, srcOffset += 4) {\n      intData[i + rowId] = Platform.getInt(src, srcOffset);\n      if (bigEndianPlatform) {\n        intData[i + rowId] = java.lang.Integer.reverseBytes(intData[i + rowId]);\n      }\n    }\n  }\n","date":"2016-05-03 04:16:46","endLine":225,"groupId":"10420","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"putIntsLittleEndian","params":"(introwId@intcount@byte[]src@intsrcIndex)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/b1/ffe4c21049babc1edd5558d7000b79fca023a4.src","preCode":"  public void putIntsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n    for (int i = 0; i < count; ++i) {\n      intData[i + rowId] = Platform.getInt(src, srcOffset);\n      srcIndex += 4;\n      srcOffset += 4;\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":217,"status":"M"},{"authorDate":"2016-05-03 04:16:46","commitOrder":5,"curCode":"  public void putLongsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n    for (int i = 0; i < count; ++i, srcOffset += 8) {\n      longData[i + rowId] = Platform.getLong(src, srcOffset);\n      if (bigEndianPlatform) {\n        longData[i + rowId] = java.lang.Long.reverseBytes(longData[i + rowId]);\n      }\n    }\n  }\n","date":"2016-05-03 04:16:46","endLine":266,"groupId":"10420","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"putLongsLittleEndian","params":"(introwId@intcount@byte[]src@intsrcIndex)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/b1/ffe4c21049babc1edd5558d7000b79fca023a4.src","preCode":"  public void putLongsLittleEndian(int rowId, int count, byte[] src, int srcIndex) {\n    int srcOffset = srcIndex + Platform.BYTE_ARRAY_OFFSET;\n    for (int i = 0; i < count; ++i) {\n      longData[i + rowId] = Platform.getLong(src, srcOffset);\n      srcIndex += 8;\n      srcOffset += 8;\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/OnHeapColumnVector.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":258,"status":"M"}],"commitId":"8a1ce4899fb9f751dedaaa34ea654dfbc8330852","commitMessage":"@@@[SPARK-13745] [SQL] Support columnar in memory representation on Big Endian platforms\n\n## What changes were proposed in this pull request?\n\nparquet datasource and ColumnarBatch tests fail on big-endian platforms This patch adds support for the little-endian byte arrays being correctly interpreted on a big-endian platform\n\n## How was this patch tested?\n\nSpark test builds ran on big endian z/Linux and regression build on little endian amd64\n\nAuthor: Pete Robbins <robbinspg@gmail.com>\n\nCloses #12397 from robbinspg/master.\n","date":"2016-05-03 04:16:46","modifiedFileCount":"3","status":"M","submitter":"Pete Robbins"}]
