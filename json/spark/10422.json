[{"authorTime":"2016-01-16 09:40:26","codes":[{"authorDate":"2016-01-16 09:40:26","commitOrder":1,"curCode":"  private int readIntLittleEndian() {\n    int ch4 = in[offset] & 255;\n    int ch3 = in[offset + 1] & 255;\n    int ch2 = in[offset + 2] & 255;\n    int ch1 = in[offset + 3] & 255;\n    offset += 4;\n    return ((ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0));\n  }\n","date":"2016-01-16 09:40:26","endLine":208,"groupId":"3238","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"readIntLittleEndian","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/49/3ec9deed49971eb75b2e86c9ab9c0f05346a5a.src","preCode":"  private int readIntLittleEndian() {\n    int ch4 = in[offset] & 255;\n    int ch3 = in[offset + 1] & 255;\n    int ch2 = in[offset + 2] & 255;\n    int ch1 = in[offset + 3] & 255;\n    offset += 4;\n    return ((ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0));\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":201,"status":"B"},{"authorDate":"2016-01-16 09:40:26","commitOrder":1,"curCode":"  private int readIntLittleEndianPaddedOnBitWidth() {\n    switch (bytesWidth) {\n      case 0:\n        return 0;\n      case 1:\n        return in[offset++] & 255;\n      case 2: {\n        int ch2 = in[offset] & 255;\n        int ch1 = in[offset + 1] & 255;\n        offset += 2;\n        return (ch1 << 8) + ch2;\n      }\n      case 3: {\n        int ch3 = in[offset] & 255;\n        int ch2 = in[offset + 1] & 255;\n        int ch1 = in[offset + 2] & 255;\n        offset += 3;\n        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n      }\n      case 4: {\n        return readIntLittleEndian();\n      }\n    }\n    throw new RuntimeException(\"Unreachable\");\n  }\n","date":"2016-01-16 09:40:26","endLine":237,"groupId":"3239","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"readIntLittleEndianPaddedOnBitWidth","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/49/3ec9deed49971eb75b2e86c9ab9c0f05346a5a.src","preCode":"  private int readIntLittleEndianPaddedOnBitWidth() {\n    switch (bytesWidth) {\n      case 0:\n        return 0;\n      case 1:\n        return in[offset++] & 255;\n      case 2: {\n        int ch2 = in[offset] & 255;\n        int ch1 = in[offset + 1] & 255;\n        offset += 2;\n        return (ch1 << 8) + ch2;\n      }\n      case 3: {\n        int ch3 = in[offset] & 255;\n        int ch2 = in[offset + 1] & 255;\n        int ch1 = in[offset + 2] & 255;\n        offset += 3;\n        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n      }\n      case 4: {\n        return readIntLittleEndian();\n      }\n    }\n    throw new RuntimeException(\"Unreachable\");\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":213,"status":"B"}],"commitId":"9039333c0a0ce4bea32f012b81c1e82e31246fc1","commitMessage":"@@@[SPARK-12644][SQL] Update parquet reader to be vectorized.\n\nThis inlines a few of the Parquet decoders and adds vectorized APIs to support decoding in batch.\nThere are a few particulars in the Parquet encodings that make this much more efficient. In\nparticular.  RLE encodings are very well suited for batch decoding. The Parquet 2.0 encodings are\nalso very suited for this.\n\nThis is a work in progress and does not affect the current execution. In subsequent patches.  we will\nsupport more encodings and types before enabling this.\n\nSimple benchmarks indicate this can decode single ints about > 3x faster.\n\nAuthor: Nong Li <nong@databricks.com>\nAuthor: Nong <nongli@gmail.com>\n\nCloses #10593 from nongli/spark-12644.\n","date":"2016-01-16 09:40:26","modifiedFileCount":"5","status":"B","submitter":"Nong Li"},{"authorTime":"2018-05-09 12:27:32","codes":[{"authorDate":"2018-05-09 12:27:32","commitOrder":2,"curCode":"  private int readIntLittleEndian() throws IOException {\n    int ch4 = in.read();\n    int ch3 = in.read();\n    int ch2 = in.read();\n    int ch1 = in.read();\n    return ((ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0));\n  }\n","date":"2018-05-09 12:27:32","endLine":575,"groupId":"1741","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"readIntLittleEndian","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/fe/3d31ae8e7467dccf5882a524a204f550ea140e.src","preCode":"  private int readIntLittleEndian() {\n    int ch4 = in[offset] & 255;\n    int ch3 = in[offset + 1] & 255;\n    int ch2 = in[offset + 2] & 255;\n    int ch1 = in[offset + 3] & 255;\n    offset += 4;\n    return ((ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0));\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":569,"status":"M"},{"authorDate":"2018-05-09 12:27:32","commitOrder":2,"curCode":"  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n    switch (bytesWidth) {\n      case 0:\n        return 0;\n      case 1:\n        return in.read();\n      case 2: {\n        int ch2 = in.read();\n        int ch1 = in.read();\n        return (ch1 << 8) + ch2;\n      }\n      case 3: {\n        int ch3 = in.read();\n        int ch2 = in.read();\n        int ch1 = in.read();\n        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n      }\n      case 4: {\n        return readIntLittleEndian();\n      }\n    }\n    throw new RuntimeException(\"Unreachable\");\n  }\n","date":"2018-05-09 12:27:32","endLine":602,"groupId":"1741","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"readIntLittleEndianPaddedOnBitWidth","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/fe/3d31ae8e7467dccf5882a524a204f550ea140e.src","preCode":"  private int readIntLittleEndianPaddedOnBitWidth() {\n    switch (bytesWidth) {\n      case 0:\n        return 0;\n      case 1:\n        return in[offset++] & 255;\n      case 2: {\n        int ch2 = in[offset] & 255;\n        int ch1 = in[offset + 1] & 255;\n        offset += 2;\n        return (ch1 << 8) + ch2;\n      }\n      case 3: {\n        int ch3 = in[offset] & 255;\n        int ch2 = in[offset + 1] & 255;\n        int ch1 = in[offset + 2] & 255;\n        offset += 3;\n        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n      }\n      case 4: {\n        return readIntLittleEndian();\n      }\n    }\n    throw new RuntimeException(\"Unreachable\");\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":580,"status":"M"}],"commitId":"cac9b1dea1bb44fa42abf77829c05bf93f70cf20","commitMessage":"@@@[SPARK-23972][BUILD][SQL] Update Parquet to 1.10.0.\n\n## What changes were proposed in this pull request?\n\nThis updates Parquet to 1.10.0 and updates the vectorized path for buffer management changes. Parquet 1.10.0 uses ByteBufferInputStream instead of byte arrays in encoders. This allows Parquet to break allocations into smaller chunks that are better for garbage collection.\n\n## How was this patch tested?\n\nExisting Parquet tests. Running in production at Netflix for about 3 months.\n\nAuthor: Ryan Blue <blue@apache.org>\n\nCloses #21070 from rdblue/SPARK-23972-update-parquet-to-1.10.0.\n","date":"2018-05-09 12:27:32","modifiedFileCount":"4","status":"M","submitter":"Ryan Blue"},{"authorTime":"2018-05-18 15:32:29","codes":[{"authorDate":"2018-05-18 15:32:29","commitOrder":3,"curCode":"  private int readIntLittleEndian() throws IOException {\n    int ch4 = in.read();\n    int ch3 = in.read();\n    int ch2 = in.read();\n    int ch1 = in.read();\n    return ((ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4));\n  }\n","date":"2018-05-18 15:32:29","endLine":575,"groupId":"1741","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"readIntLittleEndian","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/de/0d65a1e09060d3673a7cf4ca85c630dccd52dc.src","preCode":"  private int readIntLittleEndian() throws IOException {\n    int ch4 = in.read();\n    int ch3 = in.read();\n    int ch2 = in.read();\n    int ch1 = in.read();\n    return ((ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0));\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":569,"status":"M"},{"authorDate":"2018-05-18 15:32:29","commitOrder":3,"curCode":"  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n    switch (bytesWidth) {\n      case 0:\n        return 0;\n      case 1:\n        return in.read();\n      case 2: {\n        int ch2 = in.read();\n        int ch1 = in.read();\n        return (ch1 << 8) + ch2;\n      }\n      case 3: {\n        int ch3 = in.read();\n        int ch2 = in.read();\n        int ch1 = in.read();\n        return (ch1 << 16) + (ch2 << 8) + (ch3);\n      }\n      case 4: {\n        return readIntLittleEndian();\n      }\n    }\n    throw new RuntimeException(\"Unreachable\");\n  }\n","date":"2018-05-18 15:32:29","endLine":602,"groupId":"1741","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"readIntLittleEndianPaddedOnBitWidth","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/de/0d65a1e09060d3673a7cf4ca85c630dccd52dc.src","preCode":"  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n    switch (bytesWidth) {\n      case 0:\n        return 0;\n      case 1:\n        return in.read();\n      case 2: {\n        int ch2 = in.read();\n        int ch1 = in.read();\n        return (ch1 << 8) + ch2;\n      }\n      case 3: {\n        int ch3 = in.read();\n        int ch2 = in.read();\n        int ch1 = in.read();\n        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n      }\n      case 4: {\n        return readIntLittleEndian();\n      }\n    }\n    throw new RuntimeException(\"Unreachable\");\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":580,"status":"M"}],"commitId":"7b2dca5b12164b787ec4e8e7e9f92c60a3f9563e","commitMessage":"@@@[SPARK-24277][SQL] Code clean up in SQL module: HadoopMapReduceCommitProtocol\n\n## What changes were proposed in this pull request?\n\nIn HadoopMapReduceCommitProtocol and FileFormatWriter.  there are unnecessary settings in hadoop configuration.\n\nAlso clean up some code in SQL module.\n\n## How was this patch tested?\n\nUnit test\n\nAuthor: Gengliang Wang <gengliang.wang@databricks.com>\n\nCloses #21329 from gengliangwang/codeCleanWrite.\n","date":"2018-05-18 15:32:29","modifiedFileCount":"2","status":"M","submitter":"Gengliang Wang"},{"authorTime":"2018-05-19 03:51:09","codes":[{"authorDate":"2018-05-19 03:51:09","commitOrder":4,"curCode":"  private int readIntLittleEndian() throws IOException {\n    int ch4 = in.read();\n    int ch3 = in.read();\n    int ch2 = in.read();\n    int ch1 = in.read();\n    return ((ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0));\n  }\n","date":"2018-05-19 03:51:09","endLine":575,"groupId":"10422","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"readIntLittleEndian","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/fe/3d31ae8e7467dccf5882a524a204f550ea140e.src","preCode":"  private int readIntLittleEndian() throws IOException {\n    int ch4 = in.read();\n    int ch3 = in.read();\n    int ch2 = in.read();\n    int ch1 = in.read();\n    return ((ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4));\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":569,"status":"M"},{"authorDate":"2018-05-19 03:51:09","commitOrder":4,"curCode":"  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n    switch (bytesWidth) {\n      case 0:\n        return 0;\n      case 1:\n        return in.read();\n      case 2: {\n        int ch2 = in.read();\n        int ch1 = in.read();\n        return (ch1 << 8) + ch2;\n      }\n      case 3: {\n        int ch3 = in.read();\n        int ch2 = in.read();\n        int ch1 = in.read();\n        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n      }\n      case 4: {\n        return readIntLittleEndian();\n      }\n    }\n    throw new RuntimeException(\"Unreachable\");\n  }\n","date":"2018-05-19 03:51:09","endLine":602,"groupId":"10422","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"readIntLittleEndianPaddedOnBitWidth","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/fe/3d31ae8e7467dccf5882a524a204f550ea140e.src","preCode":"  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n    switch (bytesWidth) {\n      case 0:\n        return 0;\n      case 1:\n        return in.read();\n      case 2: {\n        int ch2 = in.read();\n        int ch1 = in.read();\n        return (ch1 << 8) + ch2;\n      }\n      case 3: {\n        int ch3 = in.read();\n        int ch2 = in.read();\n        int ch1 = in.read();\n        return (ch1 << 16) + (ch2 << 8) + (ch3);\n      }\n      case 4: {\n        return readIntLittleEndian();\n      }\n    }\n    throw new RuntimeException(\"Unreachable\");\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedRleValuesReader.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":580,"status":"M"}],"commitId":"1c4553d67de8089e8aa84bc736faa11f21615a6a","commitMessage":"@@@Revert \"[SPARK-24277][SQL] Code clean up in SQL module: HadoopMapReduceCommitProtocol\"\n\nThis reverts commit 7b2dca5b12164b787ec4e8e7e9f92c60a3f9563e.\n","date":"2018-05-19 03:51:09","modifiedFileCount":"2","status":"M","submitter":"gatorsmile"}]
