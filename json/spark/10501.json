[{"authorTime":"2016-02-23 09:16:56","codes":[{"authorDate":"2016-02-23 09:16:56","commitOrder":1,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"PCA Example\");\n    SparkContext sc = new SparkContext(conf);\n\n    \r\n    double[][] array = {{1.12, 2.05, 3.12}, {5.56, 6.28, 8.94}, {10.2, 8.0, 20.5}};\n    LinkedList<Vector> rowsList = new LinkedList<Vector>();\n    for (int i = 0; i < array.length; i++) {\n      Vector currentRow = Vectors.dense(array[i]);\n      rowsList.add(currentRow);\n    }\n    JavaRDD<Vector> rows = JavaSparkContext.fromSparkContext(sc).parallelize(rowsList);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    Matrix pc = mat.computePrincipalComponents(3);\n    RowMatrix projected = mat.multiply(pc);\n    \r\n    Vector[] collectPartitions = (Vector[])projected.rows().collect();\n    System.out.println(\"Projected vector of principal component:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n  }\n","date":"2016-02-23 09:16:56","endLine":64,"groupId":"3034","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/fa/f76a9540e774e9e316b4c4207e59e1ba574177.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"PCA Example\");\n    SparkContext sc = new SparkContext(conf);\n\n    \r\n    double[][] array = {{1.12, 2.05, 3.12}, {5.56, 6.28, 8.94}, {10.2, 8.0, 20.5}};\n    LinkedList<Vector> rowsList = new LinkedList<Vector>();\n    for (int i = 0; i < array.length; i++) {\n      Vector currentRow = Vectors.dense(array[i]);\n      rowsList.add(currentRow);\n    }\n    JavaRDD<Vector> rows = JavaSparkContext.fromSparkContext(sc).parallelize(rowsList);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    Matrix pc = mat.computePrincipalComponents(3);\n    RowMatrix projected = mat.multiply(pc);\n    \r\n    Vector[] collectPartitions = (Vector[])projected.rows().collect();\n    System.out.println(\"Projected vector of principal component:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":39,"status":"B"},{"authorDate":"2016-02-23 09:16:56","commitOrder":1,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"SVD Example\");\n    SparkContext sc = new SparkContext(conf);\n\n    \r\n    double[][] array = {{1.12, 2.05, 3.12}, {5.56, 6.28, 8.94}, {10.2, 8.0, 20.5}};\n    LinkedList<Vector> rowsList = new LinkedList<Vector>();\n    for (int i = 0; i < array.length; i++) {\n      Vector currentRow = Vectors.dense(array[i]);\n      rowsList.add(currentRow);\n    }\n    JavaRDD<Vector> rows = JavaSparkContext.fromSparkContext(sc).parallelize(rowsList);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    SingularValueDecomposition<RowMatrix, Matrix> svd = mat.computeSVD(3, true, 1.0E-9d);\n    RowMatrix U = svd.U();\n    Vector s = svd.s();\n    Matrix V = svd.V();\n    \r\n    Vector[] collectPartitions = (Vector[]) U.rows().collect();\n    System.out.println(\"U factor is:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n    System.out.println(\"Singular values are: \" + s);\n    System.out.println(\"V factor is:\\n\" + V);\n  }\n","date":"2016-02-23 09:16:56","endLine":69,"groupId":"3034","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/f3/685db9f2fb2a90b612afb22f2dc39a87889c86.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"SVD Example\");\n    SparkContext sc = new SparkContext(conf);\n\n    \r\n    double[][] array = {{1.12, 2.05, 3.12}, {5.56, 6.28, 8.94}, {10.2, 8.0, 20.5}};\n    LinkedList<Vector> rowsList = new LinkedList<Vector>();\n    for (int i = 0; i < array.length; i++) {\n      Vector currentRow = Vectors.dense(array[i]);\n      rowsList.add(currentRow);\n    }\n    JavaRDD<Vector> rows = JavaSparkContext.fromSparkContext(sc).parallelize(rowsList);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    SingularValueDecomposition<RowMatrix, Matrix> svd = mat.computeSVD(3, true, 1.0E-9d);\n    RowMatrix U = svd.U();\n    Vector s = svd.s();\n    Matrix V = svd.V();\n    \r\n    Vector[] collectPartitions = (Vector[]) U.rows().collect();\n    System.out.println(\"U factor is:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n    System.out.println(\"Singular values are: \" + s);\n    System.out.println(\"V factor is:\\n\" + V);\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"B"}],"commitId":"9f410871ca03f4c04bd965b2e4f80167ce543139","commitMessage":"@@@[SPARK-13016][DOCUMENTATION] Replace example code in mllib-dimensionality-reduction.md using include_example\n\nReplaced example example code in mllib-dimensionality-reduction.md using\ninclude_example\n\nAuthor: Devaraj K <devaraj@apache.org>\n\nCloses #11132 from devaraj-kavali/SPARK-13016.\n","date":"2016-02-23 09:16:56","modifiedFileCount":"0","status":"B","submitter":"Devaraj K"},{"authorTime":"2016-03-09 18:12:23","codes":[{"authorDate":"2016-02-23 09:16:56","commitOrder":2,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"PCA Example\");\n    SparkContext sc = new SparkContext(conf);\n\n    \r\n    double[][] array = {{1.12, 2.05, 3.12}, {5.56, 6.28, 8.94}, {10.2, 8.0, 20.5}};\n    LinkedList<Vector> rowsList = new LinkedList<Vector>();\n    for (int i = 0; i < array.length; i++) {\n      Vector currentRow = Vectors.dense(array[i]);\n      rowsList.add(currentRow);\n    }\n    JavaRDD<Vector> rows = JavaSparkContext.fromSparkContext(sc).parallelize(rowsList);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    Matrix pc = mat.computePrincipalComponents(3);\n    RowMatrix projected = mat.multiply(pc);\n    \r\n    Vector[] collectPartitions = (Vector[])projected.rows().collect();\n    System.out.println(\"Projected vector of principal component:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n  }\n","date":"2016-02-23 09:16:56","endLine":64,"groupId":"3034","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/fa/f76a9540e774e9e316b4c4207e59e1ba574177.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"PCA Example\");\n    SparkContext sc = new SparkContext(conf);\n\n    \r\n    double[][] array = {{1.12, 2.05, 3.12}, {5.56, 6.28, 8.94}, {10.2, 8.0, 20.5}};\n    LinkedList<Vector> rowsList = new LinkedList<Vector>();\n    for (int i = 0; i < array.length; i++) {\n      Vector currentRow = Vectors.dense(array[i]);\n      rowsList.add(currentRow);\n    }\n    JavaRDD<Vector> rows = JavaSparkContext.fromSparkContext(sc).parallelize(rowsList);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    Matrix pc = mat.computePrincipalComponents(3);\n    RowMatrix projected = mat.multiply(pc);\n    \r\n    Vector[] collectPartitions = (Vector[])projected.rows().collect();\n    System.out.println(\"Projected vector of principal component:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":39,"status":"N"},{"authorDate":"2016-03-09 18:12:23","commitOrder":2,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"SVD Example\");\n    SparkContext sc = new SparkContext(conf);\n    JavaSparkContext jsc = JavaSparkContext.fromSparkContext(sc);\n\n    \r\n    double[][] array = {{1.12, 2.05, 3.12}, {5.56, 6.28, 8.94}, {10.2, 8.0, 20.5}};\n    LinkedList<Vector> rowsList = new LinkedList<Vector>();\n    for (int i = 0; i < array.length; i++) {\n      Vector currentRow = Vectors.dense(array[i]);\n      rowsList.add(currentRow);\n    }\n    JavaRDD<Vector> rows = jsc.parallelize(rowsList);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    SingularValueDecomposition<RowMatrix, Matrix> svd = mat.computeSVD(3, true, 1.0E-9d);\n    RowMatrix U = svd.U();\n    Vector s = svd.s();\n    Matrix V = svd.V();\n    \r\n    Vector[] collectPartitions = (Vector[]) U.rows().collect();\n    System.out.println(\"U factor is:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n    System.out.println(\"Singular values are: \" + s);\n    System.out.println(\"V factor is:\\n\" + V);\n\n    jsc.stop();\n  }\n","date":"2016-03-09 18:12:23","endLine":72,"groupId":"1692","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/b4/17da8f85cf543126f64d0eb3462300199d1f84.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"SVD Example\");\n    SparkContext sc = new SparkContext(conf);\n\n    \r\n    double[][] array = {{1.12, 2.05, 3.12}, {5.56, 6.28, 8.94}, {10.2, 8.0, 20.5}};\n    LinkedList<Vector> rowsList = new LinkedList<Vector>();\n    for (int i = 0; i < array.length; i++) {\n      Vector currentRow = Vectors.dense(array[i]);\n      rowsList.add(currentRow);\n    }\n    JavaRDD<Vector> rows = JavaSparkContext.fromSparkContext(sc).parallelize(rowsList);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    SingularValueDecomposition<RowMatrix, Matrix> svd = mat.computeSVD(3, true, 1.0E-9d);\n    RowMatrix U = svd.U();\n    Vector s = svd.s();\n    Matrix V = svd.V();\n    \r\n    Vector[] collectPartitions = (Vector[]) U.rows().collect();\n    System.out.println(\"U factor is:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n    System.out.println(\"Singular values are: \" + s);\n    System.out.println(\"V factor is:\\n\" + V);\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"M"}],"commitId":"f3201aeeb06aae3b11e8cf6ee9693182dd896b32","commitMessage":"@@@[SPARK-13692][CORE][SQL] Fix trivial Coverity/Checkstyle defects\n\n## What changes were proposed in this pull request?\n\nThis issue fixes the following potential bugs and Java coding style detected by Coverity and Checkstyle.\n\n- Implement both null and type checking in equals functions.\n- Fix wrong type casting logic in SimpleJavaBean2.equals.\n- Add `implement Cloneable` to `UTF8String` and `SortedIterator`.\n- Remove dereferencing before null check in `AbstractBytesToBytesMapSuite`.\n- Fix coding style: Add '{}' to single `for` statement in mllib examples.\n- Remove unused imports in `ColumnarBatch` and `JavaKinesisStreamSuite`.\n- Remove unused fields in `ChunkFetchIntegrationSuite`.\n- Add `stop()` to prevent resource leak.\n\nPlease note that the last two checkstyle errors exist on newly added commits after [SPARK-13583](https://issues.apache.org/jira/browse/SPARK-13583).\n\n## How was this patch tested?\n\nmanual via `./dev/lint-java` and Coverity site.\n\nAuthor: Dongjoon Hyun <dongjoon@apache.org>\n\nCloses #11530 from dongjoon-hyun/SPARK-13692.\n","date":"2016-03-09 18:12:23","modifiedFileCount":"31","status":"M","submitter":"Dongjoon Hyun"},{"authorTime":"2016-03-09 18:31:26","codes":[{"authorDate":"2016-03-09 18:31:26","commitOrder":3,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"PCA Example\");\n    SparkContext sc = new SparkContext(conf);\n\n    \r\n    double[][] array = {{1.12, 2.05, 3.12}, {5.56, 6.28, 8.94}, {10.2, 8.0, 20.5}};\n    LinkedList<Vector> rowsList = new LinkedList<>();\n    for (int i = 0; i < array.length; i++) {\n      Vector currentRow = Vectors.dense(array[i]);\n      rowsList.add(currentRow);\n    }\n    JavaRDD<Vector> rows = JavaSparkContext.fromSparkContext(sc).parallelize(rowsList);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    Matrix pc = mat.computePrincipalComponents(3);\n    RowMatrix projected = mat.multiply(pc);\n    \r\n    Vector[] collectPartitions = (Vector[])projected.rows().collect();\n    System.out.println(\"Projected vector of principal component:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n  }\n","date":"2016-03-09 18:31:26","endLine":64,"groupId":"691","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/a4/2c29f52fb65de09a010e5fc22b303c78a0ddfb.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"PCA Example\");\n    SparkContext sc = new SparkContext(conf);\n\n    \r\n    double[][] array = {{1.12, 2.05, 3.12}, {5.56, 6.28, 8.94}, {10.2, 8.0, 20.5}};\n    LinkedList<Vector> rowsList = new LinkedList<Vector>();\n    for (int i = 0; i < array.length; i++) {\n      Vector currentRow = Vectors.dense(array[i]);\n      rowsList.add(currentRow);\n    }\n    JavaRDD<Vector> rows = JavaSparkContext.fromSparkContext(sc).parallelize(rowsList);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    Matrix pc = mat.computePrincipalComponents(3);\n    RowMatrix projected = mat.multiply(pc);\n    \r\n    Vector[] collectPartitions = (Vector[])projected.rows().collect();\n    System.out.println(\"Projected vector of principal component:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":39,"status":"M"},{"authorDate":"2016-03-09 18:31:26","commitOrder":3,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"SVD Example\");\n    SparkContext sc = new SparkContext(conf);\n    JavaSparkContext jsc = JavaSparkContext.fromSparkContext(sc);\n\n    \r\n    double[][] array = {{1.12, 2.05, 3.12}, {5.56, 6.28, 8.94}, {10.2, 8.0, 20.5}};\n    LinkedList<Vector> rowsList = new LinkedList<>();\n    for (int i = 0; i < array.length; i++) {\n      Vector currentRow = Vectors.dense(array[i]);\n      rowsList.add(currentRow);\n    }\n    JavaRDD<Vector> rows = jsc.parallelize(rowsList);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    SingularValueDecomposition<RowMatrix, Matrix> svd = mat.computeSVD(3, true, 1.0E-9d);\n    RowMatrix U = svd.U();\n    Vector s = svd.s();\n    Matrix V = svd.V();\n    \r\n    Vector[] collectPartitions = (Vector[]) U.rows().collect();\n    System.out.println(\"U factor is:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n    System.out.println(\"Singular values are: \" + s);\n    System.out.println(\"V factor is:\\n\" + V);\n\n    jsc.stop();\n  }\n","date":"2016-03-09 18:31:26","endLine":72,"groupId":"1692","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/37/30e60f68803876fa60bd7a6b6fa813cc2a164b.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"SVD Example\");\n    SparkContext sc = new SparkContext(conf);\n    JavaSparkContext jsc = JavaSparkContext.fromSparkContext(sc);\n\n    \r\n    double[][] array = {{1.12, 2.05, 3.12}, {5.56, 6.28, 8.94}, {10.2, 8.0, 20.5}};\n    LinkedList<Vector> rowsList = new LinkedList<Vector>();\n    for (int i = 0; i < array.length; i++) {\n      Vector currentRow = Vectors.dense(array[i]);\n      rowsList.add(currentRow);\n    }\n    JavaRDD<Vector> rows = jsc.parallelize(rowsList);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    SingularValueDecomposition<RowMatrix, Matrix> svd = mat.computeSVD(3, true, 1.0E-9d);\n    RowMatrix U = svd.U();\n    Vector s = svd.s();\n    Matrix V = svd.V();\n    \r\n    Vector[] collectPartitions = (Vector[]) U.rows().collect();\n    System.out.println(\"U factor is:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n    System.out.println(\"Singular values are: \" + s);\n    System.out.println(\"V factor is:\\n\" + V);\n\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"M"}],"commitId":"c3689bc24e03a9471cd6e8169da61963c4528252","commitMessage":"@@@[SPARK-13702][CORE][SQL][MLLIB] Use diamond operator for generic instance creation in Java code.\n\n## What changes were proposed in this pull request?\n\nIn order to make `docs/examples` (and other related code) more simple/readable/user-friendly.  this PR replaces existing codes like the followings by using `diamond` operator.\n\n```\n-    final ArrayList<Product2<Object.  Object>> dataToWrite =\n-      new ArrayList<Product2<Object.  Object>>();\n+    final ArrayList<Product2<Object.  Object>> dataToWrite = new ArrayList<>();\n```\n\nJava 7 or higher supports **diamond** operator which replaces the type arguments required to invoke the constructor of a generic class with an empty set of type parameters (<>). Currently.  Spark Java code use mixed usage of this.\n\n## How was this patch tested?\n\nManual.\nPass the existing tests.\n\nAuthor: Dongjoon Hyun <dongjoon@apache.org>\n\nCloses #11541 from dongjoon-hyun/SPARK-13702.\n","date":"2016-03-09 18:31:26","modifiedFileCount":"57","status":"M","submitter":"Dongjoon Hyun"},{"authorTime":"2016-03-09 18:31:26","codes":[{"authorDate":"2017-01-03 17:56:42","commitOrder":4,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"PCA Example\");\n    SparkContext sc = new SparkContext(conf);\n\n    \r\n    double[][] array = {{1.12, 2.05, 3.12}, {5.56, 6.28, 8.94}, {10.2, 8.0, 20.5}};\n    LinkedList<Vector> rowsList = new LinkedList<>();\n    for (int i = 0; i < array.length; i++) {\n      Vector currentRow = Vectors.dense(array[i]);\n      rowsList.add(currentRow);\n    }\n    JavaRDD<Vector> rows = JavaSparkContext.fromSparkContext(sc).parallelize(rowsList);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    Matrix pc = mat.computePrincipalComponents(3);\n    RowMatrix projected = mat.multiply(pc);\n    \r\n    Vector[] collectPartitions = (Vector[])projected.rows().collect();\n    System.out.println(\"Projected vector of principal component:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n    sc.stop();\n  }\n","date":"2017-01-03 17:56:42","endLine":65,"groupId":"691","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/30/77f557ef8867808c1d9a80740a9ab9887b9544.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"PCA Example\");\n    SparkContext sc = new SparkContext(conf);\n\n    \r\n    double[][] array = {{1.12, 2.05, 3.12}, {5.56, 6.28, 8.94}, {10.2, 8.0, 20.5}};\n    LinkedList<Vector> rowsList = new LinkedList<>();\n    for (int i = 0; i < array.length; i++) {\n      Vector currentRow = Vectors.dense(array[i]);\n      rowsList.add(currentRow);\n    }\n    JavaRDD<Vector> rows = JavaSparkContext.fromSparkContext(sc).parallelize(rowsList);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    Matrix pc = mat.computePrincipalComponents(3);\n    RowMatrix projected = mat.multiply(pc);\n    \r\n    Vector[] collectPartitions = (Vector[])projected.rows().collect();\n    System.out.println(\"Projected vector of principal component:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":39,"status":"M"},{"authorDate":"2016-03-09 18:31:26","commitOrder":4,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"SVD Example\");\n    SparkContext sc = new SparkContext(conf);\n    JavaSparkContext jsc = JavaSparkContext.fromSparkContext(sc);\n\n    \r\n    double[][] array = {{1.12, 2.05, 3.12}, {5.56, 6.28, 8.94}, {10.2, 8.0, 20.5}};\n    LinkedList<Vector> rowsList = new LinkedList<>();\n    for (int i = 0; i < array.length; i++) {\n      Vector currentRow = Vectors.dense(array[i]);\n      rowsList.add(currentRow);\n    }\n    JavaRDD<Vector> rows = jsc.parallelize(rowsList);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    SingularValueDecomposition<RowMatrix, Matrix> svd = mat.computeSVD(3, true, 1.0E-9d);\n    RowMatrix U = svd.U();\n    Vector s = svd.s();\n    Matrix V = svd.V();\n    \r\n    Vector[] collectPartitions = (Vector[]) U.rows().collect();\n    System.out.println(\"U factor is:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n    System.out.println(\"Singular values are: \" + s);\n    System.out.println(\"V factor is:\\n\" + V);\n\n    jsc.stop();\n  }\n","date":"2016-03-09 18:31:26","endLine":72,"groupId":"1692","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/37/30e60f68803876fa60bd7a6b6fa813cc2a164b.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"SVD Example\");\n    SparkContext sc = new SparkContext(conf);\n    JavaSparkContext jsc = JavaSparkContext.fromSparkContext(sc);\n\n    \r\n    double[][] array = {{1.12, 2.05, 3.12}, {5.56, 6.28, 8.94}, {10.2, 8.0, 20.5}};\n    LinkedList<Vector> rowsList = new LinkedList<>();\n    for (int i = 0; i < array.length; i++) {\n      Vector currentRow = Vectors.dense(array[i]);\n      rowsList.add(currentRow);\n    }\n    JavaRDD<Vector> rows = jsc.parallelize(rowsList);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    SingularValueDecomposition<RowMatrix, Matrix> svd = mat.computeSVD(3, true, 1.0E-9d);\n    RowMatrix U = svd.U();\n    Vector s = svd.s();\n    Matrix V = svd.V();\n    \r\n    Vector[] collectPartitions = (Vector[]) U.rows().collect();\n    System.out.println(\"U factor is:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n    System.out.println(\"Singular values are: \" + s);\n    System.out.println(\"V factor is:\\n\" + V);\n\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"N"}],"commitId":"e5c307c50a660f706799f1f7f6890bcec888d96b","commitMessage":"@@@[MINOR] Add missing sc.stop() to end of examples\n\n## What changes were proposed in this pull request?\n\nAdd `finally` clause for `sc.stop()` in the `test(\"register and deregister Spark listener from SparkContext\")`.\n\n## How was this patch tested?\nPass the build and unit tests.\n\nAuthor: Weiqing Yang <yangweiqing001@gmail.com>\n\nCloses #16426 from weiqingy/testIssue.\n","date":"2017-01-03 17:56:42","modifiedFileCount":"4","status":"M","submitter":"Weiqing Yang"},{"authorTime":"2017-05-03 16:58:05","codes":[{"authorDate":"2017-05-03 16:58:05","commitOrder":5,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"PCA Example\");\n    SparkContext sc = new SparkContext(conf);\n    JavaSparkContext jsc = JavaSparkContext.fromSparkContext(sc);\n\n    \r\n    List<Vector> data = Arrays.asList(\n            Vectors.sparse(5, new int[] {1, 3}, new double[] {1.0, 7.0}),\n            Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n            Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n    );\n\n    JavaRDD<Vector> rows = jsc.parallelize(data);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    \r\n    Matrix pc = mat.computePrincipalComponents(4);\n\n    \r\n    RowMatrix projected = mat.multiply(pc);\n    \r\n    Vector[] collectPartitions = (Vector[])projected.rows().collect();\n    System.out.println(\"Projected vector of principal component:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n    jsc.stop();\n  }\n","date":"2017-05-03 16:58:05","endLine":70,"groupId":"10501","id":9,"instanceNumber":1,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/0a/7dc621e1110a99332d39c17151d2251a71497c.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"PCA Example\");\n    SparkContext sc = new SparkContext(conf);\n\n    \r\n    double[][] array = {{1.12, 2.05, 3.12}, {5.56, 6.28, 8.94}, {10.2, 8.0, 20.5}};\n    LinkedList<Vector> rowsList = new LinkedList<>();\n    for (int i = 0; i < array.length; i++) {\n      Vector currentRow = Vectors.dense(array[i]);\n      rowsList.add(currentRow);\n    }\n    JavaRDD<Vector> rows = JavaSparkContext.fromSparkContext(sc).parallelize(rowsList);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    Matrix pc = mat.computePrincipalComponents(3);\n    RowMatrix projected = mat.multiply(pc);\n    \r\n    Vector[] collectPartitions = (Vector[])projected.rows().collect();\n    System.out.println(\"Projected vector of principal component:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n    sc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"M"},{"authorDate":"2017-05-03 16:58:05","commitOrder":5,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"SVD Example\");\n    SparkContext sc = new SparkContext(conf);\n    JavaSparkContext jsc = JavaSparkContext.fromSparkContext(sc);\n\n    \r\n    List<Vector> data = Arrays.asList(\n            Vectors.sparse(5, new int[] {1, 3}, new double[] {1.0, 7.0}),\n            Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n            Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n    );\n\n    JavaRDD<Vector> rows = jsc.parallelize(data);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    SingularValueDecomposition<RowMatrix, Matrix> svd = mat.computeSVD(5, true, 1.0E-9d);\n    RowMatrix U = svd.U();  \r\n    Vector s = svd.s();     \r\n    Matrix V = svd.V();     \r\n    \r\n    Vector[] collectPartitions = (Vector[]) U.rows().collect();\n    System.out.println(\"U factor is:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n    System.out.println(\"Singular values are: \" + s);\n    System.out.println(\"V factor is:\\n\" + V);\n\n    jsc.stop();\n  }\n","date":"2017-05-03 16:58:05","endLine":73,"groupId":"10501","id":10,"instanceNumber":2,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/80/2be3960a3378a901a0ea68ea34a508a9aefe5d.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"SVD Example\");\n    SparkContext sc = new SparkContext(conf);\n    JavaSparkContext jsc = JavaSparkContext.fromSparkContext(sc);\n\n    \r\n    double[][] array = {{1.12, 2.05, 3.12}, {5.56, 6.28, 8.94}, {10.2, 8.0, 20.5}};\n    LinkedList<Vector> rowsList = new LinkedList<>();\n    for (int i = 0; i < array.length; i++) {\n      Vector currentRow = Vectors.dense(array[i]);\n      rowsList.add(currentRow);\n    }\n    JavaRDD<Vector> rows = jsc.parallelize(rowsList);\n\n    \r\n    RowMatrix mat = new RowMatrix(rows.rdd());\n\n    \r\n    SingularValueDecomposition<RowMatrix, Matrix> svd = mat.computeSVD(3, true, 1.0E-9d);\n    RowMatrix U = svd.U();\n    Vector s = svd.s();\n    Matrix V = svd.V();\n    \r\n    Vector[] collectPartitions = (Vector[]) U.rows().collect();\n    System.out.println(\"U factor is:\");\n    for (Vector vector : collectPartitions) {\n      System.out.println(\"\\t\" + vector);\n    }\n    System.out.println(\"Singular values are: \" + s);\n    System.out.println(\"V factor is:\\n\" + V);\n\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":41,"status":"M"}],"commitId":"db2fb84b4a3c45daa449cc9232340193ce8eb37d","commitMessage":"@@@[SPARK-6227][MLLIB][PYSPARK] Implement PySpark wrappers for SVD and PCA (v2)\n\nAdd PCA and SVD to PySpark's wrappers for `RowMatrix` and `IndexedRowMatrix` (SVD only).\n\nBased on #7963.  updated.\n\n## How was this patch tested?\n\nNew doc tests and unit tests. Ran all examples locally.\n\nAuthor: MechCoder <manojkumarsivaraj334@gmail.com>\nAuthor: Nick Pentreath <nickp@za.ibm.com>\n\nCloses #17621 from MLnick/SPARK-6227-pyspark-svd-pca.\n","date":"2017-05-03 16:58:05","modifiedFileCount":"2","status":"M","submitter":"MechCoder"}]
