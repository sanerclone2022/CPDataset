[{"authorTime":"2015-07-11 07:44:51","codes":[{"authorDate":"2015-10-23 00:46:30","commitOrder":2,"curCode":"  public void expandPointerArray() {\n    final long[] oldArray = pointerArray;\n    \r\n    final int newLength = oldArray.length * 2 > 0 ? (oldArray.length * 2) : Integer.MAX_VALUE;\n    pointerArray = new long[newLength];\n    System.arraycopy(oldArray, 0, pointerArray, 0, oldArray.length);\n  }\n","date":"2015-10-23 00:46:30","endLine":59,"groupId":"3046","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"expandPointerArray","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/a8/dee6c6101c1b9a557ce38cab58d8957ec794b9.src","preCode":"  public void expandPointerArray() {\n    final long[] oldArray = pointerArray;\n    \r\n    final int newLength = oldArray.length * 2 > 0 ? (oldArray.length * 2) : Integer.MAX_VALUE;\n    pointerArray = new long[newLength];\n    System.arraycopy(oldArray, 0, pointerArray, 0, oldArray.length);\n  }\n","realPath":"core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":53,"status":"B"},{"authorDate":"2015-07-11 07:44:51","commitOrder":2,"curCode":"  public void expandPointerArray() {\n    final long[] oldArray = pointerArray;\n    \r\n    final int newLength = oldArray.length * 2 > 0 ? (oldArray.length * 2) : Integer.MAX_VALUE;\n    pointerArray = new long[newLength];\n    System.arraycopy(oldArray, 0, pointerArray, 0, oldArray.length);\n  }\n","date":"2015-07-11 07:44:51","endLine":113,"groupId":"3046","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"expandPointerArray","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/fc/34ad9cff369124aaa6546a50ef389e31b34bc5.src","preCode":"  public void expandPointerArray() {\n    final long[] oldArray = pointerArray;\n    \r\n    final int newLength = oldArray.length * 2 > 0 ? (oldArray.length * 2) : Integer.MAX_VALUE;\n    pointerArray = new long[newLength];\n    System.arraycopy(oldArray, 0, pointerArray, 0, oldArray.length);\n  }\n","realPath":"core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":107,"status":"NB"}],"commitId":"f6d06adf05afa9c5386dc2396c94e7a98730289f","commitMessage":"@@@[SPARK-10708] Consolidate sort shuffle implementations\n\nThere's a lot of duplication between SortShuffleManager and UnsafeShuffleManager. Given that these now provide the same set of functionality.  now that UnsafeShuffleManager supports large records.  I think that we should replace SortShuffleManager's serialized shuffle implementation with UnsafeShuffleManager's and should merge the two managers together.\n\nAuthor: Josh Rosen <joshrosen@databricks.com>\n\nCloses #8829 from JoshRosen/consolidate-sort-shuffle-implementations.\n","date":"2015-10-23 00:46:30","modifiedFileCount":"1","status":"M","submitter":"Josh Rosen"},{"authorTime":"2015-11-06 11:02:18","codes":[{"authorDate":"2015-11-06 11:02:18","commitOrder":3,"curCode":"  public void expandPointerArray(LongArray newArray) {\n    assert(newArray.size() > array.size());\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      array.size() * 8L\n    );\n    consumer.freeArray(array);\n    array = newArray;\n  }\n","date":"2015-11-06 11:02:18","endLine":85,"groupId":"1068","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"expandPointerArray","params":"(LongArraynewArray)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/58/ad88e1ed87bde0a5a5b5d913923aa182410dec.src","preCode":"  public void expandPointerArray() {\n    final long[] oldArray = pointerArray;\n    \r\n    final int newLength = oldArray.length * 2 > 0 ? (oldArray.length * 2) : Integer.MAX_VALUE;\n    pointerArray = new long[newLength];\n    System.arraycopy(oldArray, 0, pointerArray, 0, oldArray.length);\n  }\n","realPath":"core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":74,"status":"M"},{"authorDate":"2015-11-06 11:02:18","commitOrder":3,"curCode":"  public void expandPointerArray(LongArray newArray) {\n    if (newArray.size() < array.size()) {\n      throw new OutOfMemoryError(\"Not enough memory to grow pointer array\");\n    }\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      array.size() * 8L);\n    consumer.freeArray(array);\n    array = newArray;\n  }\n","date":"2015-11-06 11:02:18","endLine":144,"groupId":"1068","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"expandPointerArray","params":"(LongArraynewArray)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/a2/18ad4623f463e6e850fa9d305aca088ca14ecb.src","preCode":"  public void expandPointerArray() {\n    final long[] oldArray = pointerArray;\n    \r\n    final int newLength = oldArray.length * 2 > 0 ? (oldArray.length * 2) : Integer.MAX_VALUE;\n    pointerArray = new long[newLength];\n    System.arraycopy(oldArray, 0, pointerArray, 0, oldArray.length);\n  }\n","realPath":"core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":132,"status":"M"}],"commitId":"eec74ba8bde7f9446cc38e687bda103e85669d35","commitMessage":"@@@[SPARK-7542][SQL] Support off-heap index/sort buffer\n\nThis brings the support of off-heap memory for array inside BytesToBytesMap and InMemorySorter.  then we could allocate all the memory from off-heap for execution.\n\nCloses #8068\n\nAuthor: Davies Liu <davies@databricks.com>\n\nCloses #9477 from davies/unsafe_timsort.\n","date":"2015-11-06 11:02:18","modifiedFileCount":"16","status":"M","submitter":"Davies Liu"},{"authorTime":"2016-04-22 07:48:51","codes":[{"authorDate":"2016-04-22 07:48:51","commitOrder":4,"curCode":"  public void expandPointerArray(LongArray newArray) {\n    assert(newArray.size() > array.size());\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      array.size() * (Long.BYTES / memoryAllocationFactor)\n    );\n    consumer.freeArray(array);\n    array = newArray;\n  }\n","date":"2016-04-22 07:48:51","endLine":109,"groupId":"1068","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"expandPointerArray","params":"(LongArraynewArray)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/68/630946ac34c41c831fd37bad4506c1561ea1d3.src","preCode":"  public void expandPointerArray(LongArray newArray) {\n    assert(newArray.size() > array.size());\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      array.size() * 8L\n    );\n    consumer.freeArray(array);\n    array = newArray;\n  }\n","realPath":"core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":98,"status":"M"},{"authorDate":"2016-04-22 07:48:51","commitOrder":4,"curCode":"  public void expandPointerArray(LongArray newArray) {\n    if (newArray.size() < array.size()) {\n      throw new OutOfMemoryError(\"Not enough memory to grow pointer array\");\n    }\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      array.size() * (Long.BYTES / memoryAllocationFactor));\n    consumer.freeArray(array);\n    array = newArray;\n  }\n","date":"2016-04-22 07:48:51","endLine":185,"groupId":"1068","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"expandPointerArray","params":"(LongArraynewArray)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/5f/46ef9a819bc8d87e4054eab43130f6140b9ede.src","preCode":"  public void expandPointerArray(LongArray newArray) {\n    if (newArray.size() < array.size()) {\n      throw new OutOfMemoryError(\"Not enough memory to grow pointer array\");\n    }\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      array.size() * 8L);\n    consumer.freeArray(array);\n    array = newArray;\n  }\n","realPath":"core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":173,"status":"M"}],"commitId":"e2b5647ab92eb478b3f7b36a0ce6faf83e24c0e5","commitMessage":"@@@[SPARK-14724] Use radix sort for shuffles and sort operator when possible\n\n## What changes were proposed in this pull request?\n\nSpark currently uses TimSort for all in-memory sorts.  including sorts done for shuffle. One low-hanging fruit is to use radix sort when possible (e.g. sorting by integer keys). This PR adds a radix sort implementation to the unsafe sort package and switches shuffles and sorts to use it when possible.\n\nThe current implementation does not have special support for null values.  so we cannot radix-sort `LongType`. I will address this in a follow-up PR.\n\n## How was this patch tested?\n\nUnit tests.  enabling radix sort on existing tests. Microbenchmark results:\n\n```\nRunning benchmark: radix sort 25000000\nJava HotSpot(TM) 64-Bit Server VM 1.8.0_66-b17 on Linux 3.13.0-44-generic\nIntel(R) Core(TM) i7-4600U CPU  2.10GHz\n\nradix sort 25000000:                Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n-------------------------------------------------------------------------------------------\nreference TimSort key prefix array     15546 / 15859          1.6         621.9       1.0X\nreference Arrays.sort                    2416 / 2446         10.3          96.6       6.4X\nradix sort one byte                       133 /  137        188.4           5.3     117.2X\nradix sort two bytes                      255 /  258         98.2          10.2      61.1X\nradix sort eight bytes                    991 /  997         25.2          39.6      15.7X\nradix sort key prefix array              1540 / 1563         16.2          61.6      10.1X\n```\n\nI also ran a mix of the supported TPCDS queries and compared TimSort vs RadixSort metrics. The overall benchmark ran ~10% faster with radix sort on. In the breakdown below.  the radix-enabled sort phases averaged about 20x faster than TimSort.  however sorting is only a small fraction of the overall runtime. About half of the TPCDS queries were able to take advantage of radix sort.\n\n```\nTPCDS on master: 2499s real time.  8185s executor\n    - 1171s in TimSort.  avg 267 MB/s\n(note the /s accounting is weird here since dataSize counts the record sizes too)\n\nTPCDS with radix enabled: 2294s real time.  7391s executor\n    - 596s in TimSort.  avg 254 MB/s\n    - 26s in radix sort.  avg 4.2 GB/s\n```\n\ncc davies rxin\n\nAuthor: Eric Liang <ekl@databricks.com>\n\nCloses #12490 from ericl/sort-benchmark.\n","date":"2016-04-22 07:48:51","modifiedFileCount":"12","status":"M","submitter":"Eric Liang"},{"authorTime":"2016-04-22 08:52:10","codes":[{"authorDate":"2016-04-22 08:52:10","commitOrder":5,"curCode":"  public void expandPointerArray(LongArray newArray) {\n    assert(newArray.size() > array.size());\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      array.size() * (8 / memoryAllocationFactor)\n    );\n    consumer.freeArray(array);\n    array = newArray;\n  }\n","date":"2016-04-22 08:52:10","endLine":108,"groupId":"1068","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"expandPointerArray","params":"(LongArraynewArray)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/75/a0e807d76f5c7a7e4b5be5c3a2b78fef7ec2d0.src","preCode":"  public void expandPointerArray(LongArray newArray) {\n    assert(newArray.size() > array.size());\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      array.size() * (Long.BYTES / memoryAllocationFactor)\n    );\n    consumer.freeArray(array);\n    array = newArray;\n  }\n","realPath":"core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":97,"status":"M"},{"authorDate":"2016-04-22 08:52:10","commitOrder":5,"curCode":"  public void expandPointerArray(LongArray newArray) {\n    if (newArray.size() < array.size()) {\n      throw new OutOfMemoryError(\"Not enough memory to grow pointer array\");\n    }\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      array.size() * (8 / memoryAllocationFactor));\n    consumer.freeArray(array);\n    array = newArray;\n  }\n","date":"2016-04-22 08:52:10","endLine":183,"groupId":"1068","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"expandPointerArray","params":"(LongArraynewArray)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/03/973f3c124ee5823c8d0ef79c573ae8898ab68d.src","preCode":"  public void expandPointerArray(LongArray newArray) {\n    if (newArray.size() < array.size()) {\n      throw new OutOfMemoryError(\"Not enough memory to grow pointer array\");\n    }\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      array.size() * (Long.BYTES / memoryAllocationFactor));\n    consumer.freeArray(array);\n    array = newArray;\n  }\n","realPath":"core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":171,"status":"M"}],"commitId":"0bf8df250e0aeae306e2ef33e612ca27187447ed","commitMessage":"@@@[HOTFIX] Fix Java 7 compilation break\n","date":"2016-04-22 08:52:10","modifiedFileCount":"4","status":"M","submitter":"Reynold Xin"},{"authorTime":"2016-06-04 07:45:09","codes":[{"authorDate":"2016-06-04 07:45:09","commitOrder":6,"curCode":"  public void expandPointerArray(LongArray newArray) {\n    assert(newArray.size() > array.size());\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      pos * 8L\n    );\n    consumer.freeArray(array);\n    array = newArray;\n    usableCapacity = getUsableCapacity();\n  }\n","date":"2016-06-04 07:45:09","endLine":118,"groupId":"195","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"expandPointerArray","params":"(LongArraynewArray)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/dc/36809d8911fc570d95cd93e80464f048ed3a7a.src","preCode":"  public void expandPointerArray(LongArray newArray) {\n    assert(newArray.size() > array.size());\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      array.size() * (8 / memoryAllocationFactor)\n    );\n    consumer.freeArray(array);\n    array = newArray;\n  }\n","realPath":"core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":106,"status":"M"},{"authorDate":"2016-06-04 07:45:09","commitOrder":6,"curCode":"  public void expandPointerArray(LongArray newArray) {\n    if (newArray.size() < array.size()) {\n      throw new OutOfMemoryError(\"Not enough memory to grow pointer array\");\n    }\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      pos * 8L);\n    consumer.freeArray(array);\n    array = newArray;\n    usableCapacity = getUsableCapacity();\n  }\n","date":"2016-06-04 07:45:09","endLine":200,"groupId":"195","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"expandPointerArray","params":"(LongArraynewArray)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/c7/b070f519f88766cb2eb268decb6a5e0d38497a.src","preCode":"  public void expandPointerArray(LongArray newArray) {\n    if (newArray.size() < array.size()) {\n      throw new OutOfMemoryError(\"Not enough memory to grow pointer array\");\n    }\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      array.size() * (8 / memoryAllocationFactor));\n    consumer.freeArray(array);\n    array = newArray;\n  }\n","realPath":"core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":187,"status":"M"}],"commitId":"3074f575a3c84108fddab3f5f56eb1929a4b2cff","commitMessage":"@@@[SPARK-15391] [SQL] manage the temporary memory of timsort\n\n## What changes were proposed in this pull request?\n\nCurrently.  the memory for temporary buffer used by TimSort is always allocated as on-heap without bookkeeping.  it could cause OOM both in on-heap and off-heap mode.\n\nThis PR will try to manage that by preallocate it together with the pointer array.  same with RadixSort. It both works for on-heap and off-heap mode.\n\nThis PR also change the loadFactor of BytesToBytesMap to 0.5 (it was 0.70).  it enables use to radix sort also makes sure that we have enough memory for timsort.\n\n## How was this patch tested?\n\nExisting tests.\n\nAuthor: Davies Liu <davies@databricks.com>\n\nCloses #13318 from davies/fix_timsort.\n","date":"2016-06-04 07:45:09","modifiedFileCount":"9","status":"M","submitter":"Davies Liu"},{"authorTime":"2017-12-20 12:21:00","codes":[{"authorDate":"2016-06-04 07:45:09","commitOrder":7,"curCode":"  public void expandPointerArray(LongArray newArray) {\n    assert(newArray.size() > array.size());\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      pos * 8L\n    );\n    consumer.freeArray(array);\n    array = newArray;\n    usableCapacity = getUsableCapacity();\n  }\n","date":"2016-06-04 07:45:09","endLine":118,"groupId":"195","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"expandPointerArray","params":"(LongArraynewArray)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/dc/36809d8911fc570d95cd93e80464f048ed3a7a.src","preCode":"  public void expandPointerArray(LongArray newArray) {\n    assert(newArray.size() > array.size());\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      pos * 8L\n    );\n    consumer.freeArray(array);\n    array = newArray;\n    usableCapacity = getUsableCapacity();\n  }\n","realPath":"core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":106,"status":"N"},{"authorDate":"2017-12-20 12:21:00","commitOrder":7,"curCode":"  public void expandPointerArray(LongArray newArray) {\n    if (newArray.size() < array.size()) {\n      throw new SparkOutOfMemoryError(\"Not enough memory to grow pointer array\");\n    }\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      pos * 8L);\n    consumer.freeArray(array);\n    array = newArray;\n    usableCapacity = getUsableCapacity();\n  }\n","date":"2017-12-20 12:21:00","endLine":227,"groupId":"195","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"expandPointerArray","params":"(LongArraynewArray)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/95/1d076420ee6fad8b34e27c4dd95dce2ec87f23.src","preCode":"  public void expandPointerArray(LongArray newArray) {\n    if (newArray.size() < array.size()) {\n      throw new OutOfMemoryError(\"Not enough memory to grow pointer array\");\n    }\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      pos * 8L);\n    consumer.freeArray(array);\n    array = newArray;\n    usableCapacity = getUsableCapacity();\n  }\n","realPath":"core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":214,"status":"M"}],"commitId":"3a7494dfee714510f6a62d5533023f3e0d5ccdcd","commitMessage":"@@@[SPARK-22827][CORE] Avoid throwing OutOfMemoryError in case of exception in spill\n\n## What changes were proposed in this pull request?\nCurrently.  the task memory manager throws an OutofMemory error when there is an IO exception happens in spill() - https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java#L194. Similarly there any many other places in code when if a task is not able to acquire memory due to an exception we throw an OutofMemory error which kills the entire executor and hence failing all the tasks that are running on that executor instead of just failing one single task.\n\n## How was this patch tested?\n\nUnit tests\n\nAuthor: Sital Kedia <skedia@fb.com>\n\nCloses #20014 from sitalkedia/skedia/upstream_SPARK-22827.\n","date":"2017-12-20 12:21:00","modifiedFileCount":"5","status":"M","submitter":"Sital Kedia"},{"authorTime":"2018-04-06 10:13:59","codes":[{"authorDate":"2018-04-06 10:13:59","commitOrder":8,"curCode":"  public void expandPointerArray(LongArray newArray) {\n    assert(newArray.size() > array.size());\n    MemoryBlock.copyMemory(array.memoryBlock(), newArray.memoryBlock(), pos * 8L);\n    consumer.freeArray(array);\n    array = newArray;\n    usableCapacity = getUsableCapacity();\n  }\n","date":"2018-04-06 10:13:59","endLine":111,"groupId":"3440","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"expandPointerArray","params":"(LongArraynewArray)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/8f/49859746b8982fd26bb21d488e77f752479603.src","preCode":"  public void expandPointerArray(LongArray newArray) {\n    assert(newArray.size() > array.size());\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      pos * 8L\n    );\n    consumer.freeArray(array);\n    array = newArray;\n    usableCapacity = getUsableCapacity();\n  }\n","realPath":"core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":105,"status":"M"},{"authorDate":"2018-04-06 10:13:59","commitOrder":8,"curCode":"  public void expandPointerArray(LongArray newArray) {\n    if (newArray.size() < array.size()) {\n      throw new SparkOutOfMemoryError(\"Not enough memory to grow pointer array\");\n    }\n    MemoryBlock.copyMemory(array.memoryBlock(), newArray.memoryBlock(), pos * 8L);\n    consumer.freeArray(array);\n    array = newArray;\n    usableCapacity = getUsableCapacity();\n  }\n","date":"2018-04-06 10:13:59","endLine":222,"groupId":"3440","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"expandPointerArray","params":"(LongArraynewArray)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/20/a7a8b2674384488aee71a93e62ba6fdadd7312.src","preCode":"  public void expandPointerArray(LongArray newArray) {\n    if (newArray.size() < array.size()) {\n      throw new SparkOutOfMemoryError(\"Not enough memory to grow pointer array\");\n    }\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      pos * 8L);\n    consumer.freeArray(array);\n    array = newArray;\n    usableCapacity = getUsableCapacity();\n  }\n","realPath":"core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":214,"status":"M"}],"commitId":"4807d381bb113a5c61e6dad88202f23a8b6dd141","commitMessage":"@@@[SPARK-10399][CORE][SQL] Introduce multiple MemoryBlocks to choose several types of memory block\n\n## What changes were proposed in this pull request?\n\nThis PR allows us to use one of several types of `MemoryBlock`.  such as byte array.  int array.  long array.  or `java.nio.DirectByteBuffer`. To use `java.nio.DirectByteBuffer` allows to have off heap memory which is automatically deallocated by JVM. `MemoryBlock`  class has primitive accessors like `Platform.getInt()`.  `Platform.putint()`.  or `Platform.copyMemory()`.\n\nThis PR uses `MemoryBlock` for `OffHeapColumnVector`.  `UTF8String`.  and other places. This PR can improve performance of operations involving memory accesses (e.g. `UTF8String.trim`) by 1.8x.\n\nFor now.  this PR does not use `MemoryBlock` for `BufferHolder` based on cloud-fan's [suggestion](https://github.com/apache/spark/pull/11494#issuecomment-309694290).\n\nSince this PR is a successor of #11494.  close #11494. Many codes were ported from #11494. Many efforts were put here. **I think this PR should credit to yzotov.**\n\nThis PR can achieve **1.1-1.4x performance improvements** for  operations in `UTF8String` or `Murmur3_x86_32`. Other operations are almost comparable performances.\n\nWithout this PR\n```\nOpenJDK 64-Bit Server VM 1.8.0_121-8u121-b13-0ubuntu1.16.04.2-b13 on Linux 4.4.0-22-generic\nIntel(R) Xeon(R) CPU E5-2667 v3  3.20GHz\nOpenJDK 64-Bit Server VM 1.8.0_121-8u121-b13-0ubuntu1.16.04.2-b13 on Linux 4.4.0-22-generic\nIntel(R) Xeon(R) CPU E5-2667 v3  3.20GHz\nHash byte arrays with length 268435487:  Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nMurmur3_x86_32                                 526 /  536          0.0   131399881.5       1.0X\n\nUTF8String benchmark:                    Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nhashCode                                       525 /  552       1022.6           1.0       1.0X\nsubstring                                      414 /  423       1298.0           0.8       1.3X\n```\n\nWith this PR\n```\nOpenJDK 64-Bit Server VM 1.8.0_121-8u121-b13-0ubuntu1.16.04.2-b13 on Linux 4.4.0-22-generic\nIntel(R) Xeon(R) CPU E5-2667 v3  3.20GHz\nHash byte arrays with length 268435487:  Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nMurmur3_x86_32                                 474 /  488          0.0   118552232.0       1.0X\n\nUTF8String benchmark:                    Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nhashCode                                       476 /  480       1127.3           0.9       1.0X\nsubstring                                      287 /  291       1869.9           0.5       1.7X\n```\n\nBenchmark program\n```\ntest(\"benchmark Murmur3_x86_32\") {\n  val length = 8192 * 32768 + 31\n  val seed = 42L\n  val iters = 1 << 2\n  val random = new Random(seed)\n  val arrays = Array.fill[MemoryBlock](numArrays) {\n    val bytes = new Array[Byte](length)\n    random.nextBytes(bytes)\n    new ByteArrayMemoryBlock(bytes.  Platform.BYTE_ARRAY_OFFSET.  length)\n  }\n\n  val benchmark = new Benchmark(\"Hash byte arrays with length \" + length. \n    iters * numArrays.  minNumIters = 20)\n  benchmark.addCase(\"HiveHasher\") { _: Int =>\n    var sum = 0L\n    for (_ <- 0L until iters) {\n      sum += HiveHasher.hashUnsafeBytesBlock(\n        arrays(i).  Platform.BYTE_ARRAY_OFFSET.  length)\n    }\n  }\n  benchmark.run()\n}\n\ntest(\"benchmark UTF8String\") {\n  val N = 512 * 1024 * 1024\n  val iters = 2\n  val benchmark = new Benchmark(\"UTF8String benchmark\".  N.  minNumIters = 20)\n  val str0 = new java.io.StringWriter() { { for (i <- 0 until N) { write(\" \") } } }.toString\n  val s0 = UTF8String.fromString(str0)\n  benchmark.addCase(\"hashCode\") { _: Int =>\n    var h: Int = 0\n    for (_ <- 0L until iters) { h += s0.hashCode }\n  }\n  benchmark.addCase(\"substring\") { _: Int =>\n    var s: UTF8String = null\n    for (_ <- 0L until iters) { s = s0.substring(N / 2 - 5.  N / 2 + 5) }\n  }\n  benchmark.run()\n}\n```\n\nI run [this benchmark program](https://gist.github.com/kiszk/94f75b506c93a663bbbc372ffe8f05de) using [the commit](https://github.com/apache/spark/pull/19222/commits/ee5a79861c18725fb1cd9b518cdfd2489c05b81d6). I got the following results:\n\n```\nOpenJDK 64-Bit Server VM 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12 on Linux 4.4.0-66-generic\nIntel(R) Xeon(R) CPU E5-2667 v3  3.20GHz\nMemory access benchmarks:                Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nByteArrayMemoryBlock get/putInt()              220 /  221        609.3           1.6       1.0X\nPlatform get/putInt(byte[])                    220 /  236        610.9           1.6       1.0X\nPlatform get/putInt(Object)                    492 /  494        272.8           3.7       0.4X\nOnHeapMemoryBlock get/putLong()                322 /  323        416.5           2.4       0.7X\nlong[]                                         221 /  221        608.0           1.6       1.0X\nPlatform get/putLong(long[])                   321 /  321        418.7           2.4       0.7X\nPlatform get/putLong(Object)                   561 /  563        239.2           4.2       0.4X\n```\n\nI also run [this benchmark program](https://gist.github.com/kiszk/5fdb4e03733a5d110421177e289d1fb5) for comparing performance of `Platform.copyMemory()`.\n```\nOpenJDK 64-Bit Server VM 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12 on Linux 4.4.0-66-generic\nIntel(R) Xeon(R) CPU E5-2667 v3  3.20GHz\nPlatform copyMemory:                     Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative\n------------------------------------------------------------------------------------------------\nObject to Object                              1961 / 1967          8.6         116.9       1.0X\nSystem.arraycopy Object to Object             1917 / 1921          8.8         114.3       1.0X\nbyte array to byte array                      1961 / 1968          8.6         116.9       1.0X\nSystem.arraycopy byte array to byte array      1909 / 1937          8.8         113.8       1.0X\nint array to int array                        1921 / 1990          8.7         114.5       1.0X\ndouble array to double array                  1918 / 1923          8.7         114.3       1.0X\nObject to byte array                          1961 / 1967          8.6         116.9       1.0X\nObject to short array                         1965 / 1972          8.5         117.1       1.0X\nObject to int array                           1910 / 1915          8.8         113.9       1.0X\nObject to float array                         1971 / 1978          8.5         117.5       1.0X\nObject to double array                        1919 / 1944          8.7         114.4       1.0X\nbyte array to Object                          1959 / 1967          8.6         116.8       1.0X\nint array to Object                           1961 / 1970          8.6         116.9       1.0X\ndouble array to Object                        1917 / 1924          8.8         114.3       1.0X\n```\n\nThese results show three facts:\n1. According to the second/third or sixth/seventh results in the first experiment.  if we use `Platform.get/putInt(Object)`.  we achieve more than 2x worse performance than `Platform.get/putInt(byte[])` with concrete type (i.e. `byte[]`).\n2. According to the second/third or fourth/fifth/sixth results in the first experiment.  the fastest way to access an array element on Java heap is `array[]`. **Cons of `array[]` is that it is not possible to support unaligned-8byte access.**\n3. According to the first/second/third or fourth/sixth/seventh results in the first experiment.  `getInt()/putInt() or getLong()/putLong()` in subclasses of `MemoryBlock` can achieve comparable performance to `Platform.get/putInt()` or `Platform.get/putLong()` with concrete type (second or sixth result). There is no overhead regarding virtual call.\n4. According to results in the second experiment.  for `Platform.copy()`.  to pass `Object` can achieve the same performance as to pass any type of primitive array as source or destination.\n5. According to second/fourth results in the second experiment.  `Platform.copy()` can achieve the same performance as `System.arrayCopy`. **It would be good to use `Platform.copy()` since `Platform.copy()` can take any types for src and dst.**\n\nWe are incrementally replace `Platform.get/putXXX` with `MemoryBlock.get/putXXX`. This is because we have two advantages.\n1) Achieve better performance due to having a concrete type for an array.\n2) Use simple OO design instead of passing `Object`\nIt is easy to use `MemoryBlock` in `InternalRow`.  `BufferHolder`.  `TaskMemoryManager`.  and others that are already abstracted. It is not easy to use `MemoryBlock` in utility classes related to hashing or others.\n\nOther candidates are\n- UnsafeRow.  UnsafeArrayData.  UnsafeMapData.  SpecificUnsafeRowJoiner\n- UTF8StringBuffer\n- BufferHolder\n- TaskMemoryManager\n- OnHeapColumnVector\n- BytesToBytesMap\n- CachedBatch\n- classes for hash\n- others.\n\n## How was this patch tested?\n\nAdded `UnsafeMemoryAllocator`\n\nAuthor: Kazuaki Ishizaki <ishizaki@jp.ibm.com>\n\nCloses #19222 from kiszk/SPARK-10399.\n","date":"2018-04-06 10:13:59","modifiedFileCount":"27","status":"M","submitter":"Kazuaki Ishizaki"},{"authorTime":"2018-09-09 21:25:19","codes":[{"authorDate":"2018-09-09 21:25:19","commitOrder":9,"curCode":"  public void expandPointerArray(LongArray newArray) {\n    assert(newArray.size() > array.size());\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      pos * 8L\n    );\n    consumer.freeArray(array);\n    array = newArray;\n    usableCapacity = getUsableCapacity();\n  }\n","date":"2018-09-09 21:25:19","endLine":126,"groupId":"195","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"expandPointerArray","params":"(LongArraynewArray)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/0d/069125dc60efbb9e65a65e632d8a8923e44255.src","preCode":"  public void expandPointerArray(LongArray newArray) {\n    assert(newArray.size() > array.size());\n    MemoryBlock.copyMemory(array.memoryBlock(), newArray.memoryBlock(), pos * 8L);\n    consumer.freeArray(array);\n    array = newArray;\n    usableCapacity = getUsableCapacity();\n  }\n","realPath":"core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":114,"status":"M"},{"authorDate":"2018-09-09 21:25:19","commitOrder":9,"curCode":"  public void expandPointerArray(LongArray newArray) {\n    if (newArray.size() < array.size()) {\n      throw new SparkOutOfMemoryError(\"Not enough memory to grow pointer array\");\n    }\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      pos * 8L);\n    consumer.freeArray(array);\n    array = newArray;\n    usableCapacity = getUsableCapacity();\n  }\n","date":"2018-09-09 21:25:19","endLine":228,"groupId":"195","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"expandPointerArray","params":"(LongArraynewArray)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/75/690ae264838baf59ef265a2cd861f96cfa4162.src","preCode":"  public void expandPointerArray(LongArray newArray) {\n    if (newArray.size() < array.size()) {\n      throw new SparkOutOfMemoryError(\"Not enough memory to grow pointer array\");\n    }\n    MemoryBlock.copyMemory(array.memoryBlock(), newArray.memoryBlock(), pos * 8L);\n    consumer.freeArray(array);\n    array = newArray;\n    usableCapacity = getUsableCapacity();\n  }\n","realPath":"core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":215,"status":"M"}],"commitId":"0b9ccd55c2986957863dcad3b44ce80403eecfa1","commitMessage":"@@@Revert [SPARK-10399] [SPARK-23879] [SPARK-23762] [SPARK-25317]\n\n## What changes were proposed in this pull request?\n\nWhen running TPC-DS benchmarks on 2.4 release.  npoggi and winglungngai  saw more than 10% performance regression on the following queries: q67.  q24a and q24b. After we applying the PR https://github.com/apache/spark/pull/22338.  the performance regression still exists. If we revert the changes in https://github.com/apache/spark/pull/19222.  npoggi and winglungngai  found the performance regression was resolved. Thus.  this PR is to revert the related changes for unblocking the 2.4 release.\n\nIn the future release.  we still can continue the investigation and find out the root cause of the regression.\n\n## How was this patch tested?\n\nThe existing test cases\n\nCloses #22361 from gatorsmile/revertMemoryBlock.\n\nAuthored-by: gatorsmile <gatorsmile@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n","date":"2018-09-09 21:25:19","modifiedFileCount":"28","status":"M","submitter":"gatorsmile"},{"authorTime":"2020-09-29 19:05:33","codes":[{"authorDate":"2018-09-09 21:25:19","commitOrder":10,"curCode":"  public void expandPointerArray(LongArray newArray) {\n    assert(newArray.size() > array.size());\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      pos * 8L\n    );\n    consumer.freeArray(array);\n    array = newArray;\n    usableCapacity = getUsableCapacity();\n  }\n","date":"2018-09-09 21:25:19","endLine":126,"groupId":"10574","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"expandPointerArray","params":"(LongArraynewArray)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/0d/069125dc60efbb9e65a65e632d8a8923e44255.src","preCode":"  public void expandPointerArray(LongArray newArray) {\n    assert(newArray.size() > array.size());\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      pos * 8L\n    );\n    consumer.freeArray(array);\n    array = newArray;\n    usableCapacity = getUsableCapacity();\n  }\n","realPath":"core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":114,"status":"N"},{"authorDate":"2020-09-29 19:05:33","commitOrder":10,"curCode":"  public void expandPointerArray(LongArray newArray) {\n    if (array != null) {\n      if (newArray.size() < array.size()) {\n        \r\n        throw new SparkOutOfMemoryError(\"Not enough memory to grow pointer array\");\n        \r\n      }\n      Platform.copyMemory(\n        array.getBaseObject(),\n        array.getBaseOffset(),\n        newArray.getBaseObject(),\n        newArray.getBaseOffset(),\n        pos * 8L);\n      consumer.freeArray(array);\n    }\n    array = newArray;\n    usableCapacity = getUsableCapacity();\n  }\n","date":"2020-09-29 19:05:33","endLine":230,"groupId":"10574","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"expandPointerArray","params":"(LongArraynewArray)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/33/be899b6b43804ca4ee1a7d9d760a2e0fb1457d.src","preCode":"  public void expandPointerArray(LongArray newArray) {\n    if (newArray.size() < array.size()) {\n      \r\n      throw new SparkOutOfMemoryError(\"Not enough memory to grow pointer array\");\n      \r\n    }\n    Platform.copyMemory(\n      array.getBaseObject(),\n      array.getBaseOffset(),\n      newArray.getBaseObject(),\n      newArray.getBaseOffset(),\n      pos * 8L);\n    consumer.freeArray(array);\n    array = newArray;\n    usableCapacity = getUsableCapacity();\n  }\n","realPath":"core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":213,"status":"M"}],"commitId":"f167002522d50eefb261c8ba2d66a23b781a38c4","commitMessage":"@@@[SPARK-32901][CORE] Do not allocate memory while spilling UnsafeExternalSorter\n\n\n What changes were proposed in this pull request?\n\nThis PR changes `UnsafeExternalSorter` to no longer allocate any memory while spilling. In particular it removes the allocation of a new pointer array in `UnsafeInMemorySorter`. Instead the new pointer array is allocated whenever the next record is inserted into the sorter.\n\n\n Why are the changes needed?\n\nWithout this change the `UnsafeExternalSorter` could throw an OOM while spilling. The following sequence of events would have triggered an OOM:\n\n1. `UnsafeExternalSorter` runs out of space in its pointer array and attempts to allocate a new large array to replace the old one.\n2. `TaskMemoryManager` tries to allocate the memory backing the new large array using `MemoryManager`.  but `MemoryManager` is only willing to return most but not all of the memory requested.\n3. `TaskMemoryManager` asks `UnsafeExternalSorter` to spill.  which causes `UnsafeExternalSorter` to spill the current run to disk.  to free its record pages and to reset its `UnsafeInMemorySorter`.\n4. `UnsafeInMemorySorter` frees the old pointer array.  and tries to allocate a new small pointer array.\n5. `TaskMemoryManager` tries to allocate the memory backing the small array using `MemoryManager`.  but `MemoryManager` is unwilling to give it any memory.  as the `TaskMemoryManager` is still holding on to the memory it got for the new large array.\n6. `TaskMemoryManager` again asks `UnsafeExternalSorter` to spill.  but this time there is nothing to spill.\n7. `UnsafeInMemorySorter` receives less memory than it requested.  and causes a `SparkOutOfMemoryError` to be thrown.  which causes the current task to fail.\n\nWith the changes in the PR the following will happen instead:\n\n1. `UnsafeExternalSorter` runs out of space in its pointer array and attempts to allocate a new large array to replace the old one.\n2. `TaskMemoryManager` tries to allocate the memory backing the new large array using `MemoryManager`.  but `MemoryManager` is only willing to return most but not all of the memory requested.\n3. `TaskMemoryManager` asks `UnsafeExternalSorter` to spill.  which causes `UnsafeExternalSorter` to spill the current run to disk.  to free its record pages and to reset its `UnsafeInMemorySorter`.\n4. `UnsafeInMemorySorter` frees the old pointer array.\n5. `TaskMemoryManager` returns control to `UnsafeExternalSorter.growPointerArrayIfNecessary` (either by returning the the new large array or by throwing a `SparkOutOfMemoryError`).\n6. `UnsafeExternalSorter` either frees the new large array or it ignores the `SparkOutOfMemoryError` depending on what happened in the previous step.\n7. `UnsafeExternalSorter` successfully allocates a new small pointer array and operation continues as normal.\n\n\n Does this PR introduce _any_ user-facing change?\n\nNo\n\n\n How was this patch tested?\n\nTests were added in `UnsafeExternalSorterSuite` and `UnsafeInMemorySorterSuite`.\n\nCloses #29785 from tomvanbussel/SPARK-32901.\n\nAuthored-by: Tom van Bussel <tom.vanbussel@databricks.com>\nSigned-off-by: herman <herman@databricks.com>\n","date":"2020-09-29 19:05:33","modifiedFileCount":"4","status":"M","submitter":"Tom van Bussel"}]
