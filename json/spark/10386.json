[{"authorTime":"2018-10-24 09:29:40","codes":[{"authorDate":"2018-10-24 09:29:40","commitOrder":1,"curCode":"  public void testBeanWithArrayFieldDeserialization() {\n\n    Encoder<ArrayRecord> encoder = Encoders.bean(ArrayRecord.class);\n\n    Dataset<ArrayRecord> dataset = spark\n      .read()\n      .format(\"json\")\n      .schema(\"id int, intervals array<struct<startTime: bigint, endTime: bigint>>\")\n      .load(\"src/test/resources/test-data/with-array-fields.json\")\n      .as(encoder);\n\n    List<ArrayRecord> records = dataset.collectAsList();\n    Assert.assertEquals(records, ARRAY_RECORDS);\n  }\n","date":"2018-10-24 09:29:40","endLine":73,"groupId":"685","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testBeanWithArrayFieldDeserialization","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/7f/975a647c241e529f24c80c250268f400488f04.src","preCode":"  public void testBeanWithArrayFieldDeserialization() {\n\n    Encoder<ArrayRecord> encoder = Encoders.bean(ArrayRecord.class);\n\n    Dataset<ArrayRecord> dataset = spark\n      .read()\n      .format(\"json\")\n      .schema(\"id int, intervals array<struct<startTime: bigint, endTime: bigint>>\")\n      .load(\"src/test/resources/test-data/with-array-fields.json\")\n      .as(encoder);\n\n    List<ArrayRecord> records = dataset.collectAsList();\n    Assert.assertEquals(records, ARRAY_RECORDS);\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaBeanDeserializationSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":60,"status":"B"},{"authorDate":"2018-10-24 09:29:40","commitOrder":1,"curCode":"  public void testBeanWithMapFieldsDeserialization() {\n\n    Encoder<MapRecord> encoder = Encoders.bean(MapRecord.class);\n\n    Dataset<MapRecord> dataset = spark\n      .read()\n      .format(\"json\")\n      .schema(\"id int, intervals map<string, struct<startTime: bigint, endTime: bigint>>\")\n      .load(\"src/test/resources/test-data/with-map-fields.json\")\n      .as(encoder);\n\n    List<MapRecord> records = dataset.collectAsList();\n\n    Assert.assertEquals(records, MAP_RECORDS);\n  }\n","date":"2018-10-24 09:29:40","endLine":116,"groupId":"685","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testBeanWithMapFieldsDeserialization","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/7f/975a647c241e529f24c80c250268f400488f04.src","preCode":"  public void testBeanWithMapFieldsDeserialization() {\n\n    Encoder<MapRecord> encoder = Encoders.bean(MapRecord.class);\n\n    Dataset<MapRecord> dataset = spark\n      .read()\n      .format(\"json\")\n      .schema(\"id int, intervals map<string, struct<startTime: bigint, endTime: bigint>>\")\n      .load(\"src/test/resources/test-data/with-map-fields.json\")\n      .as(encoder);\n\n    List<MapRecord> records = dataset.collectAsList();\n\n    Assert.assertEquals(records, MAP_RECORDS);\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaBeanDeserializationSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":102,"status":"B"}],"commitId":"584e767d372d41071c3436f9ad4bf77a820f12b4","commitMessage":"@@@[SPARK-25772][SQL] Fix java map of structs deserialization\n\nThis is a follow-up PR for #22708. It considers another case of java beans deserialization: java maps with struct keys/values.\n\nWhen deserializing values of MapType with struct keys/values in java beans.  fields of structs get mixed up. I suggest using struct data types retrieved from resolved input data instead of inferring them from java beans.\n\n## What changes were proposed in this pull request?\n\nInvocations of \"keyArray\" and \"valueArray\" functions are used to extract arrays of keys and values. Struct type of keys or values is also inferred from java bean structure and ends up with mixed up field order.\nI created a new UnresolvedInvoke expression as a temporary substitution of Invoke expression while no actual data is available. It allows to provide the resulting data type during analysis based on the resolved input data.  not on the java bean (similar to UnresolvedMapObjects).\n\nKey and value arrays are then fed to MapObjects expression which I replaced with UnresolvedMapObjects.  just like in case of ArrayType.\n\nFinally I added resolution of UnresolvedInvoke expressions in Analyzer.resolveExpression method as an additional pattern matching case.\n\n## How was this patch tested?\n\nAdded a test case.\nBuilt complete project on travis.\n\nviirya kiszk cloud-fan michalsenkyr marmbrus liancheng\n\nCloses #22745 from vofque/SPARK-21402-FOLLOWUP.\n\nLead-authored-by: Vladimir Kuriatkov <vofque@gmail.com>\nCo-authored-by: Vladimir Kuriatkov <Vladimir_Kuriatkov@epam.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n","date":"2018-10-24 09:29:40","modifiedFileCount":"0","status":"B","submitter":"Vladimir Kuriatkov"},{"authorTime":"2018-10-24 09:29:40","codes":[{"authorDate":"2019-03-08 11:54:04","commitOrder":2,"curCode":"  public void testBeanWithArrayFieldDeserialization() {\n    Encoder<ArrayRecord> encoder = Encoders.bean(ArrayRecord.class);\n\n    Dataset<ArrayRecord> dataset = spark\n      .read()\n      .format(\"json\")\n      .schema(\"id int, intervals array<struct<startTime: bigint, endTime: bigint>>, \" +\n          \"ints array<int>\")\n      .load(\"src/test/resources/test-data/with-array-fields.json\")\n      .as(encoder);\n\n    List<ArrayRecord> records = dataset.collectAsList();\n    Assert.assertEquals(records, ARRAY_RECORDS);\n  }\n","date":"2019-03-08 11:54:04","endLine":77,"groupId":"685","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testBeanWithArrayFieldDeserialization","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/f5/9afef36a5acd7b90e39c72616035c6cf12fc0a.src","preCode":"  public void testBeanWithArrayFieldDeserialization() {\n\n    Encoder<ArrayRecord> encoder = Encoders.bean(ArrayRecord.class);\n\n    Dataset<ArrayRecord> dataset = spark\n      .read()\n      .format(\"json\")\n      .schema(\"id int, intervals array<struct<startTime: bigint, endTime: bigint>>\")\n      .load(\"src/test/resources/test-data/with-array-fields.json\")\n      .as(encoder);\n\n    List<ArrayRecord> records = dataset.collectAsList();\n    Assert.assertEquals(records, ARRAY_RECORDS);\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaBeanDeserializationSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":64,"status":"M"},{"authorDate":"2018-10-24 09:29:40","commitOrder":2,"curCode":"  public void testBeanWithMapFieldsDeserialization() {\n\n    Encoder<MapRecord> encoder = Encoders.bean(MapRecord.class);\n\n    Dataset<MapRecord> dataset = spark\n      .read()\n      .format(\"json\")\n      .schema(\"id int, intervals map<string, struct<startTime: bigint, endTime: bigint>>\")\n      .load(\"src/test/resources/test-data/with-map-fields.json\")\n      .as(encoder);\n\n    List<MapRecord> records = dataset.collectAsList();\n\n    Assert.assertEquals(records, MAP_RECORDS);\n  }\n","date":"2018-10-24 09:29:40","endLine":116,"groupId":"685","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testBeanWithMapFieldsDeserialization","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/7f/975a647c241e529f24c80c250268f400488f04.src","preCode":"  public void testBeanWithMapFieldsDeserialization() {\n\n    Encoder<MapRecord> encoder = Encoders.bean(MapRecord.class);\n\n    Dataset<MapRecord> dataset = spark\n      .read()\n      .format(\"json\")\n      .schema(\"id int, intervals map<string, struct<startTime: bigint, endTime: bigint>>\")\n      .load(\"src/test/resources/test-data/with-map-fields.json\")\n      .as(encoder);\n\n    List<MapRecord> records = dataset.collectAsList();\n\n    Assert.assertEquals(records, MAP_RECORDS);\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaBeanDeserializationSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":102,"status":"N"}],"commitId":"d8f77e11a42bf664a02124a8b6830797979550b4","commitMessage":"@@@[SPARK-27001][SQL][FOLLOWUP] Address primitive array type for serializer\n\n## What changes were proposed in this pull request?\n\nThis is follow-up PR which addresses review comment in PR for SPARK-27001:\nhttps://github.com/apache/spark/pull/23908#discussion_r261511454\n\nThis patch proposes addressing primitive array type for serializer - instead of handling it to generic one.  Spark now handles it efficiently as primitive array.\n\n## How was this patch tested?\n\nUT modified to include primitive array.\n\nCloses #24015 from HeartSaVioR/SPARK-27001-FOLLOW-UP-java-primitive-array.\n\nAuthored-by: Jungtaek Lim (HeartSaVioR) <kabhwan@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n","date":"2019-03-08 11:54:04","modifiedFileCount":"1","status":"M","submitter":"Jungtaek Lim (HeartSaVioR)"},{"authorTime":"2019-11-04 03:21:28","codes":[{"authorDate":"2019-11-04 03:21:28","commitOrder":3,"curCode":"  public void testBeanWithArrayFieldDeserialization() {\n    Encoder<ArrayRecord> encoder = Encoders.bean(ArrayRecord.class);\n\n    Dataset<ArrayRecord> dataset = spark\n      .read()\n      .format(\"json\")\n      .schema(\"id int, intervals array<struct<startTime: bigint, endTime: bigint>>, \" +\n          \"ints array<int>\")\n      .load(\"src/test/resources/test-data/with-array-fields.json\")\n      .as(encoder);\n\n    List<ArrayRecord> records = dataset.collectAsList();\n    Assert.assertEquals(ARRAY_RECORDS, records);\n  }\n","date":"2019-11-04 03:21:28","endLine":82,"groupId":"10386","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testBeanWithArrayFieldDeserialization","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/c8/b5555a135d323a1b94bce49649193b0bda807c.src","preCode":"  public void testBeanWithArrayFieldDeserialization() {\n    Encoder<ArrayRecord> encoder = Encoders.bean(ArrayRecord.class);\n\n    Dataset<ArrayRecord> dataset = spark\n      .read()\n      .format(\"json\")\n      .schema(\"id int, intervals array<struct<startTime: bigint, endTime: bigint>>, \" +\n          \"ints array<int>\")\n      .load(\"src/test/resources/test-data/with-array-fields.json\")\n      .as(encoder);\n\n    List<ArrayRecord> records = dataset.collectAsList();\n    Assert.assertEquals(records, ARRAY_RECORDS);\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaBeanDeserializationSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":69,"status":"M"},{"authorDate":"2019-11-04 03:21:28","commitOrder":3,"curCode":"  public void testBeanWithMapFieldsDeserialization() {\n\n    Encoder<MapRecord> encoder = Encoders.bean(MapRecord.class);\n\n    Dataset<MapRecord> dataset = spark\n      .read()\n      .format(\"json\")\n      .schema(\"id int, intervals map<string, struct<startTime: bigint, endTime: bigint>>\")\n      .load(\"src/test/resources/test-data/with-map-fields.json\")\n      .as(encoder);\n\n    List<MapRecord> records = dataset.collectAsList();\n\n    Assert.assertEquals(MAP_RECORDS, records);\n  }\n","date":"2019-11-04 03:21:28","endLine":125,"groupId":"10386","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testBeanWithMapFieldsDeserialization","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/c8/b5555a135d323a1b94bce49649193b0bda807c.src","preCode":"  public void testBeanWithMapFieldsDeserialization() {\n\n    Encoder<MapRecord> encoder = Encoders.bean(MapRecord.class);\n\n    Dataset<MapRecord> dataset = spark\n      .read()\n      .format(\"json\")\n      .schema(\"id int, intervals map<string, struct<startTime: bigint, endTime: bigint>>\")\n      .load(\"src/test/resources/test-data/with-map-fields.json\")\n      .as(encoder);\n\n    List<MapRecord> records = dataset.collectAsList();\n\n    Assert.assertEquals(records, MAP_RECORDS);\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaBeanDeserializationSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":111,"status":"M"}],"commitId":"80a89873b20aa07e2522bed5da0fc50e616246d9","commitMessage":"@@@[SPARK-29733][TESTS] Fix wrong order of parameters passed to `assertEquals`\n\n\n What changes were proposed in this pull request?\nThe `assertEquals` method of JUnit Assert requires the first parameter to be the expected value. In this PR.  I propose to change the order of parameters when the expected value is passed as the second parameter.\n\n\n Why are the changes needed?\nWrong order of assert parameters confuses when the assert fails and the parameters have special string representation. For example:\n```java\nassertEquals(input1.add(input2).  new CalendarInterval(5.  5.  367200000000L));\n```\n```\njava.lang.AssertionError:\nExpected :interval 5 months 5 days 101 hours\nActual   :interval 5 months 5 days 102 hours\n```\n\n\n Does this PR introduce any user-facing change?\nNo\n\n\n How was this patch tested?\nBy existing tests.\n\nCloses #26377 from MaxGekk/fix-order-in-assert-equals.\n\nAuthored-by: Maxim Gekk <max.gekk@gmail.com>\nSigned-off-by: Dongjoon Hyun <dhyun@apple.com>\n","date":"2019-11-04 03:21:28","modifiedFileCount":"21","status":"M","submitter":"Maxim Gekk"}]
