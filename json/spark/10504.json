[{"authorTime":"2015-12-08 15:26:34","codes":[{"authorDate":"2015-12-08 15:26:34","commitOrder":1,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JaveMinMaxScalerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    DataFrame dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n    MinMaxScaler scaler = new MinMaxScaler()\n      .setInputCol(\"features\")\n      .setOutputCol(\"scaledFeatures\");\n\n    \r\n    MinMaxScalerModel scalerModel = scaler.fit(dataFrame);\n\n    \r\n    DataFrame scaledData = scalerModel.transform(dataFrame);\n    \r\n    jsc.stop();\n  }\n","date":"2015-12-08 15:26:34","endLine":49,"groupId":"200","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/13/8b3ab6aba44c25b16b461023ce0d24ebfbfcb2.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JaveMinMaxScalerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    DataFrame dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n    MinMaxScaler scaler = new MinMaxScaler()\n      .setInputCol(\"features\")\n      .setOutputCol(\"scaledFeatures\");\n\n    \r\n    MinMaxScalerModel scalerModel = scaler.fit(dataFrame);\n\n    \r\n    DataFrame scaledData = scalerModel.transform(dataFrame);\n    \r\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":31,"status":"B"},{"authorDate":"2015-12-08 15:26:34","commitOrder":1,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaNormalizerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    DataFrame dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    Normalizer normalizer = new Normalizer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"normFeatures\")\n      .setP(1.0);\n\n    DataFrame l1NormData = normalizer.transform(dataFrame);\n\n    \r\n    DataFrame lInfNormData =\n      normalizer.transform(dataFrame, normalizer.p().w(Double.POSITIVE_INFINITY));\n    \r\n    jsc.stop();\n  }\n","date":"2015-12-08 15:26:34","endLine":51,"groupId":"3223","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/62/83a355e1fef143ef159807ee9ded5be0b429d9.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaNormalizerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    DataFrame dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    Normalizer normalizer = new Normalizer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"normFeatures\")\n      .setP(1.0);\n\n    DataFrame l1NormData = normalizer.transform(dataFrame);\n\n    \r\n    DataFrame lInfNormData =\n      normalizer.transform(dataFrame, normalizer.p().w(Double.POSITIVE_INFINITY));\n    \r\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":30,"status":"B"}],"commitId":"78209b0ccaf3f22b5e2345dfb2b98edfdb746819","commitMessage":"@@@[SPARK-11551][DOC][EXAMPLE] Replace example code in ml-features.md using include_example\n\nMade new patch contaning only markdown examples moved to exmaple/folder.\nOny three  java code were not shfted since they were contaning compliation error . these classes are\n1)StandardScale 2)NormalizerExample 3)VectorIndexer\n\nAuthor: Xusen Yin <yinxusen@gmail.com>\nAuthor: somideshmukh <somilde@us.ibm.com>\n\nCloses #10002 from somideshmukh/SomilBranch1.33.\n","date":"2015-12-08 15:26:34","modifiedFileCount":"0","status":"B","submitter":"somideshmukh"},{"authorTime":"2015-12-10 04:00:48","codes":[{"authorDate":"2015-12-10 04:00:48","commitOrder":2,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JaveMinMaxScalerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    DataFrame dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n    MinMaxScaler scaler = new MinMaxScaler()\n      .setInputCol(\"features\")\n      .setOutputCol(\"scaledFeatures\");\n\n    \r\n    MinMaxScalerModel scalerModel = scaler.fit(dataFrame);\n\n    \r\n    DataFrame scaledData = scalerModel.transform(dataFrame);\n    scaledData.show();\n    \r\n    jsc.stop();\n  }\n","date":"2015-12-10 04:00:48","endLine":50,"groupId":"200","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/2d/50ba7faa1a1a4dba228cae8a0ad76fa037f3c5.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JaveMinMaxScalerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    DataFrame dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n    MinMaxScaler scaler = new MinMaxScaler()\n      .setInputCol(\"features\")\n      .setOutputCol(\"scaledFeatures\");\n\n    \r\n    MinMaxScalerModel scalerModel = scaler.fit(dataFrame);\n\n    \r\n    DataFrame scaledData = scalerModel.transform(dataFrame);\n    \r\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":31,"status":"M"},{"authorDate":"2015-12-10 04:00:48","commitOrder":2,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaNormalizerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    DataFrame dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    Normalizer normalizer = new Normalizer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"normFeatures\")\n      .setP(1.0);\n\n    DataFrame l1NormData = normalizer.transform(dataFrame);\n    l1NormData.show();\n\n    \r\n    DataFrame lInfNormData =\n      normalizer.transform(dataFrame, normalizer.p().w(Double.POSITIVE_INFINITY));\n    lInfNormData.show();\n    \r\n    jsc.stop();\n  }\n","date":"2015-12-10 04:00:48","endLine":53,"groupId":"3223","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/ed/3f6163c0558f3510f186295746529765ee8be4.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaNormalizerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    DataFrame dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    Normalizer normalizer = new Normalizer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"normFeatures\")\n      .setP(1.0);\n\n    DataFrame l1NormData = normalizer.transform(dataFrame);\n\n    \r\n    DataFrame lInfNormData =\n      normalizer.transform(dataFrame, normalizer.p().w(Double.POSITIVE_INFINITY));\n    \r\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":30,"status":"M"}],"commitId":"051c6a066f7b5fcc7472412144c15b50a5319bd5","commitMessage":"@@@[SPARK-11551][DOC] Replace example code in ml-features.md using include_example\n\nPR on behalf of somideshmukh.  thanks!\n\nAuthor: Xusen Yin <yinxusen@gmail.com>\nAuthor: somideshmukh <somilde@us.ibm.com>\n\nCloses #10219 from yinxusen/SPARK-11551.\n","date":"2015-12-10 04:00:48","modifiedFileCount":"0","status":"M","submitter":"Xusen Yin"},{"authorTime":"2016-03-11 09:00:17","codes":[{"authorDate":"2016-03-11 09:00:17","commitOrder":3,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JaveMinMaxScalerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    Dataset<Row> dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n    MinMaxScaler scaler = new MinMaxScaler()\n      .setInputCol(\"features\")\n      .setOutputCol(\"scaledFeatures\");\n\n    \r\n    MinMaxScalerModel scalerModel = scaler.fit(dataFrame);\n\n    \r\n    Dataset<Row> scaledData = scalerModel.transform(dataFrame);\n    scaledData.show();\n    \r\n    jsc.stop();\n  }\n","date":"2016-03-11 09:00:17","endLine":51,"groupId":"3055","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/4a/ee18eeabfcf53b57a7addf9417a7be39a0e5b5.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JaveMinMaxScalerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    DataFrame dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n    MinMaxScaler scaler = new MinMaxScaler()\n      .setInputCol(\"features\")\n      .setOutputCol(\"scaledFeatures\");\n\n    \r\n    MinMaxScalerModel scalerModel = scaler.fit(dataFrame);\n\n    \r\n    DataFrame scaledData = scalerModel.transform(dataFrame);\n    scaledData.show();\n    \r\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":32,"status":"M"},{"authorDate":"2016-03-11 09:00:17","commitOrder":3,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaNormalizerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    Dataset<Row> dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    Normalizer normalizer = new Normalizer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"normFeatures\")\n      .setP(1.0);\n\n    Dataset<Row> l1NormData = normalizer.transform(dataFrame);\n    l1NormData.show();\n\n    \r\n    Dataset<Row> lInfNormData =\n      normalizer.transform(dataFrame, normalizer.p().w(Double.POSITIVE_INFINITY));\n    lInfNormData.show();\n    \r\n    jsc.stop();\n  }\n","date":"2016-03-11 09:00:17","endLine":54,"groupId":"3055","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/31/cd752136689756b8602d26328601d4d664b5d2.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaNormalizerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    DataFrame dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    Normalizer normalizer = new Normalizer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"normFeatures\")\n      .setP(1.0);\n\n    DataFrame l1NormData = normalizer.transform(dataFrame);\n    l1NormData.show();\n\n    \r\n    DataFrame lInfNormData =\n      normalizer.transform(dataFrame, normalizer.p().w(Double.POSITIVE_INFINITY));\n    lInfNormData.show();\n    \r\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":31,"status":"M"}],"commitId":"1d542785b9949e7f92025e6754973a779cc37c52","commitMessage":"@@@[SPARK-13244][SQL] Migrates DataFrame to Dataset\n\n## What changes were proposed in this pull request?\n\nThis PR unifies DataFrame and Dataset by migrating existing DataFrame operations to Dataset and make `DataFrame` a type alias of `Dataset[Row]`.\n\nMost Scala code changes are source compatible.  but Java API is broken as Java knows nothing about Scala type alias (mostly replacing `DataFrame` with `Dataset<Row>`).\n\nThere are several noticeable API changes related to those returning arrays:\n\n1.  `collect`/`take`\n\n    -   Old APIs in class `DataFrame`:\n\n        ```scala\n        def collect(): Array[Row]\n        def take(n: Int): Array[Row]\n        ```\n\n    -   New APIs in class `Dataset[T]`:\n\n        ```scala\n        def collect(): Array[T]\n        def take(n: Int): Array[T]\n\n        def collectRows(): Array[Row]\n        def takeRows(n: Int): Array[Row]\n        ```\n\n    Two specialized methods `collectRows` and `takeRows` are added because Java doesn't support returning generic arrays. Thus.  for example.  `DataFrame.collect(): Array[T]` actually returns `Object` instead of `Array<T>` from Java side.\n\n    Normally.  Java users may fall back to `collectAsList` and `takeAsList`.  The two new specialized versions are added to avoid performance regression in ML related code (but maybe I'm wrong and they are not necessary here).\n\n1.  `randomSplit`\n\n    -   Old APIs in class `DataFrame`:\n\n        ```scala\n        def randomSplit(weights: Array[Double].  seed: Long): Array[DataFrame]\n        def randomSplit(weights: Array[Double]): Array[DataFrame]\n        ```\n\n    -   New APIs in class `Dataset[T]`:\n\n        ```scala\n        def randomSplit(weights: Array[Double].  seed: Long): Array[Dataset[T]]\n        def randomSplit(weights: Array[Double]): Array[Dataset[T]]\n        ```\n\n    Similar problem as above.  but hasn't been addressed for Java API yet.  We can probably add `randomSplitAsList` to fix this one.\n\n1.  `groupBy`\n\n    Some original `DataFrame.groupBy` methods have conflicting signature with original `Dataset.groupBy` methods.  To distinguish these two.  typed `Dataset.groupBy` methods are renamed to `groupByKey`.\n\nOther noticeable changes:\n\n1.  Dataset always do eager analysis now\n\n    We used to support disabling DataFrame eager analysis to help reporting partially analyzed malformed logical plan on analysis failure.  However.  Dataset encoders requires eager analysi during Dataset construction.  To preserve the error reporting feature.  `AnalysisException` now takes an extra `Option[LogicalPlan]` argument to hold the partially analyzed plan.  so that we can check the plan tree when reporting test failures.  This plan is passed by `QueryExecution.assertAnalyzed`.\n\n## How was this patch tested?\n\nExisting tests do the work.\n\n## TODO\n\n- [ ] Fix all tests\n- [ ] Re-enable MiMA check\n- [ ] Update ScalaDoc (`since`.  `group`.  and example code)\n\nAuthor: Cheng Lian <lian@databricks.com>\nAuthor: Yin Huai <yhuai@databricks.com>\nAuthor: Wenchen Fan <wenchen@databricks.com>\nAuthor: Cheng Lian <liancheng@users.noreply.github.com>\n\nCloses #11443 from liancheng/ds-to-df.\n","date":"2016-03-11 09:00:17","modifiedFileCount":"87","status":"M","submitter":"Cheng Lian"},{"authorTime":"2016-05-05 05:31:36","codes":[{"authorDate":"2016-05-05 05:31:36","commitOrder":4,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession.builder().appName(\"JavaMinMaxScalerExample\").getOrCreate();\n\n    \r\n    Dataset<Row> dataFrame = spark.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n    MinMaxScaler scaler = new MinMaxScaler()\n      .setInputCol(\"features\")\n      .setOutputCol(\"scaledFeatures\");\n\n    \r\n    MinMaxScalerModel scalerModel = scaler.fit(dataFrame);\n\n    \r\n    Dataset<Row> scaledData = scalerModel.transform(dataFrame);\n    scaledData.show();\n    \r\n    spark.stop();\n  }\n","date":"2016-05-05 05:31:36","endLine":47,"groupId":"344","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/02/2940fd1e67ce125b549874bbeea9d5158d32f4.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JaveMinMaxScalerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    Dataset<Row> dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n    MinMaxScaler scaler = new MinMaxScaler()\n      .setInputCol(\"features\")\n      .setOutputCol(\"scaledFeatures\");\n\n    \r\n    MinMaxScalerModel scalerModel = scaler.fit(dataFrame);\n\n    \r\n    Dataset<Row> scaledData = scalerModel.transform(dataFrame);\n    scaledData.show();\n    \r\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":30,"status":"M"},{"authorDate":"2016-05-05 05:31:36","commitOrder":4,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession.builder().appName(\"JavaNormalizerExample\").getOrCreate();\n\n    \r\n    Dataset<Row> dataFrame =\n      spark.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    Normalizer normalizer = new Normalizer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"normFeatures\")\n      .setP(1.0);\n\n    Dataset<Row> l1NormData = normalizer.transform(dataFrame);\n    l1NormData.show();\n\n    \r\n    Dataset<Row> lInfNormData =\n      normalizer.transform(dataFrame, normalizer.p().w(Double.POSITIVE_INFINITY));\n    lInfNormData.show();\n    \r\n    spark.stop();\n  }\n","date":"2016-05-05 05:31:36","endLine":51,"groupId":"3055","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/4b/3a718ea92c841df725281fede2a09bab5a8b12.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaNormalizerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    Dataset<Row> dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    Normalizer normalizer = new Normalizer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"normFeatures\")\n      .setP(1.0);\n\n    Dataset<Row> l1NormData = normalizer.transform(dataFrame);\n    l1NormData.show();\n\n    \r\n    Dataset<Row> lInfNormData =\n      normalizer.transform(dataFrame, normalizer.p().w(Double.POSITIVE_INFINITY));\n    lInfNormData.show();\n    \r\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":29,"status":"M"}],"commitId":"cdce4e62a5674e2034e5d395578b1a60e3d8c435","commitMessage":"@@@[SPARK-15031][EXAMPLE] Use SparkSession in Scala/Python/Java example.\n\n## What changes were proposed in this pull request?\n\nThis PR aims to update Scala/Python/Java examples by replacing `SQLContext` with newly added `SparkSession`.\n\n- Use **SparkSession Builder Pattern** in 154(Scala 55.  Java 52.  Python 47) files.\n- Add `getConf` in Python SparkContext class: `python/pyspark/context.py`\n- Replace **SQLContext Singleton Pattern** with **SparkSession Singleton Pattern**:\n  - `SqlNetworkWordCount.scala`\n  - `JavaSqlNetworkWordCount.java`\n  - `sql_network_wordcount.py`\n\nNow.  `SQLContexts` are used only in R examples and the following two Python examples. The python examples are untouched in this PR since it already fails some unknown issue.\n- `simple_params_example.py`\n- `aft_survival_regression.py`\n\n## How was this patch tested?\n\nManual.\n\nAuthor: Dongjoon Hyun <dongjoon@apache.org>\n\nCloses #12809 from dongjoon-hyun/SPARK-15031.\n","date":"2016-05-05 05:31:36","modifiedFileCount":"52","status":"M","submitter":"Dongjoon Hyun"},{"authorTime":"2016-08-06 03:57:46","codes":[{"authorDate":"2016-08-06 03:57:46","commitOrder":5,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaMinMaxScalerExample\")\n      .getOrCreate();\n\n    \r\n    List<Row> data = Arrays.asList(\n        RowFactory.create(0, Vectors.dense(1.0, 0.1, -1.0)),\n        RowFactory.create(1, Vectors.dense(2.0, 1.1, 1.0)),\n        RowFactory.create(2, Vectors.dense(3.0, 10.1, 3.0))\n    );\n    StructType schema = new StructType(new StructField[]{\n        new StructField(\"id\", DataTypes.IntegerType, false, Metadata.empty()),\n        new StructField(\"features\", new VectorUDT(), false, Metadata.empty())\n    });\n    Dataset<Row> dataFrame = spark.createDataFrame(data, schema);\n\n    MinMaxScaler scaler = new MinMaxScaler()\n      .setInputCol(\"features\")\n      .setOutputCol(\"scaledFeatures\");\n\n    \r\n    MinMaxScalerModel scalerModel = scaler.fit(dataFrame);\n\n    \r\n    Dataset<Row> scaledData = scalerModel.transform(dataFrame);\n    System.out.println(\"Features scaled to range: [\" + scaler.getMin() + \", \"\n        + scaler.getMax() + \"]\");\n    scaledData.select(\"features\", \"scaledFeatures\").show();\n    \r\n\n    spark.stop();\n  }\n","date":"2016-08-06 03:57:46","endLine":73,"groupId":"10504","id":9,"instanceNumber":1,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/27/57af8d245d218122ddaa19518482848184638f.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaMinMaxScalerExample\")\n      .getOrCreate();\n\n    \r\n    Dataset<Row> dataFrame = spark\n      .read()\n      .format(\"libsvm\")\n      .load(\"data/mllib/sample_libsvm_data.txt\");\n    MinMaxScaler scaler = new MinMaxScaler()\n      .setInputCol(\"features\")\n      .setOutputCol(\"scaledFeatures\");\n\n    \r\n    MinMaxScalerModel scalerModel = scaler.fit(dataFrame);\n\n    \r\n    Dataset<Row> scaledData = scalerModel.transform(dataFrame);\n    scaledData.show();\n    \r\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"M"},{"authorDate":"2016-08-06 03:57:46","commitOrder":5,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaNormalizerExample\")\n      .getOrCreate();\n\n    \r\n    List<Row> data = Arrays.asList(\n        RowFactory.create(0, Vectors.dense(1.0, 0.1, -8.0)),\n        RowFactory.create(1, Vectors.dense(2.0, 1.0, -4.0)),\n        RowFactory.create(2, Vectors.dense(4.0, 10.0, 8.0))\n    );\n    StructType schema = new StructType(new StructField[]{\n        new StructField(\"id\", DataTypes.IntegerType, false, Metadata.empty()),\n        new StructField(\"features\", new VectorUDT(), false, Metadata.empty())\n    });\n    Dataset<Row> dataFrame = spark.createDataFrame(data, schema);\n\n    \r\n    Normalizer normalizer = new Normalizer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"normFeatures\")\n      .setP(1.0);\n\n    Dataset<Row> l1NormData = normalizer.transform(dataFrame);\n    l1NormData.show();\n\n    \r\n    Dataset<Row> lInfNormData =\n      normalizer.transform(dataFrame, normalizer.p().w(Double.POSITIVE_INFINITY));\n    lInfNormData.show();\n    \r\n\n    spark.stop();\n  }\n","date":"2016-08-06 03:57:46","endLine":73,"groupId":"10504","id":10,"instanceNumber":2,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/f8/78c420d82377ce1a4689921c6c4d63437e16e8.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaNormalizerExample\")\n      .getOrCreate();\n\n    \r\n    Dataset<Row> dataFrame =\n      spark.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    Normalizer normalizer = new Normalizer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"normFeatures\")\n      .setP(1.0);\n\n    Dataset<Row> l1NormData = normalizer.transform(dataFrame);\n    l1NormData.show();\n\n    \r\n    Dataset<Row> lInfNormData =\n      normalizer.transform(dataFrame, normalizer.p().w(Double.POSITIVE_INFINITY));\n    lInfNormData.show();\n    \r\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":39,"status":"M"}],"commitId":"180fd3e0a3426db200c97170926afb60751dfd0e","commitMessage":"@@@[SPARK-16421][EXAMPLES][ML] Improve ML Example Outputs\n\n## What changes were proposed in this pull request?\nImprove example outputs to better reflect the functionality that is being presented.  This mostly consisted of modifying what was printed at the end of the example.  such as calling show() with truncate=False.  but sometimes required minor tweaks in the example data to get relevant output.  Explicitly set parameters when they are used as part of the example.  Fixed Java examples that failed to run because of using old-style MLlib Vectors or problem with schema.  Synced examples between different APIs.\n\n## How was this patch tested?\nRan each example for Scala.  Python.  and Java and made sure output was legible on a terminal of width 100.\n\nAuthor: Bryan Cutler <cutlerb@gmail.com>\n\nCloses #14308 from BryanCutler/ml-examples-improve-output-SPARK-16260.\n","date":"2016-08-06 03:57:46","modifiedFileCount":"27","status":"M","submitter":"Bryan Cutler"}]
