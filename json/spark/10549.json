[{"authorTime":"2017-02-16 20:32:45","codes":[{"authorDate":"2017-02-16 20:32:45","commitOrder":1,"curCode":"  public void binaryFiles() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    FileChannel channel1 = fos1.getChannel();\n    ByteBuffer bbuf = ByteBuffer.wrap(content1);\n    channel1.write(bbuf);\n    channel1.close();\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName, 3);\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","date":"2017-02-16 20:32:45","endLine":1164,"groupId":"196","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"binaryFiles","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/80/aab100aced46cbe2866850ce4da7935d650b60.src","preCode":"  public void binaryFiles() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    FileChannel channel1 = fos1.getChannel();\n    ByteBuffer bbuf = ByteBuffer.wrap(content1);\n    channel1.write(bbuf);\n    channel1.close();\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName, 3);\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","realPath":"core/src/test/java/test/org/apache/spark/JavaAPISuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":1146,"status":"B"},{"authorDate":"2017-02-16 20:32:45","commitOrder":1,"curCode":"  public void binaryFilesCaching() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    FileChannel channel1 = fos1.getChannel();\n    ByteBuffer bbuf = ByteBuffer.wrap(content1);\n    channel1.write(bbuf);\n    channel1.close();\n\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName).cache();\n    readRDD.foreach(new VoidFunction<Tuple2<String,PortableDataStream>>() {\n      @Override\n      public void call(Tuple2<String, PortableDataStream> pair) {\n        pair._2().toArray(); \r\n      }\n    });\n\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","date":"2017-02-16 20:32:45","endLine":1193,"groupId":"3228","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"binaryFilesCaching","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/80/aab100aced46cbe2866850ce4da7935d650b60.src","preCode":"  public void binaryFilesCaching() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    FileChannel channel1 = fos1.getChannel();\n    ByteBuffer bbuf = ByteBuffer.wrap(content1);\n    channel1.write(bbuf);\n    channel1.close();\n\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName).cache();\n    readRDD.foreach(new VoidFunction<Tuple2<String,PortableDataStream>>() {\n      @Override\n      public void call(Tuple2<String, PortableDataStream> pair) {\n        pair._2().toArray(); \r\n      }\n    });\n\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","realPath":"core/src/test/java/test/org/apache/spark/JavaAPISuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":1167,"status":"B"}],"commitId":"0e2405490f2056728d1353abbac6f3ea177ae533","commitMessage":"@@@[SPARK-19550][BUILD][CORE][WIP] Remove Java 7 support\n\n- Move external/java8-tests tests into core.  streaming.  sql and remove\n- Remove MaxPermGen and related options\n- Fix some reflection / TODOs around Java 8+ methods\n- Update doc references to 1.7/1.8 differences\n- Remove Java 7/8 related build profiles\n- Update some plugins for better Java 8 compatibility\n- Fix a few Java-related warnings\n\nFor the future:\n\n- Update Java 8 examples to fully use Java 8\n- Update Java tests to use lambdas for simplicity\n- Update Java internal implementations to use lambdas\n\n## How was this patch tested?\n\nExisting tests\n\nAuthor: Sean Owen <sowen@cloudera.com>\n\nCloses #16871 from srowen/SPARK-19493.\n","date":"2017-02-16 20:32:45","modifiedFileCount":"51","status":"B","submitter":"Sean Owen"},{"authorTime":"2017-02-20 01:42:50","codes":[{"authorDate":"2017-02-16 20:32:45","commitOrder":2,"curCode":"  public void binaryFiles() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    FileChannel channel1 = fos1.getChannel();\n    ByteBuffer bbuf = ByteBuffer.wrap(content1);\n    channel1.write(bbuf);\n    channel1.close();\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName, 3);\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","date":"2017-02-16 20:32:45","endLine":1164,"groupId":"196","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"binaryFiles","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/80/aab100aced46cbe2866850ce4da7935d650b60.src","preCode":"  public void binaryFiles() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    FileChannel channel1 = fos1.getChannel();\n    ByteBuffer bbuf = ByteBuffer.wrap(content1);\n    channel1.write(bbuf);\n    channel1.close();\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName, 3);\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","realPath":"core/src/test/java/test/org/apache/spark/JavaAPISuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":1146,"status":"N"},{"authorDate":"2017-02-20 01:42:50","commitOrder":2,"curCode":"  public void binaryFilesCaching() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    FileChannel channel1 = fos1.getChannel();\n    ByteBuffer bbuf = ByteBuffer.wrap(content1);\n    channel1.write(bbuf);\n    channel1.close();\n\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName).cache();\n    readRDD.foreach(pair -> pair._2().toArray()); \r\n\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","date":"2017-02-20 01:42:50","endLine":1031,"groupId":"3228","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"binaryFilesCaching","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/51/2149127d72f4f8f7ed7438b9ba9015a07e254f.src","preCode":"  public void binaryFilesCaching() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    FileChannel channel1 = fos1.getChannel();\n    ByteBuffer bbuf = ByteBuffer.wrap(content1);\n    channel1.write(bbuf);\n    channel1.close();\n\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName).cache();\n    readRDD.foreach(new VoidFunction<Tuple2<String,PortableDataStream>>() {\n      @Override\n      public void call(Tuple2<String, PortableDataStream> pair) {\n        pair._2().toArray(); \r\n      }\n    });\n\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","realPath":"core/src/test/java/test/org/apache/spark/JavaAPISuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":1010,"status":"M"}],"commitId":"1487c9af20a333ead55955acf4c0aa323bea0d07","commitMessage":"@@@[SPARK-19534][TESTS] Convert Java tests to use lambdas.  Java 8 features\n\n## What changes were proposed in this pull request?\n\nConvert tests to use Java 8 lambdas.  and modest related fixes to surrounding code.\n\n## How was this patch tested?\n\nJenkins tests\n\nAuthor: Sean Owen <sowen@cloudera.com>\n\nCloses #16964 from srowen/SPARK-19534.\n","date":"2017-02-20 01:42:50","modifiedFileCount":"45","status":"M","submitter":"Sean Owen"},{"authorTime":"2018-10-05 09:58:25","codes":[{"authorDate":"2018-10-05 09:58:25","commitOrder":3,"curCode":"  public void binaryFiles() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    try (FileChannel channel1 = fos1.getChannel()) {\n      ByteBuffer bbuf = ByteBuffer.wrap(content1);\n      channel1.write(bbuf);\n    }\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName, 3);\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","date":"2018-10-05 09:58:25","endLine":1009,"groupId":"2757","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"binaryFiles","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/39/92ab7049bddd3ce5f5630f68b78c5edf378b9c.src","preCode":"  public void binaryFiles() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    FileChannel channel1 = fos1.getChannel();\n    ByteBuffer bbuf = ByteBuffer.wrap(content1);\n    channel1.write(bbuf);\n    channel1.close();\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName, 3);\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","realPath":"core/src/test/java/test/org/apache/spark/JavaAPISuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":991,"status":"M"},{"authorDate":"2018-10-05 09:58:25","commitOrder":3,"curCode":"  public void binaryFilesCaching() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    try (FileChannel channel1 = fos1.getChannel()) {\n      ByteBuffer bbuf = ByteBuffer.wrap(content1);\n      channel1.write(bbuf);\n    }\n\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName).cache();\n    readRDD.foreach(pair -> pair._2().toArray()); \r\n\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","date":"2018-10-05 09:58:25","endLine":1033,"groupId":"2758","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"binaryFilesCaching","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/39/92ab7049bddd3ce5f5630f68b78c5edf378b9c.src","preCode":"  public void binaryFilesCaching() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    FileChannel channel1 = fos1.getChannel();\n    ByteBuffer bbuf = ByteBuffer.wrap(content1);\n    channel1.write(bbuf);\n    channel1.close();\n\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName).cache();\n    readRDD.foreach(pair -> pair._2().toArray()); \r\n\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","realPath":"core/src/test/java/test/org/apache/spark/JavaAPISuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":1012,"status":"M"}],"commitId":"44c1e1ab1c26560371831b1593f96f30344c4363","commitMessage":"@@@[SPARK-25408] Move to mode ideomatic Java8\n\nWhile working on another PR.  I noticed that there is quite some legacy Java in there that can be beautified. For example the use og features from Java8.  such as:\n- Collection libraries\n- Try-with-resource blocks\n\nNo code has been changed\n\nWhat are your thoughts on this?\n\nThis makes code easier to read.  and using try-with-resource makes is less likely to forget to close something.\n\n## What changes were proposed in this pull request?\n\n(Please fill in changes proposed in this fix)\n\n## How was this patch tested?\n\n(Please explain how this patch was tested. E.g. unit tests.  integration tests.  manual tests)\n(If this patch involves UI changes.  please attach a screenshot; otherwise.  remove this)\n\nPlease review http://spark.apache.org/contributing.html before opening a pull request.\n\nCloses #22399 from Fokko/SPARK-25408.\n\nAuthored-by: Fokko Driesprong <fokkodriesprong@godatadriven.com>\nSigned-off-by: Sean Owen <sean.owen@databricks.com>\n","date":"2018-10-05 09:58:25","modifiedFileCount":"19","status":"M","submitter":"Fokko Driesprong"},{"authorTime":"2018-10-05 11:03:41","codes":[{"authorDate":"2018-10-05 11:03:41","commitOrder":4,"curCode":"  public void binaryFiles() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    FileChannel channel1 = fos1.getChannel();\n    ByteBuffer bbuf = ByteBuffer.wrap(content1);\n    channel1.write(bbuf);\n    channel1.close();\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName, 3);\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","date":"2018-10-05 11:03:41","endLine":1009,"groupId":"196","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"binaryFiles","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/01/b5fb7b466846beaee6a5018bcdb611688567c4.src","preCode":"  public void binaryFiles() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    try (FileChannel channel1 = fos1.getChannel()) {\n      ByteBuffer bbuf = ByteBuffer.wrap(content1);\n      channel1.write(bbuf);\n    }\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName, 3);\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","realPath":"core/src/test/java/test/org/apache/spark/JavaAPISuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":991,"status":"M"},{"authorDate":"2018-10-05 11:03:41","commitOrder":4,"curCode":"  public void binaryFilesCaching() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    FileChannel channel1 = fos1.getChannel();\n    ByteBuffer bbuf = ByteBuffer.wrap(content1);\n    channel1.write(bbuf);\n    channel1.close();\n\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName).cache();\n    readRDD.foreach(pair -> pair._2().toArray()); \r\n\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","date":"2018-10-05 11:03:41","endLine":1033,"groupId":"3228","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"binaryFilesCaching","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/01/b5fb7b466846beaee6a5018bcdb611688567c4.src","preCode":"  public void binaryFilesCaching() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    try (FileChannel channel1 = fos1.getChannel()) {\n      ByteBuffer bbuf = ByteBuffer.wrap(content1);\n      channel1.write(bbuf);\n    }\n\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName).cache();\n    readRDD.foreach(pair -> pair._2().toArray()); \r\n\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","realPath":"core/src/test/java/test/org/apache/spark/JavaAPISuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":1012,"status":"M"}],"commitId":"5ae20cf1a96a33f5de4435fcfb55914d64466525","commitMessage":"@@@Revert \"[SPARK-25408] Move to mode ideomatic Java8\"\n\nThis reverts commit 44c1e1ab1c26560371831b1593f96f30344c4363.\n","date":"2018-10-05 11:03:41","modifiedFileCount":"19","status":"M","submitter":"Wenchen Fan"},{"authorTime":"2018-10-08 22:58:52","codes":[{"authorDate":"2018-10-08 22:58:52","commitOrder":5,"curCode":"  public void binaryFiles() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    try (FileChannel channel1 = fos1.getChannel()) {\n      ByteBuffer bbuf = ByteBuffer.wrap(content1);\n      channel1.write(bbuf);\n    }\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName, 3);\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","date":"2018-10-08 22:58:52","endLine":1009,"groupId":"10549","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"binaryFiles","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/39/92ab7049bddd3ce5f5630f68b78c5edf378b9c.src","preCode":"  public void binaryFiles() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    FileChannel channel1 = fos1.getChannel();\n    ByteBuffer bbuf = ByteBuffer.wrap(content1);\n    channel1.write(bbuf);\n    channel1.close();\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName, 3);\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","realPath":"core/src/test/java/test/org/apache/spark/JavaAPISuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":991,"status":"M"},{"authorDate":"2018-10-08 22:58:52","commitOrder":5,"curCode":"  public void binaryFilesCaching() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    try (FileChannel channel1 = fos1.getChannel()) {\n      ByteBuffer bbuf = ByteBuffer.wrap(content1);\n      channel1.write(bbuf);\n    }\n\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName).cache();\n    readRDD.foreach(pair -> pair._2().toArray()); \r\n\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","date":"2018-10-08 22:58:52","endLine":1033,"groupId":"10549","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"binaryFilesCaching","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/39/92ab7049bddd3ce5f5630f68b78c5edf378b9c.src","preCode":"  public void binaryFilesCaching() throws Exception {\n    \r\n    byte[] content1 = \"spark is easy to use.\\n\".getBytes(StandardCharsets.UTF_8);\n\n    String tempDirName = tempDir.getAbsolutePath();\n    File file1 = new File(tempDirName + \"/part-00000\");\n\n    FileOutputStream fos1 = new FileOutputStream(file1);\n\n    FileChannel channel1 = fos1.getChannel();\n    ByteBuffer bbuf = ByteBuffer.wrap(content1);\n    channel1.write(bbuf);\n    channel1.close();\n\n    JavaPairRDD<String, PortableDataStream> readRDD = sc.binaryFiles(tempDirName).cache();\n    readRDD.foreach(pair -> pair._2().toArray()); \r\n\n    List<Tuple2<String, PortableDataStream>> result = readRDD.collect();\n    for (Tuple2<String, PortableDataStream> res : result) {\n      assertArrayEquals(content1, res._2().toArray());\n    }\n  }\n","realPath":"core/src/test/java/test/org/apache/spark/JavaAPISuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":1012,"status":"M"}],"commitId":"1a28625355d75076bde4bcc95a72e9b187cda606","commitMessage":"@@@[SPARK-25408] Move to more ideomatic Java8\n\nWhile working on another PR.  I noticed that there is quite some legacy Java in there that can be beautified. For example the use of features from Java8.  such as:\n- Collection libraries\n- Try-with-resource blocks\n\nNo logic has been changed. I think it is important to have a solid codebase with examples that will inspire next PR's to follow up on the best practices.\n\nWhat are your thoughts on this?\n\nThis makes code easier to read.  and using try-with-resource makes is less likely to forget to close something.\n\n## What changes were proposed in this pull request?\n\nNo changes in the logic of Spark.  but more in the aesthetics of the code.\n\n## How was this patch tested?\n\nUsing the existing unit tests. Since no logic is changed.  the existing unit tests should pass.\n\nPlease review http://spark.apache.org/contributing.html before opening a pull request.\n\nCloses #22637 from Fokko/SPARK-25408.\n\nAuthored-by: Fokko Driesprong <fokkodriesprong@godatadriven.com>\nSigned-off-by: Sean Owen <sean.owen@databricks.com>\n","date":"2018-10-08 22:58:52","modifiedFileCount":"17","status":"M","submitter":"Fokko Driesprong"}]
