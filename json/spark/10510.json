[{"authorTime":"2016-02-23 09:21:37","codes":[{"authorDate":"2016-02-23 09:21:37","commitOrder":1,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaPipelineExample\");\n    SparkContext sc = new SparkContext(conf);\n    SQLContext sqlContext = new SQLContext(sc);\n\n    \r\n    \r\n    DataFrame training = sqlContext.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L, \"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    PipelineModel model = pipeline.fit(training);\n\n    \r\n    DataFrame test = sqlContext.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    DataFrame predictions = model.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collect()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    sc.stop();\n  }\n","date":"2016-02-23 09:21:37","endLine":90,"groupId":"3366","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/34/07c25c83c37d56cb23742cc37260f1387f5de9.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaPipelineExample\");\n    SparkContext sc = new SparkContext(conf);\n    SQLContext sqlContext = new SQLContext(sc);\n\n    \r\n    \r\n    DataFrame training = sqlContext.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L, \"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    PipelineModel model = pipeline.fit(training);\n\n    \r\n    DataFrame test = sqlContext.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    DataFrame predictions = model.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collect()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    sc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"B"},{"authorDate":"2016-02-23 09:21:37","commitOrder":1,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf()\n      .setAppName(\"JavaModelSelectionViaCrossValidationExample\");\n    SparkContext sc = new SparkContext(conf);\n    SQLContext sqlContext = new SQLContext(sc);\n\n    \r\n    \r\n    DataFrame training = sqlContext.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L,\"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0),\n      new JavaLabeledDocument(4L, \"b spark who\", 1.0),\n      new JavaLabeledDocument(5L, \"g d a y\", 0.0),\n      new JavaLabeledDocument(6L, \"spark fly\", 1.0),\n      new JavaLabeledDocument(7L, \"was mapreduce\", 0.0),\n      new JavaLabeledDocument(8L, \"e spark program\", 1.0),\n      new JavaLabeledDocument(9L, \"a e c l\", 0.0),\n      new JavaLabeledDocument(10L, \"spark compile\", 1.0),\n      new JavaLabeledDocument(11L, \"hadoop software\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    \r\n    \r\n    ParamMap[] paramGrid = new ParamGridBuilder()\n      .addGrid(hashingTF.numFeatures(), new int[] {10, 100, 1000})\n      .addGrid(lr.regParam(), new double[] {0.1, 0.01})\n      .build();\n\n    \r\n    \r\n    \r\n    \r\n    \r\n    CrossValidator cv = new CrossValidator()\n      .setEstimator(pipeline)\n      .setEvaluator(new BinaryClassificationEvaluator())\n      .setEstimatorParamMaps(paramGrid).setNumFolds(2);  \r\n\n    \r\n    CrossValidatorModel cvModel = cv.fit(training);\n\n    \r\n    DataFrame test = sqlContext.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    DataFrame predictions = cvModel.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collect()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    sc.stop();\n  }\n","date":"2016-02-23 09:21:37","endLine":121,"groupId":"3366","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/87/ad119491e9a672a5a8252a5f1d3fa32ea4c871.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf()\n      .setAppName(\"JavaModelSelectionViaCrossValidationExample\");\n    SparkContext sc = new SparkContext(conf);\n    SQLContext sqlContext = new SQLContext(sc);\n\n    \r\n    \r\n    DataFrame training = sqlContext.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L,\"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0),\n      new JavaLabeledDocument(4L, \"b spark who\", 1.0),\n      new JavaLabeledDocument(5L, \"g d a y\", 0.0),\n      new JavaLabeledDocument(6L, \"spark fly\", 1.0),\n      new JavaLabeledDocument(7L, \"was mapreduce\", 0.0),\n      new JavaLabeledDocument(8L, \"e spark program\", 1.0),\n      new JavaLabeledDocument(9L, \"a e c l\", 0.0),\n      new JavaLabeledDocument(10L, \"spark compile\", 1.0),\n      new JavaLabeledDocument(11L, \"hadoop software\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    \r\n    \r\n    ParamMap[] paramGrid = new ParamGridBuilder()\n      .addGrid(hashingTF.numFeatures(), new int[] {10, 100, 1000})\n      .addGrid(lr.regParam(), new double[] {0.1, 0.01})\n      .build();\n\n    \r\n    \r\n    \r\n    \r\n    \r\n    CrossValidator cv = new CrossValidator()\n      .setEstimator(pipeline)\n      .setEvaluator(new BinaryClassificationEvaluator())\n      .setEstimatorParamMaps(paramGrid).setNumFolds(2);  \r\n\n    \r\n    CrossValidatorModel cvModel = cv.fit(training);\n\n    \r\n    DataFrame test = sqlContext.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    DataFrame predictions = cvModel.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collect()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    sc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":46,"status":"B"}],"commitId":"02b1fefffb00d50c1076a26f2f3f41f3c1fa0001","commitMessage":"@@@[SPARK-13012][DOCUMENTATION] Replace example code in ml-guide.md using include_example\n\nReplaced example code in ml-guide.md using include_example\n\nAuthor: Devaraj K <devaraj@apache.org>\n\nCloses #11053 from devaraj-kavali/SPARK-13012.\n","date":"2016-02-23 09:21:37","modifiedFileCount":"0","status":"B","submitter":"Devaraj K"},{"authorTime":"2016-03-11 09:00:17","codes":[{"authorDate":"2016-03-11 09:00:17","commitOrder":2,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaPipelineExample\");\n    SparkContext sc = new SparkContext(conf);\n    SQLContext sqlContext = new SQLContext(sc);\n\n    \r\n    \r\n    Dataset<Row> training = sqlContext.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L, \"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    PipelineModel model = pipeline.fit(training);\n\n    \r\n    Dataset<Row> test = sqlContext.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    Dataset<Row> predictions = model.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collectRows()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    sc.stop();\n  }\n","date":"2016-03-11 09:00:17","endLine":90,"groupId":"684","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/6a/e418d564d1fecffe02bda1200bb6223a9ec05b.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaPipelineExample\");\n    SparkContext sc = new SparkContext(conf);\n    SQLContext sqlContext = new SQLContext(sc);\n\n    \r\n    \r\n    DataFrame training = sqlContext.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L, \"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    PipelineModel model = pipeline.fit(training);\n\n    \r\n    DataFrame test = sqlContext.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    DataFrame predictions = model.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collect()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    sc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"M"},{"authorDate":"2016-03-11 09:00:17","commitOrder":2,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf()\n      .setAppName(\"JavaModelSelectionViaCrossValidationExample\");\n    SparkContext sc = new SparkContext(conf);\n    SQLContext sqlContext = new SQLContext(sc);\n\n    \r\n    \r\n    Dataset<Row> training = sqlContext.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L,\"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0),\n      new JavaLabeledDocument(4L, \"b spark who\", 1.0),\n      new JavaLabeledDocument(5L, \"g d a y\", 0.0),\n      new JavaLabeledDocument(6L, \"spark fly\", 1.0),\n      new JavaLabeledDocument(7L, \"was mapreduce\", 0.0),\n      new JavaLabeledDocument(8L, \"e spark program\", 1.0),\n      new JavaLabeledDocument(9L, \"a e c l\", 0.0),\n      new JavaLabeledDocument(10L, \"spark compile\", 1.0),\n      new JavaLabeledDocument(11L, \"hadoop software\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    \r\n    \r\n    ParamMap[] paramGrid = new ParamGridBuilder()\n      .addGrid(hashingTF.numFeatures(), new int[] {10, 100, 1000})\n      .addGrid(lr.regParam(), new double[] {0.1, 0.01})\n      .build();\n\n    \r\n    \r\n    \r\n    \r\n    \r\n    CrossValidator cv = new CrossValidator()\n      .setEstimator(pipeline)\n      .setEvaluator(new BinaryClassificationEvaluator())\n      .setEstimatorParamMaps(paramGrid).setNumFolds(2);  \r\n\n    \r\n    CrossValidatorModel cvModel = cv.fit(training);\n\n    \r\n    Dataset<Row> test = sqlContext.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    Dataset<Row> predictions = cvModel.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collectRows()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    sc.stop();\n  }\n","date":"2016-03-11 09:00:17","endLine":121,"groupId":"3582","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/e3/94605db70ea001b39b9ffe75f90ab66cd47e7e.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf()\n      .setAppName(\"JavaModelSelectionViaCrossValidationExample\");\n    SparkContext sc = new SparkContext(conf);\n    SQLContext sqlContext = new SQLContext(sc);\n\n    \r\n    \r\n    DataFrame training = sqlContext.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L,\"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0),\n      new JavaLabeledDocument(4L, \"b spark who\", 1.0),\n      new JavaLabeledDocument(5L, \"g d a y\", 0.0),\n      new JavaLabeledDocument(6L, \"spark fly\", 1.0),\n      new JavaLabeledDocument(7L, \"was mapreduce\", 0.0),\n      new JavaLabeledDocument(8L, \"e spark program\", 1.0),\n      new JavaLabeledDocument(9L, \"a e c l\", 0.0),\n      new JavaLabeledDocument(10L, \"spark compile\", 1.0),\n      new JavaLabeledDocument(11L, \"hadoop software\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    \r\n    \r\n    ParamMap[] paramGrid = new ParamGridBuilder()\n      .addGrid(hashingTF.numFeatures(), new int[] {10, 100, 1000})\n      .addGrid(lr.regParam(), new double[] {0.1, 0.01})\n      .build();\n\n    \r\n    \r\n    \r\n    \r\n    \r\n    CrossValidator cv = new CrossValidator()\n      .setEstimator(pipeline)\n      .setEvaluator(new BinaryClassificationEvaluator())\n      .setEstimatorParamMaps(paramGrid).setNumFolds(2);  \r\n\n    \r\n    CrossValidatorModel cvModel = cv.fit(training);\n\n    \r\n    DataFrame test = sqlContext.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    DataFrame predictions = cvModel.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collect()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    sc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":46,"status":"M"}],"commitId":"1d542785b9949e7f92025e6754973a779cc37c52","commitMessage":"@@@[SPARK-13244][SQL] Migrates DataFrame to Dataset\n\n## What changes were proposed in this pull request?\n\nThis PR unifies DataFrame and Dataset by migrating existing DataFrame operations to Dataset and make `DataFrame` a type alias of `Dataset[Row]`.\n\nMost Scala code changes are source compatible.  but Java API is broken as Java knows nothing about Scala type alias (mostly replacing `DataFrame` with `Dataset<Row>`).\n\nThere are several noticeable API changes related to those returning arrays:\n\n1.  `collect`/`take`\n\n    -   Old APIs in class `DataFrame`:\n\n        ```scala\n        def collect(): Array[Row]\n        def take(n: Int): Array[Row]\n        ```\n\n    -   New APIs in class `Dataset[T]`:\n\n        ```scala\n        def collect(): Array[T]\n        def take(n: Int): Array[T]\n\n        def collectRows(): Array[Row]\n        def takeRows(n: Int): Array[Row]\n        ```\n\n    Two specialized methods `collectRows` and `takeRows` are added because Java doesn't support returning generic arrays. Thus.  for example.  `DataFrame.collect(): Array[T]` actually returns `Object` instead of `Array<T>` from Java side.\n\n    Normally.  Java users may fall back to `collectAsList` and `takeAsList`.  The two new specialized versions are added to avoid performance regression in ML related code (but maybe I'm wrong and they are not necessary here).\n\n1.  `randomSplit`\n\n    -   Old APIs in class `DataFrame`:\n\n        ```scala\n        def randomSplit(weights: Array[Double].  seed: Long): Array[DataFrame]\n        def randomSplit(weights: Array[Double]): Array[DataFrame]\n        ```\n\n    -   New APIs in class `Dataset[T]`:\n\n        ```scala\n        def randomSplit(weights: Array[Double].  seed: Long): Array[Dataset[T]]\n        def randomSplit(weights: Array[Double]): Array[Dataset[T]]\n        ```\n\n    Similar problem as above.  but hasn't been addressed for Java API yet.  We can probably add `randomSplitAsList` to fix this one.\n\n1.  `groupBy`\n\n    Some original `DataFrame.groupBy` methods have conflicting signature with original `Dataset.groupBy` methods.  To distinguish these two.  typed `Dataset.groupBy` methods are renamed to `groupByKey`.\n\nOther noticeable changes:\n\n1.  Dataset always do eager analysis now\n\n    We used to support disabling DataFrame eager analysis to help reporting partially analyzed malformed logical plan on analysis failure.  However.  Dataset encoders requires eager analysi during Dataset construction.  To preserve the error reporting feature.  `AnalysisException` now takes an extra `Option[LogicalPlan]` argument to hold the partially analyzed plan.  so that we can check the plan tree when reporting test failures.  This plan is passed by `QueryExecution.assertAnalyzed`.\n\n## How was this patch tested?\n\nExisting tests do the work.\n\n## TODO\n\n- [ ] Fix all tests\n- [ ] Re-enable MiMA check\n- [ ] Update ScalaDoc (`since`.  `group`.  and example code)\n\nAuthor: Cheng Lian <lian@databricks.com>\nAuthor: Yin Huai <yhuai@databricks.com>\nAuthor: Wenchen Fan <wenchen@databricks.com>\nAuthor: Cheng Lian <liancheng@users.noreply.github.com>\n\nCloses #11443 from liancheng/ds-to-df.\n","date":"2016-03-11 09:00:17","modifiedFileCount":"87","status":"M","submitter":"Cheng Lian"},{"authorTime":"2016-03-13 12:02:52","codes":[{"authorDate":"2016-03-13 12:02:52","commitOrder":3,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaPipelineExample\");\n    SparkContext sc = new SparkContext(conf);\n    SQLContext sqlContext = new SQLContext(sc);\n\n    \r\n    \r\n    Dataset<Row> training = sqlContext.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L, \"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    PipelineModel model = pipeline.fit(training);\n\n    \r\n    Dataset<Row> test = sqlContext.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    Dataset<Row> predictions = model.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collectAsList()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    sc.stop();\n  }\n","date":"2016-03-13 12:02:52","endLine":90,"groupId":"684","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/30/5420f208b79ad5e8e82c13d876bd8abbd07982.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaPipelineExample\");\n    SparkContext sc = new SparkContext(conf);\n    SQLContext sqlContext = new SQLContext(sc);\n\n    \r\n    \r\n    Dataset<Row> training = sqlContext.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L, \"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    PipelineModel model = pipeline.fit(training);\n\n    \r\n    Dataset<Row> test = sqlContext.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    Dataset<Row> predictions = model.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collectRows()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    sc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"M"},{"authorDate":"2016-03-13 12:02:52","commitOrder":3,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf()\n      .setAppName(\"JavaModelSelectionViaCrossValidationExample\");\n    SparkContext sc = new SparkContext(conf);\n    SQLContext sqlContext = new SQLContext(sc);\n\n    \r\n    \r\n    Dataset<Row> training = sqlContext.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L,\"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0),\n      new JavaLabeledDocument(4L, \"b spark who\", 1.0),\n      new JavaLabeledDocument(5L, \"g d a y\", 0.0),\n      new JavaLabeledDocument(6L, \"spark fly\", 1.0),\n      new JavaLabeledDocument(7L, \"was mapreduce\", 0.0),\n      new JavaLabeledDocument(8L, \"e spark program\", 1.0),\n      new JavaLabeledDocument(9L, \"a e c l\", 0.0),\n      new JavaLabeledDocument(10L, \"spark compile\", 1.0),\n      new JavaLabeledDocument(11L, \"hadoop software\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    \r\n    \r\n    ParamMap[] paramGrid = new ParamGridBuilder()\n      .addGrid(hashingTF.numFeatures(), new int[] {10, 100, 1000})\n      .addGrid(lr.regParam(), new double[] {0.1, 0.01})\n      .build();\n\n    \r\n    \r\n    \r\n    \r\n    \r\n    CrossValidator cv = new CrossValidator()\n      .setEstimator(pipeline)\n      .setEvaluator(new BinaryClassificationEvaluator())\n      .setEstimatorParamMaps(paramGrid).setNumFolds(2);  \r\n\n    \r\n    CrossValidatorModel cvModel = cv.fit(training);\n\n    \r\n    Dataset<Row> test = sqlContext.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    Dataset<Row> predictions = cvModel.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collectAsList()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    sc.stop();\n  }\n","date":"2016-03-13 12:02:52","endLine":121,"groupId":"3582","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/c4/122d1247a94f5a4deb1929732f9a8accc6f9af.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf()\n      .setAppName(\"JavaModelSelectionViaCrossValidationExample\");\n    SparkContext sc = new SparkContext(conf);\n    SQLContext sqlContext = new SQLContext(sc);\n\n    \r\n    \r\n    Dataset<Row> training = sqlContext.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L,\"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0),\n      new JavaLabeledDocument(4L, \"b spark who\", 1.0),\n      new JavaLabeledDocument(5L, \"g d a y\", 0.0),\n      new JavaLabeledDocument(6L, \"spark fly\", 1.0),\n      new JavaLabeledDocument(7L, \"was mapreduce\", 0.0),\n      new JavaLabeledDocument(8L, \"e spark program\", 1.0),\n      new JavaLabeledDocument(9L, \"a e c l\", 0.0),\n      new JavaLabeledDocument(10L, \"spark compile\", 1.0),\n      new JavaLabeledDocument(11L, \"hadoop software\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    \r\n    \r\n    ParamMap[] paramGrid = new ParamGridBuilder()\n      .addGrid(hashingTF.numFeatures(), new int[] {10, 100, 1000})\n      .addGrid(lr.regParam(), new double[] {0.1, 0.01})\n      .build();\n\n    \r\n    \r\n    \r\n    \r\n    \r\n    CrossValidator cv = new CrossValidator()\n      .setEstimator(pipeline)\n      .setEvaluator(new BinaryClassificationEvaluator())\n      .setEstimatorParamMaps(paramGrid).setNumFolds(2);  \r\n\n    \r\n    CrossValidatorModel cvModel = cv.fit(training);\n\n    \r\n    Dataset<Row> test = sqlContext.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    Dataset<Row> predictions = cvModel.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collectRows()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    sc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":46,"status":"M"}],"commitId":"c079420d7c55d8972db716a2695a5ddd606d11cd","commitMessage":"@@@[SPARK-13841][SQL] Removes Dataset.collectRows()/takeRows()\n\n## What changes were proposed in this pull request?\n\nThis PR removes two methods.  `collectRows()` and `takeRows()`.  from `Dataset[T]`. These methods were added in PR #11443.  and were later considered not useful.\n\n## How was this patch tested?\n\nExisting tests should do the work.\n\nAuthor: Cheng Lian <lian@databricks.com>\n\nCloses #11678 from liancheng/remove-collect-rows-and-take-rows.\n","date":"2016-03-13 12:02:52","modifiedFileCount":"16","status":"M","submitter":"Cheng Lian"},{"authorTime":"2016-05-05 05:31:36","codes":[{"authorDate":"2016-05-05 05:31:36","commitOrder":4,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession.builder().appName(\"JavaPipelineExample\").getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> training = spark.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L, \"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    PipelineModel model = pipeline.fit(training);\n\n    \r\n    Dataset<Row> test = spark.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    Dataset<Row> predictions = model.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collectAsList()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2016-05-05 05:31:36","endLine":84,"groupId":"867","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/55/6a4573263f148bcc19a0a83dce8d21353a7fd7.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaPipelineExample\");\n    SparkContext sc = new SparkContext(conf);\n    SQLContext sqlContext = new SQLContext(sc);\n\n    \r\n    \r\n    Dataset<Row> training = sqlContext.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L, \"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    PipelineModel model = pipeline.fit(training);\n\n    \r\n    Dataset<Row> test = sqlContext.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    Dataset<Row> predictions = model.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collectAsList()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    sc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":38,"status":"M"},{"authorDate":"2016-05-05 05:31:36","commitOrder":4,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder().appName(\"JavaModelSelectionViaCrossValidationExample\").getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> training = spark.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L,\"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0),\n      new JavaLabeledDocument(4L, \"b spark who\", 1.0),\n      new JavaLabeledDocument(5L, \"g d a y\", 0.0),\n      new JavaLabeledDocument(6L, \"spark fly\", 1.0),\n      new JavaLabeledDocument(7L, \"was mapreduce\", 0.0),\n      new JavaLabeledDocument(8L, \"e spark program\", 1.0),\n      new JavaLabeledDocument(9L, \"a e c l\", 0.0),\n      new JavaLabeledDocument(10L, \"spark compile\", 1.0),\n      new JavaLabeledDocument(11L, \"hadoop software\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    \r\n    \r\n    ParamMap[] paramGrid = new ParamGridBuilder()\n      .addGrid(hashingTF.numFeatures(), new int[] {10, 100, 1000})\n      .addGrid(lr.regParam(), new double[] {0.1, 0.01})\n      .build();\n\n    \r\n    \r\n    \r\n    \r\n    \r\n    CrossValidator cv = new CrossValidator()\n      .setEstimator(pipeline)\n      .setEvaluator(new BinaryClassificationEvaluator())\n      .setEstimatorParamMaps(paramGrid).setNumFolds(2);  \r\n\n    \r\n    CrossValidatorModel cvModel = cv.fit(training);\n\n    \r\n    Dataset<Row> test = spark.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    Dataset<Row> predictions = cvModel.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collectAsList()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2016-05-05 05:31:36","endLine":117,"groupId":"741","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/a4/ec4f58154f64759d11ca2a5c11e4bb12602cbb.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf()\n      .setAppName(\"JavaModelSelectionViaCrossValidationExample\");\n    SparkContext sc = new SparkContext(conf);\n    SQLContext sqlContext = new SQLContext(sc);\n\n    \r\n    \r\n    Dataset<Row> training = sqlContext.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L,\"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0),\n      new JavaLabeledDocument(4L, \"b spark who\", 1.0),\n      new JavaLabeledDocument(5L, \"g d a y\", 0.0),\n      new JavaLabeledDocument(6L, \"spark fly\", 1.0),\n      new JavaLabeledDocument(7L, \"was mapreduce\", 0.0),\n      new JavaLabeledDocument(8L, \"e spark program\", 1.0),\n      new JavaLabeledDocument(9L, \"a e c l\", 0.0),\n      new JavaLabeledDocument(10L, \"spark compile\", 1.0),\n      new JavaLabeledDocument(11L, \"hadoop software\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    \r\n    \r\n    ParamMap[] paramGrid = new ParamGridBuilder()\n      .addGrid(hashingTF.numFeatures(), new int[] {10, 100, 1000})\n      .addGrid(lr.regParam(), new double[] {0.1, 0.01})\n      .build();\n\n    \r\n    \r\n    \r\n    \r\n    \r\n    CrossValidator cv = new CrossValidator()\n      .setEstimator(pipeline)\n      .setEvaluator(new BinaryClassificationEvaluator())\n      .setEstimatorParamMaps(paramGrid).setNumFolds(2);  \r\n\n    \r\n    CrossValidatorModel cvModel = cv.fit(training);\n\n    \r\n    Dataset<Row> test = sqlContext.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    Dataset<Row> predictions = cvModel.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collectAsList()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    sc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":44,"status":"M"}],"commitId":"cdce4e62a5674e2034e5d395578b1a60e3d8c435","commitMessage":"@@@[SPARK-15031][EXAMPLE] Use SparkSession in Scala/Python/Java example.\n\n## What changes were proposed in this pull request?\n\nThis PR aims to update Scala/Python/Java examples by replacing `SQLContext` with newly added `SparkSession`.\n\n- Use **SparkSession Builder Pattern** in 154(Scala 55.  Java 52.  Python 47) files.\n- Add `getConf` in Python SparkContext class: `python/pyspark/context.py`\n- Replace **SQLContext Singleton Pattern** with **SparkSession Singleton Pattern**:\n  - `SqlNetworkWordCount.scala`\n  - `JavaSqlNetworkWordCount.java`\n  - `sql_network_wordcount.py`\n\nNow.  `SQLContexts` are used only in R examples and the following two Python examples. The python examples are untouched in this PR since it already fails some unknown issue.\n- `simple_params_example.py`\n- `aft_survival_regression.py`\n\n## How was this patch tested?\n\nManual.\n\nAuthor: Dongjoon Hyun <dongjoon@apache.org>\n\nCloses #12809 from dongjoon-hyun/SPARK-15031.\n","date":"2016-05-05 05:31:36","modifiedFileCount":"52","status":"M","submitter":"Dongjoon Hyun"},{"authorTime":"2016-05-05 05:31:36","codes":[{"authorDate":"2016-07-14 16:12:46","commitOrder":5,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaPipelineExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> training = spark.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L, \"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.001);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    PipelineModel model = pipeline.fit(training);\n\n    \r\n    Dataset<Row> test = spark.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"spark hadoop spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    Dataset<Row> predictions = model.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collectAsList()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2016-07-14 16:12:46","endLine":87,"groupId":"867","id":9,"instanceNumber":1,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/4c/cd8f6ce265047f9423305a97582e0c62e644c6.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaPipelineExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> training = spark.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L, \"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    PipelineModel model = pipeline.fit(training);\n\n    \r\n    Dataset<Row> test = spark.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    Dataset<Row> predictions = model.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collectAsList()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":38,"status":"M"},{"authorDate":"2016-05-05 05:31:36","commitOrder":5,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder().appName(\"JavaModelSelectionViaCrossValidationExample\").getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> training = spark.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L,\"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0),\n      new JavaLabeledDocument(4L, \"b spark who\", 1.0),\n      new JavaLabeledDocument(5L, \"g d a y\", 0.0),\n      new JavaLabeledDocument(6L, \"spark fly\", 1.0),\n      new JavaLabeledDocument(7L, \"was mapreduce\", 0.0),\n      new JavaLabeledDocument(8L, \"e spark program\", 1.0),\n      new JavaLabeledDocument(9L, \"a e c l\", 0.0),\n      new JavaLabeledDocument(10L, \"spark compile\", 1.0),\n      new JavaLabeledDocument(11L, \"hadoop software\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    \r\n    \r\n    ParamMap[] paramGrid = new ParamGridBuilder()\n      .addGrid(hashingTF.numFeatures(), new int[] {10, 100, 1000})\n      .addGrid(lr.regParam(), new double[] {0.1, 0.01})\n      .build();\n\n    \r\n    \r\n    \r\n    \r\n    \r\n    CrossValidator cv = new CrossValidator()\n      .setEstimator(pipeline)\n      .setEvaluator(new BinaryClassificationEvaluator())\n      .setEstimatorParamMaps(paramGrid).setNumFolds(2);  \r\n\n    \r\n    CrossValidatorModel cvModel = cv.fit(training);\n\n    \r\n    Dataset<Row> test = spark.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    Dataset<Row> predictions = cvModel.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collectAsList()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2016-05-05 05:31:36","endLine":117,"groupId":"741","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/a4/ec4f58154f64759d11ca2a5c11e4bb12602cbb.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder().appName(\"JavaModelSelectionViaCrossValidationExample\").getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> training = spark.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L,\"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0),\n      new JavaLabeledDocument(4L, \"b spark who\", 1.0),\n      new JavaLabeledDocument(5L, \"g d a y\", 0.0),\n      new JavaLabeledDocument(6L, \"spark fly\", 1.0),\n      new JavaLabeledDocument(7L, \"was mapreduce\", 0.0),\n      new JavaLabeledDocument(8L, \"e spark program\", 1.0),\n      new JavaLabeledDocument(9L, \"a e c l\", 0.0),\n      new JavaLabeledDocument(10L, \"spark compile\", 1.0),\n      new JavaLabeledDocument(11L, \"hadoop software\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    \r\n    \r\n    ParamMap[] paramGrid = new ParamGridBuilder()\n      .addGrid(hashingTF.numFeatures(), new int[] {10, 100, 1000})\n      .addGrid(lr.regParam(), new double[] {0.1, 0.01})\n      .build();\n\n    \r\n    \r\n    \r\n    \r\n    \r\n    CrossValidator cv = new CrossValidator()\n      .setEstimator(pipeline)\n      .setEvaluator(new BinaryClassificationEvaluator())\n      .setEstimatorParamMaps(paramGrid).setNumFolds(2);  \r\n\n    \r\n    CrossValidatorModel cvModel = cv.fit(training);\n\n    \r\n    Dataset<Row> test = spark.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    Dataset<Row> predictions = cvModel.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collectAsList()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":44,"status":"N"}],"commitId":"e3f8a033679261aaee15bda0f970a1890411e743","commitMessage":"@@@[SPARK-16403][EXAMPLES] Cleanup to remove unused imports.  consistent style.  minor fixes\n\n## What changes were proposed in this pull request?\n\nCleanup of examples.  mostly from PySpark-ML to fix minor issues:  unused imports.  style consistency.  pipeline_example is a duplicate.  use future print funciton.  and a spelling error.\n\n* The \"Pipeline Example\" is duplicated by \"Simple Text Classification Pipeline\" in Scala.  Python.  and Java.\n\n* \"Estimator Transformer Param Example\" is duplicated by \"Simple Params Example\" in Scala.  Python and Java\n\n* Synced random_forest_classifier_example.py with Scala by adding IndexToString label converted\n\n* Synced train_validation_split.py (in Scala ModelSelectionViaTrainValidationExample) by adjusting data split.  adding grid for intercept.\n\n* RegexTokenizer was doing nothing in tokenizer_example.py and JavaTokenizerExample.java.  synced with Scala version\n\n## How was this patch tested?\nlocal tests and run modified examples\n\nAuthor: Bryan Cutler <cutlerb@gmail.com>\n\nCloses #14081 from BryanCutler/examples-cleanup-SPARK-16403.\n","date":"2016-07-14 16:12:46","modifiedFileCount":"3","status":"M","submitter":"Bryan Cutler"},{"authorTime":"2017-09-06 20:12:27","codes":[{"authorDate":"2016-07-14 16:12:46","commitOrder":6,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaPipelineExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> training = spark.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L, \"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.001);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    PipelineModel model = pipeline.fit(training);\n\n    \r\n    Dataset<Row> test = spark.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"spark hadoop spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    Dataset<Row> predictions = model.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collectAsList()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2016-07-14 16:12:46","endLine":87,"groupId":"10510","id":11,"instanceNumber":1,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/4c/cd8f6ce265047f9423305a97582e0c62e644c6.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaPipelineExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> training = spark.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L, \"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.001);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    PipelineModel model = pipeline.fit(training);\n\n    \r\n    Dataset<Row> test = spark.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"spark hadoop spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    Dataset<Row> predictions = model.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collectAsList()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":38,"status":"N"},{"authorDate":"2017-09-06 20:12:27","commitOrder":6,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaModelSelectionViaCrossValidationExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> training = spark.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L,\"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0),\n      new JavaLabeledDocument(4L, \"b spark who\", 1.0),\n      new JavaLabeledDocument(5L, \"g d a y\", 0.0),\n      new JavaLabeledDocument(6L, \"spark fly\", 1.0),\n      new JavaLabeledDocument(7L, \"was mapreduce\", 0.0),\n      new JavaLabeledDocument(8L, \"e spark program\", 1.0),\n      new JavaLabeledDocument(9L, \"a e c l\", 0.0),\n      new JavaLabeledDocument(10L, \"spark compile\", 1.0),\n      new JavaLabeledDocument(11L, \"hadoop software\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    \r\n    \r\n    ParamMap[] paramGrid = new ParamGridBuilder()\n      .addGrid(hashingTF.numFeatures(), new int[] {10, 100, 1000})\n      .addGrid(lr.regParam(), new double[] {0.1, 0.01})\n      .build();\n\n    \r\n    \r\n    \r\n    \r\n    \r\n    CrossValidator cv = new CrossValidator()\n      .setEstimator(pipeline)\n      .setEvaluator(new BinaryClassificationEvaluator())\n      .setEstimatorParamMaps(paramGrid)\n      .setNumFolds(2)  \r\n      .setParallelism(2);  \r\n\n    \r\n    CrossValidatorModel cvModel = cv.fit(training);\n\n    \r\n    Dataset<Row> test = spark.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    Dataset<Row> predictions = cvModel.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collectAsList()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2017-09-06 20:12:27","endLine":121,"groupId":"10510","id":12,"instanceNumber":2,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/d9/7327969ab264bb2282979c3a034b97ad923e77.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaModelSelectionViaCrossValidationExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> training = spark.createDataFrame(Arrays.asList(\n      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n      new JavaLabeledDocument(1L, \"b d\", 0.0),\n      new JavaLabeledDocument(2L,\"spark f g h\", 1.0),\n      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0),\n      new JavaLabeledDocument(4L, \"b spark who\", 1.0),\n      new JavaLabeledDocument(5L, \"g d a y\", 0.0),\n      new JavaLabeledDocument(6L, \"spark fly\", 1.0),\n      new JavaLabeledDocument(7L, \"was mapreduce\", 0.0),\n      new JavaLabeledDocument(8L, \"e spark program\", 1.0),\n      new JavaLabeledDocument(9L, \"a e c l\", 0.0),\n      new JavaLabeledDocument(10L, \"spark compile\", 1.0),\n      new JavaLabeledDocument(11L, \"hadoop software\", 0.0)\n    ), JavaLabeledDocument.class);\n\n    \r\n    Tokenizer tokenizer = new Tokenizer()\n      .setInputCol(\"text\")\n      .setOutputCol(\"words\");\n    HashingTF hashingTF = new HashingTF()\n      .setNumFeatures(1000)\n      .setInputCol(tokenizer.getOutputCol())\n      .setOutputCol(\"features\");\n    LogisticRegression lr = new LogisticRegression()\n      .setMaxIter(10)\n      .setRegParam(0.01);\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});\n\n    \r\n    \r\n    \r\n    ParamMap[] paramGrid = new ParamGridBuilder()\n      .addGrid(hashingTF.numFeatures(), new int[] {10, 100, 1000})\n      .addGrid(lr.regParam(), new double[] {0.1, 0.01})\n      .build();\n\n    \r\n    \r\n    \r\n    \r\n    \r\n    CrossValidator cv = new CrossValidator()\n      .setEstimator(pipeline)\n      .setEvaluator(new BinaryClassificationEvaluator())\n      .setEstimatorParamMaps(paramGrid).setNumFolds(2);  \r\n\n    \r\n    CrossValidatorModel cvModel = cv.fit(training);\n\n    \r\n    Dataset<Row> test = spark.createDataFrame(Arrays.asList(\n      new JavaDocument(4L, \"spark i j k\"),\n      new JavaDocument(5L, \"l m n\"),\n      new JavaDocument(6L, \"mapreduce spark\"),\n      new JavaDocument(7L, \"apache hadoop\")\n    ), JavaDocument.class);\n\n    \r\n    Dataset<Row> predictions = cvModel.transform(test);\n    for (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collectAsList()) {\n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2)\n        + \", prediction=\" + r.get(3));\n    }\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":44,"status":"M"}],"commitId":"16c4c03c71394ab30c8edaf4418973e1a2c5ebfe","commitMessage":"@@@[SPARK-19357][ML] Adding parallel model evaluation in ML tuning\n\n## What changes were proposed in this pull request?\nModified `CrossValidator` and `TrainValidationSplit` to be able to evaluate models in parallel for a given parameter grid.  The level of parallelism is controlled by a parameter `numParallelEval` used to schedule a number of models to be trained/evaluated so that the jobs can be run concurrently.  This is a naive approach that does not check the cluster for needed resources.  so care must be taken by the user to tune the parameter appropriately.  The default value is `1` which will train/evaluate in serial.\n\n## How was this patch tested?\nAdded unit tests for CrossValidator and TrainValidationSplit to verify that model selection is the same when run in serial vs parallel.  Manual testing to verify tasks run in parallel when param is > 1. Added parameter usage to relevant examples.\n\nAuthor: Bryan Cutler <cutlerb@gmail.com>\n\nCloses #16774 from BryanCutler/parallel-model-eval-SPARK-19357.\n","date":"2017-09-06 20:12:27","modifiedFileCount":"2","status":"M","submitter":"Bryan Cutler"}]
