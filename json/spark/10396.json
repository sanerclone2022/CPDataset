[{"authorTime":"2019-10-03 03:53:39","codes":[{"authorDate":"2019-10-03 03:53:39","commitOrder":1,"curCode":"    public void testTransformKeys() {\n        checkAnswer(\n            mapDf.select(transform_keys(col(\"x\"), (k, v) -> k.plus(v))),\n            toRows(\n                mapAsScalaMap(new HashMap<Integer, Integer>() {{\n                    put(2, 1);\n                    put(4, 2);\n                }}),\n                null\n            )\n        );\n    }\n","date":"2019-10-03 03:53:39","endLine":188,"groupId":"3453","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testTransformKeys","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/a5/f11d57f3ce61e638e2599b5a5813a17b435fbc.src","preCode":"    public void testTransformKeys() {\n        checkAnswer(\n            mapDf.select(transform_keys(col(\"x\"), (k, v) -> k.plus(v))),\n            toRows(\n                mapAsScalaMap(new HashMap<Integer, Integer>() {{\n                    put(2, 1);\n                    put(4, 2);\n                }}),\n                null\n            )\n        );\n    }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaHigherOrderFunctionsSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":177,"status":"B"},{"authorDate":"2019-10-03 03:53:39","commitOrder":1,"curCode":"    public void testTransformValues() {\n        checkAnswer(\n            mapDf.select(transform_values(col(\"x\"), (k, v) -> k.plus(v))),\n            toRows(\n                mapAsScalaMap(new HashMap<Integer, Integer>() {{\n                    put(1, 2);\n                    put(2, 4);\n                }}),\n                null\n            )\n        );\n    }\n","date":"2019-10-03 03:53:39","endLine":202,"groupId":"3453","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testTransformValues","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/a5/f11d57f3ce61e638e2599b5a5813a17b435fbc.src","preCode":"    public void testTransformValues() {\n        checkAnswer(\n            mapDf.select(transform_values(col(\"x\"), (k, v) -> k.plus(v))),\n            toRows(\n                mapAsScalaMap(new HashMap<Integer, Integer>() {{\n                    put(1, 2);\n                    put(2, 4);\n                }}),\n                null\n            )\n        );\n    }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaHigherOrderFunctionsSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":191,"status":"B"}],"commitId":"730a17823f533d2f08874fd5f69233a84aa7878d","commitMessage":"@@@[SPARK-27297][SQL] Add higher order functions to scala API\n\n## What changes were proposed in this pull request?\n\nThere is currently no existing Scala API equivalent for the higher order functions introduced in Spark 2.4.0.\n * transform\n * aggregate\n * filter\n * exists\n * forall\n * zip_with\n * map_zip_with\n * map_filter\n * transform_values\n * transform_keys\n\nEquivalent column based functions should be added to the Scala API for org.apache.spark.sql.functions with the following signatures:\n\n?\n```scala\ndef transform(column: Column.  f: Column => Column): Column = ???\n\ndef transform(column: Column.  f: (Column.  Column) => Column): Column = ???\n\ndef exists(column: Column.  f: Column => Column): Column = ???\n\ndef filter(column: Column.  f: Column => Column): Column = ???\n\ndef aggregate(\nexpr: Column. \nzero: Column. \nmerge: (Column.  Column) => Column. \nfinish: Column => Column): Column = ???\n\ndef aggregate(\nexpr: Column. \nzero: Column. \nmerge: (Column.  Column) => Column): Column = ???\n\ndef zip_with(\nleft: Column. \nright: Column. \nf: (Column.  Column) => Column): Column = ???\n\ndef transform_keys(expr: Column.  f: (Column.  Column) => Column): Column = ???\n\ndef transform_values(expr: Column.  f: (Column.  Column) => Column): Column = ???\n\ndef map_filter(expr: Column.  f: (Column.  Column) => Column): Column = ???\n\ndef map_zip_with(left: Column.  right: Column.  f: (Column.  Column.  Column) => Column): Column = ???\n```\n\n## How was this patch tested?\n\nI've mimicked the existing tests for the higher order functions in `org.apache.spark.sql.DataFrameFunctionsSuite` that use `expr` to test the higher order functions.\n\nAs an example of an existing test:\n```scala\n  test(\"map_zip_with function - map of primitive types\") {\n    val df = Seq(\n      (Map(8 -> 6L.  3 -> 5L.  6 -> 2L).  Map[Integer.  Integer]((6.  4).  (8.  2).  (3.  2))). \n      (Map(10 -> 6L.  8 -> 3L).  Map[Integer.  Integer]((8.  4).  (4.  null))). \n      (Map.empty[Int.  Long].  Map[Integer.  Integer]((5.  1))). \n      (Map(5 -> 1L).  null)\n    ).toDF(\"m1\".  \"m2\")\n\n    checkAnswer(df.selectExpr(\"map_zip_with(m1.  m2.  (k.  v1.  v2) -> k == v1 + v2)\"). \n      Seq(\n        Row(Map(8 -> true.  3 -> false.  6 -> true)). \n        Row(Map(10 -> null.  8 -> false.  4 -> null)). \n        Row(Map(5 -> null)). \n        Row(null)))\n}\n```\n\nI've added this test that performs the same logic.  but with the new column based API I've added.\n```scala\n    checkAnswer(df.select(map_zip_with(df(\"m1\").  df(\"m2\").  (k.  v1.  v2) => k === v1 + v2)). \n      Seq(\n        Row(Map(8 -> true.  3 -> false.  6 -> true)). \n        Row(Map(10 -> null.  8 -> false.  4 -> null)). \n        Row(Map(5 -> null)). \n        Row(null)))\n```\n\nCloses #24232 from nvander1/feature/add_higher_order_functions_to_scala_api.\n\nLead-authored-by: Nik Vanderhoof <nikolasrvanderhoof@gmail.com>\nCo-authored-by: Nik <nikolasrvanderhoof@gmail.com>\nCo-authored-by: HyukjinKwon <gurwls223@apache.org>\nSigned-off-by: Takuya UESHIN <ueshin@databricks.com>\n","date":"2019-10-03 03:53:39","modifiedFileCount":"0","status":"B","submitter":"Nik Vanderhoof"},{"authorTime":"2019-12-10 04:41:48","codes":[{"authorDate":"2019-12-10 04:41:48","commitOrder":2,"curCode":"    public void testTransformKeys() throws Exception {\n        checkAnswer(\n            mapDf.select(transform_keys(col(\"x\"), (k, v) -> k.plus(v))),\n            toRows(\n                mapAsScalaMap(new HashMap<Integer, Integer>() {{\n                    put(2, 1);\n                    put(4, 2);\n                }}),\n                null\n            )\n        );\n    }\n","date":"2019-12-10 04:41:48","endLine":232,"groupId":"10396","id":3,"instanceNumber":1,"isCurCommit":1,"methodName":"testTransformKeys","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/de/0acc295b5ea9ea696584e3c70e9d13ee906832.src","preCode":"    public void testTransformKeys() {\n        checkAnswer(\n            mapDf.select(transform_keys(col(\"x\"), (k, v) -> k.plus(v))),\n            toRows(\n                mapAsScalaMap(new HashMap<Integer, Integer>() {{\n                    put(2, 1);\n                    put(4, 2);\n                }}),\n                null\n            )\n        );\n    }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaHigherOrderFunctionsSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":221,"status":"M"},{"authorDate":"2019-12-10 04:41:48","commitOrder":2,"curCode":"    public void testTransformValues() throws Exception {\n        checkAnswer(\n            mapDf.select(transform_values(col(\"x\"), (k, v) -> k.plus(v))),\n            toRows(\n                mapAsScalaMap(new HashMap<Integer, Integer>() {{\n                    put(1, 2);\n                    put(2, 4);\n                }}),\n                null\n            )\n        );\n    }\n","date":"2019-12-10 04:41:48","endLine":246,"groupId":"10396","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"testTransformValues","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/de/0acc295b5ea9ea696584e3c70e9d13ee906832.src","preCode":"    public void testTransformValues() {\n        checkAnswer(\n            mapDf.select(transform_values(col(\"x\"), (k, v) -> k.plus(v))),\n            toRows(\n                mapAsScalaMap(new HashMap<Integer, Integer>() {{\n                    put(1, 2);\n                    put(2, 4);\n                }}),\n                null\n            )\n        );\n    }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaHigherOrderFunctionsSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":235,"status":"M"}],"commitId":"36fa1980c24c5c697982b107c8f9714f3eb57f36","commitMessage":"@@@[SPARK-30158][SQL][CORE] Seq -> Array for sc.parallelize for 2.13 compatibility; remove WrappedArray\n\n\n What changes were proposed in this pull request?\n\nUse Seq instead of Array in sc.parallelize.  with reference types.\nRemove usage of WrappedArray.\n\n\n Why are the changes needed?\n\nThese both enable building on Scala 2.13.\n\n\n Does this PR introduce any user-facing change?\n\nNone\n\n\n How was this patch tested?\n\nExisting tests\n\nCloses #26787 from srowen/SPARK-30158.\n\nAuthored-by: Sean Owen <sean.owen@databricks.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n","date":"2019-12-10 04:41:48","modifiedFileCount":"1","status":"M","submitter":"Sean Owen"}]
