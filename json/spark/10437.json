[{"authorTime":"2018-05-09 12:27:32","codes":[{"authorDate":"2018-05-09 12:27:32","commitOrder":1,"curCode":"  public final void readFloats(int total, WritableColumnVector c, int rowId) {\n    int requiredBytes = total * 4;\n    ByteBuffer buffer = getBuffer(requiredBytes);\n\n    if (buffer.hasArray()) {\n      int offset = buffer.arrayOffset() + buffer.position();\n      c.putFloats(rowId, total, buffer.array(), offset);\n    } else {\n      for (int i = 0; i < total; i += 1) {\n        c.putFloat(rowId + i, buffer.getFloat());\n      }\n    }\n  }\n","date":"2018-05-09 12:27:32","endLine":113,"groupId":"2336","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"readFloats","params":"(inttotal@WritableColumnVectorc@introwId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/aa/cefacfc1c1ab41bafb376a6cf2207e8f6a15b9.src","preCode":"  public final void readFloats(int total, WritableColumnVector c, int rowId) {\n    int requiredBytes = total * 4;\n    ByteBuffer buffer = getBuffer(requiredBytes);\n\n    if (buffer.hasArray()) {\n      int offset = buffer.arrayOffset() + buffer.position();\n      c.putFloats(rowId, total, buffer.array(), offset);\n    } else {\n      for (int i = 0; i < total; i += 1) {\n        c.putFloat(rowId + i, buffer.getFloat());\n      }\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":101,"status":"B"},{"authorDate":"2018-05-09 12:27:32","commitOrder":1,"curCode":"  public final void readDoubles(int total, WritableColumnVector c, int rowId) {\n    int requiredBytes = total * 8;\n    ByteBuffer buffer = getBuffer(requiredBytes);\n\n    if (buffer.hasArray()) {\n      int offset = buffer.arrayOffset() + buffer.position();\n      c.putDoubles(rowId, total, buffer.array(), offset);\n    } else {\n      for (int i = 0; i < total; i += 1) {\n        c.putDouble(rowId + i, buffer.getDouble());\n      }\n    }\n  }\n","date":"2018-05-09 12:27:32","endLine":128,"groupId":"1107","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"readDoubles","params":"(inttotal@WritableColumnVectorc@introwId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/aa/cefacfc1c1ab41bafb376a6cf2207e8f6a15b9.src","preCode":"  public final void readDoubles(int total, WritableColumnVector c, int rowId) {\n    int requiredBytes = total * 8;\n    ByteBuffer buffer = getBuffer(requiredBytes);\n\n    if (buffer.hasArray()) {\n      int offset = buffer.arrayOffset() + buffer.position();\n      c.putDoubles(rowId, total, buffer.array(), offset);\n    } else {\n      for (int i = 0; i < total; i += 1) {\n        c.putDouble(rowId + i, buffer.getDouble());\n      }\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":116,"status":"B"}],"commitId":"cac9b1dea1bb44fa42abf77829c05bf93f70cf20","commitMessage":"@@@[SPARK-23972][BUILD][SQL] Update Parquet to 1.10.0.\n\n## What changes were proposed in this pull request?\n\nThis updates Parquet to 1.10.0 and updates the vectorized path for buffer management changes. Parquet 1.10.0 uses ByteBufferInputStream instead of byte arrays in encoders. This allows Parquet to break allocations into smaller chunks that are better for garbage collection.\n\n## How was this patch tested?\n\nExisting Parquet tests. Running in production at Netflix for about 3 months.\n\nAuthor: Ryan Blue <blue@apache.org>\n\nCloses #21070 from rdblue/SPARK-23972-update-parquet-to-1.10.0.\n","date":"2018-05-09 12:27:32","modifiedFileCount":"4","status":"B","submitter":"Ryan Blue"},{"authorTime":"2020-08-12 14:39:10","codes":[{"authorDate":"2020-08-12 14:39:10","commitOrder":2,"curCode":"  public final void readFloats(int total, WritableColumnVector c, int rowId) {\n    int requiredBytes = total * 4;\n    ByteBuffer buffer = getBuffer(requiredBytes);\n\n    if (buffer.hasArray()) {\n      int offset = buffer.arrayOffset() + buffer.position();\n      c.putFloatsLittleEndian(rowId, total, buffer.array(), offset);\n    } else {\n      for (int i = 0; i < total; i += 1) {\n        c.putFloat(rowId + i, buffer.getFloat());\n      }\n    }\n  }\n","date":"2020-08-12 14:39:10","endLine":178,"groupId":"10437","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"readFloats","params":"(inttotal@WritableColumnVectorc@introwId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/99/4779b61882948d9c9706e559de1efa8a2b36fc.src","preCode":"  public final void readFloats(int total, WritableColumnVector c, int rowId) {\n    int requiredBytes = total * 4;\n    ByteBuffer buffer = getBuffer(requiredBytes);\n\n    if (buffer.hasArray()) {\n      int offset = buffer.arrayOffset() + buffer.position();\n      c.putFloats(rowId, total, buffer.array(), offset);\n    } else {\n      for (int i = 0; i < total; i += 1) {\n        c.putFloat(rowId + i, buffer.getFloat());\n      }\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":166,"status":"M"},{"authorDate":"2020-08-12 14:39:10","commitOrder":2,"curCode":"  public final void readDoubles(int total, WritableColumnVector c, int rowId) {\n    int requiredBytes = total * 8;\n    ByteBuffer buffer = getBuffer(requiredBytes);\n\n    if (buffer.hasArray()) {\n      int offset = buffer.arrayOffset() + buffer.position();\n      c.putDoublesLittleEndian(rowId, total, buffer.array(), offset);\n    } else {\n      for (int i = 0; i < total; i += 1) {\n        c.putDouble(rowId + i, buffer.getDouble());\n      }\n    }\n  }\n","date":"2020-08-12 14:39:10","endLine":193,"groupId":"10437","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"readDoubles","params":"(inttotal@WritableColumnVectorc@introwId)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/99/4779b61882948d9c9706e559de1efa8a2b36fc.src","preCode":"  public final void readDoubles(int total, WritableColumnVector c, int rowId) {\n    int requiredBytes = total * 8;\n    ByteBuffer buffer = getBuffer(requiredBytes);\n\n    if (buffer.hasArray()) {\n      int offset = buffer.arrayOffset() + buffer.position();\n      c.putDoubles(rowId, total, buffer.array(), offset);\n    } else {\n      for (int i = 0; i < total; i += 1) {\n        c.putDouble(rowId + i, buffer.getDouble());\n      }\n    }\n  }\n","realPath":"sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedPlainValuesReader.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":181,"status":"M"}],"commitId":"a418548dad57775fbb10b4ea690610bad1a8bfb0","commitMessage":"@@@[SPARK-31703][SQL] Parquet RLE float/double are read incorrectly on big endian platforms\n\n\n What changes were proposed in this pull request?\nThis PR fixes the issue introduced during SPARK-26985.\n\nSPARK-26985 changes the `putDoubles()` and `putFloats()` methods to respect the platform's endian-ness.  However.  that causes the RLE paths in VectorizedRleValuesReader.java to read the RLE entries in parquet as BIG_ENDIAN on big endian platforms (i.e..  as is).  even though parquet data is always in little endian format.\n\nThe comments in `WriteableColumnVector.java` say those methods are used for \"ieee formatted doubles in platform native endian\" (or floats).  but since the data in parquet is always in little endian format.  use of those methods appears to be inappropriate.\n\nTo demonstrate the problem with spark-shell:\n\n```scala\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\n\nvar data = Seq(\n  (1.0.  0.1). \n  (2.0.  0.2). \n  (0.3.  3.0). \n  (4.0.  4.0). \n  (5.0.  5.0))\n\nvar df = spark.createDataFrame(data).write.mode(SaveMode.Overwrite).parquet(\"/tmp/data.parquet2\")\nvar df2 = spark.read.parquet(\"/tmp/data.parquet2\")\ndf2.show()\n```\n\nresult:\n\n```scala\n+--------------------+--------------------+\n|                  _1|                  _2|\n+--------------------+--------------------+\n|           3.16E-322|-1.54234871366845...|\n|         2.0553E-320|         2.0553E-320|\n|          2.561E-320|          2.561E-320|\n|4.66726145843124E-62|         1.0435E-320|\n|        3.03865E-319|-1.54234871366757...|\n+--------------------+--------------------+\n```\n\nAlso tests in ParquetIOSuite that involve float/double data would fail.  e.g.. \n\n- basic data types (without binary)\n- read raw Parquet file\n\n/examples/src/main/python/mllib/isotonic_regression_example.py would fail as well.\n\nPurposed code change is to add `putDoublesLittleEndian()` and `putFloatsLittleEndian()` methods for parquet to invoke.  just like the existing `putIntsLittleEndian()` and `putLongsLittleEndian()`.  On little endian platforms they would call `putDoubles()` and `putFloats()`.  on big endian they would read the entries as little endian like pre-SPARK-26985.\n\nNo new unit-test is introduced as the existing ones are actually sufficient.\n\n\n Why are the changes needed?\nRLE float/double data in parquet files will not be read back correctly on big endian platforms.\n\n\n Does this PR introduce _any_ user-facing change?\nNo\n\n\n How was this patch tested?\nAll unit tests (mvn test) were ran and OK.\n\nCloses #29383 from tinhto-000/SPARK-31703.\n\nAuthored-by: Tin Hang To <tinto@us.ibm.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n","date":"2020-08-12 14:39:10","modifiedFileCount":"4","status":"M","submitter":"Tin Hang To"}]
