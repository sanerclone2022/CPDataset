[{"authorTime":"2015-08-03 14:41:16","codes":[{"authorDate":"2015-08-03 14:41:16","commitOrder":1,"curCode":"  public ArrayData getArray(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      return UnsafeReaders.readArray(baseObject, baseOffset + offset, size);\n    }\n  }\n","date":"2015-08-03 14:41:16","endLine":433,"groupId":"657","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"getArray","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/c5/d42d73a43a4a1758e830d614526cafa5b02e39.src","preCode":"  public ArrayData getArray(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      return UnsafeReaders.readArray(baseObject, baseOffset + offset, size);\n    }\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":424,"status":"B"},{"authorDate":"2015-08-03 14:41:16","commitOrder":1,"curCode":"  public MapData getMap(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      return UnsafeReaders.readMap(baseObject, baseOffset + offset, size);\n    }\n  }\n","date":"2015-08-03 14:41:16","endLine":445,"groupId":"657","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"getMap","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/c5/d42d73a43a4a1758e830d614526cafa5b02e39.src","preCode":"  public MapData getMap(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      return UnsafeReaders.readMap(baseObject, baseOffset + offset, size);\n    }\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":436,"status":"B"}],"commitId":"608353c8e8e50461fafff91a2c885dca8af3aaa8","commitMessage":"@@@[SPARK-9404][SPARK-9542][SQL] unsafe array data and map data\n\nThis PR adds a UnsafeArrayData.  current we encode it in this way:\n\nfirst 4 bytes is the # elements\nthen each 4 byte is the start offset of the element.  unless it is negative.  in which case the element is null.\nfollowed by the elements themselves\n\nan example:  [10.  11.  12.  13.  null.  14] will be encoded as:\n5.  28.  32.  36.  40.  -44.  44.  10.  11.  12.  13.  14\n\nNote that.  when we read a UnsafeArrayData from bytes.  we can read the first 4 bytes as numElements and take the rest(first 4 bytes skipped) as value region.\n\nunsafe map data just use 2 unsafe array data.  first 4 bytes is # of elements.  second 4 bytes is numBytes of key array.  the follows key array data and value array data.\n\nAuthor: Wenchen Fan <cloud0fan@outlook.com>\n\nCloses #7752 from cloud-fan/unsafe-array and squashes the following commits:\n\n3269bd7 [Wenchen Fan] fix a bug\n6445289 [Wenchen Fan] add unit tests\n49adf26 [Wenchen Fan] add unsafe map\n20d1039 [Wenchen Fan] add comments and unsafe converter\n821b8db [Wenchen Fan] add unsafe array\n","date":"2015-08-03 14:41:16","modifiedFileCount":"3","status":"B","submitter":"Wenchen Fan"},{"authorTime":"2015-10-06 04:00:58","codes":[{"authorDate":"2015-10-06 04:00:58","commitOrder":2,"curCode":"  public UnsafeArrayData getArray(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      return UnsafeReaders.readArray(baseObject, baseOffset + offset, size);\n    }\n  }\n","date":"2015-10-06 04:00:58","endLine":458,"groupId":"657","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"getArray","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/e8/ac2999c2d29569507ccdb6e66966c66e70e91c.src","preCode":"  public ArrayData getArray(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      return UnsafeReaders.readArray(baseObject, baseOffset + offset, size);\n    }\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":449,"status":"M"},{"authorDate":"2015-10-06 04:00:58","commitOrder":2,"curCode":"  public UnsafeMapData getMap(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      return UnsafeReaders.readMap(baseObject, baseOffset + offset, size);\n    }\n  }\n","date":"2015-10-06 04:00:58","endLine":470,"groupId":"657","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"getMap","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/e8/ac2999c2d29569507ccdb6e66966c66e70e91c.src","preCode":"  public MapData getMap(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      return UnsafeReaders.readMap(baseObject, baseOffset + offset, size);\n    }\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":461,"status":"M"}],"commitId":"c4871369db96fc33c465d11b3bbd1ffeb3b94e89","commitMessage":"@@@[SPARK-10585] [SQL] only copy data once when generate unsafe projection\n\nThis PR is a completely rewritten of GenerateUnsafeProjection.  to accomplish the goal of copying data only once. The old code of GenerateUnsafeProjection is still there to reduce review difficulty.\n\nInstead of creating unsafe conversion code for struct.  array and map.  we create code of writing the content to the global row buffer.\n\nAuthor: Wenchen Fan <cloud0fan@163.com>\nAuthor: Wenchen Fan <cloud0fan@outlook.com>\n\nCloses #8747 from cloud-fan/copy-once.\n","date":"2015-10-06 04:00:58","modifiedFileCount":"6","status":"M","submitter":"Wenchen Fan"},{"authorTime":"2015-10-20 02:02:26","codes":[{"authorDate":"2015-10-20 02:02:26","commitOrder":3,"curCode":"  public UnsafeArrayData getArray(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      final UnsafeArrayData array = new UnsafeArrayData();\n      array.pointTo(baseObject, baseOffset + offset, size);\n      return array;\n    }\n  }\n","date":"2015-10-20 02:02:26","endLine":468,"groupId":"657","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"getArray","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/36/6615f6fe69fc96425414637005dcdbb202cd23.src","preCode":"  public UnsafeArrayData getArray(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      return UnsafeReaders.readArray(baseObject, baseOffset + offset, size);\n    }\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":457,"status":"M"},{"authorDate":"2015-10-20 02:02:26","commitOrder":3,"curCode":"  public UnsafeMapData getMap(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      final UnsafeMapData map = new UnsafeMapData();\n      map.pointTo(baseObject, baseOffset + offset, size);\n      return map;\n    }\n  }\n","date":"2015-10-20 02:02:26","endLine":482,"groupId":"657","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"getMap","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/36/6615f6fe69fc96425414637005dcdbb202cd23.src","preCode":"  public UnsafeMapData getMap(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      return UnsafeReaders.readMap(baseObject, baseOffset + offset, size);\n    }\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":471,"status":"M"}],"commitId":"7893cd95db5f2caba59ff5c859d7e4964ad7938d","commitMessage":"@@@[SPARK-11119] [SQL] cleanup for unsafe array and map\n\nThe purpose of this PR is to keep the unsafe format detail only inside the unsafe class itself.  so when we use them(like use unsafe array in unsafe map.  use unsafe array and map in columnar cache).  we don't need to understand the format before use them.\n\nchange list:\n* unsafe array's 4-bytes numElements header is now required(was optional).  and become a part of unsafe array format.\n* w.r.t the previous changing.  the `sizeInBytes` of unsafe array now counts the 4-bytes header.\n* unsafe map's format was `[numElements] [key array numBytes] [key array content(without numElements header)] [value array content(without numElements header)]` before.  which is a little hacky as it makes unsafe array's header optional. I think saving 4 bytes is not a big deal.  so the format is now: `[key array numBytes] [unsafe key array] [unsafe value array]`.\n* w.r.t the previous changing.  the `sizeInBytes` of unsafe map now counts both map's header and array's header.\n\nAuthor: Wenchen Fan <wenchen@databricks.com>\n\nCloses #9131 from cloud-fan/unsafe.\n","date":"2015-10-20 02:02:26","modifiedFileCount":"5","status":"M","submitter":"Wenchen Fan"},{"authorTime":"2015-10-22 10:20:31","codes":[{"authorDate":"2015-10-22 10:20:31","commitOrder":4,"curCode":"  public UnsafeArrayData getArray(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) offsetAndSize;\n      final UnsafeArrayData array = new UnsafeArrayData();\n      array.pointTo(baseObject, baseOffset + offset, size);\n      return array;\n    }\n  }\n","date":"2015-10-22 10:20:31","endLine":468,"groupId":"10357","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"getArray","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/85/0838af9be359e2c65178b0ba79a662e241e654.src","preCode":"  public UnsafeArrayData getArray(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      final UnsafeArrayData array = new UnsafeArrayData();\n      array.pointTo(baseObject, baseOffset + offset, size);\n      return array;\n    }\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":457,"status":"M"},{"authorDate":"2015-10-22 10:20:31","commitOrder":4,"curCode":"  public UnsafeMapData getMap(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) offsetAndSize;\n      final UnsafeMapData map = new UnsafeMapData();\n      map.pointTo(baseObject, baseOffset + offset, size);\n      return map;\n    }\n  }\n","date":"2015-10-22 10:20:31","endLine":482,"groupId":"10357","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"getMap","params":"(intordinal)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/85/0838af9be359e2c65178b0ba79a662e241e654.src","preCode":"  public UnsafeMapData getMap(int ordinal) {\n    if (isNullAt(ordinal)) {\n      return null;\n    } else {\n      final long offsetAndSize = getLong(ordinal);\n      final int offset = (int) (offsetAndSize >> 32);\n      final int size = (int) (offsetAndSize & ((1L << 32) - 1));\n      final UnsafeMapData map = new UnsafeMapData();\n      map.pointTo(baseObject, baseOffset + offset, size);\n      return map;\n    }\n  }\n","realPath":"sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":471,"status":"M"}],"commitId":"1d9733271595596683a6d956a7433fa601df1cc1","commitMessage":"@@@[SPARK-11243][SQL] output UnsafeRow from columnar cache\n\nThis PR change InMemoryTableScan to output UnsafeRow.  and optimize the unrolling and scanning by coping the bytes for var-length types between UnsafeRow and ByteBuffer directly without creating the wrapper objects. When scanning the decimals in TPC-DS store_sales table.  it's 80% faster (copy it as long without create Decimal objects).\n\nAuthor: Davies Liu <davies@databricks.com>\n\nCloses #9203 from davies/unsafe_cache.\n","date":"2015-10-22 10:20:31","modifiedFileCount":"3","status":"M","submitter":"Davies Liu"}]
