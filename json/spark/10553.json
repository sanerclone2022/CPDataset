[{"authorTime":"2017-10-31 00:53:06","codes":[{"authorDate":"2019-01-18 02:29:17","commitOrder":11,"curCode":"    long previousPeakMemory = writer.getPeakMemoryUsedBytes();\n    long newPeakMemory;\n    try {\n      for (int i = 0; i < numRecordsPerPage * 10; i++) {\n        writer.insertRecordIntoSorter(new Tuple2<Object, Object>(1, 1));\n        newPeakMemory = writer.getPeakMemoryUsedBytes();\n        if (i % numRecordsPerPage == 0) {\n          \r\n          \r\n          assertEquals(previousPeakMemory + pageSizeBytes, newPeakMemory);\n        } else {\n          assertEquals(previousPeakMemory, newPeakMemory);\n        }\n        previousPeakMemory = newPeakMemory;\n      }\n\n      \r\n      writer.forceSorterToSpill();\n      newPeakMemory = writer.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n      for (int i = 0; i < numRecordsPerPage; i++) {\n        writer.insertRecordIntoSorter(new Tuple2<Object, Object>(1, 1));\n      }\n      newPeakMemory = writer.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n\n      \r\n      writer.closeAndWriteOutput();\n      newPeakMemory = writer.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n    } finally {\n","date":"2019-01-18 02:29:17","endLine":562,"groupId":"18","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"getPeakMemoryUsedBytes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/f3/4ae99c992da6e91581e2f43f525213f0568e00.src","preCode":"    long previousPeakMemory = writer.getPeakMemoryUsedBytes();\n    long newPeakMemory;\n    try {\n      for (int i = 0; i < numRecordsPerPage * 10; i++) {\n        writer.insertRecordIntoSorter(new Tuple2<Object, Object>(1, 1));\n        newPeakMemory = writer.getPeakMemoryUsedBytes();\n        if (i % numRecordsPerPage == 0) {\n          \r\n          \r\n          assertEquals(previousPeakMemory + pageSizeBytes, newPeakMemory);\n        } else {\n          assertEquals(previousPeakMemory, newPeakMemory);\n        }\n        previousPeakMemory = newPeakMemory;\n      }\n\n      \r\n      writer.forceSorterToSpill();\n      newPeakMemory = writer.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n      for (int i = 0; i < numRecordsPerPage; i++) {\n        writer.insertRecordIntoSorter(new Tuple2<Object, Object>(1, 1));\n      }\n      newPeakMemory = writer.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n\n      \r\n      writer.closeAndWriteOutput();\n      newPeakMemory = writer.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n    } finally {\n","realPath":"core/src/test/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":532,"status":"B"},{"authorDate":"2017-10-31 00:53:06","commitOrder":11,"curCode":"  public void testPeakMemoryUsed() throws Exception {\n    final long recordLengthBytes = 8;\n    final long pageSizeBytes = 256;\n    final long numRecordsPerPage = pageSizeBytes / recordLengthBytes;\n    final UnsafeExternalSorter sorter = UnsafeExternalSorter.create(\n      taskMemoryManager,\n      blockManager,\n      serializerManager,\n      taskContext,\n      () -> recordComparator,\n      prefixComparator,\n      1024,\n      pageSizeBytes,\n      spillThreshold,\n      shouldUseRadixSort());\n\n    \r\n    \r\n    long previousPeakMemory = sorter.getPeakMemoryUsedBytes();\n    long newPeakMemory;\n    try {\n      for (int i = 0; i < numRecordsPerPage * 10; i++) {\n        insertNumber(sorter, i);\n        newPeakMemory = sorter.getPeakMemoryUsedBytes();\n        if (i % numRecordsPerPage == 0) {\n          \r\n          assertEquals(previousPeakMemory + pageSizeBytes, newPeakMemory);\n        } else {\n          assertEquals(previousPeakMemory, newPeakMemory);\n        }\n        previousPeakMemory = newPeakMemory;\n      }\n\n      \r\n      sorter.spill();\n      newPeakMemory = sorter.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n      for (int i = 0; i < numRecordsPerPage; i++) {\n        insertNumber(sorter, i);\n      }\n      newPeakMemory = sorter.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n    } finally {\n      sorter.cleanupResources();\n      assertSpillFilesWereCleanedUp();\n    }\n  }\n","date":"2017-10-31 00:53:06","endLine":485,"groupId":"18","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testPeakMemoryUsed","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/af/4975c888d659b61cafc26b7eb2bd029675b529.src","preCode":"  public void testPeakMemoryUsed() throws Exception {\n    final long recordLengthBytes = 8;\n    final long pageSizeBytes = 256;\n    final long numRecordsPerPage = pageSizeBytes / recordLengthBytes;\n    final UnsafeExternalSorter sorter = UnsafeExternalSorter.create(\n      taskMemoryManager,\n      blockManager,\n      serializerManager,\n      taskContext,\n      () -> recordComparator,\n      prefixComparator,\n      1024,\n      pageSizeBytes,\n      spillThreshold,\n      shouldUseRadixSort());\n\n    \r\n    \r\n    long previousPeakMemory = sorter.getPeakMemoryUsedBytes();\n    long newPeakMemory;\n    try {\n      for (int i = 0; i < numRecordsPerPage * 10; i++) {\n        insertNumber(sorter, i);\n        newPeakMemory = sorter.getPeakMemoryUsedBytes();\n        if (i % numRecordsPerPage == 0) {\n          \r\n          assertEquals(previousPeakMemory + pageSizeBytes, newPeakMemory);\n        } else {\n          assertEquals(previousPeakMemory, newPeakMemory);\n        }\n        previousPeakMemory = newPeakMemory;\n      }\n\n      \r\n      sorter.spill();\n      newPeakMemory = sorter.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n      for (int i = 0; i < numRecordsPerPage; i++) {\n        insertNumber(sorter, i);\n      }\n      newPeakMemory = sorter.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n    } finally {\n      sorter.cleanupResources();\n      assertSpillFilesWereCleanedUp();\n    }\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":439,"status":"NB"}],"commitId":"1b575ef5d1b8e3e672b2fca5c354d6678bd78bd1","commitMessage":"@@@[SPARK-26621][CORE] Use ConfigEntry for hardcoded configs for shuffle categories.\n\n## What changes were proposed in this pull request?\n\nThe PR makes hardcoded `spark.shuffle` configs to use ConfigEntry and put them in the config package.\n\n## How was this patch tested?\nExisting unit tests\n\nCloses #23550 from 10110346/ConfigEntry_shuffle.\n\nAuthored-by: liuxian <liu.xian3@zte.com.cn>\nSigned-off-by: Sean Owen <sean.owen@databricks.com>\n","date":"2019-01-18 02:29:17","modifiedFileCount":"9","status":"M","submitter":"liuxian"},{"authorTime":"2017-10-31 00:53:06","codes":[{"authorDate":"2019-08-11 05:47:11","commitOrder":12,"curCode":"    long previousPeakMemory = writer.getPeakMemoryUsedBytes();\n    long newPeakMemory;\n    try {\n      for (int i = 0; i < numRecordsPerPage * 10; i++) {\n        writer.insertRecordIntoSorter(new Tuple2<>(1, 1));\n        newPeakMemory = writer.getPeakMemoryUsedBytes();\n        if (i % numRecordsPerPage == 0) {\n          \r\n          \r\n          assertEquals(previousPeakMemory + pageSizeBytes, newPeakMemory);\n        } else {\n          assertEquals(previousPeakMemory, newPeakMemory);\n        }\n        previousPeakMemory = newPeakMemory;\n      }\n\n      \r\n      writer.forceSorterToSpill();\n      newPeakMemory = writer.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n      for (int i = 0; i < numRecordsPerPage; i++) {\n        writer.insertRecordIntoSorter(new Tuple2<>(1, 1));\n      }\n      newPeakMemory = writer.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n\n      \r\n      writer.closeAndWriteOutput();\n      newPeakMemory = writer.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n    } finally {\n","date":"2019-08-11 05:47:11","endLine":562,"groupId":"10553","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"getPeakMemoryUsedBytes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/6b/83a984f037cd72d1019d88855410232e2a46cd.src","preCode":"    long previousPeakMemory = writer.getPeakMemoryUsedBytes();\n    long newPeakMemory;\n    try {\n      for (int i = 0; i < numRecordsPerPage * 10; i++) {\n        writer.insertRecordIntoSorter(new Tuple2<Object, Object>(1, 1));\n        newPeakMemory = writer.getPeakMemoryUsedBytes();\n        if (i % numRecordsPerPage == 0) {\n          \r\n          \r\n          assertEquals(previousPeakMemory + pageSizeBytes, newPeakMemory);\n        } else {\n          assertEquals(previousPeakMemory, newPeakMemory);\n        }\n        previousPeakMemory = newPeakMemory;\n      }\n\n      \r\n      writer.forceSorterToSpill();\n      newPeakMemory = writer.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n      for (int i = 0; i < numRecordsPerPage; i++) {\n        writer.insertRecordIntoSorter(new Tuple2<Object, Object>(1, 1));\n      }\n      newPeakMemory = writer.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n\n      \r\n      writer.closeAndWriteOutput();\n      newPeakMemory = writer.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n    } finally {\n","realPath":"core/src/test/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":532,"status":"M"},{"authorDate":"2017-10-31 00:53:06","commitOrder":12,"curCode":"  public void testPeakMemoryUsed() throws Exception {\n    final long recordLengthBytes = 8;\n    final long pageSizeBytes = 256;\n    final long numRecordsPerPage = pageSizeBytes / recordLengthBytes;\n    final UnsafeExternalSorter sorter = UnsafeExternalSorter.create(\n      taskMemoryManager,\n      blockManager,\n      serializerManager,\n      taskContext,\n      () -> recordComparator,\n      prefixComparator,\n      1024,\n      pageSizeBytes,\n      spillThreshold,\n      shouldUseRadixSort());\n\n    \r\n    \r\n    long previousPeakMemory = sorter.getPeakMemoryUsedBytes();\n    long newPeakMemory;\n    try {\n      for (int i = 0; i < numRecordsPerPage * 10; i++) {\n        insertNumber(sorter, i);\n        newPeakMemory = sorter.getPeakMemoryUsedBytes();\n        if (i % numRecordsPerPage == 0) {\n          \r\n          assertEquals(previousPeakMemory + pageSizeBytes, newPeakMemory);\n        } else {\n          assertEquals(previousPeakMemory, newPeakMemory);\n        }\n        previousPeakMemory = newPeakMemory;\n      }\n\n      \r\n      sorter.spill();\n      newPeakMemory = sorter.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n      for (int i = 0; i < numRecordsPerPage; i++) {\n        insertNumber(sorter, i);\n      }\n      newPeakMemory = sorter.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n    } finally {\n      sorter.cleanupResources();\n      assertSpillFilesWereCleanedUp();\n    }\n  }\n","date":"2017-10-31 00:53:06","endLine":485,"groupId":"10553","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testPeakMemoryUsed","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/af/4975c888d659b61cafc26b7eb2bd029675b529.src","preCode":"  public void testPeakMemoryUsed() throws Exception {\n    final long recordLengthBytes = 8;\n    final long pageSizeBytes = 256;\n    final long numRecordsPerPage = pageSizeBytes / recordLengthBytes;\n    final UnsafeExternalSorter sorter = UnsafeExternalSorter.create(\n      taskMemoryManager,\n      blockManager,\n      serializerManager,\n      taskContext,\n      () -> recordComparator,\n      prefixComparator,\n      1024,\n      pageSizeBytes,\n      spillThreshold,\n      shouldUseRadixSort());\n\n    \r\n    \r\n    long previousPeakMemory = sorter.getPeakMemoryUsedBytes();\n    long newPeakMemory;\n    try {\n      for (int i = 0; i < numRecordsPerPage * 10; i++) {\n        insertNumber(sorter, i);\n        newPeakMemory = sorter.getPeakMemoryUsedBytes();\n        if (i % numRecordsPerPage == 0) {\n          \r\n          assertEquals(previousPeakMemory + pageSizeBytes, newPeakMemory);\n        } else {\n          assertEquals(previousPeakMemory, newPeakMemory);\n        }\n        previousPeakMemory = newPeakMemory;\n      }\n\n      \r\n      sorter.spill();\n      newPeakMemory = sorter.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n      for (int i = 0; i < numRecordsPerPage; i++) {\n        insertNumber(sorter, i);\n      }\n      newPeakMemory = sorter.getPeakMemoryUsedBytes();\n      assertEquals(previousPeakMemory, newPeakMemory);\n    } finally {\n      sorter.cleanupResources();\n      assertSpillFilesWereCleanedUp();\n    }\n  }\n","realPath":"core/src/test/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorterSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":439,"status":"N"}],"commitId":"8535df72614800ba789286e569a39ea6e84b3354","commitMessage":"@@@[MINOR] Fix typos in comments and replace an explicit type with <>\n\n## What changes were proposed in this pull request?\nThis PR fixed typos in comments and replace the explicit type with '<>' for Java 8+.\n\n## How was this patch tested?\nManually tested.\n\nCloses #25338 from younggyuchun/younggyu.\n\nAuthored-by: younggyu chun <younggyuchun@gmail.com>\nSigned-off-by: Sean Owen <sean.owen@databricks.com>\n","date":"2019-08-11 05:47:11","modifiedFileCount":"14","status":"M","submitter":"younggyu chun"}]
