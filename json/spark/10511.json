[{"authorTime":"2016-03-01 15:55:26","codes":[{"authorDate":"2015-08-02 16:00:32","commitOrder":2,"curCode":"  public static void main(String[] args) {\n    if (args.length != 2) {\n      System.err.println(\"Usage: ml.JavaKMeansExample <file> <k>\");\n      System.exit(1);\n    }\n    String inputFile = args[0];\n    int k = Integer.parseInt(args[1]);\n\n    \r\n    SparkConf conf = new SparkConf().setAppName(\"JavaKMeansExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext sqlContext = new SQLContext(jsc);\n\n    \r\n    JavaRDD<Row> points = jsc.textFile(inputFile).map(new ParsePoint());\n    StructField[] fields = {new StructField(\"features\", new VectorUDT(), false, Metadata.empty())};\n    StructType schema = new StructType(fields);\n    DataFrame dataset = sqlContext.createDataFrame(points, schema);\n\n    \r\n    KMeans kmeans = new KMeans()\n      .setK(k);\n    KMeansModel model = kmeans.fit(dataset);\n\n    \r\n    Vector[] centers = model.clusterCenters();\n    System.out.println(\"Cluster Centers: \");\n    for (Vector center: centers) {\n      System.out.println(center);\n    }\n\n    jsc.stop();\n  }\n","date":"2015-08-02 16:00:58","endLine":96,"groupId":"689","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/be/2bf0c7b465c97e2c85b462c6e7b8978dbb4ad6.src","preCode":"  public static void main(String[] args) {\n    if (args.length != 2) {\n      System.err.println(\"Usage: ml.JavaKMeansExample <file> <k>\");\n      System.exit(1);\n    }\n    String inputFile = args[0];\n    int k = Integer.parseInt(args[1]);\n\n    \r\n    SparkConf conf = new SparkConf().setAppName(\"JavaKMeansExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext sqlContext = new SQLContext(jsc);\n\n    \r\n    JavaRDD<Row> points = jsc.textFile(inputFile).map(new ParsePoint());\n    StructField[] fields = {new StructField(\"features\", new VectorUDT(), false, Metadata.empty())};\n    StructType schema = new StructType(fields);\n    DataFrame dataset = sqlContext.createDataFrame(points, schema);\n\n    \r\n    KMeans kmeans = new KMeans()\n      .setK(k);\n    KMeansModel model = kmeans.fit(dataset);\n\n    \r\n    Vector[] centers = model.clusterCenters();\n    System.out.println(\"Cluster Centers: \");\n    for (Vector center: centers) {\n      System.out.println(center);\n    }\n\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":64,"status":"NB"},{"authorDate":"2016-03-01 15:55:26","commitOrder":2,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaBisectingKMeansExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    JavaRDD<Row> data = jsc.parallelize(Arrays.asList(\n      RowFactory.create(Vectors.dense(0.1, 0.1, 0.1)),\n      RowFactory.create(Vectors.dense(0.3, 0.3, 0.25)),\n      RowFactory.create(Vectors.dense(0.1, 0.1, -0.1)),\n      RowFactory.create(Vectors.dense(20.3, 20.1, 19.9)),\n      RowFactory.create(Vectors.dense(20.2, 20.1, 19.7)),\n      RowFactory.create(Vectors.dense(18.9, 20.0, 19.7))\n    ));\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"features\", new VectorUDT(), false, Metadata.empty()),\n    });\n\n    DataFrame dataset = jsql.createDataFrame(data, schema);\n\n    BisectingKMeans bkm = new BisectingKMeans().setK(2);\n    BisectingKMeansModel model = bkm.fit(dataset);\n\n    System.out.println(\"Compute Cost: \" + model.computeCost(dataset));\n\n    Vector[] clusterCenters = model.clusterCenters();\n    for (int i = 0; i < clusterCenters.length; i++) {\n      Vector clusterCenter = clusterCenters[i];\n      System.out.println(\"Cluster Center \" + i + \": \" + clusterCenter);\n    }\n    \r\n\n    jsc.stop();\n  }\n","date":"2016-03-01 15:55:26","endLine":80,"groupId":"2349","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/e1/24c1cf1855026fb20df52fd0232135f39f39fb.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaBisectingKMeansExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    JavaRDD<Row> data = jsc.parallelize(Arrays.asList(\n      RowFactory.create(Vectors.dense(0.1, 0.1, 0.1)),\n      RowFactory.create(Vectors.dense(0.3, 0.3, 0.25)),\n      RowFactory.create(Vectors.dense(0.1, 0.1, -0.1)),\n      RowFactory.create(Vectors.dense(20.3, 20.1, 19.9)),\n      RowFactory.create(Vectors.dense(20.2, 20.1, 19.7)),\n      RowFactory.create(Vectors.dense(18.9, 20.0, 19.7))\n    ));\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"features\", new VectorUDT(), false, Metadata.empty()),\n    });\n\n    DataFrame dataset = jsql.createDataFrame(data, schema);\n\n    BisectingKMeans bkm = new BisectingKMeans().setK(2);\n    BisectingKMeansModel model = bkm.fit(dataset);\n\n    System.out.println(\"Compute Cost: \" + model.computeCost(dataset));\n\n    Vector[] clusterCenters = model.clusterCenters();\n    for (int i = 0; i < clusterCenters.length; i++) {\n      Vector clusterCenter = clusterCenters[i];\n      System.out.println(\"Cluster Center \" + i + \": \" + clusterCenter);\n    }\n    \r\n\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":46,"status":"B"}],"commitId":"3c5f5e3b5c4eb69472fdd8124aa9988bd8d933b5","commitMessage":"@@@[SPARK-13550][ML] Add java example for ml.clustering.BisectingKMeans\n\nJIRA: https://issues.apache.org/jira/browse/SPARK-13550\n\n## What changes were proposed in this pull request?\n\nJust add a java example for ml.clustering.BisectingKMeans\n\n## How was this patch tested?\n\nmanual tests were done.\n\n(If this patch involves UI changes.  please attach a screenshot; otherwise.  remove this)\n\nAuthor: Zheng RuiFeng <ruifengz@foxmail.com>\n\nCloses #11428 from zhengruifeng/ml_bkm_je.\n","date":"2016-03-01 15:55:26","modifiedFileCount":"0","status":"M","submitter":"Zheng RuiFeng"},{"authorTime":"2016-03-11 09:00:17","codes":[{"authorDate":"2016-03-11 09:00:17","commitOrder":3,"curCode":"  public static void main(String[] args) {\n    if (args.length != 2) {\n      System.err.println(\"Usage: ml.JavaKMeansExample <file> <k>\");\n      System.exit(1);\n    }\n    String inputFile = args[0];\n    int k = Integer.parseInt(args[1]);\n\n    \r\n    SparkConf conf = new SparkConf().setAppName(\"JavaKMeansExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext sqlContext = new SQLContext(jsc);\n\n    \r\n    \r\n    JavaRDD<Row> points = jsc.textFile(inputFile).map(new ParsePoint());\n    StructField[] fields = {new StructField(\"features\", new VectorUDT(), false, Metadata.empty())};\n    StructType schema = new StructType(fields);\n    Dataset<Row> dataset = sqlContext.createDataFrame(points, schema);\n\n    \r\n    KMeans kmeans = new KMeans()\n      .setK(k);\n    KMeansModel model = kmeans.fit(dataset);\n\n    \r\n    Vector[] centers = model.clusterCenters();\n    System.out.println(\"Cluster Centers: \");\n    for (Vector center: centers) {\n      System.out.println(center);\n    }\n    \r\n\n    jsc.stop();\n  }\n","date":"2016-03-11 09:00:17","endLine":101,"groupId":"689","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/30/ccf308855f45cb3d0752d7a4f6c684e4a40d28.src","preCode":"  public static void main(String[] args) {\n    if (args.length != 2) {\n      System.err.println(\"Usage: ml.JavaKMeansExample <file> <k>\");\n      System.exit(1);\n    }\n    String inputFile = args[0];\n    int k = Integer.parseInt(args[1]);\n\n    \r\n    SparkConf conf = new SparkConf().setAppName(\"JavaKMeansExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext sqlContext = new SQLContext(jsc);\n\n    \r\n    \r\n    JavaRDD<Row> points = jsc.textFile(inputFile).map(new ParsePoint());\n    StructField[] fields = {new StructField(\"features\", new VectorUDT(), false, Metadata.empty())};\n    StructType schema = new StructType(fields);\n    DataFrame dataset = sqlContext.createDataFrame(points, schema);\n\n    \r\n    KMeans kmeans = new KMeans()\n      .setK(k);\n    KMeansModel model = kmeans.fit(dataset);\n\n    \r\n    Vector[] centers = model.clusterCenters();\n    System.out.println(\"Cluster Centers: \");\n    for (Vector center: centers) {\n      System.out.println(center);\n    }\n    \r\n\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":67,"status":"M"},{"authorDate":"2016-03-11 09:00:17","commitOrder":3,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaBisectingKMeansExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    JavaRDD<Row> data = jsc.parallelize(Arrays.asList(\n      RowFactory.create(Vectors.dense(0.1, 0.1, 0.1)),\n      RowFactory.create(Vectors.dense(0.3, 0.3, 0.25)),\n      RowFactory.create(Vectors.dense(0.1, 0.1, -0.1)),\n      RowFactory.create(Vectors.dense(20.3, 20.1, 19.9)),\n      RowFactory.create(Vectors.dense(20.2, 20.1, 19.7)),\n      RowFactory.create(Vectors.dense(18.9, 20.0, 19.7))\n    ));\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"features\", new VectorUDT(), false, Metadata.empty()),\n    });\n\n    Dataset<Row> dataset = jsql.createDataFrame(data, schema);\n\n    BisectingKMeans bkm = new BisectingKMeans().setK(2);\n    BisectingKMeansModel model = bkm.fit(dataset);\n\n    System.out.println(\"Compute Cost: \" + model.computeCost(dataset));\n\n    Vector[] clusterCenters = model.clusterCenters();\n    for (int i = 0; i < clusterCenters.length; i++) {\n      Vector clusterCenter = clusterCenters[i];\n      System.out.println(\"Cluster Center \" + i + \": \" + clusterCenter);\n    }\n    \r\n\n    jsc.stop();\n  }\n","date":"2016-03-11 09:00:17","endLine":80,"groupId":"2349","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/1d/1a518bbca12304e820266283bc3cbd8fce247b.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaBisectingKMeansExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    JavaRDD<Row> data = jsc.parallelize(Arrays.asList(\n      RowFactory.create(Vectors.dense(0.1, 0.1, 0.1)),\n      RowFactory.create(Vectors.dense(0.3, 0.3, 0.25)),\n      RowFactory.create(Vectors.dense(0.1, 0.1, -0.1)),\n      RowFactory.create(Vectors.dense(20.3, 20.1, 19.9)),\n      RowFactory.create(Vectors.dense(20.2, 20.1, 19.7)),\n      RowFactory.create(Vectors.dense(18.9, 20.0, 19.7))\n    ));\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"features\", new VectorUDT(), false, Metadata.empty()),\n    });\n\n    DataFrame dataset = jsql.createDataFrame(data, schema);\n\n    BisectingKMeans bkm = new BisectingKMeans().setK(2);\n    BisectingKMeansModel model = bkm.fit(dataset);\n\n    System.out.println(\"Compute Cost: \" + model.computeCost(dataset));\n\n    Vector[] clusterCenters = model.clusterCenters();\n    for (int i = 0; i < clusterCenters.length; i++) {\n      Vector clusterCenter = clusterCenters[i];\n      System.out.println(\"Cluster Center \" + i + \": \" + clusterCenter);\n    }\n    \r\n\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":46,"status":"M"}],"commitId":"1d542785b9949e7f92025e6754973a779cc37c52","commitMessage":"@@@[SPARK-13244][SQL] Migrates DataFrame to Dataset\n\n## What changes were proposed in this pull request?\n\nThis PR unifies DataFrame and Dataset by migrating existing DataFrame operations to Dataset and make `DataFrame` a type alias of `Dataset[Row]`.\n\nMost Scala code changes are source compatible.  but Java API is broken as Java knows nothing about Scala type alias (mostly replacing `DataFrame` with `Dataset<Row>`).\n\nThere are several noticeable API changes related to those returning arrays:\n\n1.  `collect`/`take`\n\n    -   Old APIs in class `DataFrame`:\n\n        ```scala\n        def collect(): Array[Row]\n        def take(n: Int): Array[Row]\n        ```\n\n    -   New APIs in class `Dataset[T]`:\n\n        ```scala\n        def collect(): Array[T]\n        def take(n: Int): Array[T]\n\n        def collectRows(): Array[Row]\n        def takeRows(n: Int): Array[Row]\n        ```\n\n    Two specialized methods `collectRows` and `takeRows` are added because Java doesn't support returning generic arrays. Thus.  for example.  `DataFrame.collect(): Array[T]` actually returns `Object` instead of `Array<T>` from Java side.\n\n    Normally.  Java users may fall back to `collectAsList` and `takeAsList`.  The two new specialized versions are added to avoid performance regression in ML related code (but maybe I'm wrong and they are not necessary here).\n\n1.  `randomSplit`\n\n    -   Old APIs in class `DataFrame`:\n\n        ```scala\n        def randomSplit(weights: Array[Double].  seed: Long): Array[DataFrame]\n        def randomSplit(weights: Array[Double]): Array[DataFrame]\n        ```\n\n    -   New APIs in class `Dataset[T]`:\n\n        ```scala\n        def randomSplit(weights: Array[Double].  seed: Long): Array[Dataset[T]]\n        def randomSplit(weights: Array[Double]): Array[Dataset[T]]\n        ```\n\n    Similar problem as above.  but hasn't been addressed for Java API yet.  We can probably add `randomSplitAsList` to fix this one.\n\n1.  `groupBy`\n\n    Some original `DataFrame.groupBy` methods have conflicting signature with original `Dataset.groupBy` methods.  To distinguish these two.  typed `Dataset.groupBy` methods are renamed to `groupByKey`.\n\nOther noticeable changes:\n\n1.  Dataset always do eager analysis now\n\n    We used to support disabling DataFrame eager analysis to help reporting partially analyzed malformed logical plan on analysis failure.  However.  Dataset encoders requires eager analysi during Dataset construction.  To preserve the error reporting feature.  `AnalysisException` now takes an extra `Option[LogicalPlan]` argument to hold the partially analyzed plan.  so that we can check the plan tree when reporting test failures.  This plan is passed by `QueryExecution.assertAnalyzed`.\n\n## How was this patch tested?\n\nExisting tests do the work.\n\n## TODO\n\n- [ ] Fix all tests\n- [ ] Re-enable MiMA check\n- [ ] Update ScalaDoc (`since`.  `group`.  and example code)\n\nAuthor: Cheng Lian <lian@databricks.com>\nAuthor: Yin Huai <yhuai@databricks.com>\nAuthor: Wenchen Fan <wenchen@databricks.com>\nAuthor: Cheng Lian <liancheng@users.noreply.github.com>\n\nCloses #11443 from liancheng/ds-to-df.\n","date":"2016-03-11 09:00:17","modifiedFileCount":"87","status":"M","submitter":"Cheng Lian"},{"authorTime":"2016-05-05 05:31:36","codes":[{"authorDate":"2016-05-05 05:31:36","commitOrder":4,"curCode":"  public static void main(String[] args) {\n    if (args.length != 2) {\n      System.err.println(\"Usage: ml.JavaKMeansExample <file> <k>\");\n      System.exit(1);\n    }\n    String inputFile = args[0];\n    int k = Integer.parseInt(args[1]);\n\n    \r\n    SparkSession spark = SparkSession.builder().appName(\"JavaKMeansExample\").getOrCreate();\n\n    \r\n    \r\n    JavaRDD<Row> points = spark.read().text(inputFile).javaRDD().map(new ParsePoint());\n    StructField[] fields = {new StructField(\"features\", new VectorUDT(), false, Metadata.empty())};\n    StructType schema = new StructType(fields);\n    Dataset<Row> dataset = spark.createDataFrame(points, schema);\n\n    \r\n    KMeans kmeans = new KMeans()\n      .setK(k);\n    KMeansModel model = kmeans.fit(dataset);\n\n    \r\n    Vector[] centers = model.clusterCenters();\n    System.out.println(\"Cluster Centers: \");\n    for (Vector center: centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2016-05-05 05:31:36","endLine":96,"groupId":"2608","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/e6/d82a0513a305821d5146398ee3d78f649deadf.src","preCode":"  public static void main(String[] args) {\n    if (args.length != 2) {\n      System.err.println(\"Usage: ml.JavaKMeansExample <file> <k>\");\n      System.exit(1);\n    }\n    String inputFile = args[0];\n    int k = Integer.parseInt(args[1]);\n\n    \r\n    SparkConf conf = new SparkConf().setAppName(\"JavaKMeansExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext sqlContext = new SQLContext(jsc);\n\n    \r\n    \r\n    JavaRDD<Row> points = jsc.textFile(inputFile).map(new ParsePoint());\n    StructField[] fields = {new StructField(\"features\", new VectorUDT(), false, Metadata.empty())};\n    StructType schema = new StructType(fields);\n    Dataset<Row> dataset = sqlContext.createDataFrame(points, schema);\n\n    \r\n    KMeans kmeans = new KMeans()\n      .setK(k);\n    KMeansModel model = kmeans.fit(dataset);\n\n    \r\n    Vector[] centers = model.clusterCenters();\n    System.out.println(\"Cluster Centers: \");\n    for (Vector center: centers) {\n      System.out.println(center);\n    }\n    \r\n\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":64,"status":"M"},{"authorDate":"2016-05-05 05:31:36","commitOrder":4,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession.builder().appName(\"JavaBisectingKMeansExample\").getOrCreate();\n\n    \r\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Vectors.dense(0.1, 0.1, 0.1)),\n      RowFactory.create(Vectors.dense(0.3, 0.3, 0.25)),\n      RowFactory.create(Vectors.dense(0.1, 0.1, -0.1)),\n      RowFactory.create(Vectors.dense(20.3, 20.1, 19.9)),\n      RowFactory.create(Vectors.dense(20.2, 20.1, 19.7)),\n      RowFactory.create(Vectors.dense(18.9, 20.0, 19.7))\n    );\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"features\", new VectorUDT(), false, Metadata.empty()),\n    });\n\n    Dataset<Row> dataset = spark.createDataFrame(data, schema);\n\n    BisectingKMeans bkm = new BisectingKMeans().setK(2);\n    BisectingKMeansModel model = bkm.fit(dataset);\n\n    System.out.println(\"Compute Cost: \" + model.computeCost(dataset));\n\n    Vector[] clusterCenters = model.clusterCenters();\n    for (int i = 0; i < clusterCenters.length; i++) {\n      Vector clusterCenter = clusterCenters[i];\n      System.out.println(\"Cluster Center \" + i + \": \" + clusterCenter);\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2016-05-05 05:31:36","endLine":76,"groupId":"1000","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/51/aa35084e845434a034741408a09e213c2822c3.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaBisectingKMeansExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    JavaRDD<Row> data = jsc.parallelize(Arrays.asList(\n      RowFactory.create(Vectors.dense(0.1, 0.1, 0.1)),\n      RowFactory.create(Vectors.dense(0.3, 0.3, 0.25)),\n      RowFactory.create(Vectors.dense(0.1, 0.1, -0.1)),\n      RowFactory.create(Vectors.dense(20.3, 20.1, 19.9)),\n      RowFactory.create(Vectors.dense(20.2, 20.1, 19.7)),\n      RowFactory.create(Vectors.dense(18.9, 20.0, 19.7))\n    ));\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"features\", new VectorUDT(), false, Metadata.empty()),\n    });\n\n    Dataset<Row> dataset = jsql.createDataFrame(data, schema);\n\n    BisectingKMeans bkm = new BisectingKMeans().setK(2);\n    BisectingKMeansModel model = bkm.fit(dataset);\n\n    System.out.println(\"Compute Cost: \" + model.computeCost(dataset));\n\n    Vector[] clusterCenters = model.clusterCenters();\n    for (int i = 0; i < clusterCenters.length; i++) {\n      Vector clusterCenter = clusterCenters[i];\n      System.out.println(\"Cluster Center \" + i + \": \" + clusterCenter);\n    }\n    \r\n\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":44,"status":"M"}],"commitId":"cdce4e62a5674e2034e5d395578b1a60e3d8c435","commitMessage":"@@@[SPARK-15031][EXAMPLE] Use SparkSession in Scala/Python/Java example.\n\n## What changes were proposed in this pull request?\n\nThis PR aims to update Scala/Python/Java examples by replacing `SQLContext` with newly added `SparkSession`.\n\n- Use **SparkSession Builder Pattern** in 154(Scala 55.  Java 52.  Python 47) files.\n- Add `getConf` in Python SparkContext class: `python/pyspark/context.py`\n- Replace **SQLContext Singleton Pattern** with **SparkSession Singleton Pattern**:\n  - `SqlNetworkWordCount.scala`\n  - `JavaSqlNetworkWordCount.java`\n  - `sql_network_wordcount.py`\n\nNow.  `SQLContexts` are used only in R examples and the following two Python examples. The python examples are untouched in this PR since it already fails some unknown issue.\n- `simple_params_example.py`\n- `aft_survival_regression.py`\n\n## How was this patch tested?\n\nManual.\n\nAuthor: Dongjoon Hyun <dongjoon@apache.org>\n\nCloses #12809 from dongjoon-hyun/SPARK-15031.\n","date":"2016-05-05 05:31:36","modifiedFileCount":"52","status":"M","submitter":"Dongjoon Hyun"},{"authorTime":"2016-05-11 15:56:36","codes":[{"authorDate":"2016-05-05 05:31:36","commitOrder":5,"curCode":"  public static void main(String[] args) {\n    if (args.length != 2) {\n      System.err.println(\"Usage: ml.JavaKMeansExample <file> <k>\");\n      System.exit(1);\n    }\n    String inputFile = args[0];\n    int k = Integer.parseInt(args[1]);\n\n    \r\n    SparkSession spark = SparkSession.builder().appName(\"JavaKMeansExample\").getOrCreate();\n\n    \r\n    \r\n    JavaRDD<Row> points = spark.read().text(inputFile).javaRDD().map(new ParsePoint());\n    StructField[] fields = {new StructField(\"features\", new VectorUDT(), false, Metadata.empty())};\n    StructType schema = new StructType(fields);\n    Dataset<Row> dataset = spark.createDataFrame(points, schema);\n\n    \r\n    KMeans kmeans = new KMeans()\n      .setK(k);\n    KMeansModel model = kmeans.fit(dataset);\n\n    \r\n    Vector[] centers = model.clusterCenters();\n    System.out.println(\"Cluster Centers: \");\n    for (Vector center: centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2016-05-05 05:31:36","endLine":96,"groupId":"2608","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/e6/d82a0513a305821d5146398ee3d78f649deadf.src","preCode":"  public static void main(String[] args) {\n    if (args.length != 2) {\n      System.err.println(\"Usage: ml.JavaKMeansExample <file> <k>\");\n      System.exit(1);\n    }\n    String inputFile = args[0];\n    int k = Integer.parseInt(args[1]);\n\n    \r\n    SparkSession spark = SparkSession.builder().appName(\"JavaKMeansExample\").getOrCreate();\n\n    \r\n    \r\n    JavaRDD<Row> points = spark.read().text(inputFile).javaRDD().map(new ParsePoint());\n    StructField[] fields = {new StructField(\"features\", new VectorUDT(), false, Metadata.empty())};\n    StructType schema = new StructType(fields);\n    Dataset<Row> dataset = spark.createDataFrame(points, schema);\n\n    \r\n    KMeans kmeans = new KMeans()\n      .setK(k);\n    KMeansModel model = kmeans.fit(dataset);\n\n    \r\n    Vector[] centers = model.clusterCenters();\n    System.out.println(\"Cluster Centers: \");\n    for (Vector center: centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":64,"status":"N"},{"authorDate":"2016-05-11 15:56:36","commitOrder":5,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaBisectingKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    BisectingKMeans bkm = new BisectingKMeans().setK(2).setSeed(1);\n    BisectingKMeansModel model = bkm.fit(dataset);\n\n    \r\n    double cost = model.computeCost(dataset);\n    System.out.println(\"Within Set Sum of Squared Errors = \" + cost);\n\n    \r\n    System.out.println(\"Cluster Centers: \");\n    Vector[] centers = model.clusterCenters();\n    for (Vector center : centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2016-05-11 15:56:36","endLine":66,"groupId":"3245","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/62/871448e36f54631c7d616ffd34d020c7383a4d.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaBisectingKMeansExample\")\n      .getOrCreate();\n\n    \r\n    List<Row> data = Arrays.asList(\n      RowFactory.create(Vectors.dense(0.1, 0.1, 0.1)),\n      RowFactory.create(Vectors.dense(0.3, 0.3, 0.25)),\n      RowFactory.create(Vectors.dense(0.1, 0.1, -0.1)),\n      RowFactory.create(Vectors.dense(20.3, 20.1, 19.9)),\n      RowFactory.create(Vectors.dense(20.2, 20.1, 19.7)),\n      RowFactory.create(Vectors.dense(18.9, 20.0, 19.7))\n    );\n\n    StructType schema = new StructType(new StructField[]{\n      new StructField(\"features\", new VectorUDT(), false, Metadata.empty()),\n    });\n\n    Dataset<Row> dataset = spark.createDataFrame(data, schema);\n\n    BisectingKMeans bkm = new BisectingKMeans().setK(2);\n    BisectingKMeansModel model = bkm.fit(dataset);\n\n    System.out.println(\"Compute Cost: \" + model.computeCost(dataset));\n\n    Vector[] clusterCenters = model.clusterCenters();\n    for (int i = 0; i < clusterCenters.length; i++) {\n      Vector clusterCenter = clusterCenters[i];\n      System.out.println(\"Cluster Center \" + i + \": \" + clusterCenter);\n    }\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":39,"status":"M"}],"commitId":"cef73b563864d5f8aa1b26e31e3b9af6f0a08a5d","commitMessage":"@@@[SPARK-14340][EXAMPLE][DOC] Update Examples and User Guide for ml.BisectingKMeans\n\n## What changes were proposed in this pull request?\n\n1.  add BisectingKMeans to ml-clustering.md\n2.  add the missing Scala BisectingKMeansExample\n3.  create a new datafile `data/mllib/sample_kmeans_data.txt`\n\n## How was this patch tested?\n\nmanual tests\n\nAuthor: Zheng RuiFeng <ruifengz@foxmail.com>\n\nCloses #11844 from zhengruifeng/doc_bkm.\n","date":"2016-05-11 15:56:36","modifiedFileCount":"1","status":"M","submitter":"Zheng RuiFeng"},{"authorTime":"2016-05-11 15:56:36","codes":[{"authorDate":"2016-05-11 16:01:43","commitOrder":6,"curCode":"  public static void main(String[] args) {\n    \r\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    KMeans kmeans = new KMeans().setK(2).setSeed(1L);\n    KMeansModel model = kmeans.fit(dataset);\n\n    \r\n    double WSSSE = model.computeCost(dataset);\n    System.out.println(\"Within Set Sum of Squared Errors = \" + WSSSE);\n\n    \r\n    Vector[] centers = model.clusterCenters();\n    System.out.println(\"Cluster Centers: \");\n    for (Vector center: centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2016-05-11 16:01:43","endLine":67,"groupId":"1373","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/24/89a9b80b074c333da5bff136b8975519927a51.src","preCode":"  public static void main(String[] args) {\n    if (args.length != 2) {\n      System.err.println(\"Usage: ml.JavaKMeansExample <file> <k>\");\n      System.exit(1);\n    }\n    String inputFile = args[0];\n    int k = Integer.parseInt(args[1]);\n\n    \r\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    JavaRDD<Row> points = spark.read().text(inputFile).javaRDD().map(new ParsePoint());\n    StructField[] fields = {new StructField(\"features\", new VectorUDT(), false, Metadata.empty())};\n    StructType schema = new StructType(fields);\n    Dataset<Row> dataset = spark.createDataFrame(points, schema);\n\n    \r\n    KMeans kmeans = new KMeans()\n      .setK(k);\n    KMeansModel model = kmeans.fit(dataset);\n\n    \r\n    Vector[] centers = model.clusterCenters();\n    System.out.println(\"Cluster Centers: \");\n    for (Vector center: centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":39,"status":"M"},{"authorDate":"2016-05-11 15:56:36","commitOrder":6,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaBisectingKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    BisectingKMeans bkm = new BisectingKMeans().setK(2).setSeed(1);\n    BisectingKMeansModel model = bkm.fit(dataset);\n\n    \r\n    double cost = model.computeCost(dataset);\n    System.out.println(\"Within Set Sum of Squared Errors = \" + cost);\n\n    \r\n    System.out.println(\"Cluster Centers: \");\n    Vector[] centers = model.clusterCenters();\n    for (Vector center : centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2016-05-11 15:56:36","endLine":66,"groupId":"3245","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/62/871448e36f54631c7d616ffd34d020c7383a4d.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaBisectingKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    BisectingKMeans bkm = new BisectingKMeans().setK(2).setSeed(1);\n    BisectingKMeansModel model = bkm.fit(dataset);\n\n    \r\n    double cost = model.computeCost(dataset);\n    System.out.println(\"Within Set Sum of Squared Errors = \" + cost);\n\n    \r\n    System.out.println(\"Cluster Centers: \");\n    Vector[] centers = model.clusterCenters();\n    for (Vector center : centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":39,"status":"N"}],"commitId":"8beae59144827d81491eed385dc2aa6aedd6a7b4","commitMessage":"@@@[SPARK-15149][EXAMPLE][DOC] update kmeans example\n\n## What changes were proposed in this pull request?\nPython example for ml.kmeans already exists.  but not included in user guide.\n1. small changes like: `example_on` `example_off`\n2. add it to user guide\n3. update examples to directly read datafile\n\n## How was this patch tested?\nmanual tests\n`./bin/spark-submit examples/src/main/python/ml/kmeans_example.py\n\nAuthor: Zheng RuiFeng <ruifengz@foxmail.com>\n\nCloses #12925 from zhengruifeng/km_pe.\n","date":"2016-05-11 16:01:43","modifiedFileCount":"1","status":"M","submitter":"Zheng RuiFeng"},{"authorTime":"2016-05-11 15:56:36","codes":[{"authorDate":"2017-12-11 20:35:31","commitOrder":7,"curCode":"  public static void main(String[] args) {\n    \r\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    KMeans kmeans = new KMeans().setK(2).setSeed(1L);\n    KMeansModel model = kmeans.fit(dataset);\n\n    \r\n    Dataset<Row> predictions = model.transform(dataset);\n\n    \r\n    ClusteringEvaluator evaluator = new ClusteringEvaluator();\n\n    double silhouette = evaluator.evaluate(predictions);\n    System.out.println(\"Silhouette with squared euclidean distance = \" + silhouette);\n\n    \r\n    Vector[] centers = model.clusterCenters();\n    System.out.println(\"Cluster Centers: \");\n    for (Vector center: centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2017-12-11 20:35:31","endLine":73,"groupId":"0","id":11,"instanceNumber":1,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/dc/4b0bcb5965742a7a85a318a904fc087da79065.src","preCode":"  public static void main(String[] args) {\n    \r\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    KMeans kmeans = new KMeans().setK(2).setSeed(1L);\n    KMeansModel model = kmeans.fit(dataset);\n\n    \r\n    double WSSSE = model.computeCost(dataset);\n    System.out.println(\"Within Set Sum of Squared Errors = \" + WSSSE);\n\n    \r\n    Vector[] centers = model.clusterCenters();\n    System.out.println(\"Cluster Centers: \");\n    for (Vector center: centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"M"},{"authorDate":"2016-05-11 15:56:36","commitOrder":7,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaBisectingKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    BisectingKMeans bkm = new BisectingKMeans().setK(2).setSeed(1);\n    BisectingKMeansModel model = bkm.fit(dataset);\n\n    \r\n    double cost = model.computeCost(dataset);\n    System.out.println(\"Within Set Sum of Squared Errors = \" + cost);\n\n    \r\n    System.out.println(\"Cluster Centers: \");\n    Vector[] centers = model.clusterCenters();\n    for (Vector center : centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2016-05-11 15:56:36","endLine":66,"groupId":"3245","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/62/871448e36f54631c7d616ffd34d020c7383a4d.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaBisectingKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    BisectingKMeans bkm = new BisectingKMeans().setK(2).setSeed(1);\n    BisectingKMeansModel model = bkm.fit(dataset);\n\n    \r\n    double cost = model.computeCost(dataset);\n    System.out.println(\"Within Set Sum of Squared Errors = \" + cost);\n\n    \r\n    System.out.println(\"Cluster Centers: \");\n    Vector[] centers = model.clusterCenters();\n    for (Vector center : centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":39,"status":"N"}],"commitId":"ec873a4fd20a47cf0791456bfb301f25a34ae014","commitMessage":"@@@[SPARK-14516][FOLLOWUP] Adding ClusteringEvaluator to examples\n\n## What changes were proposed in this pull request?\n\nIn SPARK-14516 we have introduced ClusteringEvaluator.  but we didn't put any reference in the documentation and the examples were still relying on the sum of squared errors to show a way to evaluate the clustering model.\n\nThe PR adds the ClusteringEvaluator in the examples.\n\n## How was this patch tested?\n\nManual runs of the examples.\n\nAuthor: Marco Gaido <mgaido@hortonworks.com>\n\nCloses #19676 from mgaido91/SPARK-14516_examples.\n","date":"2017-12-11 20:35:31","modifiedFileCount":"1","status":"M","submitter":"Marco Gaido"},{"authorTime":"2018-10-19 09:33:46","codes":[{"authorDate":"2017-12-11 20:35:31","commitOrder":8,"curCode":"  public static void main(String[] args) {\n    \r\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    KMeans kmeans = new KMeans().setK(2).setSeed(1L);\n    KMeansModel model = kmeans.fit(dataset);\n\n    \r\n    Dataset<Row> predictions = model.transform(dataset);\n\n    \r\n    ClusteringEvaluator evaluator = new ClusteringEvaluator();\n\n    double silhouette = evaluator.evaluate(predictions);\n    System.out.println(\"Silhouette with squared euclidean distance = \" + silhouette);\n\n    \r\n    Vector[] centers = model.clusterCenters();\n    System.out.println(\"Cluster Centers: \");\n    for (Vector center: centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2017-12-11 20:35:31","endLine":73,"groupId":"0","id":13,"instanceNumber":1,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/dc/4b0bcb5965742a7a85a318a904fc087da79065.src","preCode":"  public static void main(String[] args) {\n    \r\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    KMeans kmeans = new KMeans().setK(2).setSeed(1L);\n    KMeansModel model = kmeans.fit(dataset);\n\n    \r\n    Dataset<Row> predictions = model.transform(dataset);\n\n    \r\n    ClusteringEvaluator evaluator = new ClusteringEvaluator();\n\n    double silhouette = evaluator.evaluate(predictions);\n    System.out.println(\"Silhouette with squared euclidean distance = \" + silhouette);\n\n    \r\n    Vector[] centers = model.clusterCenters();\n    System.out.println(\"Cluster Centers: \");\n    for (Vector center: centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"N"},{"authorDate":"2018-10-19 09:33:46","commitOrder":8,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaBisectingKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    BisectingKMeans bkm = new BisectingKMeans().setK(2).setSeed(1);\n    BisectingKMeansModel model = bkm.fit(dataset);\n\n    \r\n    Dataset<Row> predictions = model.transform(dataset);\n\n    \r\n    ClusteringEvaluator evaluator = new ClusteringEvaluator();\n\n    double silhouette = evaluator.evaluate(predictions);\n    System.out.println(\"Silhouette with squared euclidean distance = \" + silhouette);\n\n    \r\n    System.out.println(\"Cluster Centers: \");\n    Vector[] centers = model.clusterCenters();\n    for (Vector center : centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2018-10-19 09:33:46","endLine":72,"groupId":"3245","id":14,"instanceNumber":2,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/f5/17dc314b2b75a37429499a63fba6e4b506b508.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaBisectingKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    BisectingKMeans bkm = new BisectingKMeans().setK(2).setSeed(1);\n    BisectingKMeansModel model = bkm.fit(dataset);\n\n    \r\n    double cost = model.computeCost(dataset);\n    System.out.println(\"Within Set Sum of Squared Errors = \" + cost);\n\n    \r\n    System.out.println(\"Cluster Centers: \");\n    Vector[] centers = model.clusterCenters();\n    for (Vector center : centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"M"}],"commitId":"d0ecff28545ac81f5ba7ac06957ced65b6e3ebcd","commitMessage":"@@@[SPARK-25764][ML][EXAMPLES] Update BisectingKMeans example to use ClusteringEvaluator\n\n## What changes were proposed in this pull request?\n\nThe PR updates the examples for `BisectingKMeans` so that they don't use the deprecated method `computeCost` (see SPARK-25758).\n\n## How was this patch tested?\n\nrunning examples\n\nCloses #22763 from mgaido91/SPARK-25764.\n\nAuthored-by: Marco Gaido <marcogaido91@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n","date":"2018-10-19 09:33:46","modifiedFileCount":"1","status":"M","submitter":"Marco Gaido"},{"authorTime":"2018-10-20 09:28:53","codes":[{"authorDate":"2017-12-11 20:35:31","commitOrder":9,"curCode":"  public static void main(String[] args) {\n    \r\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    KMeans kmeans = new KMeans().setK(2).setSeed(1L);\n    KMeansModel model = kmeans.fit(dataset);\n\n    \r\n    Dataset<Row> predictions = model.transform(dataset);\n\n    \r\n    ClusteringEvaluator evaluator = new ClusteringEvaluator();\n\n    double silhouette = evaluator.evaluate(predictions);\n    System.out.println(\"Silhouette with squared euclidean distance = \" + silhouette);\n\n    \r\n    Vector[] centers = model.clusterCenters();\n    System.out.println(\"Cluster Centers: \");\n    for (Vector center: centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2017-12-11 20:35:31","endLine":73,"groupId":"0","id":15,"instanceNumber":1,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/dc/4b0bcb5965742a7a85a318a904fc087da79065.src","preCode":"  public static void main(String[] args) {\n    \r\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    KMeans kmeans = new KMeans().setK(2).setSeed(1L);\n    KMeansModel model = kmeans.fit(dataset);\n\n    \r\n    Dataset<Row> predictions = model.transform(dataset);\n\n    \r\n    ClusteringEvaluator evaluator = new ClusteringEvaluator();\n\n    double silhouette = evaluator.evaluate(predictions);\n    System.out.println(\"Silhouette with squared euclidean distance = \" + silhouette);\n\n    \r\n    Vector[] centers = model.clusterCenters();\n    System.out.println(\"Cluster Centers: \");\n    for (Vector center: centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"N"},{"authorDate":"2018-10-20 09:28:53","commitOrder":9,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaBisectingKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    BisectingKMeans bkm = new BisectingKMeans().setK(2).setSeed(1);\n    BisectingKMeansModel model = bkm.fit(dataset);\n\n    \r\n    double cost = model.computeCost(dataset);\n    System.out.println(\"Within Set Sum of Squared Errors = \" + cost);\n\n    \r\n    System.out.println(\"Cluster Centers: \");\n    Vector[] centers = model.clusterCenters();\n    for (Vector center : centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2018-10-20 09:28:53","endLine":66,"groupId":"3245","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/8c/82aaaacca38ffb8b1aabdaa4d0db198fe767ca.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaBisectingKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    BisectingKMeans bkm = new BisectingKMeans().setK(2).setSeed(1);\n    BisectingKMeansModel model = bkm.fit(dataset);\n\n    \r\n    Dataset<Row> predictions = model.transform(dataset);\n\n    \r\n    ClusteringEvaluator evaluator = new ClusteringEvaluator();\n\n    double silhouette = evaluator.evaluate(predictions);\n    System.out.println(\"Silhouette with squared euclidean distance = \" + silhouette);\n\n    \r\n    System.out.println(\"Cluster Centers: \");\n    Vector[] centers = model.clusterCenters();\n    for (Vector center : centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":39,"status":"M"}],"commitId":"4acbda4a96a5d6ef9065544631a3457e8d7b1748","commitMessage":"@@@Revert \"[SPARK-25764][ML][EXAMPLES] Update BisectingKMeans example to use ClusteringEvaluator\"\n\nThis reverts commit d0ecff28545ac81f5ba7ac06957ced65b6e3ebcd.\n","date":"2018-10-20 09:28:53","modifiedFileCount":"1","status":"M","submitter":"Wenchen Fan"},{"authorTime":"2018-11-06 06:42:04","codes":[{"authorDate":"2017-12-11 20:35:31","commitOrder":10,"curCode":"  public static void main(String[] args) {\n    \r\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    KMeans kmeans = new KMeans().setK(2).setSeed(1L);\n    KMeansModel model = kmeans.fit(dataset);\n\n    \r\n    Dataset<Row> predictions = model.transform(dataset);\n\n    \r\n    ClusteringEvaluator evaluator = new ClusteringEvaluator();\n\n    double silhouette = evaluator.evaluate(predictions);\n    System.out.println(\"Silhouette with squared euclidean distance = \" + silhouette);\n\n    \r\n    Vector[] centers = model.clusterCenters();\n    System.out.println(\"Cluster Centers: \");\n    for (Vector center: centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2017-12-11 20:35:31","endLine":73,"groupId":"10511","id":17,"instanceNumber":1,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/dc/4b0bcb5965742a7a85a318a904fc087da79065.src","preCode":"  public static void main(String[] args) {\n    \r\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    KMeans kmeans = new KMeans().setK(2).setSeed(1L);\n    KMeansModel model = kmeans.fit(dataset);\n\n    \r\n    Dataset<Row> predictions = model.transform(dataset);\n\n    \r\n    ClusteringEvaluator evaluator = new ClusteringEvaluator();\n\n    double silhouette = evaluator.evaluate(predictions);\n    System.out.println(\"Silhouette with squared euclidean distance = \" + silhouette);\n\n    \r\n    Vector[] centers = model.clusterCenters();\n    System.out.println(\"Cluster Centers: \");\n    for (Vector center: centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"N"},{"authorDate":"2018-11-06 06:42:04","commitOrder":10,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaBisectingKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    BisectingKMeans bkm = new BisectingKMeans().setK(2).setSeed(1);\n    BisectingKMeansModel model = bkm.fit(dataset);\n\n    \r\n    Dataset<Row> predictions = model.transform(dataset);\n\n    \r\n    ClusteringEvaluator evaluator = new ClusteringEvaluator();\n\n    double silhouette = evaluator.evaluate(predictions);\n    System.out.println(\"Silhouette with squared euclidean distance = \" + silhouette);\n\n    \r\n    System.out.println(\"Cluster Centers: \");\n    Vector[] centers = model.clusterCenters();\n    for (Vector center : centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","date":"2018-11-06 06:42:04","endLine":72,"groupId":"10511","id":18,"instanceNumber":2,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/f5/17dc314b2b75a37429499a63fba6e4b506b508.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaBisectingKMeansExample\")\n      .getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> dataset = spark.read().format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\");\n\n    \r\n    BisectingKMeans bkm = new BisectingKMeans().setK(2).setSeed(1);\n    BisectingKMeansModel model = bkm.fit(dataset);\n\n    \r\n    double cost = model.computeCost(dataset);\n    System.out.println(\"Within Set Sum of Squared Errors = \" + cost);\n\n    \r\n    System.out.println(\"Cluster Centers: \");\n    Vector[] centers = model.clusterCenters();\n    for (Vector center : centers) {\n      System.out.println(center);\n    }\n    \r\n\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"M"}],"commitId":"0b59170001be1cc1198cfc1c0486ca34633e64d5","commitMessage":"@@@[SPARK-25764][ML][EXAMPLES] Update BisectingKMeans example to use ClusteringEvaluator\n\n## What changes were proposed in this pull request?\n\nUsing `computeCost` for evaluating a model is a very poor approach. We should advice the users to a better approach which is available.  ie. using the `ClusteringEvaluator` to evaluate their models. The PR updates the examples for `BisectingKMeans` in order to do that.\n\n## How was this patch tested?\n\nrunning examples\n\nCloses #22786 from mgaido91/SPARK-25764.\n\nAuthored-by: Marco Gaido <marcogaido91@gmail.com>\nSigned-off-by: DB Tsai <d_tsai@apple.com>\n","date":"2018-11-06 06:42:04","modifiedFileCount":"1","status":"M","submitter":"Marco Gaido"}]
