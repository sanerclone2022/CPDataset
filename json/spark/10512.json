[{"authorTime":"2015-11-14 00:43:05","codes":[{"authorDate":"2015-11-18 15:44:06","commitOrder":3,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaGradientBoostedTreeRegressorExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext sqlContext = new SQLContext(jsc);\n\n    \r\n    \r\n    DataFrame data = sqlContext.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    \r\n    VectorIndexerModel featureIndexer = new VectorIndexer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"indexedFeatures\")\n      .setMaxCategories(4)\n      .fit(data);\n\n    \r\n    DataFrame[] splits = data.randomSplit(new double[] {0.7, 0.3});\n    DataFrame trainingData = splits[0];\n    DataFrame testData = splits[1];\n\n    \r\n    GBTRegressor gbt = new GBTRegressor()\n      .setLabelCol(\"label\")\n      .setFeaturesCol(\"indexedFeatures\")\n      .setMaxIter(10);\n\n    \r\n    Pipeline pipeline = new Pipeline().setStages(new PipelineStage[] {featureIndexer, gbt});\n\n    \r\n    PipelineModel model = pipeline.fit(trainingData);\n\n    \r\n    DataFrame predictions = model.transform(testData);\n\n    \r\n    predictions.select(\"prediction\", \"label\", \"features\").show(5);\n\n    \r\n    RegressionEvaluator evaluator = new RegressionEvaluator()\n      .setLabelCol(\"label\")\n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"rmse\");\n    double rmse = evaluator.evaluate(predictions);\n    System.out.println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse);\n\n    GBTRegressionModel gbtModel = (GBTRegressionModel)(model.stages()[1]);\n    System.out.println(\"Learned regression GBT model:\\n\" + gbtModel.toDebugString());\n    \r\n\n    jsc.stop();\n  }\n","date":"2015-11-18 15:44:06","endLine":89,"groupId":"512","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/1f/67b0842db0d7b889f8a684e55a07c19394e2af.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaGradientBoostedTreeRegressorExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext sqlContext = new SQLContext(jsc);\n\n    \r\n    \r\n    DataFrame data = sqlContext.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    \r\n    VectorIndexerModel featureIndexer = new VectorIndexer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"indexedFeatures\")\n      .setMaxCategories(4)\n      .fit(data);\n\n    \r\n    DataFrame[] splits = data.randomSplit(new double[] {0.7, 0.3});\n    DataFrame trainingData = splits[0];\n    DataFrame testData = splits[1];\n\n    \r\n    GBTRegressor gbt = new GBTRegressor()\n      .setLabelCol(\"label\")\n      .setFeaturesCol(\"indexedFeatures\")\n      .setMaxIter(10);\n\n    \r\n    Pipeline pipeline = new Pipeline().setStages(new PipelineStage[] {featureIndexer, gbt});\n\n    \r\n    PipelineModel model = pipeline.fit(trainingData);\n\n    \r\n    DataFrame predictions = model.transform(testData);\n\n    \r\n    predictions.select(\"prediction\", \"label\", \"features\").show(5);\n\n    \r\n    RegressionEvaluator evaluator = new RegressionEvaluator()\n      .setLabelCol(\"label\")\n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"rmse\");\n    double rmse = evaluator.evaluate(predictions);\n    System.out.println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse);\n\n    GBTRegressionModel gbtModel = (GBTRegressionModel)(model.stages()[1]);\n    System.out.println(\"Learned regression GBT model:\\n\" + gbtModel.toDebugString());\n    \r\n\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":36,"status":"B"},{"authorDate":"2015-11-14 00:43:05","commitOrder":3,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaDecisionTreeRegressionExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext sqlContext = new SQLContext(jsc);\n    \r\n    \r\n    DataFrame data = sqlContext.read().format(\"libsvm\")\n      .load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    \r\n    VectorIndexerModel featureIndexer = new VectorIndexer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"indexedFeatures\")\n      .setMaxCategories(4)\n      .fit(data);\n\n    \r\n    DataFrame[] splits = data.randomSplit(new double[]{0.7, 0.3});\n    DataFrame trainingData = splits[0];\n    DataFrame testData = splits[1];\n\n    \r\n    DecisionTreeRegressor dt = new DecisionTreeRegressor()\n      .setFeaturesCol(\"indexedFeatures\");\n\n    \r\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[]{featureIndexer, dt});\n\n    \r\n    PipelineModel model = pipeline.fit(trainingData);\n\n    \r\n    DataFrame predictions = model.transform(testData);\n\n    \r\n    predictions.select(\"label\", \"features\").show(5);\n\n    \r\n    RegressionEvaluator evaluator = new RegressionEvaluator()\n      .setLabelCol(\"label\")\n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"rmse\");\n    double rmse = evaluator.evaluate(predictions);\n    System.out.println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse);\n\n    DecisionTreeRegressionModel treeModel =\n      (DecisionTreeRegressionModel) (model.stages()[1]);\n    System.out.println(\"Learned regression tree model:\\n\" + treeModel.toDebugString());\n    \r\n  }\n","date":"2015-11-14 00:43:05","endLine":86,"groupId":"512","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/c7/f1868dd105a426113c479fafa194b207a09a6f.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaDecisionTreeRegressionExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext sqlContext = new SQLContext(jsc);\n    \r\n    \r\n    DataFrame data = sqlContext.read().format(\"libsvm\")\n      .load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    \r\n    VectorIndexerModel featureIndexer = new VectorIndexer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"indexedFeatures\")\n      .setMaxCategories(4)\n      .fit(data);\n\n    \r\n    DataFrame[] splits = data.randomSplit(new double[]{0.7, 0.3});\n    DataFrame trainingData = splits[0];\n    DataFrame testData = splits[1];\n\n    \r\n    DecisionTreeRegressor dt = new DecisionTreeRegressor()\n      .setFeaturesCol(\"indexedFeatures\");\n\n    \r\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[]{featureIndexer, dt});\n\n    \r\n    PipelineModel model = pipeline.fit(trainingData);\n\n    \r\n    DataFrame predictions = model.transform(testData);\n\n    \r\n    predictions.select(\"label\", \"features\").show(5);\n\n    \r\n    RegressionEvaluator evaluator = new RegressionEvaluator()\n      .setLabelCol(\"label\")\n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"rmse\");\n    double rmse = evaluator.evaluate(predictions);\n    System.out.println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse);\n\n    DecisionTreeRegressionModel treeModel =\n      (DecisionTreeRegressionModel) (model.stages()[1]);\n    System.out.println(\"Learned regression tree model:\\n\" + treeModel.toDebugString());\n    \r\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":35,"status":"NB"}],"commitId":"9154f89befb7a33d4853cea95efd7dc6b25d033b","commitMessage":"@@@[SPARK-11728] Replace example code in ml-ensembles.md using include_example\n\nJIRA issue https://issues.apache.org/jira/browse/SPARK-11728.\n\nThe ml-ensembles.md file contains `OneVsRestExample`. Instead of writing new code files of two `OneVsRestExample`s.  I use two existing files in the examples directory.  they are `OneVsRestExample.scala` and `JavaOneVsRestExample.scala`.\n\nAuthor: Xusen Yin <yinxusen@gmail.com>\n\nCloses #9716 from yinxusen/SPARK-11728.\n","date":"2015-11-18 15:44:06","modifiedFileCount":"1","status":"M","submitter":"Xusen Yin"},{"authorTime":"2016-03-09 18:12:23","codes":[{"authorDate":"2015-11-18 15:44:06","commitOrder":4,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaGradientBoostedTreeRegressorExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext sqlContext = new SQLContext(jsc);\n\n    \r\n    \r\n    DataFrame data = sqlContext.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    \r\n    VectorIndexerModel featureIndexer = new VectorIndexer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"indexedFeatures\")\n      .setMaxCategories(4)\n      .fit(data);\n\n    \r\n    DataFrame[] splits = data.randomSplit(new double[] {0.7, 0.3});\n    DataFrame trainingData = splits[0];\n    DataFrame testData = splits[1];\n\n    \r\n    GBTRegressor gbt = new GBTRegressor()\n      .setLabelCol(\"label\")\n      .setFeaturesCol(\"indexedFeatures\")\n      .setMaxIter(10);\n\n    \r\n    Pipeline pipeline = new Pipeline().setStages(new PipelineStage[] {featureIndexer, gbt});\n\n    \r\n    PipelineModel model = pipeline.fit(trainingData);\n\n    \r\n    DataFrame predictions = model.transform(testData);\n\n    \r\n    predictions.select(\"prediction\", \"label\", \"features\").show(5);\n\n    \r\n    RegressionEvaluator evaluator = new RegressionEvaluator()\n      .setLabelCol(\"label\")\n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"rmse\");\n    double rmse = evaluator.evaluate(predictions);\n    System.out.println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse);\n\n    GBTRegressionModel gbtModel = (GBTRegressionModel)(model.stages()[1]);\n    System.out.println(\"Learned regression GBT model:\\n\" + gbtModel.toDebugString());\n    \r\n\n    jsc.stop();\n  }\n","date":"2015-11-18 15:44:06","endLine":89,"groupId":"512","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/1f/67b0842db0d7b889f8a684e55a07c19394e2af.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaGradientBoostedTreeRegressorExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext sqlContext = new SQLContext(jsc);\n\n    \r\n    \r\n    DataFrame data = sqlContext.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    \r\n    VectorIndexerModel featureIndexer = new VectorIndexer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"indexedFeatures\")\n      .setMaxCategories(4)\n      .fit(data);\n\n    \r\n    DataFrame[] splits = data.randomSplit(new double[] {0.7, 0.3});\n    DataFrame trainingData = splits[0];\n    DataFrame testData = splits[1];\n\n    \r\n    GBTRegressor gbt = new GBTRegressor()\n      .setLabelCol(\"label\")\n      .setFeaturesCol(\"indexedFeatures\")\n      .setMaxIter(10);\n\n    \r\n    Pipeline pipeline = new Pipeline().setStages(new PipelineStage[] {featureIndexer, gbt});\n\n    \r\n    PipelineModel model = pipeline.fit(trainingData);\n\n    \r\n    DataFrame predictions = model.transform(testData);\n\n    \r\n    predictions.select(\"prediction\", \"label\", \"features\").show(5);\n\n    \r\n    RegressionEvaluator evaluator = new RegressionEvaluator()\n      .setLabelCol(\"label\")\n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"rmse\");\n    double rmse = evaluator.evaluate(predictions);\n    System.out.println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse);\n\n    GBTRegressionModel gbtModel = (GBTRegressionModel)(model.stages()[1]);\n    System.out.println(\"Learned regression GBT model:\\n\" + gbtModel.toDebugString());\n    \r\n\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":36,"status":"N"},{"authorDate":"2016-03-09 18:12:23","commitOrder":4,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaDecisionTreeRegressionExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext sqlContext = new SQLContext(jsc);\n    \r\n    \r\n    DataFrame data = sqlContext.read().format(\"libsvm\")\n      .load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    \r\n    VectorIndexerModel featureIndexer = new VectorIndexer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"indexedFeatures\")\n      .setMaxCategories(4)\n      .fit(data);\n\n    \r\n    DataFrame[] splits = data.randomSplit(new double[]{0.7, 0.3});\n    DataFrame trainingData = splits[0];\n    DataFrame testData = splits[1];\n\n    \r\n    DecisionTreeRegressor dt = new DecisionTreeRegressor()\n      .setFeaturesCol(\"indexedFeatures\");\n\n    \r\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[]{featureIndexer, dt});\n\n    \r\n    PipelineModel model = pipeline.fit(trainingData);\n\n    \r\n    DataFrame predictions = model.transform(testData);\n\n    \r\n    predictions.select(\"label\", \"features\").show(5);\n\n    \r\n    RegressionEvaluator evaluator = new RegressionEvaluator()\n      .setLabelCol(\"label\")\n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"rmse\");\n    double rmse = evaluator.evaluate(predictions);\n    System.out.println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse);\n\n    DecisionTreeRegressionModel treeModel =\n      (DecisionTreeRegressionModel) (model.stages()[1]);\n    System.out.println(\"Learned regression tree model:\\n\" + treeModel.toDebugString());\n    \r\n\n    jsc.stop();\n  }\n","date":"2016-03-09 18:12:23","endLine":88,"groupId":"512","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/9c/b67be04a7b1d8f7f77e41f54931c9409050e68.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaDecisionTreeRegressionExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext sqlContext = new SQLContext(jsc);\n    \r\n    \r\n    DataFrame data = sqlContext.read().format(\"libsvm\")\n      .load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    \r\n    VectorIndexerModel featureIndexer = new VectorIndexer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"indexedFeatures\")\n      .setMaxCategories(4)\n      .fit(data);\n\n    \r\n    DataFrame[] splits = data.randomSplit(new double[]{0.7, 0.3});\n    DataFrame trainingData = splits[0];\n    DataFrame testData = splits[1];\n\n    \r\n    DecisionTreeRegressor dt = new DecisionTreeRegressor()\n      .setFeaturesCol(\"indexedFeatures\");\n\n    \r\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[]{featureIndexer, dt});\n\n    \r\n    PipelineModel model = pipeline.fit(trainingData);\n\n    \r\n    DataFrame predictions = model.transform(testData);\n\n    \r\n    predictions.select(\"label\", \"features\").show(5);\n\n    \r\n    RegressionEvaluator evaluator = new RegressionEvaluator()\n      .setLabelCol(\"label\")\n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"rmse\");\n    double rmse = evaluator.evaluate(predictions);\n    System.out.println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse);\n\n    DecisionTreeRegressionModel treeModel =\n      (DecisionTreeRegressionModel) (model.stages()[1]);\n    System.out.println(\"Learned regression tree model:\\n\" + treeModel.toDebugString());\n    \r\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":35,"status":"M"}],"commitId":"f3201aeeb06aae3b11e8cf6ee9693182dd896b32","commitMessage":"@@@[SPARK-13692][CORE][SQL] Fix trivial Coverity/Checkstyle defects\n\n## What changes were proposed in this pull request?\n\nThis issue fixes the following potential bugs and Java coding style detected by Coverity and Checkstyle.\n\n- Implement both null and type checking in equals functions.\n- Fix wrong type casting logic in SimpleJavaBean2.equals.\n- Add `implement Cloneable` to `UTF8String` and `SortedIterator`.\n- Remove dereferencing before null check in `AbstractBytesToBytesMapSuite`.\n- Fix coding style: Add '{}' to single `for` statement in mllib examples.\n- Remove unused imports in `ColumnarBatch` and `JavaKinesisStreamSuite`.\n- Remove unused fields in `ChunkFetchIntegrationSuite`.\n- Add `stop()` to prevent resource leak.\n\nPlease note that the last two checkstyle errors exist on newly added commits after [SPARK-13583](https://issues.apache.org/jira/browse/SPARK-13583).\n\n## How was this patch tested?\n\nmanual via `./dev/lint-java` and Coverity site.\n\nAuthor: Dongjoon Hyun <dongjoon@apache.org>\n\nCloses #11530 from dongjoon-hyun/SPARK-13692.\n","date":"2016-03-09 18:12:23","modifiedFileCount":"31","status":"M","submitter":"Dongjoon Hyun"},{"authorTime":"2016-03-11 09:00:17","codes":[{"authorDate":"2016-03-11 09:00:17","commitOrder":5,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaGradientBoostedTreeRegressorExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext sqlContext = new SQLContext(jsc);\n\n    \r\n    \r\n    Dataset<Row> data =\n        sqlContext.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    \r\n    VectorIndexerModel featureIndexer = new VectorIndexer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"indexedFeatures\")\n      .setMaxCategories(4)\n      .fit(data);\n\n    \r\n    Dataset<Row>[] splits = data.randomSplit(new double[] {0.7, 0.3});\n    Dataset<Row> trainingData = splits[0];\n    Dataset<Row> testData = splits[1];\n\n    \r\n    GBTRegressor gbt = new GBTRegressor()\n      .setLabelCol(\"label\")\n      .setFeaturesCol(\"indexedFeatures\")\n      .setMaxIter(10);\n\n    \r\n    Pipeline pipeline = new Pipeline().setStages(new PipelineStage[] {featureIndexer, gbt});\n\n    \r\n    PipelineModel model = pipeline.fit(trainingData);\n\n    \r\n    Dataset<Row> predictions = model.transform(testData);\n\n    \r\n    predictions.select(\"prediction\", \"label\", \"features\").show(5);\n\n    \r\n    RegressionEvaluator evaluator = new RegressionEvaluator()\n      .setLabelCol(\"label\")\n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"rmse\");\n    double rmse = evaluator.evaluate(predictions);\n    System.out.println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse);\n\n    GBTRegressionModel gbtModel = (GBTRegressionModel)(model.stages()[1]);\n    System.out.println(\"Learned regression GBT model:\\n\" + gbtModel.toDebugString());\n    \r\n\n    jsc.stop();\n  }\n","date":"2016-03-11 09:00:17","endLine":91,"groupId":"2765","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/83/fd89e3bd59b7feac94ef103545585a3c7f6500.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaGradientBoostedTreeRegressorExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext sqlContext = new SQLContext(jsc);\n\n    \r\n    \r\n    DataFrame data = sqlContext.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    \r\n    VectorIndexerModel featureIndexer = new VectorIndexer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"indexedFeatures\")\n      .setMaxCategories(4)\n      .fit(data);\n\n    \r\n    DataFrame[] splits = data.randomSplit(new double[] {0.7, 0.3});\n    DataFrame trainingData = splits[0];\n    DataFrame testData = splits[1];\n\n    \r\n    GBTRegressor gbt = new GBTRegressor()\n      .setLabelCol(\"label\")\n      .setFeaturesCol(\"indexedFeatures\")\n      .setMaxIter(10);\n\n    \r\n    Pipeline pipeline = new Pipeline().setStages(new PipelineStage[] {featureIndexer, gbt});\n\n    \r\n    PipelineModel model = pipeline.fit(trainingData);\n\n    \r\n    DataFrame predictions = model.transform(testData);\n\n    \r\n    predictions.select(\"prediction\", \"label\", \"features\").show(5);\n\n    \r\n    RegressionEvaluator evaluator = new RegressionEvaluator()\n      .setLabelCol(\"label\")\n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"rmse\");\n    double rmse = evaluator.evaluate(predictions);\n    System.out.println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse);\n\n    GBTRegressionModel gbtModel = (GBTRegressionModel)(model.stages()[1]);\n    System.out.println(\"Learned regression GBT model:\\n\" + gbtModel.toDebugString());\n    \r\n\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":37,"status":"M"},{"authorDate":"2016-03-11 09:00:17","commitOrder":5,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaDecisionTreeRegressionExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext sqlContext = new SQLContext(jsc);\n    \r\n    \r\n    Dataset<Row> data = sqlContext.read().format(\"libsvm\")\n      .load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    \r\n    VectorIndexerModel featureIndexer = new VectorIndexer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"indexedFeatures\")\n      .setMaxCategories(4)\n      .fit(data);\n\n    \r\n    Dataset<Row>[] splits = data.randomSplit(new double[]{0.7, 0.3});\n    Dataset<Row> trainingData = splits[0];\n    Dataset<Row> testData = splits[1];\n\n    \r\n    DecisionTreeRegressor dt = new DecisionTreeRegressor()\n      .setFeaturesCol(\"indexedFeatures\");\n\n    \r\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[]{featureIndexer, dt});\n\n    \r\n    PipelineModel model = pipeline.fit(trainingData);\n\n    \r\n    Dataset<Row> predictions = model.transform(testData);\n\n    \r\n    predictions.select(\"label\", \"features\").show(5);\n\n    \r\n    RegressionEvaluator evaluator = new RegressionEvaluator()\n      .setLabelCol(\"label\")\n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"rmse\");\n    double rmse = evaluator.evaluate(predictions);\n    System.out.println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse);\n\n    DecisionTreeRegressionModel treeModel =\n      (DecisionTreeRegressionModel) (model.stages()[1]);\n    System.out.println(\"Learned regression tree model:\\n\" + treeModel.toDebugString());\n    \r\n\n    jsc.stop();\n  }\n","date":"2016-03-11 09:00:17","endLine":89,"groupId":"2765","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/a4/f3e97bf318a3b017c045ce5002f2cd662ee56b.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaDecisionTreeRegressionExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext sqlContext = new SQLContext(jsc);\n    \r\n    \r\n    DataFrame data = sqlContext.read().format(\"libsvm\")\n      .load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    \r\n    VectorIndexerModel featureIndexer = new VectorIndexer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"indexedFeatures\")\n      .setMaxCategories(4)\n      .fit(data);\n\n    \r\n    DataFrame[] splits = data.randomSplit(new double[]{0.7, 0.3});\n    DataFrame trainingData = splits[0];\n    DataFrame testData = splits[1];\n\n    \r\n    DecisionTreeRegressor dt = new DecisionTreeRegressor()\n      .setFeaturesCol(\"indexedFeatures\");\n\n    \r\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[]{featureIndexer, dt});\n\n    \r\n    PipelineModel model = pipeline.fit(trainingData);\n\n    \r\n    DataFrame predictions = model.transform(testData);\n\n    \r\n    predictions.select(\"label\", \"features\").show(5);\n\n    \r\n    RegressionEvaluator evaluator = new RegressionEvaluator()\n      .setLabelCol(\"label\")\n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"rmse\");\n    double rmse = evaluator.evaluate(predictions);\n    System.out.println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse);\n\n    DecisionTreeRegressionModel treeModel =\n      (DecisionTreeRegressionModel) (model.stages()[1]);\n    System.out.println(\"Learned regression tree model:\\n\" + treeModel.toDebugString());\n    \r\n\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":36,"status":"M"}],"commitId":"1d542785b9949e7f92025e6754973a779cc37c52","commitMessage":"@@@[SPARK-13244][SQL] Migrates DataFrame to Dataset\n\n## What changes were proposed in this pull request?\n\nThis PR unifies DataFrame and Dataset by migrating existing DataFrame operations to Dataset and make `DataFrame` a type alias of `Dataset[Row]`.\n\nMost Scala code changes are source compatible.  but Java API is broken as Java knows nothing about Scala type alias (mostly replacing `DataFrame` with `Dataset<Row>`).\n\nThere are several noticeable API changes related to those returning arrays:\n\n1.  `collect`/`take`\n\n    -   Old APIs in class `DataFrame`:\n\n        ```scala\n        def collect(): Array[Row]\n        def take(n: Int): Array[Row]\n        ```\n\n    -   New APIs in class `Dataset[T]`:\n\n        ```scala\n        def collect(): Array[T]\n        def take(n: Int): Array[T]\n\n        def collectRows(): Array[Row]\n        def takeRows(n: Int): Array[Row]\n        ```\n\n    Two specialized methods `collectRows` and `takeRows` are added because Java doesn't support returning generic arrays. Thus.  for example.  `DataFrame.collect(): Array[T]` actually returns `Object` instead of `Array<T>` from Java side.\n\n    Normally.  Java users may fall back to `collectAsList` and `takeAsList`.  The two new specialized versions are added to avoid performance regression in ML related code (but maybe I'm wrong and they are not necessary here).\n\n1.  `randomSplit`\n\n    -   Old APIs in class `DataFrame`:\n\n        ```scala\n        def randomSplit(weights: Array[Double].  seed: Long): Array[DataFrame]\n        def randomSplit(weights: Array[Double]): Array[DataFrame]\n        ```\n\n    -   New APIs in class `Dataset[T]`:\n\n        ```scala\n        def randomSplit(weights: Array[Double].  seed: Long): Array[Dataset[T]]\n        def randomSplit(weights: Array[Double]): Array[Dataset[T]]\n        ```\n\n    Similar problem as above.  but hasn't been addressed for Java API yet.  We can probably add `randomSplitAsList` to fix this one.\n\n1.  `groupBy`\n\n    Some original `DataFrame.groupBy` methods have conflicting signature with original `Dataset.groupBy` methods.  To distinguish these two.  typed `Dataset.groupBy` methods are renamed to `groupByKey`.\n\nOther noticeable changes:\n\n1.  Dataset always do eager analysis now\n\n    We used to support disabling DataFrame eager analysis to help reporting partially analyzed malformed logical plan on analysis failure.  However.  Dataset encoders requires eager analysi during Dataset construction.  To preserve the error reporting feature.  `AnalysisException` now takes an extra `Option[LogicalPlan]` argument to hold the partially analyzed plan.  so that we can check the plan tree when reporting test failures.  This plan is passed by `QueryExecution.assertAnalyzed`.\n\n## How was this patch tested?\n\nExisting tests do the work.\n\n## TODO\n\n- [ ] Fix all tests\n- [ ] Re-enable MiMA check\n- [ ] Update ScalaDoc (`since`.  `group`.  and example code)\n\nAuthor: Cheng Lian <lian@databricks.com>\nAuthor: Yin Huai <yhuai@databricks.com>\nAuthor: Wenchen Fan <wenchen@databricks.com>\nAuthor: Cheng Lian <liancheng@users.noreply.github.com>\n\nCloses #11443 from liancheng/ds-to-df.\n","date":"2016-03-11 09:00:17","modifiedFileCount":"87","status":"M","submitter":"Cheng Lian"},{"authorTime":"2016-05-05 05:31:36","codes":[{"authorDate":"2016-05-05 05:31:36","commitOrder":6,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder().appName(\"JavaGradientBoostedTreeRegressorExample\").getOrCreate();\n\n    \r\n    \r\n    Dataset<Row> data = spark.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    \r\n    VectorIndexerModel featureIndexer = new VectorIndexer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"indexedFeatures\")\n      .setMaxCategories(4)\n      .fit(data);\n\n    \r\n    Dataset<Row>[] splits = data.randomSplit(new double[] {0.7, 0.3});\n    Dataset<Row> trainingData = splits[0];\n    Dataset<Row> testData = splits[1];\n\n    \r\n    GBTRegressor gbt = new GBTRegressor()\n      .setLabelCol(\"label\")\n      .setFeaturesCol(\"indexedFeatures\")\n      .setMaxIter(10);\n\n    \r\n    Pipeline pipeline = new Pipeline().setStages(new PipelineStage[] {featureIndexer, gbt});\n\n    \r\n    PipelineModel model = pipeline.fit(trainingData);\n\n    \r\n    Dataset<Row> predictions = model.transform(testData);\n\n    \r\n    predictions.select(\"prediction\", \"label\", \"features\").show(5);\n\n    \r\n    RegressionEvaluator evaluator = new RegressionEvaluator()\n      .setLabelCol(\"label\")\n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"rmse\");\n    double rmse = evaluator.evaluate(predictions);\n    System.out.println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse);\n\n    GBTRegressionModel gbtModel = (GBTRegressionModel)(model.stages()[1]);\n    System.out.println(\"Learned regression GBT model:\\n\" + gbtModel.toDebugString());\n    \r\n\n    spark.stop();\n  }\n","date":"2016-05-05 05:31:36","endLine":87,"groupId":"10512","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/6d/3f21fdafa047c8cbe6283e6c480dcf041b2fb6.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaGradientBoostedTreeRegressorExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext sqlContext = new SQLContext(jsc);\n\n    \r\n    \r\n    Dataset<Row> data =\n        sqlContext.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    \r\n    VectorIndexerModel featureIndexer = new VectorIndexer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"indexedFeatures\")\n      .setMaxCategories(4)\n      .fit(data);\n\n    \r\n    Dataset<Row>[] splits = data.randomSplit(new double[] {0.7, 0.3});\n    Dataset<Row> trainingData = splits[0];\n    Dataset<Row> testData = splits[1];\n\n    \r\n    GBTRegressor gbt = new GBTRegressor()\n      .setLabelCol(\"label\")\n      .setFeaturesCol(\"indexedFeatures\")\n      .setMaxIter(10);\n\n    \r\n    Pipeline pipeline = new Pipeline().setStages(new PipelineStage[] {featureIndexer, gbt});\n\n    \r\n    PipelineModel model = pipeline.fit(trainingData);\n\n    \r\n    Dataset<Row> predictions = model.transform(testData);\n\n    \r\n    predictions.select(\"prediction\", \"label\", \"features\").show(5);\n\n    \r\n    RegressionEvaluator evaluator = new RegressionEvaluator()\n      .setLabelCol(\"label\")\n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"rmse\");\n    double rmse = evaluator.evaluate(predictions);\n    System.out.println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse);\n\n    GBTRegressionModel gbtModel = (GBTRegressionModel)(model.stages()[1]);\n    System.out.println(\"Learned regression GBT model:\\n\" + gbtModel.toDebugString());\n    \r\n\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":35,"status":"M"},{"authorDate":"2016-05-05 05:31:36","commitOrder":6,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder().appName(\"JavaDecisionTreeRegressionExample\").getOrCreate();\n    \r\n    \r\n    Dataset<Row> data = spark.read().format(\"libsvm\")\n      .load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    \r\n    VectorIndexerModel featureIndexer = new VectorIndexer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"indexedFeatures\")\n      .setMaxCategories(4)\n      .fit(data);\n\n    \r\n    Dataset<Row>[] splits = data.randomSplit(new double[]{0.7, 0.3});\n    Dataset<Row> trainingData = splits[0];\n    Dataset<Row> testData = splits[1];\n\n    \r\n    DecisionTreeRegressor dt = new DecisionTreeRegressor()\n      .setFeaturesCol(\"indexedFeatures\");\n\n    \r\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[]{featureIndexer, dt});\n\n    \r\n    PipelineModel model = pipeline.fit(trainingData);\n\n    \r\n    Dataset<Row> predictions = model.transform(testData);\n\n    \r\n    predictions.select(\"label\", \"features\").show(5);\n\n    \r\n    RegressionEvaluator evaluator = new RegressionEvaluator()\n      .setLabelCol(\"label\")\n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"rmse\");\n    double rmse = evaluator.evaluate(predictions);\n    System.out.println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse);\n\n    DecisionTreeRegressionModel treeModel =\n      (DecisionTreeRegressionModel) (model.stages()[1]);\n    System.out.println(\"Learned regression tree model:\\n\" + treeModel.toDebugString());\n    \r\n\n    spark.stop();\n  }\n","date":"2016-05-05 05:31:36","endLine":86,"groupId":"10512","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/bd/6dc3edd36313ae0c54c32f355c96a681d252fe.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaDecisionTreeRegressionExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext sqlContext = new SQLContext(jsc);\n    \r\n    \r\n    Dataset<Row> data = sqlContext.read().format(\"libsvm\")\n      .load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    \r\n    VectorIndexerModel featureIndexer = new VectorIndexer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"indexedFeatures\")\n      .setMaxCategories(4)\n      .fit(data);\n\n    \r\n    Dataset<Row>[] splits = data.randomSplit(new double[]{0.7, 0.3});\n    Dataset<Row> trainingData = splits[0];\n    Dataset<Row> testData = splits[1];\n\n    \r\n    DecisionTreeRegressor dt = new DecisionTreeRegressor()\n      .setFeaturesCol(\"indexedFeatures\");\n\n    \r\n    Pipeline pipeline = new Pipeline()\n      .setStages(new PipelineStage[]{featureIndexer, dt});\n\n    \r\n    PipelineModel model = pipeline.fit(trainingData);\n\n    \r\n    Dataset<Row> predictions = model.transform(testData);\n\n    \r\n    predictions.select(\"label\", \"features\").show(5);\n\n    \r\n    RegressionEvaluator evaluator = new RegressionEvaluator()\n      .setLabelCol(\"label\")\n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"rmse\");\n    double rmse = evaluator.evaluate(predictions);\n    System.out.println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse);\n\n    DecisionTreeRegressionModel treeModel =\n      (DecisionTreeRegressionModel) (model.stages()[1]);\n    System.out.println(\"Learned regression tree model:\\n\" + treeModel.toDebugString());\n    \r\n\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":34,"status":"M"}],"commitId":"cdce4e62a5674e2034e5d395578b1a60e3d8c435","commitMessage":"@@@[SPARK-15031][EXAMPLE] Use SparkSession in Scala/Python/Java example.\n\n## What changes were proposed in this pull request?\n\nThis PR aims to update Scala/Python/Java examples by replacing `SQLContext` with newly added `SparkSession`.\n\n- Use **SparkSession Builder Pattern** in 154(Scala 55.  Java 52.  Python 47) files.\n- Add `getConf` in Python SparkContext class: `python/pyspark/context.py`\n- Replace **SQLContext Singleton Pattern** with **SparkSession Singleton Pattern**:\n  - `SqlNetworkWordCount.scala`\n  - `JavaSqlNetworkWordCount.java`\n  - `sql_network_wordcount.py`\n\nNow.  `SQLContexts` are used only in R examples and the following two Python examples. The python examples are untouched in this PR since it already fails some unknown issue.\n- `simple_params_example.py`\n- `aft_survival_regression.py`\n\n## How was this patch tested?\n\nManual.\n\nAuthor: Dongjoon Hyun <dongjoon@apache.org>\n\nCloses #12809 from dongjoon-hyun/SPARK-15031.\n","date":"2016-05-05 05:31:36","modifiedFileCount":"52","status":"M","submitter":"Dongjoon Hyun"}]
