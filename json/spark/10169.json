[{"authorTime":"2019-05-24 04:15:39","codes":[{"authorDate":"2016-02-29 09:25:07","commitOrder":2,"curCode":"  public boolean equals(Object other) {\n    if (other != null && other instanceof OpenBlocks) {\n      OpenBlocks o = (OpenBlocks) other;\n      return Objects.equal(appId, o.appId)\n        && Objects.equal(execId, o.execId)\n        && Arrays.equals(blockIds, o.blockIds);\n    }\n    return false;\n  }\n","date":"2016-02-29 09:25:07","endLine":68,"groupId":"1485","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"equals","params":"(Objectother)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/ce/954b8a289e4808aead298b3426ad8b1795e1a1.src","preCode":"  public boolean equals(Object other) {\n    if (other != null && other instanceof OpenBlocks) {\n      OpenBlocks o = (OpenBlocks) other;\n      return Objects.equal(appId, o.appId)\n        && Objects.equal(execId, o.execId)\n        && Arrays.equals(blockIds, o.blockIds);\n    }\n    return false;\n  }\n","realPath":"common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/OpenBlocks.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":60,"status":"NB"},{"authorDate":"2019-05-24 04:15:39","commitOrder":2,"curCode":"  public boolean equals(Object other) {\n    if (other != null && other instanceof RemoveBlocks) {\n      RemoveBlocks o = (RemoveBlocks) other;\n      return Objects.equal(appId, o.appId)\n        && Objects.equal(execId, o.execId)\n        && Arrays.equals(blockIds, o.blockIds);\n    }\n    return false;\n  }\n","date":"2019-05-24 04:15:39","endLine":67,"groupId":"1485","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"equals","params":"(Objectother)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/1c/718d307753fcbeae4eaaf16fb001b89a3dd6ed.src","preCode":"  public boolean equals(Object other) {\n    if (other != null && other instanceof RemoveBlocks) {\n      RemoveBlocks o = (RemoveBlocks) other;\n      return Objects.equal(appId, o.appId)\n        && Objects.equal(execId, o.execId)\n        && Arrays.equals(blockIds, o.blockIds);\n    }\n    return false;\n  }\n","realPath":"common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/RemoveBlocks.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":59,"status":"B"}],"commitId":"e9f3f62b2c0f521f3cc23fef381fc6754853ad4f","commitMessage":"@@@[SPARK-27677][CORE] Serve local disk persisted blocks by the external service after releasing executor by dynamic allocation\n\n# What changes were proposed in this pull request?\n\n## Problem statement\n\nAn executor which has persisted blocks does not consider to be idle and this way ready to be released by dynamic allocation after the regular timeout `spark.dynamicAllocation.executorIdleTimeout` but there is separate configuration `spark.dynamicAllocation.cachedExecutorIdleTimeout` which defaults to `Integer.MAX_VALUE`. This is because releasing the executor also means losing the persisted blocks (as the metadata for individual blocks called `BlockInfo` are kept in memory) and when the RDD is referenced latter on this lost blocks will be recomputed.\nOn the other hand keeping the executors too long without any task to work on is also a waste of resources (as executors are reserved for the application by the resource manager).\n\n## Solution\n\nThis PR focuses on the first part of SPARK-25888: it extends the external shuffle service with the capability to serve RDD blocks which are persisted on the local disk store by the executors. Moreover when this feature is enabled by setting the `spark.shuffle.service.fetch.rdd.enabled` config to true and a block is reported to be persisted on to disk the external shuffle service instance running on the same host as the executor is also registered (along with the reporting block manager) as a possible location for fetching it.\n\n## Some implementation detail\n\nSome explanation about the decisions made during the development:\n- the location list to fetch a block was randomized but the groups (same host.  same rack.  others) order was kept. In this PR the order of groups are kept and external shuffle service added to the end of the each group.\n- `BlockManagerInfo` is not introduced for external shuffle service but only a lightweight solution is taken. A hash map from `BlockId` to `BlockStatus` is introduced. A type alias would make the source more readable but I know it is discouraged. On the other hand a new class wrapping this hash map would introduce unnecessary indirection.\n- when this feature is on the cleanup triggered during removing of executors (which is handled in `ExternalShuffleBlockResolver`) is modified to keep the disk persisted RDD blocks. This cleanup is triggered in standalone mode when the `spark.storage.cleanupFilesAfterExecutorExit` config is set.\n- the unpersisting of an RDD is extended to use the external shuffle service for disk persisted RDD blocks when the original executor which created the blocks are already released. New block transport messages are introduced to support this: `RemoveBlocks` and `BlocksRemoved`.\n\n# How was this patch tested?\n\n## Unit tests\n\n\n ExternalShuffleServiceSuite\n\nHere the complete use case is tested by the \"SPARK-25888: using external shuffle service fetching disk persisted blocks\" with a tiny difference: here the executor is killed manually.  this way the test is a bit faster than waiting for the idle timeout.\n\n\n ExternalShuffleBlockHandlerSuite\n\nTests the fetching of the RDD blocks via the external shuffle service.\n\n\n BlockManagerInfoSuite\n\nThis a new suite. As the `BlockManagerInfo` behaviour depends very much on whether the external shuffle service enabled or not all the tests are executed with and without it.\n\n\n BlockManagerSuite\n\nTests the sorting of the block locations.\n\n## Manually on YARN\n\nSpark App was:\n\n~~~scala\npackage com.mycompany\n\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkContext.  SparkConf}\nimport org.apache.spark.storage.StorageLevel\n\nobject TestAppDiskOnlyLevel {\n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf().setAppName(\"test-app\")\n\n    println(\"Attila: START\")\n    val sc = new SparkContext(conf)\n    val rdd = sc.parallelize(0 until 100.  10)\n      .map { i =>\n        println(s\"Attila: calculate first rdd i=$i\")\n        Thread.sleep(1000)\n        i\n      }\n\n    rdd.persist(StorageLevel.DISK_ONLY)\n    rdd.count()\n\n    println(\"Attila: First RDD is processed.  waiting for 60 sec\")\n\n    Thread.sleep(60 * 1000)\n\n    println(\"Attila: Num executors must be 0 as executorIdleTimeout is way over\")\n\n    val rdd2 = sc.parallelize(0 until 10.  1)\n      .map(i => (i.  1))\n      .persist(StorageLevel.DISK_ONLY)\n\n    rdd2.count()\n\n    println(\"Attila: Second RDD with one partition (only one executors must be alive)\")\n\n    // reduce runs as user code to detect the empty seq (empty blocks)\n    println(\"Calling collect on the first RDD: \" + rdd.collect().reduce(_ + _))\n\n    println(\"Attila: STOP\")\n  }\n}\n~~~\n\nI have submitted with the following configuration:\n\n~~~bash\nspark-submit --master yarn \\\n  --conf spark.dynamicAllocation.enabled=true \\\n  --conf spark.dynamicAllocation.executorIdleTimeout=30 \\\n  --conf spark.dynamicAllocation.cachedExecutorIdleTimeout=90 \\\n  --class com.mycompany.TestAppDiskOnlyLevel dyn_alloc_demo-core_2.11-0.1.0-SNAPSHOT-jar-with-dependencies.jar\n~~~\n\nChecked the result by filtering for the side effect of the task calculations:\n\n~~~bash\n[userserver ~]$ yarn logs -applicationId application_1556299359453_0001 | grep \"Attila: calculate\" | wc -l\nWARNING: YARN_OPTS has been replaced by HADOOP_OPTS. Using value of YARN_OPTS.\n19/04/26 10:31:59 INFO client.RMProxy: Connecting to ResourceManager at apiros-1.gce.company.com/172.31.115.165:8032\n100\n~~~\n\nSo it is only 100 task execution and not 200 (which would be the case for re-computation).\n\nMoreover from the submit/launcher log we can see executors really stopped in between (see the new total is 0 before the last line):\n~~~\n[userserver ~]$ grep \"Attila: Num executors must be 0\" -B 2 spark-submit.log\n19/04/26 10:24:27 INFO cluster.YarnScheduler: Executor 9 on apiros-3.gce.company.com killed by driver.\n19/04/26 10:24:27 INFO spark.ExecutorAllocationManager: Existing executor 9 has been removed (new total is 0)\nAttila: Num executors must be 0 as executorIdleTimeout is way over\n~~~\n\n[Full spark submit log](https://github.com/attilapiros/spark/files/3122465/spark-submit.log)\n\nI have done a test also after changing the `DISK_ONLY` storage level to `MEMORY_ONLY` for the first RDD. After this change during the 60sec waiting no executor was removed.\n\nCloses #24499 from attilapiros/SPARK-25888-final.\n\nAuthored-by: ?attilapiros? <piros.attila.zsolt@gmail.com>\nSigned-off-by: Marcelo Vanzin <vanzin@cloudera.com>\n","date":"2019-05-24 04:15:39","modifiedFileCount":"13","status":"M","submitter":"?attilapiros?"},{"authorTime":"2019-12-20 22:55:04","codes":[{"authorDate":"2019-12-20 22:55:04","commitOrder":3,"curCode":"  public boolean equals(Object other) {\n    if (other != null && other instanceof OpenBlocks) {\n      OpenBlocks o = (OpenBlocks) other;\n      return Objects.equals(appId, o.appId)\n        && Objects.equals(execId, o.execId)\n        && Arrays.equals(blockIds, o.blockIds);\n    }\n    return false;\n  }\n","date":"2019-12-20 22:55:04","endLine":70,"groupId":"10169","id":3,"instanceNumber":1,"isCurCommit":1,"methodName":"equals","params":"(Objectother)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/77/1e17b3233ec0b646a6e0cc6caf6a9c22195538.src","preCode":"  public boolean equals(Object other) {\n    if (other != null && other instanceof OpenBlocks) {\n      OpenBlocks o = (OpenBlocks) other;\n      return Objects.equal(appId, o.appId)\n        && Objects.equal(execId, o.execId)\n        && Arrays.equals(blockIds, o.blockIds);\n    }\n    return false;\n  }\n","realPath":"common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/OpenBlocks.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":62,"status":"M"},{"authorDate":"2019-12-20 22:55:04","commitOrder":3,"curCode":"  public boolean equals(Object other) {\n    if (other != null && other instanceof RemoveBlocks) {\n      RemoveBlocks o = (RemoveBlocks) other;\n      return Objects.equals(appId, o.appId)\n        && Objects.equals(execId, o.execId)\n        && Arrays.equals(blockIds, o.blockIds);\n    }\n    return false;\n  }\n","date":"2019-12-20 22:55:04","endLine":70,"groupId":"10169","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"equals","params":"(Objectother)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/ad/e838bd4286c4d169375f815757ff7103fb717f.src","preCode":"  public boolean equals(Object other) {\n    if (other != null && other instanceof RemoveBlocks) {\n      RemoveBlocks o = (RemoveBlocks) other;\n      return Objects.equal(appId, o.appId)\n        && Objects.equal(execId, o.execId)\n        && Arrays.equals(blockIds, o.blockIds);\n    }\n    return false;\n  }\n","realPath":"common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/RemoveBlocks.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":62,"status":"M"}],"commitId":"7dff3b125de23a4d6ce834217ee08973b259414c","commitMessage":"@@@[SPARK-30272][SQL][CORE] Remove usage of Guava that breaks in 27; replace with workalikes\n\n\n What changes were proposed in this pull request?\n\nRemove usages of Guava that no longer work in Guava 27.  and replace with workalikes. I'll comment on key types of changes below.\n\n\n Why are the changes needed?\n\nHadoop 3.2.1 uses Guava 27.  so this helps us avoid problems running on Hadoop 3.2.1+ and generally lowers our exposure to Guava.\n\n\n Does this PR introduce any user-facing change?\n\nShould not be.  but see notes below on hash codes and toString.\n\n\n How was this patch tested?\n\nExisting tests will verify whether these changes break anything for Guava 14.\nI manually built with an updated version and it compiles with Guava 27; tests running manually locally now.\n\nCloses #26911 from srowen/SPARK-30272.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n","date":"2019-12-20 22:55:04","modifiedFileCount":"34","status":"M","submitter":"Sean Owen"}]
