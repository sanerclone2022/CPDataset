[{"authorTime":"2015-04-26 03:27:19","codes":[{"authorDate":"2015-04-26 03:27:19","commitOrder":1,"curCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<Integer, Integer>();\n    DataFrame dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    GBTClassifier rf = new GBTClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setMaxIter(3)\n      .setStepSize(0.1)\n      .setMaxDepth(2); \r\n    for (String lossType: GBTClassifier.supportedLossTypes()) {\n      rf.setLossType(lossType);\n    }\n    GBTClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","date":"2015-04-26 03:27:19","endLine":99,"groupId":"2934","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"runDT","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/3c/69467fa119ee823c57250e3ace62539b3322d9.src","preCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<Integer, Integer>();\n    DataFrame dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    GBTClassifier rf = new GBTClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setMaxIter(3)\n      .setStepSize(0.1)\n      .setMaxDepth(2); \r\n    for (String lossType: GBTClassifier.supportedLossTypes()) {\n      rf.setLossType(lossType);\n    }\n    GBTClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/classification/JavaGBTClassifierSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":52,"status":"B"},{"authorDate":"2015-04-26 03:27:19","commitOrder":1,"curCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<Integer, Integer>();\n    DataFrame dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    RandomForestClassifier rf = new RandomForestClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setNumTrees(3)\n      .setMaxDepth(2); \r\n    for (String impurity: RandomForestClassifier.supportedImpurities()) {\n      rf.setImpurity(impurity);\n    }\n    for (String featureSubsetStrategy: RandomForestClassifier.supportedFeatureSubsetStrategies()) {\n      rf.setFeatureSubsetStrategy(featureSubsetStrategy);\n    }\n    RandomForestClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","date":"2015-04-26 03:27:19","endLine":102,"groupId":"1362","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"runDT","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/32/d0b3856b7e258fc4792820c7d3fa5186e6aa76.src","preCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<Integer, Integer>();\n    DataFrame dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    RandomForestClassifier rf = new RandomForestClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setNumTrees(3)\n      .setMaxDepth(2); \r\n    for (String impurity: RandomForestClassifier.supportedImpurities()) {\n      rf.setImpurity(impurity);\n    }\n    for (String featureSubsetStrategy: RandomForestClassifier.supportedFeatureSubsetStrategies()) {\n      rf.setFeatureSubsetStrategy(featureSubsetStrategy);\n    }\n    RandomForestClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/classification/JavaRandomForestClassifierSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":52,"status":"B"}],"commitId":"a7160c4e3aae22600d05e257d0b4d2428754b8ea","commitMessage":"@@@[SPARK-6113] [ML] Tree ensembles for Pipelines API\n\nThis is a continuation of [https://github.com/apache/spark/pull/5530] (which was for Decision Trees).  but for ensembles: Random Forests and Gradient-Boosted Trees.  Please refer to the JIRA [https://issues.apache.org/jira/browse/SPARK-6113].  the design doc linked from the JIRA.  and the previous PR linked above for design discussions.\n\nThis PR follows the example set by the previous PR for Decision Trees.  It includes a few cleanups to Decision Trees.\n\nNote: There is one issue which will be addressed in a separate PR: Ensembles' component Models have no parent or fittingParamMap.  I plan to submit a separate PR which makes those values in Model be Options.  It does not matter much which PR gets merged first.\n\nCC: mengxr manishamde codedeft chouqin\n\nAuthor: Joseph K. Bradley <joseph@databricks.com>\n\nCloses #5626 from jkbradley/dt-api-ensembles and squashes the following commits:\n\n729167a [Joseph K. Bradley] small cleanups based on code review\nbbae2a2 [Joseph K. Bradley] Updated per all comments in code review\n855aa9a [Joseph K. Bradley] scala style fix\nea3d901 [Joseph K. Bradley] Added GBT to spark.ml.  with tests and examples\nc0f30c1 [Joseph K. Bradley] Added random forests and test suites to spark.ml.  Not tested yet.  Need to add example as well\nd045ebd [Joseph K. Bradley] some more updates.  but far from done\nee1a10b [Joseph K. Bradley] Added files from old PR and did some initial updates.\n","date":"2015-04-26 03:27:19","modifiedFileCount":"2","status":"B","submitter":"Joseph K. Bradley"},{"authorTime":"2015-08-04 03:17:46","codes":[{"authorDate":"2015-04-26 03:27:19","commitOrder":2,"curCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<Integer, Integer>();\n    DataFrame dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    GBTClassifier rf = new GBTClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setMaxIter(3)\n      .setStepSize(0.1)\n      .setMaxDepth(2); \r\n    for (String lossType: GBTClassifier.supportedLossTypes()) {\n      rf.setLossType(lossType);\n    }\n    GBTClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","date":"2015-04-26 03:27:19","endLine":99,"groupId":"2934","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"runDT","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/3c/69467fa119ee823c57250e3ace62539b3322d9.src","preCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<Integer, Integer>();\n    DataFrame dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    GBTClassifier rf = new GBTClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setMaxIter(3)\n      .setStepSize(0.1)\n      .setMaxDepth(2); \r\n    for (String lossType: GBTClassifier.supportedLossTypes()) {\n      rf.setLossType(lossType);\n    }\n    GBTClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/classification/JavaGBTClassifierSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":52,"status":"N"},{"authorDate":"2015-08-04 03:17:46","commitOrder":2,"curCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<Integer, Integer>();\n    DataFrame dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    RandomForestClassifier rf = new RandomForestClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setNumTrees(3)\n      .setMaxDepth(2); \r\n    for (String impurity: RandomForestClassifier.supportedImpurities()) {\n      rf.setImpurity(impurity);\n    }\n    for (String featureSubsetStrategy: RandomForestClassifier.supportedFeatureSubsetStrategies()) {\n      rf.setFeatureSubsetStrategy(featureSubsetStrategy);\n    }\n    RandomForestClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n    Vector importances = model.featureImportances();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","date":"2015-08-04 03:17:46","endLine":104,"groupId":"1362","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"runDT","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/a6/6a1e12927beaf1fbb1fa269c4f4a439b6e4150.src","preCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<Integer, Integer>();\n    DataFrame dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    RandomForestClassifier rf = new RandomForestClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setNumTrees(3)\n      .setMaxDepth(2); \r\n    for (String impurity: RandomForestClassifier.supportedImpurities()) {\n      rf.setImpurity(impurity);\n    }\n    for (String featureSubsetStrategy: RandomForestClassifier.supportedFeatureSubsetStrategies()) {\n      rf.setFeatureSubsetStrategy(featureSubsetStrategy);\n    }\n    RandomForestClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/classification/JavaRandomForestClassifierSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":53,"status":"M"}],"commitId":"ff9169a002f1b75231fd25b7d04157a912503038","commitMessage":"@@@[SPARK-5133] [ML] Added featureImportance to RandomForestClassifier and Regressor\n\nAdded featureImportance to RandomForestClassifier and Regressor.\n\nThis follows the scikit-learn implementation here: [https://github.com/scikit-learn/scikit-learn/blob/a95203b249c1cf392f86d001ad999e29b2392739/sklearn/tree/_tree.pyx#L3341]\n\nCC: yanboliang  Would you mind taking a look?  Thanks!\n\nAuthor: Joseph K. Bradley <joseph@databricks.com>\nAuthor: Feynman Liang <fliang@databricks.com>\n\nCloses #7838 from jkbradley/dt-feature-importance and squashes the following commits:\n\n72a167a [Joseph K. Bradley] fixed unit test\n86cea5f [Joseph K. Bradley] Modified RF featuresImportances to return Vector instead of Map\n5aa74f0 [Joseph K. Bradley] finally fixed unit test for real\n33df5db [Joseph K. Bradley] fix unit test\n42a2d3b [Joseph K. Bradley] fix unit test\nfe94e72 [Joseph K. Bradley] modified feature importance unit tests\ncc693ee [Feynman Liang] Add classifier tests\n79a6f87 [Feynman Liang] Compare dense vectors in test\n21d01fc [Feynman Liang] Added failing SKLearn test\nac0b254 [Joseph K. Bradley] Added featureImportance to RandomForestClassifier/Regressor.  Need to add unit tests\n","date":"2015-08-04 03:17:46","modifiedFileCount":"2","status":"M","submitter":"Joseph K. Bradley"},{"authorTime":"2016-03-09 18:31:26","codes":[{"authorDate":"2016-03-09 18:31:26","commitOrder":3,"curCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<>();\n    DataFrame dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    GBTClassifier rf = new GBTClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setMaxIter(3)\n      .setStepSize(0.1)\n      .setMaxDepth(2); \r\n    for (String lossType: GBTClassifier.supportedLossTypes()) {\n      rf.setLossType(lossType);\n    }\n    GBTClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","date":"2016-03-09 18:31:26","endLine":99,"groupId":"2934","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"runDT","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/59/b6fba7a928adc6b52be3057fea3c2f3920ff37.src","preCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<Integer, Integer>();\n    DataFrame dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    GBTClassifier rf = new GBTClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setMaxIter(3)\n      .setStepSize(0.1)\n      .setMaxDepth(2); \r\n    for (String lossType: GBTClassifier.supportedLossTypes()) {\n      rf.setLossType(lossType);\n    }\n    GBTClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/classification/JavaGBTClassifierSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":52,"status":"M"},{"authorDate":"2016-03-09 18:31:26","commitOrder":3,"curCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<>();\n    DataFrame dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    RandomForestClassifier rf = new RandomForestClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setNumTrees(3)\n      .setMaxDepth(2); \r\n    for (String impurity: RandomForestClassifier.supportedImpurities()) {\n      rf.setImpurity(impurity);\n    }\n    for (String featureSubsetStrategy: RandomForestClassifier.supportedFeatureSubsetStrategies()) {\n      rf.setFeatureSubsetStrategy(featureSubsetStrategy);\n    }\n    RandomForestClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n    Vector importances = model.featureImportances();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","date":"2016-03-09 18:31:26","endLine":104,"groupId":"1362","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"runDT","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/54/85fcbf01bda8631893e2413d8be882bd33df16.src","preCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<Integer, Integer>();\n    DataFrame dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    RandomForestClassifier rf = new RandomForestClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setNumTrees(3)\n      .setMaxDepth(2); \r\n    for (String impurity: RandomForestClassifier.supportedImpurities()) {\n      rf.setImpurity(impurity);\n    }\n    for (String featureSubsetStrategy: RandomForestClassifier.supportedFeatureSubsetStrategies()) {\n      rf.setFeatureSubsetStrategy(featureSubsetStrategy);\n    }\n    RandomForestClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n    Vector importances = model.featureImportances();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/classification/JavaRandomForestClassifierSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":53,"status":"M"}],"commitId":"c3689bc24e03a9471cd6e8169da61963c4528252","commitMessage":"@@@[SPARK-13702][CORE][SQL][MLLIB] Use diamond operator for generic instance creation in Java code.\n\n## What changes were proposed in this pull request?\n\nIn order to make `docs/examples` (and other related code) more simple/readable/user-friendly.  this PR replaces existing codes like the followings by using `diamond` operator.\n\n```\n-    final ArrayList<Product2<Object.  Object>> dataToWrite =\n-      new ArrayList<Product2<Object.  Object>>();\n+    final ArrayList<Product2<Object.  Object>> dataToWrite = new ArrayList<>();\n```\n\nJava 7 or higher supports **diamond** operator which replaces the type arguments required to invoke the constructor of a generic class with an empty set of type parameters (<>). Currently.  Spark Java code use mixed usage of this.\n\n## How was this patch tested?\n\nManual.\nPass the existing tests.\n\nAuthor: Dongjoon Hyun <dongjoon@apache.org>\n\nCloses #11541 from dongjoon-hyun/SPARK-13702.\n","date":"2016-03-09 18:31:26","modifiedFileCount":"57","status":"M","submitter":"Dongjoon Hyun"},{"authorTime":"2016-03-11 09:00:17","codes":[{"authorDate":"2016-03-11 09:00:17","commitOrder":4,"curCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<>();\n    Dataset<Row> dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    GBTClassifier rf = new GBTClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setMaxIter(3)\n      .setStepSize(0.1)\n      .setMaxDepth(2); \r\n    for (String lossType: GBTClassifier.supportedLossTypes()) {\n      rf.setLossType(lossType);\n    }\n    GBTClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","date":"2016-03-11 09:00:17","endLine":100,"groupId":"976","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"runDT","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/f4/70f4ada6397078324f411ddc66c90dda986a2d.src","preCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<>();\n    DataFrame dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    GBTClassifier rf = new GBTClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setMaxIter(3)\n      .setStepSize(0.1)\n      .setMaxDepth(2); \r\n    for (String lossType: GBTClassifier.supportedLossTypes()) {\n      rf.setLossType(lossType);\n    }\n    GBTClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/classification/JavaGBTClassifierSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":53,"status":"M"},{"authorDate":"2016-03-11 09:00:17","commitOrder":4,"curCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<>();\n    Dataset<Row> dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    RandomForestClassifier rf = new RandomForestClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setNumTrees(3)\n      .setMaxDepth(2); \r\n    for (String impurity: RandomForestClassifier.supportedImpurities()) {\n      rf.setImpurity(impurity);\n    }\n    for (String featureSubsetStrategy: RandomForestClassifier.supportedFeatureSubsetStrategies()) {\n      rf.setFeatureSubsetStrategy(featureSubsetStrategy);\n    }\n    RandomForestClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n    Vector importances = model.featureImportances();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","date":"2016-03-11 09:00:17","endLine":105,"groupId":"1362","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"runDT","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/9a/63cef2a8f7267775fc2b7c163039dcbdfdc73e.src","preCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<>();\n    DataFrame dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    RandomForestClassifier rf = new RandomForestClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setNumTrees(3)\n      .setMaxDepth(2); \r\n    for (String impurity: RandomForestClassifier.supportedImpurities()) {\n      rf.setImpurity(impurity);\n    }\n    for (String featureSubsetStrategy: RandomForestClassifier.supportedFeatureSubsetStrategies()) {\n      rf.setFeatureSubsetStrategy(featureSubsetStrategy);\n    }\n    RandomForestClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n    Vector importances = model.featureImportances();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/classification/JavaRandomForestClassifierSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":54,"status":"M"}],"commitId":"1d542785b9949e7f92025e6754973a779cc37c52","commitMessage":"@@@[SPARK-13244][SQL] Migrates DataFrame to Dataset\n\n## What changes were proposed in this pull request?\n\nThis PR unifies DataFrame and Dataset by migrating existing DataFrame operations to Dataset and make `DataFrame` a type alias of `Dataset[Row]`.\n\nMost Scala code changes are source compatible.  but Java API is broken as Java knows nothing about Scala type alias (mostly replacing `DataFrame` with `Dataset<Row>`).\n\nThere are several noticeable API changes related to those returning arrays:\n\n1.  `collect`/`take`\n\n    -   Old APIs in class `DataFrame`:\n\n        ```scala\n        def collect(): Array[Row]\n        def take(n: Int): Array[Row]\n        ```\n\n    -   New APIs in class `Dataset[T]`:\n\n        ```scala\n        def collect(): Array[T]\n        def take(n: Int): Array[T]\n\n        def collectRows(): Array[Row]\n        def takeRows(n: Int): Array[Row]\n        ```\n\n    Two specialized methods `collectRows` and `takeRows` are added because Java doesn't support returning generic arrays. Thus.  for example.  `DataFrame.collect(): Array[T]` actually returns `Object` instead of `Array<T>` from Java side.\n\n    Normally.  Java users may fall back to `collectAsList` and `takeAsList`.  The two new specialized versions are added to avoid performance regression in ML related code (but maybe I'm wrong and they are not necessary here).\n\n1.  `randomSplit`\n\n    -   Old APIs in class `DataFrame`:\n\n        ```scala\n        def randomSplit(weights: Array[Double].  seed: Long): Array[DataFrame]\n        def randomSplit(weights: Array[Double]): Array[DataFrame]\n        ```\n\n    -   New APIs in class `Dataset[T]`:\n\n        ```scala\n        def randomSplit(weights: Array[Double].  seed: Long): Array[Dataset[T]]\n        def randomSplit(weights: Array[Double]): Array[Dataset[T]]\n        ```\n\n    Similar problem as above.  but hasn't been addressed for Java API yet.  We can probably add `randomSplitAsList` to fix this one.\n\n1.  `groupBy`\n\n    Some original `DataFrame.groupBy` methods have conflicting signature with original `Dataset.groupBy` methods.  To distinguish these two.  typed `Dataset.groupBy` methods are renamed to `groupByKey`.\n\nOther noticeable changes:\n\n1.  Dataset always do eager analysis now\n\n    We used to support disabling DataFrame eager analysis to help reporting partially analyzed malformed logical plan on analysis failure.  However.  Dataset encoders requires eager analysi during Dataset construction.  To preserve the error reporting feature.  `AnalysisException` now takes an extra `Option[LogicalPlan]` argument to hold the partially analyzed plan.  so that we can check the plan tree when reporting test failures.  This plan is passed by `QueryExecution.assertAnalyzed`.\n\n## How was this patch tested?\n\nExisting tests do the work.\n\n## TODO\n\n- [ ] Fix all tests\n- [ ] Re-enable MiMA check\n- [ ] Update ScalaDoc (`since`.  `group`.  and example code)\n\nAuthor: Cheng Lian <lian@databricks.com>\nAuthor: Yin Huai <yhuai@databricks.com>\nAuthor: Wenchen Fan <wenchen@databricks.com>\nAuthor: Cheng Lian <liancheng@users.noreply.github.com>\n\nCloses #11443 from liancheng/ds-to-df.\n","date":"2016-03-11 09:00:17","modifiedFileCount":"87","status":"M","submitter":"Cheng Lian"},{"authorTime":"2016-04-12 22:53:26","codes":[{"authorDate":"2016-03-11 09:00:17","commitOrder":5,"curCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<>();\n    Dataset<Row> dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    GBTClassifier rf = new GBTClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setMaxIter(3)\n      .setStepSize(0.1)\n      .setMaxDepth(2); \r\n    for (String lossType: GBTClassifier.supportedLossTypes()) {\n      rf.setLossType(lossType);\n    }\n    GBTClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","date":"2016-03-11 09:00:17","endLine":100,"groupId":"976","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"runDT","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/f4/70f4ada6397078324f411ddc66c90dda986a2d.src","preCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<>();\n    Dataset<Row> dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    GBTClassifier rf = new GBTClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setMaxIter(3)\n      .setStepSize(0.1)\n      .setMaxDepth(2); \r\n    for (String lossType: GBTClassifier.supportedLossTypes()) {\n      rf.setLossType(lossType);\n    }\n    GBTClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/classification/JavaGBTClassifierSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":53,"status":"N"},{"authorDate":"2016-04-12 22:53:26","commitOrder":5,"curCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<>();\n    Dataset<Row> dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    RandomForestClassifier rf = new RandomForestClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setNumTrees(3)\n      .setMaxDepth(2); \r\n    for (String impurity: RandomForestClassifier.supportedImpurities()) {\n      rf.setImpurity(impurity);\n    }\n    for (String featureSubsetStrategy: RandomForestClassifier.supportedFeatureSubsetStrategies()) {\n      rf.setFeatureSubsetStrategy(featureSubsetStrategy);\n    }\n    String realStrategies[] = {\".1\", \".10\", \"0.10\", \"0.1\", \"0.9\", \"1.0\"};\n    for (String strategy: realStrategies) {\n      rf.setFeatureSubsetStrategy(strategy);\n    }\n    String integerStrategies[] = {\"1\", \"10\", \"100\", \"1000\", \"10000\"};\n    for (String strategy: integerStrategies) {\n      rf.setFeatureSubsetStrategy(strategy);\n    }\n    String invalidStrategies[] = {\"-.1\", \"-.10\", \"-0.10\", \".0\", \"0.0\", \"1.1\", \"0\"};\n    for (String strategy: invalidStrategies) {\n      try {\n        rf.setFeatureSubsetStrategy(strategy);\n        Assert.fail(\"Expected exception to be thrown for invalid strategies\");\n      } catch (Exception e) {\n        Assert.assertTrue(e instanceof IllegalArgumentException);\n      }\n    }\n\n    RandomForestClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n    Vector importances = model.featureImportances();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","date":"2016-04-12 22:53:26","endLine":124,"groupId":"1362","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"runDT","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/5a/ec52ac72b180773a84cf66fc25a135852409d3.src","preCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<>();\n    Dataset<Row> dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    RandomForestClassifier rf = new RandomForestClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setNumTrees(3)\n      .setMaxDepth(2); \r\n    for (String impurity: RandomForestClassifier.supportedImpurities()) {\n      rf.setImpurity(impurity);\n    }\n    for (String featureSubsetStrategy: RandomForestClassifier.supportedFeatureSubsetStrategies()) {\n      rf.setFeatureSubsetStrategy(featureSubsetStrategy);\n    }\n    RandomForestClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n    Vector importances = model.featureImportances();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/classification/JavaRandomForestClassifierSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":55,"status":"M"}],"commitId":"da60b34d2f6eba19633e4f1b46504ce92cd6c179","commitMessage":"@@@[SPARK-3724][ML] RandomForest: More options for feature subset size.\n\n## What changes were proposed in this pull request?\n\nThis PR tries to support more options for feature subset size in RandomForest implementation. Previously.  RandomForest only support \"auto\".  \"all\".  \"sort\".  \"log2\".  \"onethird\". This PR tries to support any given value to allow model search.\n\nIn this PR.  `featureSubsetStrategy` could be passed with:\na) a real number in the range of `(0.0-1.0]` that represents the fraction of the number of features in each subset. \nb)  an integer number (`>0`) that represents the number of features in each subset.\n\n## How was this patch tested?\n\nTwo tests `JavaRandomForestClassifierSuite` and `JavaRandomForestRegressorSuite` have been updated to check the additional options for params in this PR.\nAn additional test has been added to `org.apache.spark.mllib.tree.RandomForestSuite` to cover the cases in this PR.\n\nAuthor: Yong Tang <yong.tang.github@outlook.com>\n\nCloses #11989 from yongtang/SPARK-3724.\n","date":"2016-04-12 22:53:26","modifiedFileCount":"2","status":"M","submitter":"Yong Tang"},{"authorTime":"2016-04-25 11:40:03","codes":[{"authorDate":"2016-03-11 09:00:17","commitOrder":6,"curCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<>();\n    Dataset<Row> dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    GBTClassifier rf = new GBTClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setMaxIter(3)\n      .setStepSize(0.1)\n      .setMaxDepth(2); \r\n    for (String lossType: GBTClassifier.supportedLossTypes()) {\n      rf.setLossType(lossType);\n    }\n    GBTClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","date":"2016-03-11 09:00:17","endLine":100,"groupId":"976","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"runDT","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/f4/70f4ada6397078324f411ddc66c90dda986a2d.src","preCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<>();\n    Dataset<Row> dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    GBTClassifier rf = new GBTClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setMaxIter(3)\n      .setStepSize(0.1)\n      .setMaxDepth(2); \r\n    for (String lossType: GBTClassifier.supportedLossTypes()) {\n      rf.setLossType(lossType);\n    }\n    GBTClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/classification/JavaGBTClassifierSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":53,"status":"N"},{"authorDate":"2016-04-25 11:40:03","commitOrder":6,"curCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<>();\n    Dataset<Row> dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    RandomForestClassifier rf = new RandomForestClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setNumTrees(3)\n      .setMaxDepth(2); \r\n    for (String impurity: RandomForestClassifier.supportedImpurities()) {\n      rf.setImpurity(impurity);\n    }\n    for (String featureSubsetStrategy: RandomForestClassifier.supportedFeatureSubsetStrategies()) {\n      rf.setFeatureSubsetStrategy(featureSubsetStrategy);\n    }\n    String[] realStrategies = {\".1\", \".10\", \"0.10\", \"0.1\", \"0.9\", \"1.0\"};\n    for (String strategy: realStrategies) {\n      rf.setFeatureSubsetStrategy(strategy);\n    }\n    String[] integerStrategies = {\"1\", \"10\", \"100\", \"1000\", \"10000\"};\n    for (String strategy: integerStrategies) {\n      rf.setFeatureSubsetStrategy(strategy);\n    }\n    String[] invalidStrategies = {\"-.1\", \"-.10\", \"-0.10\", \".0\", \"0.0\", \"1.1\", \"0\"};\n    for (String strategy: invalidStrategies) {\n      try {\n        rf.setFeatureSubsetStrategy(strategy);\n        Assert.fail(\"Expected exception to be thrown for invalid strategies\");\n      } catch (Exception e) {\n        Assert.assertTrue(e instanceof IllegalArgumentException);\n      }\n    }\n\n    RandomForestClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n    Vector importances = model.featureImportances();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","date":"2016-04-25 11:40:03","endLine":124,"groupId":"1362","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"runDT","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/4f/40fd65b9f11f3c8c8787ca25f5505cf24327cd.src","preCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<>();\n    Dataset<Row> dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    RandomForestClassifier rf = new RandomForestClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setNumTrees(3)\n      .setMaxDepth(2); \r\n    for (String impurity: RandomForestClassifier.supportedImpurities()) {\n      rf.setImpurity(impurity);\n    }\n    for (String featureSubsetStrategy: RandomForestClassifier.supportedFeatureSubsetStrategies()) {\n      rf.setFeatureSubsetStrategy(featureSubsetStrategy);\n    }\n    String realStrategies[] = {\".1\", \".10\", \"0.10\", \"0.1\", \"0.9\", \"1.0\"};\n    for (String strategy: realStrategies) {\n      rf.setFeatureSubsetStrategy(strategy);\n    }\n    String integerStrategies[] = {\"1\", \"10\", \"100\", \"1000\", \"10000\"};\n    for (String strategy: integerStrategies) {\n      rf.setFeatureSubsetStrategy(strategy);\n    }\n    String invalidStrategies[] = {\"-.1\", \"-.10\", \"-0.10\", \".0\", \"0.0\", \"1.1\", \"0\"};\n    for (String strategy: invalidStrategies) {\n      try {\n        rf.setFeatureSubsetStrategy(strategy);\n        Assert.fail(\"Expected exception to be thrown for invalid strategies\");\n      } catch (Exception e) {\n        Assert.assertTrue(e instanceof IllegalArgumentException);\n      }\n    }\n\n    RandomForestClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n    Vector importances = model.featureImportances();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/classification/JavaRandomForestClassifierSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":55,"status":"M"}],"commitId":"d34d6503786bbe429c10ddb1879519cc9bd709b6","commitMessage":"@@@[SPARK-14868][BUILD] Enable NewLineAtEofChecker in checkstyle and fix lint-java errors\n\n## What changes were proposed in this pull request?\n\nSpark uses `NewLineAtEofChecker` rule in Scala by ScalaStyle. And.  most Java code also comply with the rule. This PR aims to enforce the same rule `NewlineAtEndOfFile` by CheckStyle explicitly. Also.  this fixes lint-java errors since SPARK-14465. The followings are the items.\n\n- Adds a new line at the end of the files (19 files)\n- Fixes 25 lint-java errors (12 RedundantModifier.  6 **ArrayTypeStyle**.  2 LineLength.  2 UnusedImports.  2 RegexpSingleline.  1 ModifierOrder)\n\n## How was this patch tested?\n\nAfter the Jenkins test succeeds.  `dev/lint-java` should pass. (Currently.  Jenkins dose not run lint-java.)\n```bash\n$ dev/lint-java\nUsing `mvn` from path: /usr/local/bin/mvn\nCheckstyle checks passed.\n```\n\nAuthor: Dongjoon Hyun <dongjoon@apache.org>\n\nCloses #12632 from dongjoon-hyun/SPARK-14868.\n","date":"2016-04-25 11:40:03","modifiedFileCount":"24","status":"M","submitter":"Dongjoon Hyun"},{"authorTime":"2016-05-11 02:17:47","codes":[{"authorDate":"2016-05-11 02:17:47","commitOrder":7,"curCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = jsc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<>();\n    Dataset<Row> dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    GBTClassifier rf = new GBTClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setMaxIter(3)\n      .setStepSize(0.1)\n      .setMaxDepth(2); \r\n    for (String lossType : GBTClassifier.supportedLossTypes()) {\n      rf.setLossType(lossType);\n    }\n    GBTClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","date":"2016-05-11 02:17:47","endLine":106,"groupId":"10484","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"runDT","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/68/2371eb9e4d5132dfd9a40a263b98f13cf110dd.src","preCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<>();\n    Dataset<Row> dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    GBTClassifier rf = new GBTClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setMaxIter(3)\n      .setStepSize(0.1)\n      .setMaxDepth(2); \r\n    for (String lossType: GBTClassifier.supportedLossTypes()) {\n      rf.setLossType(lossType);\n    }\n    GBTClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/classification/JavaGBTClassifierSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":59,"status":"M"},{"authorDate":"2016-05-11 02:17:47","commitOrder":7,"curCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = jsc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<>();\n    Dataset<Row> dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    RandomForestClassifier rf = new RandomForestClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setNumTrees(3)\n      .setMaxDepth(2); \r\n    for (String impurity : RandomForestClassifier.supportedImpurities()) {\n      rf.setImpurity(impurity);\n    }\n    for (String featureSubsetStrategy : RandomForestClassifier.supportedFeatureSubsetStrategies()) {\n      rf.setFeatureSubsetStrategy(featureSubsetStrategy);\n    }\n    String[] realStrategies = {\".1\", \".10\", \"0.10\", \"0.1\", \"0.9\", \"1.0\"};\n    for (String strategy : realStrategies) {\n      rf.setFeatureSubsetStrategy(strategy);\n    }\n    String[] integerStrategies = {\"1\", \"10\", \"100\", \"1000\", \"10000\"};\n    for (String strategy : integerStrategies) {\n      rf.setFeatureSubsetStrategy(strategy);\n    }\n    String[] invalidStrategies = {\"-.1\", \"-.10\", \"-0.10\", \".0\", \"0.0\", \"1.1\", \"0\"};\n    for (String strategy : invalidStrategies) {\n      try {\n        rf.setFeatureSubsetStrategy(strategy);\n        Assert.fail(\"Expected exception to be thrown for invalid strategies\");\n      } catch (Exception e) {\n        Assert.assertTrue(e instanceof IllegalArgumentException);\n      }\n    }\n\n    RandomForestClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n    Vector importances = model.featureImportances();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","date":"2016-05-11 02:17:47","endLine":130,"groupId":"10484","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"runDT","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/e3/855662fb6de1e65e550e553bf5d04b3b75679e.src","preCode":"  public void runDT() {\n    int nPoints = 20;\n    double A = 2.0;\n    double B = -1.5;\n\n    JavaRDD<LabeledPoint> data = sc.parallelize(\n      LogisticRegressionSuite.generateLogisticInputAsList(A, B, nPoints, 42), 2).cache();\n    Map<Integer, Integer> categoricalFeatures = new HashMap<>();\n    Dataset<Row> dataFrame = TreeTests.setMetadata(data, categoricalFeatures, 2);\n\n    \r\n    RandomForestClassifier rf = new RandomForestClassifier()\n      .setMaxDepth(2)\n      .setMaxBins(10)\n      .setMinInstancesPerNode(5)\n      .setMinInfoGain(0.0)\n      .setMaxMemoryInMB(256)\n      .setCacheNodeIds(false)\n      .setCheckpointInterval(10)\n      .setSubsamplingRate(1.0)\n      .setSeed(1234)\n      .setNumTrees(3)\n      .setMaxDepth(2); \r\n    for (String impurity: RandomForestClassifier.supportedImpurities()) {\n      rf.setImpurity(impurity);\n    }\n    for (String featureSubsetStrategy: RandomForestClassifier.supportedFeatureSubsetStrategies()) {\n      rf.setFeatureSubsetStrategy(featureSubsetStrategy);\n    }\n    String[] realStrategies = {\".1\", \".10\", \"0.10\", \"0.1\", \"0.9\", \"1.0\"};\n    for (String strategy: realStrategies) {\n      rf.setFeatureSubsetStrategy(strategy);\n    }\n    String[] integerStrategies = {\"1\", \"10\", \"100\", \"1000\", \"10000\"};\n    for (String strategy: integerStrategies) {\n      rf.setFeatureSubsetStrategy(strategy);\n    }\n    String[] invalidStrategies = {\"-.1\", \"-.10\", \"-0.10\", \".0\", \"0.0\", \"1.1\", \"0\"};\n    for (String strategy: invalidStrategies) {\n      try {\n        rf.setFeatureSubsetStrategy(strategy);\n        Assert.fail(\"Expected exception to be thrown for invalid strategies\");\n      } catch (Exception e) {\n        Assert.assertTrue(e instanceof IllegalArgumentException);\n      }\n    }\n\n    RandomForestClassificationModel model = rf.fit(dataFrame);\n\n    model.transform(dataFrame);\n    model.totalNumNodes();\n    model.toDebugString();\n    model.trees();\n    model.treeWeights();\n    Vector importances = model.featureImportances();\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","realPath":"mllib/src/test/java/org/apache/spark/ml/classification/JavaRandomForestClassifierSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":61,"status":"M"}],"commitId":"ed0b4070fb50054b1ecf66ff6c32458a4967dfd3","commitMessage":"@@@[SPARK-15037][SQL][MLLIB] Use SparkSession instead of SQLContext in Scala/Java TestSuites\n\n## What changes were proposed in this pull request?\nUse SparkSession instead of SQLContext in Scala/Java TestSuites\nas this PR already very big working Python TestSuites in a diff PR.\n\n## How was this patch tested?\nExisting tests\n\nAuthor: Sandeep Singh <sandeep@techaddict.me>\n\nCloses #12907 from techaddict/SPARK-15037.\n","date":"2016-05-11 02:17:47","modifiedFileCount":"63","status":"M","submitter":"Sandeep Singh"}]
