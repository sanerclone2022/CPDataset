[{"authorTime":"2015-02-25 10:51:41","codes":[{"authorDate":"2015-02-25 10:51:41","commitOrder":1,"curCode":"  public void applySchema() {\n    List<Person> personList = new ArrayList<Person>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n      new Function<Person, Row>() {\n        public Row call(Person person) throws Exception {\n          return RowFactory.create(person.getName(), person.getAge());\n        }\n      });\n\n    List<StructField> fields = new ArrayList<StructField>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    DataFrame df = sqlContext.applySchema(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    Row[] actual = sqlContext.sql(\"SELECT * FROM people\").collect();\n\n    List<Row> expected = new ArrayList<Row>(2);\n    expected.add(RowFactory.create(\"Michael\", 29));\n    expected.add(RowFactory.create(\"Yin\", 28));\n\n    Assert.assertEquals(expected, Arrays.asList(actual));\n  }\n","date":"2015-02-25 10:51:41","endLine":110,"groupId":"1395","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"applySchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/c3/44a9b095c52b2bde2b6161e5b97f8f8e764c51.src","preCode":"  public void applySchema() {\n    List<Person> personList = new ArrayList<Person>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n      new Function<Person, Row>() {\n        public Row call(Person person) throws Exception {\n          return RowFactory.create(person.getName(), person.getAge());\n        }\n      });\n\n    List<StructField> fields = new ArrayList<StructField>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    DataFrame df = sqlContext.applySchema(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    Row[] actual = sqlContext.sql(\"SELECT * FROM people\").collect();\n\n    List<Row> expected = new ArrayList<Row>(2);\n    expected.add(RowFactory.create(\"Michael\", 29));\n    expected.add(RowFactory.create(\"Yin\", 28));\n\n    Assert.assertEquals(expected, Arrays.asList(actual));\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaApplySchemaSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":78,"status":"B"},{"authorDate":"2015-02-25 10:51:41","commitOrder":1,"curCode":"  public void dataFrameRDDOperations() {\n    List<Person> personList = new ArrayList<Person>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n            new Function<Person, Row>() {\n              public Row call(Person person) throws Exception {\n                return RowFactory.create(person.getName(), person.getAge());\n              }\n            });\n\n    List<StructField> fields = new ArrayList<StructField>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    DataFrame df = sqlContext.applySchema(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    List<String> actual = sqlContext.sql(\"SELECT * FROM people\").toJavaRDD().map(new Function<Row, String>() {\n\n      public String call(Row row) {\n        return row.getString(0) + \"_\" + row.get(1).toString();\n      }\n    }).collect();\n\n    List<String> expected = new ArrayList<String>(2);\n    expected.add(\"Michael_29\");\n    expected.add(\"Yin_28\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","date":"2015-02-25 10:51:41","endLine":150,"groupId":"1395","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"dataFrameRDDOperations","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/c3/44a9b095c52b2bde2b6161e5b97f8f8e764c51.src","preCode":"  public void dataFrameRDDOperations() {\n    List<Person> personList = new ArrayList<Person>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n            new Function<Person, Row>() {\n              public Row call(Person person) throws Exception {\n                return RowFactory.create(person.getName(), person.getAge());\n              }\n            });\n\n    List<StructField> fields = new ArrayList<StructField>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    DataFrame df = sqlContext.applySchema(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    List<String> actual = sqlContext.sql(\"SELECT * FROM people\").toJavaRDD().map(new Function<Row, String>() {\n\n      public String call(Row row) {\n        return row.getString(0) + \"_\" + row.get(1).toString();\n      }\n    }).collect();\n\n    List<String> expected = new ArrayList<String>(2);\n    expected.add(\"Michael_29\");\n    expected.add(\"Yin_28\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaApplySchemaSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":113,"status":"B"}],"commitId":"53a1ebf33b5c349ae3a40d7eebf357b839b363af","commitMessage":"@@@[SPARK-5904][SQL] DataFrame Java API test suites.\n\nAdded a new test suite to make sure Java DF programs can use varargs properly.\nAlso moved all suites into test.org.apache.spark package to make sure the suites also test for method visibility.\n\nAuthor: Reynold Xin <rxin@databricks.com>\n\nCloses #4751 from rxin/df-tests and squashes the following commits:\n\n1e8b8e4 [Reynold Xin] Fixed imports and renamed JavaAPISuite.\na6ca53b [Reynold Xin] [SPARK-5904][SQL] DataFrame Java API test suites.\n","date":"2015-02-25 10:51:41","modifiedFileCount":"0","status":"B","submitter":"Reynold Xin"},{"authorTime":"2015-09-12 17:40:10","codes":[{"authorDate":"2015-09-12 17:40:10","commitOrder":2,"curCode":"  public void applySchema() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n      new Function<Person, Row>() {\n        @Override\n        public Row call(Person person) throws Exception {\n          return RowFactory.create(person.getName(), person.getAge());\n        }\n      });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    DataFrame df = sqlContext.applySchema(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    Row[] actual = sqlContext.sql(\"SELECT * FROM people\").collect();\n\n    List<Row> expected = new ArrayList<Row>(2);\n    expected.add(RowFactory.create(\"Michael\", 29));\n    expected.add(RowFactory.create(\"Yin\", 28));\n\n    Assert.assertEquals(expected, Arrays.asList(actual));\n  }\n","date":"2015-09-12 17:40:10","endLine":119,"groupId":"2660","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"applySchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/7b/50aad4ad498d25167253fef7d3912d5e36a65e.src","preCode":"  public void applySchema() {\n    List<Person> personList = new ArrayList<Person>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n      new Function<Person, Row>() {\n        public Row call(Person person) throws Exception {\n          return RowFactory.create(person.getName(), person.getAge());\n        }\n      });\n\n    List<StructField> fields = new ArrayList<StructField>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    DataFrame df = sqlContext.applySchema(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    Row[] actual = sqlContext.sql(\"SELECT * FROM people\").collect();\n\n    List<Row> expected = new ArrayList<Row>(2);\n    expected.add(RowFactory.create(\"Michael\", 29));\n    expected.add(RowFactory.create(\"Yin\", 28));\n\n    Assert.assertEquals(expected, Arrays.asList(actual));\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaApplySchemaSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"M"},{"authorDate":"2015-09-12 17:40:10","commitOrder":2,"curCode":"  public void dataFrameRDDOperations() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n        new Function<Person, Row>() {\n          @Override\n          public Row call(Person person) {\n            return RowFactory.create(person.getName(), person.getAge());\n          }\n        });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    DataFrame df = sqlContext.applySchema(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    List<String> actual = sqlContext.sql(\"SELECT * FROM people\").toJavaRDD().map(new Function<Row, String>() {\n      @Override\n      public String call(Row row) {\n        return row.getString(0) + \"_\" + row.get(1);\n      }\n    }).collect();\n\n    List<String> expected = new ArrayList<>(2);\n    expected.add(\"Michael_29\");\n    expected.add(\"Yin_28\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","date":"2015-09-12 17:40:10","endLine":160,"groupId":"0","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"dataFrameRDDOperations","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/7b/50aad4ad498d25167253fef7d3912d5e36a65e.src","preCode":"  public void dataFrameRDDOperations() {\n    List<Person> personList = new ArrayList<Person>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n            new Function<Person, Row>() {\n              public Row call(Person person) throws Exception {\n                return RowFactory.create(person.getName(), person.getAge());\n              }\n            });\n\n    List<StructField> fields = new ArrayList<StructField>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    DataFrame df = sqlContext.applySchema(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    List<String> actual = sqlContext.sql(\"SELECT * FROM people\").toJavaRDD().map(new Function<Row, String>() {\n\n      public String call(Row row) {\n        return row.getString(0) + \"_\" + row.get(1).toString();\n      }\n    }).collect();\n\n    List<String> expected = new ArrayList<String>(2);\n    expected.add(\"Michael_29\");\n    expected.add(\"Yin_28\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaApplySchemaSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":122,"status":"M"}],"commitId":"22730ad54d681ad30e63fe910e8d89360853177d","commitMessage":"@@@[SPARK-10547] [TEST] Streamline / improve style of Java API tests\n\nFix a few Java API test style issues: unused generic types.  exceptions.  wrong assert argument order\n\nAuthor: Sean Owen <sowen@cloudera.com>\n\nCloses #8706 from srowen/SPARK-10547.\n","date":"2015-09-12 17:40:10","modifiedFileCount":"15","status":"M","submitter":"Sean Owen"},{"authorTime":"2016-01-05 10:02:38","codes":[{"authorDate":"2016-01-05 10:02:38","commitOrder":3,"curCode":"  public void applySchema() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n      new Function<Person, Row>() {\n        @Override\n        public Row call(Person person) throws Exception {\n          return RowFactory.create(person.getName(), person.getAge());\n        }\n      });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    DataFrame df = sqlContext.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    Row[] actual = sqlContext.sql(\"SELECT * FROM people\").collect();\n\n    List<Row> expected = new ArrayList<Row>(2);\n    expected.add(RowFactory.create(\"Michael\", 29));\n    expected.add(RowFactory.create(\"Yin\", 28));\n\n    Assert.assertEquals(expected, Arrays.asList(actual));\n  }\n","date":"2016-01-05 10:02:38","endLine":119,"groupId":"2660","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"applySchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/64/0efcc737eaa54c3323bdba1a6c304ff16233ff.src","preCode":"  public void applySchema() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n      new Function<Person, Row>() {\n        @Override\n        public Row call(Person person) throws Exception {\n          return RowFactory.create(person.getName(), person.getAge());\n        }\n      });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    DataFrame df = sqlContext.applySchema(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    Row[] actual = sqlContext.sql(\"SELECT * FROM people\").collect();\n\n    List<Row> expected = new ArrayList<Row>(2);\n    expected.add(RowFactory.create(\"Michael\", 29));\n    expected.add(RowFactory.create(\"Yin\", 28));\n\n    Assert.assertEquals(expected, Arrays.asList(actual));\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaApplySchemaSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"M"},{"authorDate":"2016-01-05 10:02:38","commitOrder":3,"curCode":"  public void dataFrameRDDOperations() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n        new Function<Person, Row>() {\n          @Override\n          public Row call(Person person) {\n            return RowFactory.create(person.getName(), person.getAge());\n          }\n        });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    DataFrame df = sqlContext.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    List<String> actual = sqlContext.sql(\"SELECT * FROM people\").toJavaRDD().map(new Function<Row, String>() {\n      @Override\n      public String call(Row row) {\n        return row.getString(0) + \"_\" + row.get(1);\n      }\n    }).collect();\n\n    List<String> expected = new ArrayList<>(2);\n    expected.add(\"Michael_29\");\n    expected.add(\"Yin_28\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","date":"2016-01-05 10:02:38","endLine":160,"groupId":"1639","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"dataFrameRDDOperations","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/64/0efcc737eaa54c3323bdba1a6c304ff16233ff.src","preCode":"  public void dataFrameRDDOperations() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n        new Function<Person, Row>() {\n          @Override\n          public Row call(Person person) {\n            return RowFactory.create(person.getName(), person.getAge());\n          }\n        });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    DataFrame df = sqlContext.applySchema(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    List<String> actual = sqlContext.sql(\"SELECT * FROM people\").toJavaRDD().map(new Function<Row, String>() {\n      @Override\n      public String call(Row row) {\n        return row.getString(0) + \"_\" + row.get(1);\n      }\n    }).collect();\n\n    List<String> expected = new ArrayList<>(2);\n    expected.add(\"Michael_29\");\n    expected.add(\"Yin_28\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaApplySchemaSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":122,"status":"M"}],"commitId":"77ab49b8575d2ebd678065fa70b0343d532ab9c2","commitMessage":"@@@[SPARK-12600][SQL] Remove deprecated methods in Spark SQL\n\nAuthor: Reynold Xin <rxin@databricks.com>\n\nCloses #10559 from rxin/remove-deprecated-sql.\n","date":"2016-01-05 10:02:38","modifiedFileCount":"1","status":"M","submitter":"Reynold Xin"},{"authorTime":"2016-01-05 10:02:38","codes":[{"authorDate":"2016-03-09 18:31:26","commitOrder":4,"curCode":"  public void applySchema() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n      new Function<Person, Row>() {\n        @Override\n        public Row call(Person person) throws Exception {\n          return RowFactory.create(person.getName(), person.getAge());\n        }\n      });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    DataFrame df = sqlContext.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    Row[] actual = sqlContext.sql(\"SELECT * FROM people\").collect();\n\n    List<Row> expected = new ArrayList<>(2);\n    expected.add(RowFactory.create(\"Michael\", 29));\n    expected.add(RowFactory.create(\"Yin\", 28));\n\n    Assert.assertEquals(expected, Arrays.asList(actual));\n  }\n","date":"2016-03-09 18:31:26","endLine":119,"groupId":"2660","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"applySchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/51/f987fda9de71693f8a44d30f1353118bf396de.src","preCode":"  public void applySchema() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n      new Function<Person, Row>() {\n        @Override\n        public Row call(Person person) throws Exception {\n          return RowFactory.create(person.getName(), person.getAge());\n        }\n      });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    DataFrame df = sqlContext.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    Row[] actual = sqlContext.sql(\"SELECT * FROM people\").collect();\n\n    List<Row> expected = new ArrayList<Row>(2);\n    expected.add(RowFactory.create(\"Michael\", 29));\n    expected.add(RowFactory.create(\"Yin\", 28));\n\n    Assert.assertEquals(expected, Arrays.asList(actual));\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaApplySchemaSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"M"},{"authorDate":"2016-01-05 10:02:38","commitOrder":4,"curCode":"  public void dataFrameRDDOperations() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n        new Function<Person, Row>() {\n          @Override\n          public Row call(Person person) {\n            return RowFactory.create(person.getName(), person.getAge());\n          }\n        });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    DataFrame df = sqlContext.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    List<String> actual = sqlContext.sql(\"SELECT * FROM people\").toJavaRDD().map(new Function<Row, String>() {\n      @Override\n      public String call(Row row) {\n        return row.getString(0) + \"_\" + row.get(1);\n      }\n    }).collect();\n\n    List<String> expected = new ArrayList<>(2);\n    expected.add(\"Michael_29\");\n    expected.add(\"Yin_28\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","date":"2016-01-05 10:02:38","endLine":160,"groupId":"1639","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"dataFrameRDDOperations","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/64/0efcc737eaa54c3323bdba1a6c304ff16233ff.src","preCode":"  public void dataFrameRDDOperations() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n        new Function<Person, Row>() {\n          @Override\n          public Row call(Person person) {\n            return RowFactory.create(person.getName(), person.getAge());\n          }\n        });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    DataFrame df = sqlContext.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    List<String> actual = sqlContext.sql(\"SELECT * FROM people\").toJavaRDD().map(new Function<Row, String>() {\n      @Override\n      public String call(Row row) {\n        return row.getString(0) + \"_\" + row.get(1);\n      }\n    }).collect();\n\n    List<String> expected = new ArrayList<>(2);\n    expected.add(\"Michael_29\");\n    expected.add(\"Yin_28\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaApplySchemaSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":122,"status":"N"}],"commitId":"c3689bc24e03a9471cd6e8169da61963c4528252","commitMessage":"@@@[SPARK-13702][CORE][SQL][MLLIB] Use diamond operator for generic instance creation in Java code.\n\n## What changes were proposed in this pull request?\n\nIn order to make `docs/examples` (and other related code) more simple/readable/user-friendly.  this PR replaces existing codes like the followings by using `diamond` operator.\n\n```\n-    final ArrayList<Product2<Object.  Object>> dataToWrite =\n-      new ArrayList<Product2<Object.  Object>>();\n+    final ArrayList<Product2<Object.  Object>> dataToWrite = new ArrayList<>();\n```\n\nJava 7 or higher supports **diamond** operator which replaces the type arguments required to invoke the constructor of a generic class with an empty set of type parameters (<>). Currently.  Spark Java code use mixed usage of this.\n\n## How was this patch tested?\n\nManual.\nPass the existing tests.\n\nAuthor: Dongjoon Hyun <dongjoon@apache.org>\n\nCloses #11541 from dongjoon-hyun/SPARK-13702.\n","date":"2016-03-09 18:31:26","modifiedFileCount":"57","status":"M","submitter":"Dongjoon Hyun"},{"authorTime":"2016-03-11 09:00:17","codes":[{"authorDate":"2016-03-11 09:00:17","commitOrder":5,"curCode":"  public void applySchema() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n      new Function<Person, Row>() {\n        @Override\n        public Row call(Person person) throws Exception {\n          return RowFactory.create(person.getName(), person.getAge());\n        }\n      });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    Dataset<Row> df = sqlContext.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    Row[] actual = sqlContext.sql(\"SELECT * FROM people\").collectRows();\n\n    List<Row> expected = new ArrayList<>(2);\n    expected.add(RowFactory.create(\"Michael\", 29));\n    expected.add(RowFactory.create(\"Yin\", 28));\n\n    Assert.assertEquals(expected, Arrays.asList(actual));\n  }\n","date":"2016-03-11 09:00:17","endLine":119,"groupId":"2618","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"applySchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/42/af813bc1cd3256f82864516c83acd7f74fd83e.src","preCode":"  public void applySchema() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n      new Function<Person, Row>() {\n        @Override\n        public Row call(Person person) throws Exception {\n          return RowFactory.create(person.getName(), person.getAge());\n        }\n      });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    DataFrame df = sqlContext.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    Row[] actual = sqlContext.sql(\"SELECT * FROM people\").collect();\n\n    List<Row> expected = new ArrayList<>(2);\n    expected.add(RowFactory.create(\"Michael\", 29));\n    expected.add(RowFactory.create(\"Yin\", 28));\n\n    Assert.assertEquals(expected, Arrays.asList(actual));\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaApplySchemaSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"M"},{"authorDate":"2016-03-11 09:00:17","commitOrder":5,"curCode":"  public void dataFrameRDDOperations() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n        new Function<Person, Row>() {\n          @Override\n          public Row call(Person person) {\n            return RowFactory.create(person.getName(), person.getAge());\n          }\n        });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    Dataset<Row> df = sqlContext.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    List<String> actual = sqlContext.sql(\"SELECT * FROM people\").toJavaRDD().map(new Function<Row, String>() {\n      @Override\n      public String call(Row row) {\n        return row.getString(0) + \"_\" + row.get(1);\n      }\n    }).collect();\n\n    List<String> expected = new ArrayList<>(2);\n    expected.add(\"Michael_29\");\n    expected.add(\"Yin_28\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","date":"2016-03-11 09:00:17","endLine":160,"groupId":"1639","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"dataFrameRDDOperations","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/42/af813bc1cd3256f82864516c83acd7f74fd83e.src","preCode":"  public void dataFrameRDDOperations() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n        new Function<Person, Row>() {\n          @Override\n          public Row call(Person person) {\n            return RowFactory.create(person.getName(), person.getAge());\n          }\n        });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    DataFrame df = sqlContext.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    List<String> actual = sqlContext.sql(\"SELECT * FROM people\").toJavaRDD().map(new Function<Row, String>() {\n      @Override\n      public String call(Row row) {\n        return row.getString(0) + \"_\" + row.get(1);\n      }\n    }).collect();\n\n    List<String> expected = new ArrayList<>(2);\n    expected.add(\"Michael_29\");\n    expected.add(\"Yin_28\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaApplySchemaSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":122,"status":"M"}],"commitId":"1d542785b9949e7f92025e6754973a779cc37c52","commitMessage":"@@@[SPARK-13244][SQL] Migrates DataFrame to Dataset\n\n## What changes were proposed in this pull request?\n\nThis PR unifies DataFrame and Dataset by migrating existing DataFrame operations to Dataset and make `DataFrame` a type alias of `Dataset[Row]`.\n\nMost Scala code changes are source compatible.  but Java API is broken as Java knows nothing about Scala type alias (mostly replacing `DataFrame` with `Dataset<Row>`).\n\nThere are several noticeable API changes related to those returning arrays:\n\n1.  `collect`/`take`\n\n    -   Old APIs in class `DataFrame`:\n\n        ```scala\n        def collect(): Array[Row]\n        def take(n: Int): Array[Row]\n        ```\n\n    -   New APIs in class `Dataset[T]`:\n\n        ```scala\n        def collect(): Array[T]\n        def take(n: Int): Array[T]\n\n        def collectRows(): Array[Row]\n        def takeRows(n: Int): Array[Row]\n        ```\n\n    Two specialized methods `collectRows` and `takeRows` are added because Java doesn't support returning generic arrays. Thus.  for example.  `DataFrame.collect(): Array[T]` actually returns `Object` instead of `Array<T>` from Java side.\n\n    Normally.  Java users may fall back to `collectAsList` and `takeAsList`.  The two new specialized versions are added to avoid performance regression in ML related code (but maybe I'm wrong and they are not necessary here).\n\n1.  `randomSplit`\n\n    -   Old APIs in class `DataFrame`:\n\n        ```scala\n        def randomSplit(weights: Array[Double].  seed: Long): Array[DataFrame]\n        def randomSplit(weights: Array[Double]): Array[DataFrame]\n        ```\n\n    -   New APIs in class `Dataset[T]`:\n\n        ```scala\n        def randomSplit(weights: Array[Double].  seed: Long): Array[Dataset[T]]\n        def randomSplit(weights: Array[Double]): Array[Dataset[T]]\n        ```\n\n    Similar problem as above.  but hasn't been addressed for Java API yet.  We can probably add `randomSplitAsList` to fix this one.\n\n1.  `groupBy`\n\n    Some original `DataFrame.groupBy` methods have conflicting signature with original `Dataset.groupBy` methods.  To distinguish these two.  typed `Dataset.groupBy` methods are renamed to `groupByKey`.\n\nOther noticeable changes:\n\n1.  Dataset always do eager analysis now\n\n    We used to support disabling DataFrame eager analysis to help reporting partially analyzed malformed logical plan on analysis failure.  However.  Dataset encoders requires eager analysi during Dataset construction.  To preserve the error reporting feature.  `AnalysisException` now takes an extra `Option[LogicalPlan]` argument to hold the partially analyzed plan.  so that we can check the plan tree when reporting test failures.  This plan is passed by `QueryExecution.assertAnalyzed`.\n\n## How was this patch tested?\n\nExisting tests do the work.\n\n## TODO\n\n- [ ] Fix all tests\n- [ ] Re-enable MiMA check\n- [ ] Update ScalaDoc (`since`.  `group`.  and example code)\n\nAuthor: Cheng Lian <lian@databricks.com>\nAuthor: Yin Huai <yhuai@databricks.com>\nAuthor: Wenchen Fan <wenchen@databricks.com>\nAuthor: Cheng Lian <liancheng@users.noreply.github.com>\n\nCloses #11443 from liancheng/ds-to-df.\n","date":"2016-03-11 09:00:17","modifiedFileCount":"87","status":"M","submitter":"Cheng Lian"},{"authorTime":"2016-03-11 09:00:17","codes":[{"authorDate":"2016-03-13 12:02:52","commitOrder":6,"curCode":"  public void applySchema() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n      new Function<Person, Row>() {\n        @Override\n        public Row call(Person person) throws Exception {\n          return RowFactory.create(person.getName(), person.getAge());\n        }\n      });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    Dataset<Row> df = sqlContext.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    List<Row> actual = sqlContext.sql(\"SELECT * FROM people\").collectAsList();\n\n    List<Row> expected = new ArrayList<>(2);\n    expected.add(RowFactory.create(\"Michael\", 29));\n    expected.add(RowFactory.create(\"Yin\", 28));\n\n    Assert.assertEquals(expected, actual);\n  }\n","date":"2016-03-13 12:02:52","endLine":119,"groupId":"2618","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"applySchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/ae/9c8cc1ba9ff574eae99d822b2881be0ac1e6f6.src","preCode":"  public void applySchema() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n      new Function<Person, Row>() {\n        @Override\n        public Row call(Person person) throws Exception {\n          return RowFactory.create(person.getName(), person.getAge());\n        }\n      });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    Dataset<Row> df = sqlContext.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    Row[] actual = sqlContext.sql(\"SELECT * FROM people\").collectRows();\n\n    List<Row> expected = new ArrayList<>(2);\n    expected.add(RowFactory.create(\"Michael\", 29));\n    expected.add(RowFactory.create(\"Yin\", 28));\n\n    Assert.assertEquals(expected, Arrays.asList(actual));\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaApplySchemaSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"M"},{"authorDate":"2016-03-11 09:00:17","commitOrder":6,"curCode":"  public void dataFrameRDDOperations() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n        new Function<Person, Row>() {\n          @Override\n          public Row call(Person person) {\n            return RowFactory.create(person.getName(), person.getAge());\n          }\n        });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    Dataset<Row> df = sqlContext.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    List<String> actual = sqlContext.sql(\"SELECT * FROM people\").toJavaRDD().map(new Function<Row, String>() {\n      @Override\n      public String call(Row row) {\n        return row.getString(0) + \"_\" + row.get(1);\n      }\n    }).collect();\n\n    List<String> expected = new ArrayList<>(2);\n    expected.add(\"Michael_29\");\n    expected.add(\"Yin_28\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","date":"2016-03-11 09:00:17","endLine":160,"groupId":"1639","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"dataFrameRDDOperations","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/42/af813bc1cd3256f82864516c83acd7f74fd83e.src","preCode":"  public void dataFrameRDDOperations() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n        new Function<Person, Row>() {\n          @Override\n          public Row call(Person person) {\n            return RowFactory.create(person.getName(), person.getAge());\n          }\n        });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    Dataset<Row> df = sqlContext.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    List<String> actual = sqlContext.sql(\"SELECT * FROM people\").toJavaRDD().map(new Function<Row, String>() {\n      @Override\n      public String call(Row row) {\n        return row.getString(0) + \"_\" + row.get(1);\n      }\n    }).collect();\n\n    List<String> expected = new ArrayList<>(2);\n    expected.add(\"Michael_29\");\n    expected.add(\"Yin_28\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaApplySchemaSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":122,"status":"N"}],"commitId":"c079420d7c55d8972db716a2695a5ddd606d11cd","commitMessage":"@@@[SPARK-13841][SQL] Removes Dataset.collectRows()/takeRows()\n\n## What changes were proposed in this pull request?\n\nThis PR removes two methods.  `collectRows()` and `takeRows()`.  from `Dataset[T]`. These methods were added in PR #11443.  and were later considered not useful.\n\n## How was this patch tested?\n\nExisting tests should do the work.\n\nAuthor: Cheng Lian <lian@databricks.com>\n\nCloses #11678 from liancheng/remove-collect-rows-and-take-rows.\n","date":"2016-03-13 12:02:52","modifiedFileCount":"16","status":"M","submitter":"Cheng Lian"},{"authorTime":"2016-05-11 02:17:47","codes":[{"authorDate":"2016-05-11 02:17:47","commitOrder":7,"curCode":"  public void applySchema() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = jsc.parallelize(personList).map(\n      new Function<Person, Row>() {\n        @Override\n        public Row call(Person person) throws Exception {\n          return RowFactory.create(person.getName(), person.getAge());\n        }\n      });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    Dataset<Row> df = spark.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    List<Row> actual = spark.sql(\"SELECT * FROM people\").collectAsList();\n\n    List<Row> expected = new ArrayList<>(2);\n    expected.add(RowFactory.create(\"Michael\", 29));\n    expected.add(RowFactory.create(\"Yin\", 28));\n\n    Assert.assertEquals(expected, actual);\n  }\n","date":"2016-05-11 02:17:47","endLine":119,"groupId":"3419","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"applySchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/f2/ae40e644245db9b6c49c68c4f9b77a37f0bfb7.src","preCode":"  public void applySchema() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n      new Function<Person, Row>() {\n        @Override\n        public Row call(Person person) throws Exception {\n          return RowFactory.create(person.getName(), person.getAge());\n        }\n      });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    Dataset<Row> df = sqlContext.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    List<Row> actual = sqlContext.sql(\"SELECT * FROM people\").collectAsList();\n\n    List<Row> expected = new ArrayList<>(2);\n    expected.add(RowFactory.create(\"Michael\", 29));\n    expected.add(RowFactory.create(\"Yin\", 28));\n\n    Assert.assertEquals(expected, actual);\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaApplySchemaSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"M"},{"authorDate":"2016-05-11 02:17:47","commitOrder":7,"curCode":"  public void dataFrameRDDOperations() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = jsc.parallelize(personList).map(\n        new Function<Person, Row>() {\n          @Override\n          public Row call(Person person) {\n            return RowFactory.create(person.getName(), person.getAge());\n          }\n        });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    Dataset<Row> df = spark.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    List<String> actual = spark.sql(\"SELECT * FROM people\").toJavaRDD()\n      .map(new Function<Row, String>() {\n        @Override\n        public String call(Row row) {\n          return row.getString(0) + \"_\" + row.get(1);\n        }\n      }).collect();\n\n    List<String> expected = new ArrayList<>(2);\n    expected.add(\"Michael_29\");\n    expected.add(\"Yin_28\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","date":"2016-05-11 02:17:47","endLine":161,"groupId":"3419","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"dataFrameRDDOperations","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/f2/ae40e644245db9b6c49c68c4f9b77a37f0bfb7.src","preCode":"  public void dataFrameRDDOperations() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = javaCtx.parallelize(personList).map(\n        new Function<Person, Row>() {\n          @Override\n          public Row call(Person person) {\n            return RowFactory.create(person.getName(), person.getAge());\n          }\n        });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    Dataset<Row> df = sqlContext.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    List<String> actual = sqlContext.sql(\"SELECT * FROM people\").toJavaRDD()\n      .map(new Function<Row, String>() {\n        @Override\n        public String call(Row row) {\n          return row.getString(0) + \"_\" + row.get(1);\n        }\n      }).collect();\n\n    List<String> expected = new ArrayList<>(2);\n    expected.add(\"Michael_29\");\n    expected.add(\"Yin_28\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaApplySchemaSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":122,"status":"M"}],"commitId":"ed0b4070fb50054b1ecf66ff6c32458a4967dfd3","commitMessage":"@@@[SPARK-15037][SQL][MLLIB] Use SparkSession instead of SQLContext in Scala/Java TestSuites\n\n## What changes were proposed in this pull request?\nUse SparkSession instead of SQLContext in Scala/Java TestSuites\nas this PR already very big working Python TestSuites in a diff PR.\n\n## How was this patch tested?\nExisting tests\n\nAuthor: Sandeep Singh <sandeep@techaddict.me>\n\nCloses #12907 from techaddict/SPARK-15037.\n","date":"2016-05-11 02:17:47","modifiedFileCount":"63","status":"M","submitter":"Sandeep Singh"},{"authorTime":"2016-05-18 09:01:59","codes":[{"authorDate":"2016-05-18 09:01:59","commitOrder":8,"curCode":"  public void applySchema() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = jsc.parallelize(personList).map(\n      new Function<Person, Row>() {\n        @Override\n        public Row call(Person person) throws Exception {\n          return RowFactory.create(person.getName(), person.getAge());\n        }\n      });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    Dataset<Row> df = spark.createDataFrame(rowRDD, schema);\n    df.createOrReplaceTempView(\"people\");\n    List<Row> actual = spark.sql(\"SELECT * FROM people\").collectAsList();\n\n    List<Row> expected = new ArrayList<>(2);\n    expected.add(RowFactory.create(\"Michael\", 29));\n    expected.add(RowFactory.create(\"Yin\", 28));\n\n    Assert.assertEquals(expected, actual);\n  }\n","date":"2016-05-18 09:01:59","endLine":119,"groupId":"3419","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"applySchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/57/3d0e35943634c764a56a19d4fae4f943afa26e.src","preCode":"  public void applySchema() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = jsc.parallelize(personList).map(\n      new Function<Person, Row>() {\n        @Override\n        public Row call(Person person) throws Exception {\n          return RowFactory.create(person.getName(), person.getAge());\n        }\n      });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    Dataset<Row> df = spark.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    List<Row> actual = spark.sql(\"SELECT * FROM people\").collectAsList();\n\n    List<Row> expected = new ArrayList<>(2);\n    expected.add(RowFactory.create(\"Michael\", 29));\n    expected.add(RowFactory.create(\"Yin\", 28));\n\n    Assert.assertEquals(expected, actual);\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaApplySchemaSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"M"},{"authorDate":"2016-05-18 09:01:59","commitOrder":8,"curCode":"  public void dataFrameRDDOperations() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = jsc.parallelize(personList).map(\n        new Function<Person, Row>() {\n          @Override\n          public Row call(Person person) {\n            return RowFactory.create(person.getName(), person.getAge());\n          }\n        });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    Dataset<Row> df = spark.createDataFrame(rowRDD, schema);\n    df.createOrReplaceTempView(\"people\");\n    List<String> actual = spark.sql(\"SELECT * FROM people\").toJavaRDD()\n      .map(new Function<Row, String>() {\n        @Override\n        public String call(Row row) {\n          return row.getString(0) + \"_\" + row.get(1);\n        }\n      }).collect();\n\n    List<String> expected = new ArrayList<>(2);\n    expected.add(\"Michael_29\");\n    expected.add(\"Yin_28\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","date":"2016-05-18 09:01:59","endLine":161,"groupId":"3419","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"dataFrameRDDOperations","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/57/3d0e35943634c764a56a19d4fae4f943afa26e.src","preCode":"  public void dataFrameRDDOperations() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = jsc.parallelize(personList).map(\n        new Function<Person, Row>() {\n          @Override\n          public Row call(Person person) {\n            return RowFactory.create(person.getName(), person.getAge());\n          }\n        });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    Dataset<Row> df = spark.createDataFrame(rowRDD, schema);\n    df.registerTempTable(\"people\");\n    List<String> actual = spark.sql(\"SELECT * FROM people\").toJavaRDD()\n      .map(new Function<Row, String>() {\n        @Override\n        public String call(Row row) {\n          return row.getString(0) + \"_\" + row.get(1);\n        }\n      }).collect();\n\n    List<String> expected = new ArrayList<>(2);\n    expected.add(\"Michael_29\");\n    expected.add(\"Yin_28\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaApplySchemaSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":122,"status":"M"}],"commitId":"25b315e6cad7c27b62dcaa2c194293c1115fdfb3","commitMessage":"@@@[SPARK-15171][SQL] Remove the references to deprecated method dataset.registerTempTable\n\n## What changes were proposed in this pull request?\n\nUpdate the unit test code.  examples.  and documents to remove calls to deprecated method `dataset.registerTempTable`.\n\n## How was this patch tested?\n\nThis PR only changes the unit test code.  examples.  and comments. It should be safe.\nThis is a follow up of PR https://github.com/apache/spark/pull/12945 which was merged.\n\nAuthor: Sean Zhong <seanzhong@databricks.com>\n\nCloses #13098 from clockfly/spark-15171-remove-deprecation.\n","date":"2016-05-18 09:01:59","modifiedFileCount":"9","status":"M","submitter":"Sean Zhong"},{"authorTime":"2017-02-20 01:42:50","codes":[{"authorDate":"2017-02-20 01:42:50","commitOrder":9,"curCode":"  public void applySchema() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = jsc.parallelize(personList).map(\n        person -> RowFactory.create(person.getName(), person.getAge()));\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    Dataset<Row> df = spark.createDataFrame(rowRDD, schema);\n    df.createOrReplaceTempView(\"people\");\n    List<Row> actual = spark.sql(\"SELECT * FROM people\").collectAsList();\n\n    List<Row> expected = new ArrayList<>(2);\n    expected.add(RowFactory.create(\"Michael\", 29));\n    expected.add(RowFactory.create(\"Yin\", 28));\n\n    Assert.assertEquals(expected, actual);\n  }\n","date":"2017-02-20 01:42:50","endLine":113,"groupId":"10385","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"applySchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/bf/8ff61eae39e23de0a25d510e69e96c6a3c287d.src","preCode":"  public void applySchema() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = jsc.parallelize(personList).map(\n      new Function<Person, Row>() {\n        @Override\n        public Row call(Person person) throws Exception {\n          return RowFactory.create(person.getName(), person.getAge());\n        }\n      });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"name\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    Dataset<Row> df = spark.createDataFrame(rowRDD, schema);\n    df.createOrReplaceTempView(\"people\");\n    List<Row> actual = spark.sql(\"SELECT * FROM people\").collectAsList();\n\n    List<Row> expected = new ArrayList<>(2);\n    expected.add(RowFactory.create(\"Michael\", 29));\n    expected.add(RowFactory.create(\"Yin\", 28));\n\n    Assert.assertEquals(expected, actual);\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaApplySchemaSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":85,"status":"M"},{"authorDate":"2017-02-20 01:42:50","commitOrder":9,"curCode":"  public void dataFrameRDDOperations() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = jsc.parallelize(personList).map(\n        person -> RowFactory.create(person.getName(), person.getAge()));\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    Dataset<Row> df = spark.createDataFrame(rowRDD, schema);\n    df.createOrReplaceTempView(\"people\");\n    List<String> actual = spark.sql(\"SELECT * FROM people\").toJavaRDD()\n      .map(row -> row.getString(0) + \"_\" + row.get(1)).collect();\n\n    List<String> expected = new ArrayList<>(2);\n    expected.add(\"Michael_29\");\n    expected.add(\"Yin_28\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","date":"2017-02-20 01:42:50","endLine":145,"groupId":"10385","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"dataFrameRDDOperations","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/bf/8ff61eae39e23de0a25d510e69e96c6a3c287d.src","preCode":"  public void dataFrameRDDOperations() {\n    List<Person> personList = new ArrayList<>(2);\n    Person person1 = new Person();\n    person1.setName(\"Michael\");\n    person1.setAge(29);\n    personList.add(person1);\n    Person person2 = new Person();\n    person2.setName(\"Yin\");\n    person2.setAge(28);\n    personList.add(person2);\n\n    JavaRDD<Row> rowRDD = jsc.parallelize(personList).map(\n        new Function<Person, Row>() {\n          @Override\n          public Row call(Person person) {\n            return RowFactory.create(person.getName(), person.getAge());\n          }\n        });\n\n    List<StructField> fields = new ArrayList<>(2);\n    fields.add(DataTypes.createStructField(\"\", DataTypes.StringType, false));\n    fields.add(DataTypes.createStructField(\"age\", DataTypes.IntegerType, false));\n    StructType schema = DataTypes.createStructType(fields);\n\n    Dataset<Row> df = spark.createDataFrame(rowRDD, schema);\n    df.createOrReplaceTempView(\"people\");\n    List<String> actual = spark.sql(\"SELECT * FROM people\").toJavaRDD()\n      .map(new Function<Row, String>() {\n        @Override\n        public String call(Row row) {\n          return row.getString(0) + \"_\" + row.get(1);\n        }\n      }).collect();\n\n    List<String> expected = new ArrayList<>(2);\n    expected.add(\"Michael_29\");\n    expected.add(\"Yin_28\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","realPath":"sql/core/src/test/java/test/org/apache/spark/sql/JavaApplySchemaSuite.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":116,"status":"M"}],"commitId":"1487c9af20a333ead55955acf4c0aa323bea0d07","commitMessage":"@@@[SPARK-19534][TESTS] Convert Java tests to use lambdas.  Java 8 features\n\n## What changes were proposed in this pull request?\n\nConvert tests to use Java 8 lambdas.  and modest related fixes to surrounding code.\n\n## How was this patch tested?\n\nJenkins tests\n\nAuthor: Sean Owen <sowen@cloudera.com>\n\nCloses #16964 from srowen/SPARK-19534.\n","date":"2017-02-20 01:42:50","modifiedFileCount":"45","status":"M","submitter":"Sean Owen"}]
