[{"authorTime":"2016-03-11 09:00:17","codes":[{"authorDate":"2016-03-11 15:31:35","commitOrder":4,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaMaxAbsScalerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    DataFrame dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n    MaxAbsScaler scaler = new MaxAbsScaler()\n        .setInputCol(\"features\")\n        .setOutputCol(\"scaledFeatures\");\n\n    \r\n    MaxAbsScalerModel scalerModel = scaler.fit(dataFrame);\n\n    \r\n    DataFrame scaledData = scalerModel.transform(dataFrame);\n    scaledData.show();\n    \r\n    jsc.stop();\n  }\n","date":"2016-03-11 15:31:35","endLine":50,"groupId":"200","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/b1/e3b9137f213e72a64a0f9cd9e789c0b1428929.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaMaxAbsScalerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    DataFrame dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n    MaxAbsScaler scaler = new MaxAbsScaler()\n        .setInputCol(\"features\")\n        .setOutputCol(\"scaledFeatures\");\n\n    \r\n    MaxAbsScalerModel scalerModel = scaler.fit(dataFrame);\n\n    \r\n    DataFrame scaledData = scalerModel.transform(dataFrame);\n    scaledData.show();\n    \r\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":31,"status":"B"},{"authorDate":"2016-03-11 09:00:17","commitOrder":4,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaNormalizerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    Dataset<Row> dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    Normalizer normalizer = new Normalizer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"normFeatures\")\n      .setP(1.0);\n\n    Dataset<Row> l1NormData = normalizer.transform(dataFrame);\n    l1NormData.show();\n\n    \r\n    Dataset<Row> lInfNormData =\n      normalizer.transform(dataFrame, normalizer.p().w(Double.POSITIVE_INFINITY));\n    lInfNormData.show();\n    \r\n    jsc.stop();\n  }\n","date":"2016-03-11 09:00:17","endLine":54,"groupId":"3055","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/31/cd752136689756b8602d26328601d4d664b5d2.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaNormalizerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    Dataset<Row> dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    Normalizer normalizer = new Normalizer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"normFeatures\")\n      .setP(1.0);\n\n    Dataset<Row> l1NormData = normalizer.transform(dataFrame);\n    l1NormData.show();\n\n    \r\n    Dataset<Row> lInfNormData =\n      normalizer.transform(dataFrame, normalizer.p().w(Double.POSITIVE_INFINITY));\n    lInfNormData.show();\n    \r\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":31,"status":"NB"}],"commitId":"0b713e0455d01999d5a027ddc2ea8527eb085b34","commitMessage":"@@@[SPARK-13512][ML] add example and doc for MaxAbsScaler\n\n## What changes were proposed in this pull request?\n\njira: https://issues.apache.org/jira/browse/SPARK-13512\nAdd example and doc for ml.feature.MaxAbsScaler.\n\n## How was this patch tested?\n unit tests\n\nAuthor: Yuhao Yang <hhbyyh@gmail.com>\n\nCloses #11392 from hhbyyh/maxabsdoc.\n","date":"2016-03-11 15:31:35","modifiedFileCount":"0","status":"M","submitter":"Yuhao Yang"},{"authorTime":"2016-03-11 09:00:17","codes":[{"authorDate":"2016-03-11 16:20:39","commitOrder":5,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaMaxAbsScalerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    Dataset<Row> dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n    MaxAbsScaler scaler = new MaxAbsScaler()\n        .setInputCol(\"features\")\n        .setOutputCol(\"scaledFeatures\");\n\n    \r\n    MaxAbsScalerModel scalerModel = scaler.fit(dataFrame);\n\n    \r\n    Dataset<Row> scaledData = scalerModel.transform(dataFrame);\n    scaledData.show();\n    \r\n    jsc.stop();\n  }\n","date":"2016-03-11 16:20:39","endLine":51,"groupId":"3055","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/a2/a072b253f39db66fceb288b5bebf7fbd5aef92.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaMaxAbsScalerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    DataFrame dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n    MaxAbsScaler scaler = new MaxAbsScaler()\n        .setInputCol(\"features\")\n        .setOutputCol(\"scaledFeatures\");\n\n    \r\n    MaxAbsScalerModel scalerModel = scaler.fit(dataFrame);\n\n    \r\n    DataFrame scaledData = scalerModel.transform(dataFrame);\n    scaledData.show();\n    \r\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":32,"status":"M"},{"authorDate":"2016-03-11 09:00:17","commitOrder":5,"curCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaNormalizerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    Dataset<Row> dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    Normalizer normalizer = new Normalizer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"normFeatures\")\n      .setP(1.0);\n\n    Dataset<Row> l1NormData = normalizer.transform(dataFrame);\n    l1NormData.show();\n\n    \r\n    Dataset<Row> lInfNormData =\n      normalizer.transform(dataFrame, normalizer.p().w(Double.POSITIVE_INFINITY));\n    lInfNormData.show();\n    \r\n    jsc.stop();\n  }\n","date":"2016-03-11 09:00:17","endLine":54,"groupId":"3055","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/31/cd752136689756b8602d26328601d4d664b5d2.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaNormalizerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    Dataset<Row> dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    Normalizer normalizer = new Normalizer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"normFeatures\")\n      .setP(1.0);\n\n    Dataset<Row> l1NormData = normalizer.transform(dataFrame);\n    l1NormData.show();\n\n    \r\n    Dataset<Row> lInfNormData =\n      normalizer.transform(dataFrame, normalizer.p().w(Double.POSITIVE_INFINITY));\n    lInfNormData.show();\n    \r\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":31,"status":"N"}],"commitId":"8fff0f92a4aca90b62c6e272eabcbb0257ba38d5","commitMessage":"@@@[HOT-FIX][SQL][ML] Fix compile error from use of DataFrame in Java MaxAbsScaler example\n\n## What changes were proposed in this pull request?\n\nFix build failure introduced in #11392 (change `DataFrame` -> `Dataset<Row>`).\n\n## How was this patch tested?\n\nExisting build/unit tests\n\nAuthor: Nick Pentreath <nick.pentreath@gmail.com>\n\nCloses #11653 from MLnick/java-maxabs-example-fix.\n","date":"2016-03-11 16:20:39","modifiedFileCount":"1","status":"M","submitter":"Nick Pentreath"},{"authorTime":"2016-05-05 05:31:36","codes":[{"authorDate":"2016-05-05 05:31:36","commitOrder":6,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession.builder().appName(\"JavaMaxAbsScalerExample\").getOrCreate();\n\n    \r\n    Dataset<Row> dataFrame = spark.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n    MaxAbsScaler scaler = new MaxAbsScaler()\n        .setInputCol(\"features\")\n        .setOutputCol(\"scaledFeatures\");\n\n    \r\n    MaxAbsScalerModel scalerModel = scaler.fit(dataFrame);\n\n    \r\n    Dataset<Row> scaledData = scalerModel.transform(dataFrame);\n    scaledData.show();\n    \r\n    spark.stop();\n  }\n","date":"2016-05-05 05:31:36","endLine":47,"groupId":"344","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/80/cdd364b93765143b3a1437f26eb6b7f272b45b.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaMaxAbsScalerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    Dataset<Row> dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n    MaxAbsScaler scaler = new MaxAbsScaler()\n        .setInputCol(\"features\")\n        .setOutputCol(\"scaledFeatures\");\n\n    \r\n    MaxAbsScalerModel scalerModel = scaler.fit(dataFrame);\n\n    \r\n    Dataset<Row> scaledData = scalerModel.transform(dataFrame);\n    scaledData.show();\n    \r\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":30,"status":"M"},{"authorDate":"2016-05-05 05:31:36","commitOrder":6,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession.builder().appName(\"JavaNormalizerExample\").getOrCreate();\n\n    \r\n    Dataset<Row> dataFrame =\n      spark.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    Normalizer normalizer = new Normalizer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"normFeatures\")\n      .setP(1.0);\n\n    Dataset<Row> l1NormData = normalizer.transform(dataFrame);\n    l1NormData.show();\n\n    \r\n    Dataset<Row> lInfNormData =\n      normalizer.transform(dataFrame, normalizer.p().w(Double.POSITIVE_INFINITY));\n    lInfNormData.show();\n    \r\n    spark.stop();\n  }\n","date":"2016-05-05 05:31:36","endLine":51,"groupId":"3055","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/4b/3a718ea92c841df725281fede2a09bab5a8b12.src","preCode":"  public static void main(String[] args) {\n    SparkConf conf = new SparkConf().setAppName(\"JavaNormalizerExample\");\n    JavaSparkContext jsc = new JavaSparkContext(conf);\n    SQLContext jsql = new SQLContext(jsc);\n\n    \r\n    Dataset<Row> dataFrame = jsql.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    Normalizer normalizer = new Normalizer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"normFeatures\")\n      .setP(1.0);\n\n    Dataset<Row> l1NormData = normalizer.transform(dataFrame);\n    l1NormData.show();\n\n    \r\n    Dataset<Row> lInfNormData =\n      normalizer.transform(dataFrame, normalizer.p().w(Double.POSITIVE_INFINITY));\n    lInfNormData.show();\n    \r\n    jsc.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":29,"status":"M"}],"commitId":"cdce4e62a5674e2034e5d395578b1a60e3d8c435","commitMessage":"@@@[SPARK-15031][EXAMPLE] Use SparkSession in Scala/Python/Java example.\n\n## What changes were proposed in this pull request?\n\nThis PR aims to update Scala/Python/Java examples by replacing `SQLContext` with newly added `SparkSession`.\n\n- Use **SparkSession Builder Pattern** in 154(Scala 55.  Java 52.  Python 47) files.\n- Add `getConf` in Python SparkContext class: `python/pyspark/context.py`\n- Replace **SQLContext Singleton Pattern** with **SparkSession Singleton Pattern**:\n  - `SqlNetworkWordCount.scala`\n  - `JavaSqlNetworkWordCount.java`\n  - `sql_network_wordcount.py`\n\nNow.  `SQLContexts` are used only in R examples and the following two Python examples. The python examples are untouched in this PR since it already fails some unknown issue.\n- `simple_params_example.py`\n- `aft_survival_regression.py`\n\n## How was this patch tested?\n\nManual.\n\nAuthor: Dongjoon Hyun <dongjoon@apache.org>\n\nCloses #12809 from dongjoon-hyun/SPARK-15031.\n","date":"2016-05-05 05:31:36","modifiedFileCount":"52","status":"M","submitter":"Dongjoon Hyun"},{"authorTime":"2016-08-06 03:57:46","codes":[{"authorDate":"2016-08-06 03:57:46","commitOrder":7,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaMaxAbsScalerExample\")\n      .getOrCreate();\n\n    \r\n    List<Row> data = Arrays.asList(\n        RowFactory.create(0, Vectors.dense(1.0, 0.1, -8.0)),\n        RowFactory.create(1, Vectors.dense(2.0, 1.0, -4.0)),\n        RowFactory.create(2, Vectors.dense(4.0, 10.0, 8.0))\n    );\n    StructType schema = new StructType(new StructField[]{\n        new StructField(\"id\", DataTypes.IntegerType, false, Metadata.empty()),\n        new StructField(\"features\", new VectorUDT(), false, Metadata.empty())\n    });\n    Dataset<Row> dataFrame = spark.createDataFrame(data, schema);\n\n    MaxAbsScaler scaler = new MaxAbsScaler()\n      .setInputCol(\"features\")\n      .setOutputCol(\"scaledFeatures\");\n\n    \r\n    MaxAbsScalerModel scalerModel = scaler.fit(dataFrame);\n\n    \r\n    Dataset<Row> scaledData = scalerModel.transform(dataFrame);\n    scaledData.select(\"features\", \"scaledFeatures\").show();\n    \r\n\n    spark.stop();\n  }\n","date":"2016-08-06 03:57:46","endLine":71,"groupId":"10505","id":7,"instanceNumber":1,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/9f/1ce463cf309e13d72a5d4df3d017bcef11dbea.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaMaxAbsScalerExample\")\n      .getOrCreate();\n\n    \r\n    Dataset<Row> dataFrame = spark\n      .read()\n      .format(\"libsvm\")\n      .load(\"data/mllib/sample_libsvm_data.txt\");\n    MaxAbsScaler scaler = new MaxAbsScaler()\n      .setInputCol(\"features\")\n      .setOutputCol(\"scaledFeatures\");\n\n    \r\n    MaxAbsScalerModel scalerModel = scaler.fit(dataFrame);\n\n    \r\n    Dataset<Row> scaledData = scalerModel.transform(dataFrame);\n    scaledData.show();\n    \r\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"M"},{"authorDate":"2016-08-06 03:57:46","commitOrder":7,"curCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaNormalizerExample\")\n      .getOrCreate();\n\n    \r\n    List<Row> data = Arrays.asList(\n        RowFactory.create(0, Vectors.dense(1.0, 0.1, -8.0)),\n        RowFactory.create(1, Vectors.dense(2.0, 1.0, -4.0)),\n        RowFactory.create(2, Vectors.dense(4.0, 10.0, 8.0))\n    );\n    StructType schema = new StructType(new StructField[]{\n        new StructField(\"id\", DataTypes.IntegerType, false, Metadata.empty()),\n        new StructField(\"features\", new VectorUDT(), false, Metadata.empty())\n    });\n    Dataset<Row> dataFrame = spark.createDataFrame(data, schema);\n\n    \r\n    Normalizer normalizer = new Normalizer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"normFeatures\")\n      .setP(1.0);\n\n    Dataset<Row> l1NormData = normalizer.transform(dataFrame);\n    l1NormData.show();\n\n    \r\n    Dataset<Row> lInfNormData =\n      normalizer.transform(dataFrame, normalizer.p().w(Double.POSITIVE_INFINITY));\n    lInfNormData.show();\n    \r\n\n    spark.stop();\n  }\n","date":"2016-08-06 03:57:46","endLine":73,"groupId":"10505","id":8,"instanceNumber":2,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-spark-10-0.7/blobInfo/CC_OUT/blobs/f8/78c420d82377ce1a4689921c6c4d63437e16e8.src","preCode":"  public static void main(String[] args) {\n    SparkSession spark = SparkSession\n      .builder()\n      .appName(\"JavaNormalizerExample\")\n      .getOrCreate();\n\n    \r\n    Dataset<Row> dataFrame =\n      spark.read().format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\");\n\n    \r\n    Normalizer normalizer = new Normalizer()\n      .setInputCol(\"features\")\n      .setOutputCol(\"normFeatures\")\n      .setP(1.0);\n\n    Dataset<Row> l1NormData = normalizer.transform(dataFrame);\n    l1NormData.show();\n\n    \r\n    Dataset<Row> lInfNormData =\n      normalizer.transform(dataFrame, normalizer.p().w(Double.POSITIVE_INFINITY));\n    lInfNormData.show();\n    \r\n    spark.stop();\n  }\n","realPath":"examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java","repoName":"spark","snippetEndLine":0,"snippetStartLine":0,"startLine":39,"status":"M"}],"commitId":"180fd3e0a3426db200c97170926afb60751dfd0e","commitMessage":"@@@[SPARK-16421][EXAMPLES][ML] Improve ML Example Outputs\n\n## What changes were proposed in this pull request?\nImprove example outputs to better reflect the functionality that is being presented.  This mostly consisted of modifying what was printed at the end of the example.  such as calling show() with truncate=False.  but sometimes required minor tweaks in the example data to get relevant output.  Explicitly set parameters when they are used as part of the example.  Fixed Java examples that failed to run because of using old-style MLlib Vectors or problem with schema.  Synced examples between different APIs.\n\n## How was this patch tested?\nRan each example for Scala.  Python.  and Java and made sure output was legible on a terminal of width 100.\n\nAuthor: Bryan Cutler <cutlerb@gmail.com>\n\nCloses #14308 from BryanCutler/ml-examples-improve-output-SPARK-16260.\n","date":"2016-08-06 03:57:46","modifiedFileCount":"27","status":"M","submitter":"Bryan Cutler"}]
