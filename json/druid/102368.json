[{"authorTime":"2018-10-29 20:02:43","codes":[{"authorDate":"2018-12-22 03:49:24","commitOrder":5,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        false,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        },\n        new Counters()\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        MoreExecutors.sameThreadExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2018-12-22 03:49:24","endLine":2785,"groupId":"5738","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ce/f97955db2e8ceaae814ce65540385404536749.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        false,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        },\n        new Counters()\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        MoreExecutors.sameThreadExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2635,"status":"B"},{"authorDate":"2018-10-29 20:02:43","commitOrder":5,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        false,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        },\n        new Counters()\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        MoreExecutors.sameThreadExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2018-10-29 20:02:43","endLine":2269,"groupId":"5738","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/6f/dfcd8b935473cdb9fc4afccdd2d103e2a358f3.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        false,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        },\n        new Counters()\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        MoreExecutors.sameThreadExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2124,"status":"NB"}],"commitId":"7c7997e8a1183a7bffad731ca94e8b4c381e8665","commitMessage":"@@@Add Kinesis Indexing Service to core Druid (#6431)\n\n* created seekablestream classes\n\n* created seekablestreamsupervisor class\n\n* first attempt to integrate kafa indexing service to use SeekableStream\n\n* seekablestream bug fixes\n\n* kafkarecordsupplier\n\n* integrated kafka indexing service with seekablestream\n\n* implemented resume/suspend and refactored some package names\n\n* moved kinesis indexing service into core druid extensions\n\n* merged some changes from kafka supervisor race condition\n\n* integrated kinesis-indexing-service with seekablestream\n\n* unite tests for kinesis-indexing-service\n\n* various bug fixes for kinesis-indexing-service\n\n* refactored kinesisindexingtask\n\n* finished up more kinesis unit tests\n\n* more bug fixes for kinesis-indexing-service\n\n* finsihed refactoring kinesis unit tests\n\n* removed KinesisParititons and KafkaPartitions to use SeekableStreamPartitions\n\n* kinesis-indexing-service code cleanup and docs\n\n* merge #6291\n\nmerge #6337\n\nmerge #6383\n\n* added more docs and reordered methods\n\n* fixd kinesis tests after merging master and added docs in seekablestream\n\n* fix various things from pr comment\n\n* improve recordsupplier and add unit tests\n\n* migrated to aws-java-sdk-kinesis\n\n* merge changes from master\n\n* fix pom files and forbiddenapi checks\n\n* checkpoint JavaType bug fix\n\n* fix pom and stuff\n\n* disable checkpointing in kinesis\n\n* fix kinesis sequence number null in closed shard\n\n* merge changes from master\n\n* fixes for kinesis tasks\n\n* capitalized <partitionType.  sequenceType>\n\n* removed abstract class loggers\n\n* conform to guava api restrictions\n\n* add docker for travis other modules test\n\n* address comments\n\n* improve RecordSupplier to supply records in batch\n\n* fix strict compile issue\n\n* add test scope for localstack dependency\n\n* kinesis indexing task refactoring\n\n* comments\n\n* github comments\n\n* minor fix\n\n* removed unneeded readme\n\n* fix deserialization bug\n\n* fix various bugs\n\n* KinesisRecordSupplier unable to catch up to earliest position in stream bug fix\n\n* minor changes to kinesis\n\n* implement deaggregate for kinesis\n\n* Merge remote-tracking branch 'upstream/master' into seekablestream\n\n* fix kinesis offset discrepancy with kafka\n\n* kinesis record supplier disable getPosition\n\n* pr comments\n\n* mock for kinesis tests and remove docker dependency for unit tests\n\n* PR comments\n\n* avg lag in kafkasupervisor #6587\n\n* refacotred SequenceMetadata in taskRunners\n\n* small fix\n\n* more small fix\n\n* recordsupplier resource leak\n\n* revert .travis.yml formatting\n\n* fix style\n\n* kinesis docs\n\n* doc part2\n\n* more docs\n\n* comments\n\n* comments*2\n\n* revert string replace changes\n\n* comments\n\n* teamcity\n\n* comments part 1\n\n* comments part 2\n\n* comments part 3\n\n* merge #6754\n\n* fix injection binding\n\n* comments\n\n* KinesisRegion refactor\n\n* comments part idk lol\n\n* can't think of a commit msg anymore\n\n* remove possiblyResetDataSourceMetadata() for IncrementalPublishingTaskRunner\n\n* commmmmmmmmmments\n\n* extra error handling in KinesisRecordSupplier getRecords\n\n* comments\n\n* quickfix\n\n* typo\n\n* oof\n","date":"2018-12-22 03:49:24","modifiedFileCount":"22","status":"M","submitter":"Joshua Sun"},{"authorTime":"2019-01-12 02:42:19","codes":[{"authorDate":"2019-01-12 02:42:19","commitOrder":6,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        false,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        },\n        new Counters()\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-01-12 02:42:19","endLine":2785,"groupId":"5738","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/31/bedd9ae892a9fbd3482e5df4cd33aaa7feb135.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        false,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        },\n        new Counters()\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        MoreExecutors.sameThreadExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2635,"status":"M"},{"authorDate":"2019-01-12 02:42:19","commitOrder":6,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        false,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        },\n        new Counters()\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-01-12 02:42:19","endLine":2472,"groupId":"5738","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0b/6e7c9e0002e323d097f4858d4d2f8decf4523a.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        false,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        },\n        new Counters()\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        MoreExecutors.sameThreadExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2327,"status":"M"}],"commitId":"5d2947cd5229d683494b904a4146249f5dfac05f","commitMessage":"@@@Use Guava Compatible immediate executor service (#6815)\n\n* Use multi-guava version friendly direct executor implementation\n\n* Don't use a singleton\n\n* Fix strict compliation complaints\n\n* Copy Guava's DirectExecutor\n\n* Fix javadoc\n\n* Imports are the devil\n","date":"2019-01-12 02:42:19","modifiedFileCount":"41","status":"M","submitter":"Charles Allen"},{"authorTime":"2019-01-26 07:43:06","codes":[{"authorDate":"2019-01-26 07:43:06","commitOrder":7,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        },\n        new Counters()\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-01-26 07:43:06","endLine":2785,"groupId":"5738","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/a6/2d45ae451eebcd895299119ff824b335b5eb2a.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        false,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        },\n        new Counters()\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2635,"status":"M"},{"authorDate":"2019-01-26 07:43:06","commitOrder":7,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        },\n        new Counters()\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-01-26 07:43:06","endLine":2472,"groupId":"5738","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/7b/f83ddd3a031288880dcd0c141cd2f31ea868d6.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        false,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        },\n        new Counters()\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2327,"status":"M"}],"commitId":"8492d94f599da1f7851add2a0e7500515abd881d","commitMessage":"@@@Kill Hadoop MR task on kill of Hadoop ingestion task  (#6828)\n\n* KillTask from overlord UI now makes sure that it terminates the underlying MR job.  thus saving unnecessary compute\n\nRun in jobby is now split into 2\n 1. submitAndGetHadoopJobId followed by 2. run\n  submitAndGetHadoopJobId is responsible for submitting the job and returning the jobId as a string.  run monitors this job for completion\n\nJobHelper writes this jobId in the path provided by HadoopIndexTask which in turn is provided by the ForkingTaskRunner\n\nHadoopIndexTask reads this path when kill task is clicked to get hte jobId and fire the kill command via the yarn api. This is taken care in the stopGracefully method which is called in SingleTaskBackgroundRunner. Have enabled `canRestore` method to return `true` for HadoopIndexTask in order for the stopGracefully method to be called\n\nHadoop*Job files have been changed to incorporate the changes to jobby\n\n* Addressing PR comments\n\n* Addressing PR comments - Fix taskDir\n\n* Addressing PR comments - For changing the contract of Task.stopGracefully()\n`SingleTaskBackgroundRunner` calls stopGracefully in stop() and then checks for canRestore condition to return the status of the task\n\n* Addressing PR comments\n 1. Formatting\n 2. Removing `submitAndGetHadoopJobId` from `Jobby` and calling writeJobIdToFile in the job itself\n\n* Addressing PR comments\n 1. POM change. Moving hadoop dependency to indexing-hadoop\n\n* Addressing PR comments\n 1. stopGracefully now accepts TaskConfig as a param\n     Handling isRestoreOnRestart in stopGracefully for `AppenderatorDriverRealtimeIndexTask.  RealtimeIndexTask.  SeekableStreamIndexTask`\n     Changing tests to make TaskConfig param isRestoreOnRestart to true\n","date":"2019-01-26 07:43:06","modifiedFileCount":"20","status":"M","submitter":"Ankit Kothari"},{"authorTime":"2019-02-05 01:18:12","codes":[{"authorDate":"2019-02-05 01:18:12","commitOrder":8,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-02-05 01:18:12","endLine":2783,"groupId":"5738","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/a8/130b9c0e5b7bd74b96e7746fece32d82efe386.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        },\n        new Counters()\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2634,"status":"M"},{"authorDate":"2019-02-05 01:18:12","commitOrder":8,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-02-05 01:18:12","endLine":2470,"groupId":"5738","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/04/d3802a66cbb8f81cda4666cb6aee1485e9a58f.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        },\n        new Counters()\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2326,"status":"M"}],"commitId":"0e926e865276f892ddc5b62685d331b822104022","commitMessage":"@@@Prohibit assigning concurrent maps into Map-typed variables and fields and fix a race condition in CoordinatorRuleManager (#6898)\n\n* Prohibit assigning concurrent maps into Map-types variables and fields; Fix a race condition in CoordinatorRuleManager; improve logic in DirectDruidClient and ResourcePool\n\n* Enforce that if compute().  computeIfAbsent().  computeIfPresent() or merge() is called on a ConcurrentHashMap.  it's stored in a ConcurrentHashMap-typed variable.  not ConcurrentMap; add comments explaining get()-before-computeIfAbsent() optimization; refactor Counters; fix a race condition in Intialization.java\n\n* Remove unnecessary comment\n\n* Checkstyle\n\n* Fix getFromExtensions()\n\n* Add a reference to the comment about guarded computeIfAbsent() optimization; IdentityHashMap optimization\n\n* Fix UriCacheGeneratorTest\n\n* Workaround issue with MaterializedViewQueryQueryToolChest\n\n* Strengthen Appenderator's contract regarding concurrency\n","date":"2019-02-05 01:18:12","modifiedFileCount":"101","status":"M","submitter":"Roman Leventov"},{"authorTime":"2019-02-19 03:50:08","codes":[{"authorDate":"2019-02-05 01:18:12","commitOrder":9,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-02-05 01:18:12","endLine":2783,"groupId":"5738","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/a8/130b9c0e5b7bd74b96e7746fece32d82efe386.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2634,"status":"N"},{"authorDate":"2019-02-19 03:50:08","commitOrder":9,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-02-19 03:50:08","endLine":2577,"groupId":"5738","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3d/6308cb0a96b01947db7e17b51cab010dd61129.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2433,"status":"M"}],"commitId":"80a2ef7be46c4fc807ac2a74507b8ba8b6a44049","commitMessage":"@@@Support kafka transactional topics (#5404) (#6496)\n\n* Support kafka transactional topics\n\n* update kafka to version 2.0.0\n* Remove the skipOffsetGaps option since it's not used anymore\n* Adjust kafka consumer to use transactional semantics\n* Update tests\n\n* Remove unused import from test\n\n* Fix compilation\n\n* Invoke transaction api to fix a unit test\n\n* temporary modification of travis.yml for debugging\n\n* another attempt to get travis tasklogs\n\n* update kafka to 2.0.1 at all places\n\n* Remove druid-kafka-eight dependency from integration-tests.  remove the kafka firehose test and deprecate kafka-eight classes\n\n* Add deprecated in docs for kafka-eight and kafka-simple extensions\n\n* Remove skipOffsetGaps and code changes for transaction support\n\n* Fix indentation\n\n* remove skipOffsetGaps from kinesis\n\n* Add transaction api to KafkaRecordSupplierTest\n\n* Fix indent\n\n* Fix test\n\n* update kafka version to 2.1.0\n","date":"2019-02-19 03:50:08","modifiedFileCount":"25","status":"M","submitter":"Surekha"},{"authorTime":"2019-02-21 07:10:29","codes":[{"authorDate":"2019-02-21 07:10:29","commitOrder":10,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-02-21 07:10:29","endLine":2783,"groupId":"5738","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f0/ef143e5eec039d6ed4c2871a4c7bc1718eb84e.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2634,"status":"M"},{"authorDate":"2019-02-21 07:10:29","commitOrder":10,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-02-21 07:10:29","endLine":2577,"groupId":"5738","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/1a/5512889d9410e360872c171342c8b1ba914a90.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig, objectMapper);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2433,"status":"M"}],"commitId":"4e2b0852012ab4aed20189878a202e3fe590710b","commitMessage":"@@@Remove DataSegmentFinder.  InsertSegmentToDb.  and descriptor.json file in deep storage (#6911)\n\n* Remove DataSegmentFinder.  InsertSegmentToDb.  and descriptor.json file\n\n* delete descriptor.file when killing segments\n\n* fix test\n\n* Add doc for ha\n\n* improve warning\n","date":"2019-02-21 07:10:29","modifiedFileCount":"37","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-02-24 09:02:56","codes":[{"authorDate":"2019-02-24 09:02:56","commitOrder":11,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-02-24 09:02:56","endLine":2780,"groupId":"5738","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/52/941166c002a200c6f413be4080c41c74a2a836.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2633,"status":"M"},{"authorDate":"2019-02-24 09:02:56","commitOrder":11,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-02-24 09:02:56","endLine":2574,"groupId":"5738","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/89/2349f755feeaecb366e9deedc2e5294c47b4e8.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(\n            new SegmentLoaderLocalCacheManager(null, segmentLoaderConfig, testUtils.getTestObjectMapper())\n        ),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2432,"status":"M"}],"commitId":"1c2753ab9033ff0c785d6e80f5f7c07dc34c3889","commitMessage":"@@@ParallelIndexSubTask: support ingestSegment in delegating factories (#7089)\n\nIndexTask had special-cased code to properly send a TaskToolbox to a\nIngestSegmentFirehoseFactory that's nested inside a CombiningFirehoseFactory. \nbut ParallelIndexSubTask didn't.\n\nThis change refactors IngestSegmentFirehoseFactory so that it doesn't need a\nTaskToolbox; it instead gets a CoordinatorClient and a SegmentLoaderFactory\ndirectly injected into it.\n\nThis also refactors SegmentLoaderFactory so it doesn't depend on\nan injectable SegmentLoaderConfig.  since its only method always\nreplaces the preconfigured SegmentLoaderConfig anyway.\nThis makes it possible to use SegmentLoaderFactory without setting\ndruid.segmentCaches.locations to some dummy value.\n\nAnother goal of this PR is to make it possible for IngestSegmentFirehoseFactory\nto list data segments outside of connect() --- specifically.  to make it a\nFiniteFirehoseFactory which can query the coordinator in order to calculate its\nsplits. See #7048.\n\nThis also adds missing datasource name URL-encoding to an API used by\nCoordinatorBasedSegmentHandoffNotifier.","date":"2019-02-24 09:02:56","modifiedFileCount":"21","status":"M","submitter":"David Glasser"},{"authorTime":"2019-02-24 09:02:56","codes":[{"authorDate":"2019-03-15 11:31:08","commitOrder":12,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-03-15 11:31:08","endLine":3074,"groupId":"5738","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/16/85d8acec03bc2b7e467af8b4f1f363648244d5.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2935,"status":"M"},{"authorDate":"2019-02-24 09:02:56","commitOrder":12,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-02-24 09:02:56","endLine":2574,"groupId":"5738","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/89/2349f755feeaecb366e9deedc2e5294c47b4e8.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2432,"status":"N"}],"commitId":"69a6f1154aed5b466a6f76d73b4997c5952e8898","commitMessage":"@@@Fix testIncrementalHandOffReadsThroughEndOffsets in Kafka/KinesisIndexTaskTest (#7264)\n\n* Fix testIncrementalHandOffReadsThroughEndOffsets in Kafka/KinesisIndexTaskTest\n\n* revert unnecessary change\n\n* fix test\n\n* remove debug log\n","date":"2019-03-15 11:31:08","modifiedFileCount":"2","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-03-27 05:39:07","codes":[{"authorDate":"2019-03-27 05:39:07","commitOrder":13,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-03-27 05:39:07","endLine":2864,"groupId":"5738","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/bb/dd2ddc713f78b36973de6029d2b174f92e4413.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2725,"status":"M"},{"authorDate":"2019-03-27 05:39:07","commitOrder":13,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-03-27 05:39:07","endLine":2738,"groupId":"5738","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/24/573da46a297d18d51cf875ec92e4f7b39b0362.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"taskBaseDir\").getPath(),\n        null,\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2596,"status":"M"}],"commitId":"5294277cb4a382ae77baec2111d9383f0b34bed3","commitMessage":"@@@Fix exclusive start partitions for sequenceMetadata (#7339)\n\n* Fix exclusvie start partitions for sequenceMetadata\n\n* add empty check\n","date":"2019-03-27 05:39:07","modifiedFileCount":"10","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-05-24 00:25:35","codes":[{"authorDate":"2019-03-27 05:39:07","commitOrder":14,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-03-27 05:39:07","endLine":2864,"groupId":"5738","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/bb/dd2ddc713f78b36973de6029d2b174f92e4413.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2725,"status":"N"},{"authorDate":"2019-05-24 00:25:35","commitOrder":14,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-05-24 00:25:35","endLine":2712,"groupId":"5738","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/78/3af47ca8bb71623d67c9415e3f7a2ce8d26cb8.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    SegmentLoaderConfig segmentLoaderConfig = new SegmentLoaderConfig()\n    {\n      @Override\n      public List<StorageLocationConfig> getLocations()\n      {\n        return new ArrayList<>();\n      }\n    };\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2578,"status":"M"}],"commitId":"eff2be4f8f9d7aad0f01516f5425f3ebccaa006c","commitMessage":"@@@Remove LegacyKafkaIndexTaskRunner (#7735)\n\n","date":"2019-05-24 00:25:35","modifiedFileCount":"4","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-05-24 00:25:35","codes":[{"authorDate":"2019-07-07 00:33:12","commitOrder":15,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-07-07 00:33:12","endLine":2867,"groupId":"5738","id":21,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d8/bf83ce2a3969f1243df5bd1dad6429b383e83e.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2728,"status":"M"},{"authorDate":"2019-05-24 00:25:35","commitOrder":15,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-05-24 00:25:35","endLine":2712,"groupId":"5738","id":22,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/78/3af47ca8bb71623d67c9415e3f7a2ce8d26cb8.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2578,"status":"N"}],"commitId":"1166bbcb75d432817715fdd429737f86730b5591","commitMessage":"@@@Remove static imports from tests (#8036)\n\nMake static imports forbidden in tests and remove all occurrences to be\nconsistent with the non-test code.\n\nAlso.  various changes to files affected by above:\n- Reformat to adhere to druid style guide\n- Fix various IntelliJ warnings\n- Fix various SonarLint warnings (e.g..  the expected/actual args to\n  Assert.assertEquals() were flipped)","date":"2019-07-07 00:33:12","modifiedFileCount":"98","status":"M","submitter":"Chi Cao Minh"},{"authorTime":"2019-07-19 05:46:47","codes":[{"authorDate":"2019-07-19 05:46:47","commitOrder":16,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-07-19 05:46:47","endLine":2869,"groupId":"5738","id":23,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/4f/c7ba1b4457f762df48e166f7e2c3b400ac636f.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2729,"status":"M"},{"authorDate":"2019-07-19 05:46:47","commitOrder":16,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-07-19 05:46:47","endLine":2683,"groupId":"5738","id":24,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3f/d6511c752769371e9f45ca7159b203f3ced300.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2548,"status":"M"}],"commitId":"c7eb7cd01837c48914ba284d08b6096b47c957b0","commitMessage":"@@@Add intermediary data server for shuffle (#8088)\n\n* Add intermediary data server for shuffle\n\n* javadoc\n\n* adjust timeout\n\n* resolved todo\n\n* fix test\n\n* style\n\n* address comments\n\n* rename to shuffleDataLocations\n\n* Address comments\n\n* bit adjustment StorageLocation\n\n* fix test\n\n* address comment & fix test\n\n* handle interrupted exception\n","date":"2019-07-19 05:46:47","modifiedFileCount":"29","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-07-25 08:35:46","codes":[{"authorDate":"2019-07-25 08:35:46","commitOrder":17,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-07-25 08:35:46","endLine":2910,"groupId":"14801","id":25,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/00/6677b0a28185be95d0eb677acfab7ede656120.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2770,"status":"M"},{"authorDate":"2019-07-25 08:35:46","commitOrder":17,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-07-25 08:35:46","endLine":2731,"groupId":"14801","id":26,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/87/f522eae18ccef1c23a5fd9ae035dbbe3410dfc.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2596,"status":"M"}],"commitId":"db149462073d59e7563f0d3834e69d44a2bb4011","commitMessage":"@@@Add support minor compaction with segment locking (#7547)\n\n* Segment locking\n\n* Allow both timeChunk and segment lock in the same gruop\n\n* fix it test\n\n* Fix adding same chunk to atomicUpdateGroup\n\n* resolving todos\n\n* Fix segments to lock\n\n* fix segments to lock\n\n* fix kill task\n\n* resolving todos\n\n* resolving todos\n\n* fix teamcity\n\n* remove unused class\n\n* fix single map\n\n* resolving todos\n\n* fix build\n\n* fix SQLMetadataSegmentManager\n\n* fix findInputSegments\n\n* adding more tests\n\n* fixing task lock checks\n\n* add SegmentTransactionalOverwriteAction\n\n* changing publisher\n\n* fixing something\n\n* fix for perfect rollup\n\n* fix test\n\n* adjust package-lock.json\n\n* fix test\n\n* fix style\n\n* adding javadocs\n\n* remove unused classes\n\n* add more javadocs\n\n* unused import\n\n* fix test\n\n* fix test\n\n* Support forceTimeChunk context and force timeChunk lock for parallel index task if intervals are missing\n\n* fix travis\n\n* fix travis\n\n* unused import\n\n* spotbug\n\n* revert getMaxVersion\n\n* address comments\n\n* fix tc\n\n* add missing error handling\n\n* fix backward compatibility\n\n* unused import\n\n* Fix perf of versionedIntervalTimeline\n\n* fix timeline\n\n* fix tc\n\n* remove remaining todos\n\n* add comment for parallel index\n\n* fix javadoc and typos\n\n* typo\n\n* address comments\n","date":"2019-07-25 08:35:46","modifiedFileCount":"130","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-07-30 08:06:33","codes":[{"authorDate":"2019-07-30 08:06:33","commitOrder":18,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-07-30 08:06:33","endLine":2917,"groupId":"14801","id":27,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/09/d0b4580583ae5cd14c197408f5c47e40c01678.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2776,"status":"M"},{"authorDate":"2019-07-30 08:06:33","commitOrder":18,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-07-30 08:06:33","endLine":2737,"groupId":"14801","id":28,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/97/02493e3b16fcacf9caf57b6c6f8a6bef8c1ff2.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new TaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2601,"status":"M"}],"commitId":"640b7afc1cee911a27de7bf938dda24a85ba1510","commitMessage":"@@@Add CliIndexer process type and initial task runner implementation (#8107)\n\n* Add CliIndexer process type and initial task runner implementation\n\n* Fix HttpRemoteTaskRunnerTest\n\n* Remove batch sanity check on PeonAppenderatorsManager\n\n* Fix paralle index tests\n\n* PR comments\n\n* Adjust Jersey resource logging\n\n* Additional cleanup\n\n* Fix SystemSchemaTest\n\n* Add comment to LocalDataSegmentPusherTest absolute path test\n\n* More PR comments\n\n* Use Server annotated with RemoteChatHandler\n\n* More PR comments\n\n* Checkstyle\n\n* PR comments\n\n* Add task shutdown to stopGracefully\n\n* Small cleanup\n\n* Compile fix\n\n* Address PR comments\n\n* Adjust TaskReportFileWriter and fix nits\n\n* Remove unnecessary closer\n\n* More PR comments\n\n* Minor adjustments\n\n* PR comments\n\n* ThreadingTaskRunner: cancel  task run future not shutdownFuture and remove thread from workitem\n","date":"2019-07-30 08:06:33","modifiedFileCount":"64","status":"M","submitter":"Jonathan Wei"},{"authorTime":"2019-08-03 06:34:44","codes":[{"authorDate":"2019-08-03 06:34:44","commitOrder":19,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-08-03 06:34:44","endLine":2918,"groupId":"14801","id":29,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/fd/d92b278f9c06996bedabc6f9950efcb61dab6c.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2776,"status":"M"},{"authorDate":"2019-08-03 06:34:44","commitOrder":19,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile)\n    );\n  }\n","date":"2019-08-03 06:34:44","endLine":2738,"groupId":"14801","id":30,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b5/f3c6258f4deb9303a9728a25fb7045ab7ea6e7.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2601,"status":"M"}],"commitId":"8a16a8e97ff3c80907451bc11c4cd0ea8a421650","commitMessage":"@@@Teach tasks what machine they are running on (#8190)\n\n* Teach the middleManager port to tasks\n\n* parent annotation\n\n* Bind parent for indexer\n","date":"2019-08-03 06:34:44","modifiedFileCount":"19","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-08-16 08:43:35","codes":[{"authorDate":"2019-08-16 08:43:35","commitOrder":20,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","date":"2019-08-16 08:43:35","endLine":2919,"groupId":"14801","id":31,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/59/fe45a5928bb93215a0c191a237c672dcacab55.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2776,"status":"M"},{"authorDate":"2019-08-16 08:43:35","commitOrder":20,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","date":"2019-08-16 08:43:35","endLine":2739,"groupId":"14801","id":32,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/bc/019dec0ad3b860a5b1fc8afe5a75824615aa6e.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile)\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2601,"status":"M"}],"commitId":"5dac6375f382ba485ec000802ff0e922929df95d","commitMessage":"@@@Add support for parallel native indexing with shuffle for perfect rollup (#8257)\n\n* Add TaskResourceCleaner; fix a couple of concurrency bugs in batch tasks\n\n* kill runner when it's ready\n\n* add comment\n\n* kill run thread\n\n* fix test\n\n* Take closeable out of Appenderator\n\n* add javadoc\n\n* fix test\n\n* fix test\n\n* update javadoc\n\n* add javadoc about killed task\n\n* address comment\n\n* Add support for parallel native indexing with shuffle for perfect rollup.\n\n* Add comment about volatiles\n\n* fix test\n\n* fix test\n\n* handling missing exceptions\n\n* more clear javadoc for stopGracefully\n\n* unused import\n\n* update javadoc\n\n* Add missing statement in javadoc\n\n* address comments; fix doc\n\n* add javadoc for isGuaranteedRollup\n\n* Rename confusing variable name and fix typos\n\n* fix typos; move fetch() to a better home; fix the expiration time\n\n* add support https\n","date":"2019-08-16 08:43:35","modifiedFileCount":"49","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-08-22 01:58:22","codes":[{"authorDate":"2019-08-22 01:58:22","commitOrder":21,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","date":"2019-08-22 01:58:22","endLine":2913,"groupId":"8771","id":33,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d8/b98a272a298b563509af89b00decbb917bc831.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2772,"status":"M"},{"authorDate":"2019-08-22 01:58:22","commitOrder":21,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","date":"2019-08-22 01:58:22","endLine":2727,"groupId":"8771","id":34,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/14/55d2ab29c9b3d7854f443cccacf03a9f645d06.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata,\n              @Nullable DataSourceMetadata currentDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata,\n                    currentDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2591,"status":"M"}],"commitId":"22d6384d364a851fd2b5cecafea9d72b004cb03b","commitMessage":"@@@Fix unrealistic test variables in KafkaSupervisorTest and tidy up unused variable in checkpointing process (#7319)\n\n* Fix unrealistic test arguments in KafkaSupervisorTest\n\n* remove currentCheckpoint from checkpoint action\n\n* rename variable\n","date":"2019-08-22 01:58:22","modifiedFileCount":"12","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-10-24 07:51:16","codes":[{"authorDate":"2019-10-24 07:51:16","commitOrder":22,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","date":"2019-10-24 07:51:16","endLine":2998,"groupId":"4919","id":35,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/81/de1e2e9e58c7b384f04232123d2bf6faebb691.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2858,"status":"M"},{"authorDate":"2019-10-24 07:51:16","commitOrder":22,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","date":"2019-10-24 07:51:16","endLine":2790,"groupId":"4919","id":36,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/a1/21d6141447eb672902ddf8078f0f027a96bf50.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              @Nullable Integer taskGroupId,\n              String baseSequenceName,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2655,"status":"M"}],"commitId":"2518478b20dfb22f7133be375f354fbe7bf1ff1e","commitMessage":"@@@Remove deprecated parameter for Checkpoint request (#8707)\n\n* Remove deprecated parameter for Checkpoint request\n\n* fix wrong doc\n","date":"2019-10-24 07:51:16","modifiedFileCount":"11","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-01-25 05:10:01","codes":[{"authorDate":"2020-01-25 05:10:01","commitOrder":23,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","date":"2020-01-25 05:10:01","endLine":2908,"groupId":"4919","id":37,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ae/f7accc709981eafcaa46bb42b28d8d13a183a3.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2767,"status":"M"},{"authorDate":"2020-01-25 05:10:01","commitOrder":23,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","date":"2020-01-25 05:10:01","endLine":2672,"groupId":"4919","id":38,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/43/253b067e143164c90e347bc5e69d3d33724572.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2536,"status":"M"}],"commitId":"19b427e8f371bed0b0100272ac890b3775471654","commitMessage":"@@@Add JoinableFactory interface and use it in the query stack. (#9247)\n\n* Add JoinableFactory interface and use it in the query stack.\n\nAlso includes InlineJoinableFactory.  which enables joining against\ninline datasources. This is the first patch where a basic join query\nactually works. It includes integration tests.\n\n* Fix test issues.\n\n* Adjustments from code review.\n","date":"2020-01-25 05:10:01","modifiedFileCount":"51","status":"M","submitter":"Gian Merlino"},{"authorTime":"2020-06-30 12:03:07","codes":[{"authorDate":"2020-06-30 12:03:07","commitOrder":24,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","date":"2020-06-30 12:03:07","endLine":2981,"groupId":"4919","id":39,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c8/93c9c738ee3323a47ba600e656ef840c5b495e.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2840,"status":"M"},{"authorDate":"2020-06-30 12:03:07","commitOrder":24,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","date":"2020-06-30 12:03:07","endLine":2721,"groupId":"4919","id":40,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c2/57df296790d22457998217eb956c5244720bdc.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2585,"status":"M"}],"commitId":"363d0d86be9e83a5d24d5cf002cac57d99a43baa","commitMessage":"@@@QueryCountStatsMonitor can be injected in the Peon (#10092)\n\n* QueryCountStatsMonitor can be injected in the Peon\n\nThis change fixes a dependency injection bug where there is a circular\ndependency on getting the MonitorScheduler when a user configures the\nQueryCountStatsMonitor to be used.\n\n* fix tests\n\n* Actually fix the tests this time","date":"2020-06-30 12:03:07","modifiedFileCount":"13","status":"M","submitter":"Suneet Saldanha"},{"authorTime":"2020-08-27 08:08:12","codes":[{"authorDate":"2020-08-27 08:08:12","commitOrder":25,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","date":"2020-08-27 08:08:12","endLine":2986,"groupId":"4919","id":41,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/8a/05464d7a1309f62eebadc15d7e7f330cdb2315.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2837,"status":"M"},{"authorDate":"2020-08-27 08:08:12","commitOrder":25,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","date":"2020-08-27 08:08:12","endLine":2728,"groupId":"4919","id":42,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/a6/9dbecd5a7cc5b6810f78eef8f1e1cf3fe547a4.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2584,"status":"M"}],"commitId":"f82fd22fa7de175200b7127c34c2eb2900bf7317","commitMessage":"@@@Move tools for indexing to TaskToolbox instead of injecting them in constructor (#10308)\n\n* Move tools for indexing to TaskToolbox instead of injecting them in constructor\n\n* oops.  other changes\n\n* fix test\n\n* unnecessary new file\n\n* fix test\n\n* fix build","date":"2020-08-27 08:08:12","modifiedFileCount":"67","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-08-27 08:08:12","codes":[{"authorDate":"2020-09-12 07:31:10","commitOrder":26,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","date":"2020-09-12 07:31:10","endLine":2979,"groupId":"4919","id":43,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/4a/ff50b40ba4b202a8f2d220993a41cf489583f7.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2831,"status":"M"},{"authorDate":"2020-08-27 08:08:12","commitOrder":26,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","date":"2020-08-27 08:08:12","endLine":2728,"groupId":"4919","id":44,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/a6/9dbecd5a7cc5b6810f78eef8f1e1cf3fe547a4.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2584,"status":"N"}],"commitId":"8f14ac814e1fdf11878a5ba9bdba58fb745b4c8f","commitMessage":"@@@More structured way to handle parse exceptions (#10336)\n\n* More structured way to handle parse exceptions\n\n* checkstyle; add more tests\n\n* forbidden api; test\n\n* address comment; new test\n\n* address review comments\n\n* javadoc for parseException; remove redundant parseException in streaming ingestion\n\n* fix tests\n\n* unnecessary catch\n\n* unused imports\n\n* appenderator test\n\n* unused import","date":"2020-09-12 07:31:10","modifiedFileCount":"116","status":"M","submitter":"Jihoon Son"},{"authorTime":"2021-01-09 08:04:37","codes":[{"authorDate":"2020-09-12 07:31:10","commitOrder":27,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","date":"2020-09-12 07:31:10","endLine":2979,"groupId":"4919","id":45,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/4a/ff50b40ba4b202a8f2d220993a41cf489583f7.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2831,"status":"N"},{"authorDate":"2021-01-09 08:04:37","commitOrder":27,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    objectMapper.registerModule(TEST_MODULE);\n\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","date":"2021-01-09 08:04:37","endLine":2906,"groupId":"4919","id":46,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3c/a1c0ba4a6726ee443d344ac7a3d28f439bac51.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2759,"status":"M"}],"commitId":"118b50195e5c2989e04e0f5290aa72cae114db39","commitMessage":"@@@Introduce KafkaRecordEntity to support Kafka headers in InputFormats (#10730)\n\nToday Kafka message support in streaming indexing tasks is limited to\nmessage values.  and does not provide a way to expose Kafka headers. \ntimestamps.  or keys.  which may be of interest to more specialized\nDruid input formats. For instance.  Kafka headers may be used to indicate\npayload format/encoding or additional metadata.  and timestamps are often\nomitted from values in Kafka streams applications.  since they are\nincluded in the record.\n\nThis change proposes to introduce KafkaRecordEntity as InputEntity. \nwhich would give input formats full access to the underlying Kafka record. \nincluding headers.  key.  timestamps. It would also open access to low-level\ninformation such as topic.  partition.  offset if needed.\n\nKafkaEntity is a subclass of ByteEntity for backwards compatibility with\nexisting input formats.  and to avoid introducing unnecessary complexity\nfor Kinesis indexing tasks.","date":"2021-01-09 08:04:37","modifiedFileCount":"30","status":"M","submitter":"Xavier L?aut?"},{"authorTime":"2021-03-26 01:32:21","codes":[{"authorDate":"2021-03-26 01:32:21","commitOrder":28,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null,\n        false\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","date":"2021-03-26 01:32:21","endLine":2998,"groupId":"4919","id":47,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/2f/e4d333d495f3fe26db4dca6a6cf1b5c299f2ab.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2849,"status":"M"},{"authorDate":"2021-03-26 01:32:21","commitOrder":28,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    objectMapper.registerModule(TEST_MODULE);\n\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null,\n        false\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","date":"2021-03-26 01:32:21","endLine":2911,"groupId":"4919","id":48,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3d/9dc1c4242aca14ec9958912396b278f42915a1.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    objectMapper.registerModule(TEST_MODULE);\n\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2763,"status":"M"}],"commitId":"bf20f9e9798417c9293a195690b6adcb48f44d3f","commitMessage":"@@@DruidInputSource: Fix issues in column projection.  timestamp handling. (#10267)\n\n* DruidInputSource: Fix issues in column projection.  timestamp handling.\n\nDruidInputSource.  DruidSegmentReader changes:\n\n1) Remove \"dimensions\" and \"metrics\". They are not necessary.  because we\n   can compute which columns we need to read based on what is going to\n   be used by the timestamp.  transform.  dimensions.  and metrics.\n2) Start using ColumnsFilter (see below) to decide which columns we need\n   to read.\n3) Actually respect the \"timestampSpec\". Previously.  it was ignored.  and\n   the timestamp of the returned InputRows was set to the `__time` column\n   of the input datasource.\n\n(1) and (2) together fix a bug in which the DruidInputSource would not\nproperly read columns that are used as inputs to a transformSpec.\n\n(3) fixes a bug where the timestampSpec would be ignored if you attempted\nto set the column to something other than `__time`.\n\n(1) and (3) are breaking changes.\n\nWeb console changes:\n\n1) Remove \"Dimensions\" and \"Metrics\" from the Druid input source.\n2) Set timestampSpec to `{\"column\": \"__time\".  \"format\": \"millis\"}` for\n   compatibility with the new behavior.\n\nOther changes:\n\n1) Add ColumnsFilter.  a new class that allows input readers to determine\n   which columns they need to read. Currently.  it's only used by the\n   DruidInputSource.  but it could be used by other columnar input sources\n   in the future.\n2) Add a ColumnsFilter to InputRowSchema.\n3) Remove the metric names from InputRowSchema (they were unused).\n4) Add InputRowSchemas.fromDataSchema method that computes the proper\n   ColumnsFilter for given timestamp.  dimensions.  transform.  and metrics.\n5) Add \"getRequiredColumns\" method to TransformSpec to support the above.\n\n* Various fixups.\n\n* Uncomment incorrectly commented lines.\n\n* Move TransformSpecTest to the proper module.\n\n* Add druid.indexer.task.ignoreTimestampSpecForDruidInputSource setting.\n\n* Fix.\n\n* Fix build.\n\n* Checkstyle.\n\n* Misc fixes.\n\n* Fix test.\n\n* Move config.\n\n* Fix imports.\n\n* Fixup.\n\n* Fix ShuffleResourceTest.\n\n* Add import.\n\n* Smarter exclusions.\n\n* Fixes based on tests.\n\nAlso.  add TIME_COLUMN constant in the web console.\n\n* Adjustments for tests.\n\n* Reorder test data.\n\n* Update docs.\n\n* Update docs to say Druid 0.22.0 instead of 0.21.0.\n\n* Fix test.\n\n* Fix ITAutoCompactionTest.\n\n* Changes from review & from merging.","date":"2021-03-26 01:32:21","modifiedFileCount":"60","status":"M","submitter":"Gian Merlino"},{"authorTime":"2021-05-12 05:34:26","codes":[{"authorDate":"2021-05-12 05:34:26","commitOrder":29,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null,\n        false,\n        false\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","date":"2021-05-12 05:34:26","endLine":2999,"groupId":"4919","id":49,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/19/d2445f7ee21005eae6718e836233310d6ed8b0.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null,\n        false\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2849,"status":"M"},{"authorDate":"2021-05-12 05:34:26","commitOrder":29,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    objectMapper.registerModule(TEST_MODULE);\n\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null,\n        false,\n        false\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","date":"2021-05-12 05:34:26","endLine":2912,"groupId":"4919","id":50,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/34/ef75cc87a3ef7f69e078358c751636d3c119b2.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    objectMapper.registerModule(TEST_MODULE);\n\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null,\n        false\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2763,"status":"M"}],"commitId":"8e5048e643f95c25cc95be91317475fecbdce456","commitMessage":"@@@Avoid memory mapping hydrants after they are persisted & after they are merged for native batch ingestion (#11123)\n\n* Avoid mapping hydrants in create segments phase for native ingestion\n\n* Drop queriable indices after a given sink is fully merged\n\n* Do not drop memory mappings for realtime ingestion\n\n* Style fixes\n\n* Renamed to match use case better\n\n* Rollback memoization code and use the real time flag instead\n\n* Null ptr fix in FireHydrant toString plus adjustments to memory pressure tracking calculations\n\n* Style\n\n* Log some count stats\n\n* Make sure sinks size is obtained at the right time\n\n* BatchAppenderator unit test\n\n* Fix comment typos\n\n* Renamed methods to make them more readable\n\n* Move persisted metadata from FireHydrant class to AppenderatorImpl. Removed superfluous differences and fix comment typo. Removed custom comparator\n\n* Missing dependency\n\n* Make persisted hydrant metadata map concurrent and better reflect the fact that keys are Java references. Maintain persisted metadata when dropping/closing segments.\n\n* Replaced concurrent variables with normal ones\n\n* Added   batchMemoryMappedIndex \"fallback\" flag with default \"false\". Set this to \"true\" make code fallback to previous code path.\n\n* Style fix.\n\n* Added note to new setting in doc.  using Iterables.size (and removing a dependency).  and fixing a typo in a comment.\n\n* Forgot to commit this edited documentation message","date":"2021-05-12 05:34:26","modifiedFileCount":"33","status":"M","submitter":"Agustin Gonzalez"},{"authorTime":"2021-07-01 18:33:08","codes":[{"authorDate":"2021-07-01 18:33:08","commitOrder":30,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null,\n        false,\n        false\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        DirectQueryProcessingPool.INSTANCE,\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","date":"2021-07-01 18:33:08","endLine":2999,"groupId":"4919","id":51,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5b/3a77cff91f9c0df5a601129f83118f96b6db44.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null,\n        false,\n        false\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2849,"status":"M"},{"authorDate":"2021-07-01 18:33:08","commitOrder":30,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    objectMapper.registerModule(TEST_MODULE);\n\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null,\n        false,\n        false\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        DirectQueryProcessingPool.INSTANCE,\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","date":"2021-07-01 18:33:08","endLine":2912,"groupId":"4919","id":52,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ca/3630d635d0f3411e48b018ecc49cd71e0ddb03.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    objectMapper.registerModule(TEST_MODULE);\n\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null,\n        false,\n        false\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        Execs.directExecutor(), \r\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2763,"status":"M"}],"commitId":"03a6a6d6e1f7024c2691e6fef0685bd137d223e7","commitMessage":"@@@Replace Processing ExecutorService with QueryProcessingPool (#11382)\n\nThis PR refactors the code for QueryRunnerFactory#mergeRunners to accept a new interface called QueryProcessingPool instead of ExecutorService for concurrent execution of query runners. This interface will let custom extensions inject their own implementation for deciding which query-runner to prioritize first. The default implementation is the same as today that takes the priority of query into account. QueryProcessingPool can also be used as a regular executor service. It has a dedicated method for accepting query execution work so implementations can differentiate between regular async tasks and query execution tasks. This dedicated method also passes the QueryRunner object as part of the task information. This hook will let custom extensions carry any state from QuerySegmentWalker to QueryProcessingPool#mergeRunners which is not possible currently.","date":"2021-07-01 18:33:08","modifiedFileCount":"52","status":"M","submitter":"Abhishek Agarwal"},{"authorTime":"2021-07-21 02:44:19","codes":[{"authorDate":"2021-07-21 02:44:19","commitOrder":31,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null,\n        false,\n        false\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        DirectQueryProcessingPool.INSTANCE,\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentCacheManagerFactory(testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","date":"2021-07-21 02:44:19","endLine":2999,"groupId":"4919","id":53,"instanceNumber":1,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3d/0f86d67f53ce956ff79c605fce973671a15ee8.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null,\n        false,\n        false\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        DirectQueryProcessingPool.INSTANCE,\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2849,"status":"M"},{"authorDate":"2021-07-21 02:44:19","commitOrder":31,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    objectMapper.registerModule(TEST_MODULE);\n\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null,\n        false,\n        false\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        DirectQueryProcessingPool.INSTANCE,\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentCacheManagerFactory(testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","date":"2021-07-21 02:44:19","endLine":2912,"groupId":"4919","id":54,"instanceNumber":2,"isCurCommit":0,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/54/242883c01b28b43d780f36b91618048b3c4871.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    objectMapper.registerModule(TEST_MODULE);\n\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null,\n        false,\n        false\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        DirectQueryProcessingPool.INSTANCE,\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentLoaderFactory(null, testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2763,"status":"M"}],"commitId":"94c1671eaf7b050972602fdedcb1971cdbde692d","commitMessage":"@@@Split SegmentLoader into SegmentLoader and SegmentCacheManager (#11466)\n\nThis PR splits current SegmentLoader into SegmentLoader and SegmentCacheManager.\n\nSegmentLoader - this class is responsible for building the segment object but does not expose any methods for downloading.  cache space management.  etc. Default implementation delegates the download operations to SegmentCacheManager and only contains the logic for building segments once downloaded. . This class will be used in SegmentManager to construct Segment objects.\n\nSegmentCacheManager - this class manages the segment cache on the local disk. It fetches the segment files to the local disk.  can clean up the cache.  and in the future.  support reserve and release on cache space. [See https://github.com/Make SegmentLoader extensible and customizable #11398]. This class will be used in ingestion tasks such as compaction.  re-indexing where segment files need to be downloaded locally.","date":"2021-07-21 02:44:19","modifiedFileCount":"41","status":"M","submitter":"Abhishek Agarwal"},{"authorTime":"2021-09-09 04:31:52","codes":[{"authorDate":"2021-09-09 04:31:52","commitOrder":32,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null,\n        false,\n        false,\n        TaskConfig.BATCH_PROCESSING_MODE_DEFAULT.name()\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        DirectQueryProcessingPool.INSTANCE,\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentCacheManagerFactory(testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","date":"2021-09-09 04:31:52","endLine":3000,"groupId":"102368","id":55,"instanceNumber":1,"isCurCommit":1,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5a/781a86f43ef7a3c6e6563f470d09f4b7ca8f06.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n    objectMapper.setInjectableValues(((InjectableValues.Std) objectMapper.getInjectableValues()).addValue(\n        AWSCredentialsConfig.class,\n        new AWSCredentialsConfig()\n    ));\n    for (Module module : new KinesisIndexingServiceModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null,\n        false,\n        false\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata checkpointMetadata\n          )\n          {\n            LOG.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    checkpointMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesOnlyConglomerate,\n        DirectQueryProcessingPool.INSTANCE,\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentCacheManagerFactory(testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","realPath":"extensions-core/kinesis-indexing-service/src/test/java/org/apache/druid/indexing/kinesis/KinesisIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2849,"status":"M"},{"authorDate":"2021-09-09 04:31:52","commitOrder":32,"curCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    objectMapper.registerModule(TEST_MODULE);\n\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null,\n        false,\n        false,\n        TaskConfig.BATCH_PROCESSING_MODE_DEFAULT.name()\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        DirectQueryProcessingPool.INSTANCE,\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentCacheManagerFactory(testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","date":"2021-09-09 04:31:52","endLine":2913,"groupId":"102368","id":56,"instanceNumber":2,"isCurCommit":1,"methodName":"makeToolboxFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/87/32ed7f1bf78a3d526dd5365e15a3dbb5b56945.src","preCode":"  private void makeToolboxFactory() throws IOException\n  {\n    directory = tempFolder.newFolder();\n    final TestUtils testUtils = new TestUtils();\n    rowIngestionMetersFactory = testUtils.getRowIngestionMetersFactory();\n    final ObjectMapper objectMapper = testUtils.getTestObjectMapper();\n\n    for (Module module : new KafkaIndexTaskModule().getJacksonModules()) {\n      objectMapper.registerModule(module);\n    }\n    objectMapper.registerModule(TEST_MODULE);\n\n    final TaskConfig taskConfig = new TaskConfig(\n        new File(directory, \"baseDir\").getPath(),\n        new File(directory, \"baseTaskDir\").getPath(),\n        null,\n        50000,\n        null,\n        true,\n        null,\n        null,\n        null,\n        false,\n        false\n    );\n    final TestDerbyConnector derbyConnector = derby.getConnector();\n    derbyConnector.createDataSourceTable();\n    derbyConnector.createPendingSegmentsTable();\n    derbyConnector.createSegmentTable();\n    derbyConnector.createRulesTable();\n    derbyConnector.createConfigTable();\n    derbyConnector.createTaskTables();\n    derbyConnector.createAuditTable();\n    taskStorage = new MetadataTaskStorage(\n        derbyConnector,\n        new TaskStorageConfig(null),\n        new DerbyMetadataStorageActionHandlerFactory(\n            derbyConnector,\n            derby.metadataTablesConfigSupplier().get(),\n            objectMapper\n        )\n    );\n    metadataStorageCoordinator = new IndexerSQLMetadataStorageCoordinator(\n        testUtils.getTestObjectMapper(),\n        derby.metadataTablesConfigSupplier().get(),\n        derbyConnector\n    );\n    taskLockbox = new TaskLockbox(taskStorage, metadataStorageCoordinator);\n    final TaskActionToolbox taskActionToolbox = new TaskActionToolbox(\n        taskLockbox,\n        taskStorage,\n        metadataStorageCoordinator,\n        emitter,\n        new SupervisorManager(null)\n        {\n          @Override\n          public boolean checkPointDataSourceMetadata(\n              String supervisorId,\n              int taskGroupId,\n              @Nullable DataSourceMetadata previousDataSourceMetadata\n          )\n          {\n            log.info(\"Adding checkpoint hash to the set\");\n            checkpointRequestsHash.add(\n                Objects.hash(\n                    supervisorId,\n                    taskGroupId,\n                    previousDataSourceMetadata\n                )\n            );\n            return true;\n          }\n        }\n    );\n    final TaskActionClientFactory taskActionClientFactory = new LocalTaskActionClientFactory(\n        taskStorage,\n        taskActionToolbox,\n        new TaskAuditLogConfig(false)\n    );\n    final SegmentHandoffNotifierFactory handoffNotifierFactory = dataSource -> new SegmentHandoffNotifier()\n    {\n      @Override\n      public boolean registerSegmentHandoffCallback(\n          SegmentDescriptor descriptor,\n          Executor exec,\n          Runnable handOffRunnable\n      )\n      {\n        if (doHandoff) {\n          \r\n          exec.execute(handOffRunnable);\n        }\n        return true;\n      }\n\n      @Override\n      public void start()\n      {\n        \r\n      }\n\n      @Override\n      public void close()\n      {\n        \r\n      }\n    };\n    final LocalDataSegmentPusherConfig dataSegmentPusherConfig = new LocalDataSegmentPusherConfig();\n    dataSegmentPusherConfig.storageDirectory = getSegmentDirectory();\n    final DataSegmentPusher dataSegmentPusher = new LocalDataSegmentPusher(dataSegmentPusherConfig);\n\n    toolboxFactory = new TaskToolboxFactory(\n        taskConfig,\n        null, \r\n        taskActionClientFactory,\n        emitter,\n        dataSegmentPusher,\n        new TestDataSegmentKiller(),\n        null, \r\n        null, \r\n        new TestDataSegmentAnnouncer(),\n        EasyMock.createNiceMock(DataSegmentServerAnnouncer.class),\n        handoffNotifierFactory,\n        this::makeTimeseriesAndScanConglomerate,\n        DirectQueryProcessingPool.INSTANCE,\n        NoopJoinableFactory.INSTANCE,\n        () -> EasyMock.createMock(MonitorScheduler.class),\n        new SegmentCacheManagerFactory(testUtils.getTestObjectMapper()),\n        testUtils.getTestObjectMapper(),\n        testUtils.getTestIndexIO(),\n        MapCache.create(1024),\n        new CacheConfig(),\n        new CachePopulatorStats(),\n        testUtils.getTestIndexMergerV9(),\n        EasyMock.createNiceMock(DruidNodeAnnouncer.class),\n        EasyMock.createNiceMock(DruidNode.class),\n        new LookupNodeService(\"tier\"),\n        new DataNodeService(\"tier\", 1, ServerType.INDEXER_EXECUTOR, 0),\n        new SingleFileTaskReportFileWriter(reportsFile),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        testUtils.getRowIngestionMetersFactory(),\n        new TestAppenderatorsManager(),\n        new NoopIndexingServiceClient(),\n        null,\n        null,\n        null\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2763,"status":"M"}],"commitId":"9efa6cc9c85aab66b325e9f37f7bad13826aea39","commitMessage":"@@@Make persists concurrent with adding rows in batch ingestion (#11536)\n\n* Make persists concurrent with ingestion\n\n* Remove semaphore but keep concurrent persists (with add) and add push in the backround as well\n\n* Go back to documented default persists (zero)\n\n* Move to debug\n\n* Remove unnecessary Atomics\n\n* Comments on synchronization (or not) for sinks & sinkMetadata\n\n* Some cleanup for unit tests but they still need further work\n\n* Shutdown & wait for persists and push on close\n\n* Provide support for three existing batch appenderators using batchProcessingMode flag\n\n* Fix reference to wrong appenderator\n\n* Fix doc typos\n\n* Add BatchAppenderators class test coverage\n\n* Add log message to batchProcessingMode final value.  fix typo in enum name\n\n* Another typo and minor fix to log message\n\n* LEGACY->OPEN_SEGMENTS.  Edit docs\n\n* Minor update legacy->open segments log message\n\n* More code comments.  mostly small adjustments to naming etc\n\n* fix spelling\n\n* Exclude BtachAppenderators from Jacoco since it is fully tested but Jacoco still refuses to ack coverage\n\n* Coverage for Appenderators & BatchAppenderators.  name change of a method that was still using \"legacy\" rather than \"openSegments\"\n\nCo-authored-by: Clint Wylie <cjwylie@gmail.com>","date":"2021-09-09 04:31:52","modifiedFileCount":"33","status":"M","submitter":"Agustin Gonzalez"}]
