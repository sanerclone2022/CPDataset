[{"authorTime":"2019-12-10 15:05:49","codes":[{"authorDate":"2019-11-21 09:24:12","commitOrder":2,"curCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    \r\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedHashPartitionsReport> indexingRunner = createRunner(\n        toolbox,\n        this::createPartialHashSegmentGenerateRunner\n    );\n\n    TaskState state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n\n    \r\n    Map<Pair<Interval, Integer>, List<HashPartitionLocation>> partitionToLocations =\n        groupHashPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialHashSegmentMergeIOConfig> ioConfigs = createHashMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexTaskRunner<PartialHashSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialHashSegmentMergeRunner(tb, ioConfigs)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), state);\n  }\n","date":"2019-11-21 09:24:12","endLine":520,"groupId":"2879","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"runHashPartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/28/bfc7c421b1728933e09761ab1b2d45b4aa5ab4.src","preCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    \r\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedHashPartitionsReport> indexingRunner = createRunner(\n        toolbox,\n        this::createPartialHashSegmentGenerateRunner\n    );\n\n    TaskState state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n\n    \r\n    Map<Pair<Interval, Integer>, List<HashPartitionLocation>> partitionToLocations =\n        groupHashPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialHashSegmentMergeIOConfig> ioConfigs = createHashMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexTaskRunner<PartialHashSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialHashSegmentMergeRunner(tb, ioConfigs)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), state);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":486,"status":"NB"},{"authorDate":"2019-12-10 15:05:49","commitOrder":2,"curCode":"  private TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    ParallelIndexTaskRunner<PartialDimensionDistributionTask, DimensionDistributionReport> distributionRunner =\n        createRunner(\n            toolbox,\n            this::createPartialDimensionDistributionRunner\n        );\n\n    TaskState distributionState = runNextPhase(distributionRunner);\n    if (distributionState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialDimensionDistributionTask.TYPE + \" failed\");\n    }\n\n    Map<Interval, PartitionBoundaries> intervalToPartitions =\n        determineAllRangePartitions(distributionRunner.getReports().values());\n\n    if (intervalToPartitions.isEmpty()) {\n      String msg = \"No valid rows for single dimension partitioning.\"\n          + \" All rows may have invalid timestamps or multiple dimension values.\";\n      LOG.warn(msg);\n      return TaskStatus.success(getId(), msg);\n    }\n\n    ParallelIndexTaskRunner<PartialRangeSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(toolbox, tb -> createPartialRangeSegmentGenerateRunner(tb, intervalToPartitions));\n\n    TaskState indexingState = runNextPhase(indexingRunner);\n    if (indexingState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialRangeSegmentGenerateTask.TYPE + \" failed\");\n    }\n\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs)\n    );\n    TaskState mergeState = runNextPhase(mergeRunner);\n    if (mergeState.isSuccess()) {\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), mergeState);\n  }\n","date":"2019-12-10 15:05:49","endLine":648,"groupId":"2879","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"runRangePartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/db/31af67d91a21a5ff6b201661d6f091cda50de1.src","preCode":"  private TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    ParallelIndexTaskRunner<PartialDimensionDistributionTask, DimensionDistributionReport> distributionRunner =\n        createRunner(\n            toolbox,\n            this::createPartialDimensionDistributionRunner\n        );\n\n    TaskState distributionState = runNextPhase(distributionRunner);\n    if (distributionState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialDimensionDistributionTask.TYPE + \" failed\");\n    }\n\n    Map<Interval, PartitionBoundaries> intervalToPartitions =\n        determineAllRangePartitions(distributionRunner.getReports().values());\n\n    if (intervalToPartitions.isEmpty()) {\n      String msg = \"No valid rows for single dimension partitioning.\"\n          + \" All rows may have invalid timestamps or multiple dimension values.\";\n      LOG.warn(msg);\n      return TaskStatus.success(getId(), msg);\n    }\n\n    ParallelIndexTaskRunner<PartialRangeSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(toolbox, tb -> createPartialRangeSegmentGenerateRunner(tb, intervalToPartitions));\n\n    TaskState indexingState = runNextPhase(indexingRunner);\n    if (indexingState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialRangeSegmentGenerateTask.TYPE + \" failed\");\n    }\n\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs)\n    );\n    TaskState mergeState = runNextPhase(mergeRunner);\n    if (mergeState.isSuccess()) {\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), mergeState);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":599,"status":"B"}],"commitId":"bab78fc80e8ee2929d6d2c277d3999695c8402a4","commitMessage":"@@@Parallel indexing single dim partitions (#8925)\n\n* Parallel indexing single dim partitions\n\nImplements single dimension range partitioning for native parallel batch\nindexing as described in #8769. This initial version requires the\ndruid-datasketches extension to be loaded.\n\nThe algorithm has 5 phases that are orchestrated by the supervisor in\n`ParallelIndexSupervisorTask#runRangePartitionMultiPhaseParallel()`.\nThese phases and the main classes involved are described below:\n\n1) In parallel.  determine the distribution of dimension values for each\n   input source split.\n\n   `PartialDimensionDistributionTask` uses `StringSketch` to generate\n   the approximate distribution of dimension values for each input\n   source split. If the rows are ungrouped. \n   `PartialDimensionDistributionTask.UngroupedRowDimensionValueFilter`\n   uses a Bloom filter to skip rows that would be grouped. The final\n   distribution is sent back to the supervisor via\n   `DimensionDistributionReport`.\n\n2) The range partitions are determined.\n\n   In `ParallelIndexSupervisorTask#determineAllRangePartitions()`.  the\n   supervisor uses `StringSketchMerger` to merge the individual\n   `StringSketch`es created in the preceding phase. The merged sketch is\n   then used to create the range partitions.\n\n3) In parallel.  generate partial range-partitioned segments.\n\n   `PartialRangeSegmentGenerateTask` uses the range partitions\n   determined in the preceding phase and\n   `RangePartitionCachingLocalSegmentAllocator` to generate\n   `SingleDimensionShardSpec`s.  The partition information is sent back\n   to the supervisor via `GeneratedGenericPartitionsReport`.\n\n4) The partial range segments are grouped.\n\n   In `ParallelIndexSupervisorTask#groupGenericPartitionLocationsPerPartition()`. \n   the supervisor creates the `PartialGenericSegmentMergeIOConfig`s\n   necessary for the next phase.\n\n5) In parallel.  merge partial range-partitioned segments.\n\n   `PartialGenericSegmentMergeTask` uses `GenericPartitionLocation` to\n   retrieve the partial range-partitioned segments generated earlier and\n   then merges and publishes them.\n\n* Fix dependencies & forbidden apis\n\n* Fixes for integration test\n\n* Address review comments\n\n* Fix docs.  strict compile.  sketch check.  rollup check\n\n* Fix first shard spec.  partition serde.  single subtask\n\n* Fix first partition check in test\n\n* Misc rewording/refactoring to address code review\n\n* Fix doc link\n\n* Split batch index integration test\n\n* Do not run parallel-batch-index twice\n\n* Adjust last partition\n\n* Split ITParallelIndexTest to reduce runtime\n\n* Rename test class\n\n* Allow null values in range partitions\n\n* Indicate which phase failed\n\n* Improve asserts in tests\n","date":"2019-12-10 15:05:49","modifiedFileCount":"18","status":"M","submitter":"Chi Cao Minh"},{"authorTime":"2019-12-10 15:05:49","codes":[{"authorDate":"2020-06-19 09:40:43","commitOrder":3,"curCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    \r\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner\n        = createRunner(toolbox, this::createPartialHashSegmentGenerateRunner);\n\n    TaskState state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), state);\n  }\n","date":"2020-06-19 09:40:43","endLine":561,"groupId":"2879","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"runHashPartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/be/d85dea6926fce5c22c4ce268c98a7ec15fcaf4.src","preCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    \r\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedHashPartitionsReport> indexingRunner = createRunner(\n        toolbox,\n        this::createPartialHashSegmentGenerateRunner\n    );\n\n    TaskState state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n\n    \r\n    Map<Pair<Interval, Integer>, List<HashPartitionLocation>> partitionToLocations =\n        groupHashPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialHashSegmentMergeIOConfig> ioConfigs = createHashMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexTaskRunner<PartialHashSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialHashSegmentMergeRunner(tb, ioConfigs)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), state);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":529,"status":"M"},{"authorDate":"2019-12-10 15:05:49","commitOrder":3,"curCode":"  private TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    ParallelIndexTaskRunner<PartialDimensionDistributionTask, DimensionDistributionReport> distributionRunner =\n        createRunner(\n            toolbox,\n            this::createPartialDimensionDistributionRunner\n        );\n\n    TaskState distributionState = runNextPhase(distributionRunner);\n    if (distributionState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialDimensionDistributionTask.TYPE + \" failed\");\n    }\n\n    Map<Interval, PartitionBoundaries> intervalToPartitions =\n        determineAllRangePartitions(distributionRunner.getReports().values());\n\n    if (intervalToPartitions.isEmpty()) {\n      String msg = \"No valid rows for single dimension partitioning.\"\n          + \" All rows may have invalid timestamps or multiple dimension values.\";\n      LOG.warn(msg);\n      return TaskStatus.success(getId(), msg);\n    }\n\n    ParallelIndexTaskRunner<PartialRangeSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(toolbox, tb -> createPartialRangeSegmentGenerateRunner(tb, intervalToPartitions));\n\n    TaskState indexingState = runNextPhase(indexingRunner);\n    if (indexingState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialRangeSegmentGenerateTask.TYPE + \" failed\");\n    }\n\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs)\n    );\n    TaskState mergeState = runNextPhase(mergeRunner);\n    if (mergeState.isSuccess()) {\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), mergeState);\n  }\n","date":"2019-12-10 15:05:49","endLine":648,"groupId":"2879","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"runRangePartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/db/31af67d91a21a5ff6b201661d6f091cda50de1.src","preCode":"  private TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    ParallelIndexTaskRunner<PartialDimensionDistributionTask, DimensionDistributionReport> distributionRunner =\n        createRunner(\n            toolbox,\n            this::createPartialDimensionDistributionRunner\n        );\n\n    TaskState distributionState = runNextPhase(distributionRunner);\n    if (distributionState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialDimensionDistributionTask.TYPE + \" failed\");\n    }\n\n    Map<Interval, PartitionBoundaries> intervalToPartitions =\n        determineAllRangePartitions(distributionRunner.getReports().values());\n\n    if (intervalToPartitions.isEmpty()) {\n      String msg = \"No valid rows for single dimension partitioning.\"\n          + \" All rows may have invalid timestamps or multiple dimension values.\";\n      LOG.warn(msg);\n      return TaskStatus.success(getId(), msg);\n    }\n\n    ParallelIndexTaskRunner<PartialRangeSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(toolbox, tb -> createPartialRangeSegmentGenerateRunner(tb, intervalToPartitions));\n\n    TaskState indexingState = runNextPhase(indexingRunner);\n    if (indexingState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialRangeSegmentGenerateTask.TYPE + \" failed\");\n    }\n\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs)\n    );\n    TaskState mergeState = runNextPhase(mergeRunner);\n    if (mergeState.isSuccess()) {\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), mergeState);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":599,"status":"N"}],"commitId":"d644a27f1a545105a4b1a4110f3ed83d7c46a46f","commitMessage":"@@@Create packed core partitions for hash/range-partitioned segments in native batch ingestion (#10025)\n\n* Fill in the core partition set size properly for batch ingestion with\ndynamic partitioning\n\n* incomplete javadoc\n\n* Address comments\n\n* fix tests\n\n* fix json serde.  add tests\n\n* checkstyle\n\n* Set core partition set size for hash-partitioned segments properly in\nbatch ingestion\n\n* test for both parallel and single-threaded task\n\n* unused variables\n\n* fix test\n\n* unused imports\n\n* add hash/range buckets\n\n* some test adjustment and missing json serde\n\n* centralized partition id allocation in parallel and simple tasks\n\n* remove string partition chunk\n\n* revive string partition chunk\n\n* fill numCorePartitions for hadoop\n\n* clean up hash stuffs\n\n* resolved todos\n\n* javadocs\n\n* Fix tests\n\n* add more tests\n\n* doc\n\n* unused imports","date":"2020-06-19 09:40:43","modifiedFileCount":"78","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-12-10 15:05:49","codes":[{"authorDate":"2020-09-25 04:47:53","commitOrder":4,"curCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Integer numShardsOverride;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    if (partitionsSpec.getNumShards() == null) {\n      \r\n      LOG.info(\"numShards is unspecified, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      if (cardinalityRunner == null) {\n        throw new ISE(\"Could not create cardinality runner for hash partitioning.\");\n      }\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        return TaskStatus.failure(getId());\n      }\n\n      int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                       ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                       : partitionsSpec.getMaxRowsPerSegment();\n      LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n      if (cardinalityRunner.getReports() == null) {\n        throw new ISE(\"Could not determine cardinalities for hash partitioning.\");\n      }\n      numShardsOverride = determineNumShardsFromCardinalityReport(\n          cardinalityRunner.getReports().values(),\n          effectiveMaxRowsPerSegment\n      );\n\n      LOG.info(\"Automatically determined numShards: \" + numShardsOverride);\n    } else {\n      numShardsOverride = null;\n    }\n\n    \r\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, numShardsOverride)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), state);\n  }\n","date":"2020-09-25 04:47:53","endLine":602,"groupId":"2879","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"runHashPartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/dd/0e75980c9ca76bbe3de2e249cb0b6e0658ee2d.src","preCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    \r\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner\n        = createRunner(toolbox, this::createPartialHashSegmentGenerateRunner);\n\n    TaskState state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), state);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":520,"status":"M"},{"authorDate":"2019-12-10 15:05:49","commitOrder":4,"curCode":"  private TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    ParallelIndexTaskRunner<PartialDimensionDistributionTask, DimensionDistributionReport> distributionRunner =\n        createRunner(\n            toolbox,\n            this::createPartialDimensionDistributionRunner\n        );\n\n    TaskState distributionState = runNextPhase(distributionRunner);\n    if (distributionState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialDimensionDistributionTask.TYPE + \" failed\");\n    }\n\n    Map<Interval, PartitionBoundaries> intervalToPartitions =\n        determineAllRangePartitions(distributionRunner.getReports().values());\n\n    if (intervalToPartitions.isEmpty()) {\n      String msg = \"No valid rows for single dimension partitioning.\"\n          + \" All rows may have invalid timestamps or multiple dimension values.\";\n      LOG.warn(msg);\n      return TaskStatus.success(getId(), msg);\n    }\n\n    ParallelIndexTaskRunner<PartialRangeSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(toolbox, tb -> createPartialRangeSegmentGenerateRunner(tb, intervalToPartitions));\n\n    TaskState indexingState = runNextPhase(indexingRunner);\n    if (indexingState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialRangeSegmentGenerateTask.TYPE + \" failed\");\n    }\n\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs)\n    );\n    TaskState mergeState = runNextPhase(mergeRunner);\n    if (mergeState.isSuccess()) {\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), mergeState);\n  }\n","date":"2019-12-10 15:05:49","endLine":648,"groupId":"2879","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"runRangePartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/db/31af67d91a21a5ff6b201661d6f091cda50de1.src","preCode":"  private TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    ParallelIndexTaskRunner<PartialDimensionDistributionTask, DimensionDistributionReport> distributionRunner =\n        createRunner(\n            toolbox,\n            this::createPartialDimensionDistributionRunner\n        );\n\n    TaskState distributionState = runNextPhase(distributionRunner);\n    if (distributionState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialDimensionDistributionTask.TYPE + \" failed\");\n    }\n\n    Map<Interval, PartitionBoundaries> intervalToPartitions =\n        determineAllRangePartitions(distributionRunner.getReports().values());\n\n    if (intervalToPartitions.isEmpty()) {\n      String msg = \"No valid rows for single dimension partitioning.\"\n          + \" All rows may have invalid timestamps or multiple dimension values.\";\n      LOG.warn(msg);\n      return TaskStatus.success(getId(), msg);\n    }\n\n    ParallelIndexTaskRunner<PartialRangeSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(toolbox, tb -> createPartialRangeSegmentGenerateRunner(tb, intervalToPartitions));\n\n    TaskState indexingState = runNextPhase(indexingRunner);\n    if (indexingState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialRangeSegmentGenerateTask.TYPE + \" failed\");\n    }\n\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs)\n    );\n    TaskState mergeState = runNextPhase(mergeRunner);\n    if (mergeState.isSuccess()) {\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), mergeState);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":599,"status":"N"}],"commitId":"cb30b1fe2353dc28601c50984def9e83adb89571","commitMessage":"@@@Automatically determine numShards for parallel ingestion hash partitioning (#10419)\n\n* Automatically determine numShards for parallel ingestion hash partitioning\n\n* Fix inspection.  tests.  coverage\n\n* Docs and some PR comments\n\n* Adjust locking\n\n* Use HllSketch instead of HyperLogLogCollector\n\n* Fix tests\n\n* Address some PR comments\n\n* Fix granularity bug\n\n* Small doc fix","date":"2020-09-25 04:47:53","modifiedFileCount":"14","status":"M","submitter":"Jonathan Wei"},{"authorTime":"2020-11-26 06:50:22","codes":[{"authorDate":"2020-11-26 06:50:22","commitOrder":5,"curCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Map<Interval, Integer> intervalToNumShards;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    final boolean needsInputSampling =\n        partitionsSpec.getNumShards() == null\n        || ingestionSchemaToUse.getDataSchema().getGranularitySpec().inputIntervals().isEmpty();\n    if (needsInputSampling) {\n      \r\n      LOG.info(\"Needs to determine intervals or numShards, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        return TaskStatus.failure(getId());\n      }\n\n      if (cardinalityRunner.getReports().isEmpty()) {\n        String msg = \"No valid rows for hash partitioning.\"\n                     + \" All rows may have invalid timestamps or have been filtered out.\";\n        LOG.warn(msg);\n        return TaskStatus.success(getId(), msg);\n      }\n\n      if (partitionsSpec.getNumShards() == null) {\n        int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                         ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                         : partitionsSpec.getMaxRowsPerSegment();\n        LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n        intervalToNumShards = determineNumShardsFromCardinalityReport(\n            cardinalityRunner.getReports().values(),\n            effectiveMaxRowsPerSegment\n        );\n      } else {\n        intervalToNumShards = CollectionUtils.mapValues(\n            mergeCardinalityReports(cardinalityRunner.getReports().values()),\n            k -> partitionsSpec.getNumShards()\n        );\n      }\n\n      ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n          ingestionSchemaToUse,\n          intervalToNumShards.keySet()\n      );\n    } else {\n      \r\n      intervalToNumShards = null;\n    }\n\n    \r\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, segmentCreateIngestionSpec, intervalToNumShards)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), state);\n  }\n","date":"2020-11-26 06:50:22","endLine":650,"groupId":"2879","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"runHashPartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3d/272b43813c4d222e918d7c69cdad0ae3e419df.src","preCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Integer numShardsOverride;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    if (partitionsSpec.getNumShards() == null) {\n      \r\n      LOG.info(\"numShards is unspecified, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      if (cardinalityRunner == null) {\n        throw new ISE(\"Could not create cardinality runner for hash partitioning.\");\n      }\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        return TaskStatus.failure(getId());\n      }\n\n      int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                       ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                       : partitionsSpec.getMaxRowsPerSegment();\n      LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n      if (cardinalityRunner.getReports() == null) {\n        throw new ISE(\"Could not determine cardinalities for hash partitioning.\");\n      }\n      numShardsOverride = determineNumShardsFromCardinalityReport(\n          cardinalityRunner.getReports().values(),\n          effectiveMaxRowsPerSegment\n      );\n\n      LOG.info(\"Automatically determined numShards: \" + numShardsOverride);\n    } else {\n      numShardsOverride = null;\n    }\n\n    \r\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, numShardsOverride)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), state);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":551,"status":"M"},{"authorDate":"2020-11-26 06:50:22","commitOrder":5,"curCode":"  private TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n    ParallelIndexTaskRunner<PartialDimensionDistributionTask, DimensionDistributionReport> distributionRunner =\n        createRunner(\n            toolbox,\n            this::createPartialDimensionDistributionRunner\n        );\n\n    TaskState distributionState = runNextPhase(distributionRunner);\n    if (distributionState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialDimensionDistributionTask.TYPE + \" failed\");\n    }\n\n    Map<Interval, PartitionBoundaries> intervalToPartitions =\n        determineAllRangePartitions(distributionRunner.getReports().values());\n\n    if (intervalToPartitions.isEmpty()) {\n      String msg = \"No valid rows for single dimension partitioning.\"\n                   + \" All rows may have invalid timestamps or multiple dimension values.\";\n      LOG.warn(msg);\n      return TaskStatus.success(getId(), msg);\n    }\n\n    ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n        ingestionSchemaToUse,\n        intervalToPartitions.keySet()\n    );\n\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialRangeSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            tb -> createPartialRangeSegmentGenerateRunner(tb, intervalToPartitions, segmentCreateIngestionSpec)\n        );\n\n    TaskState indexingState = runNextPhase(indexingRunner);\n    if (indexingState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialRangeSegmentGenerateTask.TYPE + \" failed\");\n    }\n\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    TaskState mergeState = runNextPhase(mergeRunner);\n    if (mergeState.isSuccess()) {\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), mergeState);\n  }\n","date":"2020-11-26 06:50:22","endLine":712,"groupId":"2879","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"runRangePartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3d/272b43813c4d222e918d7c69cdad0ae3e419df.src","preCode":"  private TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    ParallelIndexTaskRunner<PartialDimensionDistributionTask, DimensionDistributionReport> distributionRunner =\n        createRunner(\n            toolbox,\n            this::createPartialDimensionDistributionRunner\n        );\n\n    TaskState distributionState = runNextPhase(distributionRunner);\n    if (distributionState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialDimensionDistributionTask.TYPE + \" failed\");\n    }\n\n    Map<Interval, PartitionBoundaries> intervalToPartitions =\n        determineAllRangePartitions(distributionRunner.getReports().values());\n\n    if (intervalToPartitions.isEmpty()) {\n      String msg = \"No valid rows for single dimension partitioning.\"\n          + \" All rows may have invalid timestamps or multiple dimension values.\";\n      LOG.warn(msg);\n      return TaskStatus.success(getId(), msg);\n    }\n\n    ParallelIndexTaskRunner<PartialRangeSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(toolbox, tb -> createPartialRangeSegmentGenerateRunner(tb, intervalToPartitions));\n\n    TaskState indexingState = runNextPhase(indexingRunner);\n    if (indexingState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialRangeSegmentGenerateTask.TYPE + \" failed\");\n    }\n\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs)\n    );\n    TaskState mergeState = runNextPhase(mergeRunner);\n    if (mergeState.isSuccess()) {\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), mergeState);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":652,"status":"M"}],"commitId":"7462b0b953e87890a683a0bee7b7480465450342","commitMessage":"@@@Allow missing intervals for Parallel task with hash/range partitioning (#10592)\n\n* Allow missing intervals for Parallel task\n\n* fix row filter\n\n* fix tests\n\n* fix log","date":"2020-11-26 06:50:22","modifiedFileCount":"22","status":"M","submitter":"Jihoon Son"},{"authorTime":"2021-04-09 12:03:00","codes":[{"authorDate":"2021-04-09 12:03:00","commitOrder":6,"curCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Map<Interval, Integer> intervalToNumShards;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    final boolean needsInputSampling =\n        partitionsSpec.getNumShards() == null\n        || ingestionSchemaToUse.getDataSchema().getGranularitySpec().inputIntervals().isEmpty();\n    if (needsInputSampling) {\n      \r\n      LOG.info(\"Needs to determine intervals or numShards, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        return TaskStatus.failure(getId());\n      }\n\n      if (cardinalityRunner.getReports().isEmpty()) {\n        String msg = \"No valid rows for hash partitioning.\"\n                     + \" All rows may have invalid timestamps or have been filtered out.\";\n        LOG.warn(msg);\n        return TaskStatus.success(getId(), msg);\n      }\n\n      if (partitionsSpec.getNumShards() == null) {\n        int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                         ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                         : partitionsSpec.getMaxRowsPerSegment();\n        LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n        intervalToNumShards = determineNumShardsFromCardinalityReport(\n            cardinalityRunner.getReports().values(),\n            effectiveMaxRowsPerSegment\n        );\n      } else {\n        intervalToNumShards = CollectionUtils.mapValues(\n            mergeCardinalityReports(cardinalityRunner.getReports().values()),\n            k -> partitionsSpec.getNumShards()\n        );\n      }\n\n      ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n          ingestionSchemaToUse,\n          intervalToNumShards.keySet()\n      );\n    } else {\n      \r\n      intervalToNumShards = null;\n    }\n\n    \r\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, segmentCreateIngestionSpec, intervalToNumShards)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(mergeRunner.getReports());\n      }\n    }\n\n    TaskStatus taskStatus = TaskStatus.fromCode(getId(), state);\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","date":"2021-04-09 12:03:00","endLine":694,"groupId":"2879","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"runHashPartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/60/984e8a9dbb447af7d2cec5781c4c746fcb2eff.src","preCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Map<Interval, Integer> intervalToNumShards;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    final boolean needsInputSampling =\n        partitionsSpec.getNumShards() == null\n        || ingestionSchemaToUse.getDataSchema().getGranularitySpec().inputIntervals().isEmpty();\n    if (needsInputSampling) {\n      \r\n      LOG.info(\"Needs to determine intervals or numShards, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        return TaskStatus.failure(getId());\n      }\n\n      if (cardinalityRunner.getReports().isEmpty()) {\n        String msg = \"No valid rows for hash partitioning.\"\n                     + \" All rows may have invalid timestamps or have been filtered out.\";\n        LOG.warn(msg);\n        return TaskStatus.success(getId(), msg);\n      }\n\n      if (partitionsSpec.getNumShards() == null) {\n        int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                         ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                         : partitionsSpec.getMaxRowsPerSegment();\n        LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n        intervalToNumShards = determineNumShardsFromCardinalityReport(\n            cardinalityRunner.getReports().values(),\n            effectiveMaxRowsPerSegment\n        );\n      } else {\n        intervalToNumShards = CollectionUtils.mapValues(\n            mergeCardinalityReports(cardinalityRunner.getReports().values()),\n            k -> partitionsSpec.getNumShards()\n        );\n      }\n\n      ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n          ingestionSchemaToUse,\n          intervalToNumShards.keySet()\n      );\n    } else {\n      \r\n      intervalToNumShards = null;\n    }\n\n    \r\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, segmentCreateIngestionSpec, intervalToNumShards)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), state);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":587,"status":"M"},{"authorDate":"2021-04-09 12:03:00","commitOrder":6,"curCode":"  private TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n    ParallelIndexTaskRunner<PartialDimensionDistributionTask, DimensionDistributionReport> distributionRunner =\n        createRunner(\n            toolbox,\n            this::createPartialDimensionDistributionRunner\n        );\n\n    TaskState distributionState = runNextPhase(distributionRunner);\n    if (distributionState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialDimensionDistributionTask.TYPE + \" failed\");\n    }\n\n    Map<Interval, PartitionBoundaries> intervalToPartitions =\n        determineAllRangePartitions(distributionRunner.getReports().values());\n\n    if (intervalToPartitions.isEmpty()) {\n      String msg = \"No valid rows for single dimension partitioning.\"\n                   + \" All rows may have invalid timestamps or multiple dimension values.\";\n      LOG.warn(msg);\n      return TaskStatus.success(getId(), msg);\n    }\n\n    ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n        ingestionSchemaToUse,\n        intervalToPartitions.keySet()\n    );\n\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialRangeSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            tb -> createPartialRangeSegmentGenerateRunner(tb, intervalToPartitions, segmentCreateIngestionSpec)\n        );\n\n    TaskState indexingState = runNextPhase(indexingRunner);\n    if (indexingState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialRangeSegmentGenerateTask.TYPE + \" failed\");\n    }\n\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    TaskState mergeState = runNextPhase(mergeRunner);\n    if (mergeState.isSuccess()) {\n      publishSegments(toolbox, mergeRunner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(mergeRunner.getReports());\n      }\n    }\n\n    TaskStatus taskStatus = TaskStatus.fromCode(getId(), mergeState);\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","date":"2021-04-09 12:03:00","endLine":764,"groupId":"2879","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"runRangePartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/60/984e8a9dbb447af7d2cec5781c4c746fcb2eff.src","preCode":"  private TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n    ParallelIndexTaskRunner<PartialDimensionDistributionTask, DimensionDistributionReport> distributionRunner =\n        createRunner(\n            toolbox,\n            this::createPartialDimensionDistributionRunner\n        );\n\n    TaskState distributionState = runNextPhase(distributionRunner);\n    if (distributionState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialDimensionDistributionTask.TYPE + \" failed\");\n    }\n\n    Map<Interval, PartitionBoundaries> intervalToPartitions =\n        determineAllRangePartitions(distributionRunner.getReports().values());\n\n    if (intervalToPartitions.isEmpty()) {\n      String msg = \"No valid rows for single dimension partitioning.\"\n                   + \" All rows may have invalid timestamps or multiple dimension values.\";\n      LOG.warn(msg);\n      return TaskStatus.success(getId(), msg);\n    }\n\n    ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n        ingestionSchemaToUse,\n        intervalToPartitions.keySet()\n    );\n\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialRangeSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            tb -> createPartialRangeSegmentGenerateRunner(tb, intervalToPartitions, segmentCreateIngestionSpec)\n        );\n\n    TaskState indexingState = runNextPhase(indexingRunner);\n    if (indexingState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialRangeSegmentGenerateTask.TYPE + \" failed\");\n    }\n\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    TaskState mergeState = runNextPhase(mergeRunner);\n    if (mergeState.isSuccess()) {\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), mergeState);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":696,"status":"M"}],"commitId":"8264203cee688607091232897749e959e7706010","commitMessage":"@@@Allow client to configure batch ingestion task to wait to complete until segments are confirmed to be available by other (#10676)\n\n* Add ability to wait for segment availability for batch jobs\n\n* IT updates\n\n* fix queries in legacy hadoop IT\n\n* Fix broken indexing integration tests\n\n* address an lgtm flag\n\n* spell checker still flagging for hadoop doc. adding under that file header too\n\n* fix compaction IT\n\n* Updates to wait for availability method\n\n* improve unit testing for patch\n\n* fix bad indentation\n\n* refactor waitForSegmentAvailability\n\n* Fixes based off of review comments\n\n* cleanup to get compile after merging with master\n\n* fix failing test after previous logic update\n\n* add back code that must have gotten deleted during conflict resolution\n\n* update some logging code\n\n* fixes to get compilation working after merge with master\n\n* reset interrupt flag in catch block after code review pointed it out\n\n* small changes following self-review\n\n* fixup some issues brought on by merge with master\n\n* small changes after review\n\n* cleanup a little bit after merge with master\n\n* Fix potential resource leak in AbstractBatchIndexTask\n\n* syntax fix\n\n* Add a Compcation TuningConfig type\n\n* add docs stipulating the lack of support by Compaction tasks for the new config\n\n* Fixup compilation errors after merge with master\n\n* Remove erreneous newline","date":"2021-04-09 12:03:00","modifiedFileCount":"106","status":"M","submitter":"Lucas Capistrant"},{"authorTime":"2021-08-03 03:11:28","codes":[{"authorDate":"2021-08-03 03:11:28","commitOrder":7,"curCode":"  TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Map<Interval, Integer> intervalToNumShards;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    final boolean needsInputSampling =\n        partitionsSpec.getNumShards() == null\n        || ingestionSchemaToUse.getDataSchema().getGranularitySpec().inputIntervals().isEmpty();\n    if (needsInputSampling) {\n      \r\n      LOG.info(\"Needs to determine intervals or numShards, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        String errMsg = StringUtils.format(\n            TASK_PHASE_FAILURE_MSG,\n            cardinalityRunner.getName()\n        );\n        return TaskStatus.failure(getId(), errMsg);\n      }\n\n      if (cardinalityRunner.getReports().isEmpty()) {\n        String msg = \"No valid rows for hash partitioning.\"\n                     + \" All rows may have invalid timestamps or have been filtered out.\";\n        LOG.warn(msg);\n        return TaskStatus.success(getId(), msg);\n      }\n\n      if (partitionsSpec.getNumShards() == null) {\n        int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                         ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                         : partitionsSpec.getMaxRowsPerSegment();\n        LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n        intervalToNumShards = determineNumShardsFromCardinalityReport(\n            cardinalityRunner.getReports().values(),\n            effectiveMaxRowsPerSegment\n        );\n      } else {\n        intervalToNumShards = CollectionUtils.mapValues(\n            mergeCardinalityReports(cardinalityRunner.getReports().values()),\n            k -> partitionsSpec.getNumShards()\n        );\n      }\n\n      ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n          ingestionSchemaToUse,\n          intervalToNumShards.keySet()\n      );\n    } else {\n      \r\n      intervalToNumShards = null;\n    }\n\n    \r\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, segmentCreateIngestionSpec, intervalToNumShards)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          indexingRunner.getName()\n      );\n      return TaskStatus.failure(getId(), errMsg);\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    state = runNextPhase(mergeRunner);\n    TaskStatus taskStatus;\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(mergeRunner.getReports());\n      }\n      taskStatus = TaskStatus.success(getId());\n    } else {\n      \r\n      Preconditions.checkState(state.isFailure(), \"Unrecognized state after task is complete[%s]\", state);\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          mergeRunner.getName()\n      );\n      taskStatus = TaskStatus.failure(getId(), errMsg);\n    }\n\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","date":"2021-08-03 03:11:28","endLine":749,"groupId":"10658","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"runHashPartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d2/a3ca7819df3dc7fa78aaaf3e72308e8fdbff1d.src","preCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Map<Interval, Integer> intervalToNumShards;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    final boolean needsInputSampling =\n        partitionsSpec.getNumShards() == null\n        || ingestionSchemaToUse.getDataSchema().getGranularitySpec().inputIntervals().isEmpty();\n    if (needsInputSampling) {\n      \r\n      LOG.info(\"Needs to determine intervals or numShards, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        return TaskStatus.failure(getId());\n      }\n\n      if (cardinalityRunner.getReports().isEmpty()) {\n        String msg = \"No valid rows for hash partitioning.\"\n                     + \" All rows may have invalid timestamps or have been filtered out.\";\n        LOG.warn(msg);\n        return TaskStatus.success(getId(), msg);\n      }\n\n      if (partitionsSpec.getNumShards() == null) {\n        int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                         ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                         : partitionsSpec.getMaxRowsPerSegment();\n        LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n        intervalToNumShards = determineNumShardsFromCardinalityReport(\n            cardinalityRunner.getReports().values(),\n            effectiveMaxRowsPerSegment\n        );\n      } else {\n        intervalToNumShards = CollectionUtils.mapValues(\n            mergeCardinalityReports(cardinalityRunner.getReports().values()),\n            k -> partitionsSpec.getNumShards()\n        );\n      }\n\n      ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n          ingestionSchemaToUse,\n          intervalToNumShards.keySet()\n      );\n    } else {\n      \r\n      intervalToNumShards = null;\n    }\n\n    \r\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, segmentCreateIngestionSpec, intervalToNumShards)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(mergeRunner.getReports());\n      }\n    }\n\n    TaskStatus taskStatus = TaskStatus.fromCode(getId(), state);\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":625,"status":"M"},{"authorDate":"2021-08-03 03:11:28","commitOrder":7,"curCode":"  TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n    ParallelIndexTaskRunner<PartialDimensionDistributionTask, DimensionDistributionReport> distributionRunner =\n        createRunner(\n            toolbox,\n            this::createPartialDimensionDistributionRunner\n        );\n\n    TaskState distributionState = runNextPhase(distributionRunner);\n    if (distributionState.isFailure()) {\n      String errMsg = StringUtils.format(TASK_PHASE_FAILURE_MSG, distributionRunner.getName());\n      return TaskStatus.failure(getId(), errMsg);\n    }\n\n    Map<Interval, PartitionBoundaries> intervalToPartitions =\n        determineAllRangePartitions(distributionRunner.getReports().values());\n\n    if (intervalToPartitions.isEmpty()) {\n      String msg = \"No valid rows for single dimension partitioning.\"\n                   + \" All rows may have invalid timestamps or multiple dimension values.\";\n      LOG.warn(msg);\n      return TaskStatus.success(getId(), msg);\n    }\n\n    ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n        ingestionSchemaToUse,\n        intervalToPartitions.keySet()\n    );\n\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialRangeSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            tb -> createPartialRangeSegmentGenerateRunner(tb, intervalToPartitions, segmentCreateIngestionSpec)\n        );\n\n    TaskState indexingState = runNextPhase(indexingRunner);\n    if (indexingState.isFailure()) {\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          indexingRunner.getName()\n      );\n      return TaskStatus.failure(getId(), errMsg);\n    }\n\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    TaskState mergeState = runNextPhase(mergeRunner);\n    TaskStatus taskStatus;\n    if (mergeState.isSuccess()) {\n      publishSegments(toolbox, mergeRunner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(mergeRunner.getReports());\n      }\n      taskStatus = TaskStatus.success(getId());\n    } else {\n      \r\n      Preconditions.checkState(mergeState.isFailure(), \"Unrecognized state after task is complete[%s]\", mergeState);\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          mergeRunner.getName()\n      );\n      taskStatus = TaskStatus.failure(getId(), errMsg);\n    }\n\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","date":"2021-08-03 03:11:28","endLine":834,"groupId":"10658","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"runRangePartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d2/a3ca7819df3dc7fa78aaaf3e72308e8fdbff1d.src","preCode":"  private TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n    ParallelIndexTaskRunner<PartialDimensionDistributionTask, DimensionDistributionReport> distributionRunner =\n        createRunner(\n            toolbox,\n            this::createPartialDimensionDistributionRunner\n        );\n\n    TaskState distributionState = runNextPhase(distributionRunner);\n    if (distributionState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialDimensionDistributionTask.TYPE + \" failed\");\n    }\n\n    Map<Interval, PartitionBoundaries> intervalToPartitions =\n        determineAllRangePartitions(distributionRunner.getReports().values());\n\n    if (intervalToPartitions.isEmpty()) {\n      String msg = \"No valid rows for single dimension partitioning.\"\n                   + \" All rows may have invalid timestamps or multiple dimension values.\";\n      LOG.warn(msg);\n      return TaskStatus.success(getId(), msg);\n    }\n\n    ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n        ingestionSchemaToUse,\n        intervalToPartitions.keySet()\n    );\n\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialRangeSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            tb -> createPartialRangeSegmentGenerateRunner(tb, intervalToPartitions, segmentCreateIngestionSpec)\n        );\n\n    TaskState indexingState = runNextPhase(indexingRunner);\n    if (indexingState.isFailure()) {\n      return TaskStatus.failure(getId(), PartialRangeSegmentGenerateTask.TYPE + \" failed\");\n    }\n\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    TaskState mergeState = runNextPhase(mergeRunner);\n    if (mergeState.isSuccess()) {\n      publishSegments(toolbox, mergeRunner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(mergeRunner.getReports());\n      }\n    }\n\n    TaskStatus taskStatus = TaskStatus.fromCode(getId(), mergeState);\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":752,"status":"M"}],"commitId":"a2da407b704db8eed85b97cd7c1d22541f2b1102","commitMessage":"@@@Add error msg to parallel task's TaskStatus (#11486)\n\n* Add error msg to parallel task's TaskStatus\n\n* Consolidate failure block\n\n* Add failure test\n\n* Make it fail\n\n* Add fail while stopped\n\n* Simplify hash task test using a runner that fails after so many runs (parameter)\n\n* Remove unthrown exception\n\n* Use runner names to identify phase\n\n* Added range partition kill test & fixed a timing bug with the custom runner\n\n* Forbidden api\n\n* Style\n\n* Unit test code cleanup\n\n* Added message to invalid state exception and improved readability  of the phase error messages for the parallel task failure unit tests","date":"2021-08-03 03:11:28","modifiedFileCount":"19","status":"M","submitter":"Agustin Gonzalez"},{"authorTime":"2021-08-14 04:40:25","codes":[{"authorDate":"2021-08-14 04:40:25","commitOrder":8,"curCode":"  TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Map<Interval, Integer> intervalToNumShards;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    final boolean needsInputSampling =\n        partitionsSpec.getNumShards() == null\n        || ingestionSchemaToUse.getDataSchema().getGranularitySpec().inputIntervals().isEmpty();\n    if (needsInputSampling) {\n      \r\n      LOG.info(\"Needs to determine intervals or numShards, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        String errMsg = StringUtils.format(\n            TASK_PHASE_FAILURE_MSG,\n            cardinalityRunner.getName()\n        );\n        return TaskStatus.failure(getId(), errMsg);\n      }\n\n      if (cardinalityRunner.getReports().isEmpty()) {\n        String msg = \"No valid rows for hash partitioning.\"\n                     + \" All rows may have invalid timestamps or have been filtered out.\";\n        LOG.warn(msg);\n        return TaskStatus.success(getId(), msg);\n      }\n\n      if (partitionsSpec.getNumShards() == null) {\n        int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                         ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                         : partitionsSpec.getMaxRowsPerSegment();\n        LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n        intervalToNumShards = determineNumShardsFromCardinalityReport(\n            cardinalityRunner.getReports().values(),\n            effectiveMaxRowsPerSegment\n        );\n      } else {\n        intervalToNumShards = CollectionUtils.mapValues(\n            mergeCardinalityReports(cardinalityRunner.getReports().values()),\n            k -> partitionsSpec.getNumShards()\n        );\n      }\n\n      ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n          ingestionSchemaToUse,\n          intervalToNumShards.keySet()\n      );\n    } else {\n      \r\n      intervalToNumShards = null;\n    }\n\n    \r\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, segmentCreateIngestionSpec, intervalToNumShards)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          indexingRunner.getName()\n      );\n      return TaskStatus.failure(getId(), errMsg);\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<PartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    state = runNextPhase(mergeRunner);\n    TaskStatus taskStatus;\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(mergeRunner.getReports());\n      }\n      taskStatus = TaskStatus.success(getId());\n    } else {\n      \r\n      Preconditions.checkState(state.isFailure(), \"Unrecognized state after task is complete[%s]\", state);\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          mergeRunner.getName()\n      );\n      taskStatus = TaskStatus.failure(getId(), errMsg);\n    }\n\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","date":"2021-08-14 04:40:25","endLine":749,"groupId":"104782","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"runHashPartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b6/6f49f7a6c8a7b15f150b69f2416af8f16759e2.src","preCode":"  TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Map<Interval, Integer> intervalToNumShards;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    final boolean needsInputSampling =\n        partitionsSpec.getNumShards() == null\n        || ingestionSchemaToUse.getDataSchema().getGranularitySpec().inputIntervals().isEmpty();\n    if (needsInputSampling) {\n      \r\n      LOG.info(\"Needs to determine intervals or numShards, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        String errMsg = StringUtils.format(\n            TASK_PHASE_FAILURE_MSG,\n            cardinalityRunner.getName()\n        );\n        return TaskStatus.failure(getId(), errMsg);\n      }\n\n      if (cardinalityRunner.getReports().isEmpty()) {\n        String msg = \"No valid rows for hash partitioning.\"\n                     + \" All rows may have invalid timestamps or have been filtered out.\";\n        LOG.warn(msg);\n        return TaskStatus.success(getId(), msg);\n      }\n\n      if (partitionsSpec.getNumShards() == null) {\n        int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                         ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                         : partitionsSpec.getMaxRowsPerSegment();\n        LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n        intervalToNumShards = determineNumShardsFromCardinalityReport(\n            cardinalityRunner.getReports().values(),\n            effectiveMaxRowsPerSegment\n        );\n      } else {\n        intervalToNumShards = CollectionUtils.mapValues(\n            mergeCardinalityReports(cardinalityRunner.getReports().values()),\n            k -> partitionsSpec.getNumShards()\n        );\n      }\n\n      ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n          ingestionSchemaToUse,\n          intervalToNumShards.keySet()\n      );\n    } else {\n      \r\n      intervalToNumShards = null;\n    }\n\n    \r\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, segmentCreateIngestionSpec, intervalToNumShards)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          indexingRunner.getName()\n      );\n      return TaskStatus.failure(getId(), errMsg);\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    state = runNextPhase(mergeRunner);\n    TaskStatus taskStatus;\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(mergeRunner.getReports());\n      }\n      taskStatus = TaskStatus.success(getId());\n    } else {\n      \r\n      Preconditions.checkState(state.isFailure(), \"Unrecognized state after task is complete[%s]\", state);\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          mergeRunner.getName()\n      );\n      taskStatus = TaskStatus.failure(getId(), errMsg);\n    }\n\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":625,"status":"M"},{"authorDate":"2021-08-14 04:40:25","commitOrder":8,"curCode":"  TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n    ParallelIndexTaskRunner<PartialDimensionDistributionTask, DimensionDistributionReport> distributionRunner =\n        createRunner(\n            toolbox,\n            this::createPartialDimensionDistributionRunner\n        );\n\n    TaskState distributionState = runNextPhase(distributionRunner);\n    if (distributionState.isFailure()) {\n      String errMsg = StringUtils.format(TASK_PHASE_FAILURE_MSG, distributionRunner.getName());\n      return TaskStatus.failure(getId(), errMsg);\n    }\n\n    Map<Interval, PartitionBoundaries> intervalToPartitions =\n        determineAllRangePartitions(distributionRunner.getReports().values());\n\n    if (intervalToPartitions.isEmpty()) {\n      String msg = \"No valid rows for single dimension partitioning.\"\n                   + \" All rows may have invalid timestamps or multiple dimension values.\";\n      LOG.warn(msg);\n      return TaskStatus.success(getId(), msg);\n    }\n\n    ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n        ingestionSchemaToUse,\n        intervalToPartitions.keySet()\n    );\n\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialRangeSegmentGenerateTask, GeneratedPartitionsReport> indexingRunner =\n        createRunner(\n            toolbox,\n            tb -> createPartialRangeSegmentGenerateRunner(tb, intervalToPartitions, segmentCreateIngestionSpec)\n        );\n\n    TaskState indexingState = runNextPhase(indexingRunner);\n    if (indexingState.isFailure()) {\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          indexingRunner.getName()\n      );\n      return TaskStatus.failure(getId(), errMsg);\n    }\n\n    \r\n    Map<Pair<Interval, Integer>, List<PartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    TaskState mergeState = runNextPhase(mergeRunner);\n    TaskStatus taskStatus;\n    if (mergeState.isSuccess()) {\n      publishSegments(toolbox, mergeRunner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(mergeRunner.getReports());\n      }\n      taskStatus = TaskStatus.success(getId());\n    } else {\n      \r\n      Preconditions.checkState(mergeState.isFailure(), \"Unrecognized state after task is complete[%s]\", mergeState);\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          mergeRunner.getName()\n      );\n      taskStatus = TaskStatus.failure(getId(), errMsg);\n    }\n\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","date":"2021-08-14 04:40:25","endLine":834,"groupId":"104782","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"runRangePartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b6/6f49f7a6c8a7b15f150b69f2416af8f16759e2.src","preCode":"  TaskStatus runRangePartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n    ParallelIndexTaskRunner<PartialDimensionDistributionTask, DimensionDistributionReport> distributionRunner =\n        createRunner(\n            toolbox,\n            this::createPartialDimensionDistributionRunner\n        );\n\n    TaskState distributionState = runNextPhase(distributionRunner);\n    if (distributionState.isFailure()) {\n      String errMsg = StringUtils.format(TASK_PHASE_FAILURE_MSG, distributionRunner.getName());\n      return TaskStatus.failure(getId(), errMsg);\n    }\n\n    Map<Interval, PartitionBoundaries> intervalToPartitions =\n        determineAllRangePartitions(distributionRunner.getReports().values());\n\n    if (intervalToPartitions.isEmpty()) {\n      String msg = \"No valid rows for single dimension partitioning.\"\n                   + \" All rows may have invalid timestamps or multiple dimension values.\";\n      LOG.warn(msg);\n      return TaskStatus.success(getId(), msg);\n    }\n\n    ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n        ingestionSchemaToUse,\n        intervalToPartitions.keySet()\n    );\n\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialRangeSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            tb -> createPartialRangeSegmentGenerateRunner(tb, intervalToPartitions, segmentCreateIngestionSpec)\n        );\n\n    TaskState indexingState = runNextPhase(indexingRunner);\n    if (indexingState.isFailure()) {\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          indexingRunner.getName()\n      );\n      return TaskStatus.failure(getId(), errMsg);\n    }\n\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    TaskState mergeState = runNextPhase(mergeRunner);\n    TaskStatus taskStatus;\n    if (mergeState.isSuccess()) {\n      publishSegments(toolbox, mergeRunner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(mergeRunner.getReports());\n      }\n      taskStatus = TaskStatus.success(getId());\n    } else {\n      \r\n      Preconditions.checkState(mergeState.isFailure(), \"Unrecognized state after task is complete[%s]\", mergeState);\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          mergeRunner.getName()\n      );\n      taskStatus = TaskStatus.failure(getId(), errMsg);\n    }\n\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":752,"status":"M"}],"commitId":"c7b46671b311e841f7ddc3596dc4e9b4cf96b24d","commitMessage":"@@@option to use deep storage for storing shuffle data (#11507)\n\nFixes #11297.\nDescription\n\nDescription and design in the proposal #11297\nKey changed/added classes in this PR\n\n    *DataSegmentPusher\n    *ShuffleClient\n    *PartitionStat\n    *PartitionLocation\n    *IntermediaryDataManager\n","date":"2021-08-14 04:40:25","modifiedFileCount":"47","status":"M","submitter":"Parag Jain"}]
