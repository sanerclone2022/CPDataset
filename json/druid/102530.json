[{"authorTime":"2019-03-14 05:29:39","codes":[{"authorDate":"2019-02-24 12:10:31","commitOrder":7,"curCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","date":"2019-02-24 12:10:31","endLine":1840,"groupId":"2275","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testRestore","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b1/9bf8ad3716e1bda5912843b5946146e1e703d6.src","preCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1747,"status":"NB"},{"authorDate":"2019-03-14 05:29:39","commitOrder":7,"curCode":"  public void testRestoreAfterPersistingSequences() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n\n    records = generateSinglePartitionRecords(topic);\n    maxRowsPerSegment = 2;\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 0L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final SeekableStreamPartitions<Integer, Long> checkpoint = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(0, 5L)\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (task1.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task1.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    \r\n    task1.getRunner().setEndOffsets(ImmutableMap.of(0, 5L), false);\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 0L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n    \r\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(5, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(4, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2008/P1D\", 1);\n    SegmentDescriptor desc3 = sd(task1, \"2009/P1D\", 0);\n    SegmentDescriptor desc4 = sd(task1, \"2009/P1D\", 1);\n    SegmentDescriptor desc5 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc6 = sd(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc7 = sd(task1, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","date":"2019-03-14 05:29:39","endLine":1969,"groupId":"9632","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testRestoreAfterPersistingSequences","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/07/0396f467226bd526e1ed0b20b37652ef6a8c9f.src","preCode":"  public void testRestoreAfterPersistingSequences() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n\n    records = generateSinglePartitionRecords(topic);\n    maxRowsPerSegment = 2;\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 0L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final SeekableStreamPartitions<Integer, Long> checkpoint = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(0, 5L)\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (task1.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task1.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    \r\n    task1.getRunner().setEndOffsets(ImmutableMap.of(0, 5L), false);\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 0L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n    \r\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(5, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(4, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2008/P1D\", 1);\n    SegmentDescriptor desc3 = sd(task1, \"2009/P1D\", 0);\n    SegmentDescriptor desc4 = sd(task1, \"2009/P1D\", 1);\n    SegmentDescriptor desc5 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc6 = sd(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc7 = sd(task1, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1859,"status":"B"}],"commitId":"fb1489d31380106bf4002a1df816fef7dc99c05b","commitMessage":"@@@fix SequenceMetadata deserialization (#7256)\n\n* wip\n\n* fix tests.  stop reading if we are at end offset\n\n* fix build\n\n* remove restore at end offsets fix in favor of a separate PR\n\n* use typereference from method for serialization too\n","date":"2019-03-14 05:29:39","modifiedFileCount":"7","status":"M","submitter":"Clint Wylie"},{"authorTime":"2019-03-22 04:12:22","codes":[{"authorDate":"2019-03-22 04:12:22","commitOrder":8,"curCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","date":"2019-03-22 04:12:22","endLine":1858,"groupId":"2275","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testRestore","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/eb/ef3fc4596a43b59efefcaf094105a8ae34ab5f.src","preCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 2L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1765,"status":"M"},{"authorDate":"2019-03-22 04:12:22","commitOrder":8,"curCode":"  public void testRestoreAfterPersistingSequences() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n\n    records = generateSinglePartitionRecords(topic);\n    maxRowsPerSegment = 2;\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 5L),\n        ImmutableSet.of(0)\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (task1.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task1.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    \r\n    task1.getRunner().setEndOffsets(ImmutableMap.of(0, 5L), false);\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n    \r\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(5, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(4, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2008/P1D\", 1);\n    SegmentDescriptor desc3 = sd(task1, \"2009/P1D\", 0);\n    SegmentDescriptor desc4 = sd(task1, \"2009/P1D\", 1);\n    SegmentDescriptor desc5 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc6 = sd(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc7 = sd(task1, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","date":"2019-03-22 04:12:22","endLine":1972,"groupId":"11127","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testRestoreAfterPersistingSequences","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/eb/ef3fc4596a43b59efefcaf094105a8ae34ab5f.src","preCode":"  public void testRestoreAfterPersistingSequences() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n\n    records = generateSinglePartitionRecords(topic);\n    maxRowsPerSegment = 2;\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 0L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final SeekableStreamPartitions<Integer, Long> checkpoint = new SeekableStreamPartitions<>(\n        topic,\n        ImmutableMap.of(0, 5L)\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (task1.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task1.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    \r\n    task1.getRunner().setEndOffsets(ImmutableMap.of(0, 5L), false);\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 0L)),\n            new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n    \r\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(5, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(4, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2008/P1D\", 1);\n    SegmentDescriptor desc3 = sd(task1, \"2009/P1D\", 0);\n    SegmentDescriptor desc4 = sd(task1, \"2009/P1D\", 1);\n    SegmentDescriptor desc5 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc6 = sd(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc7 = sd(task1, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1861,"status":"M"}],"commitId":"0c5dcf5586e33607849e397209f3eb0b10661f1e","commitMessage":"@@@Fix exclusivity for start offset in kinesis indexing service & check exclusivity properly in IndexerSQLMetadataStorageCoordinator (#7291)\n\n* Fix exclusivity for start offset in kinesis indexing service\n\n* some adjustment\n\n* Fix SeekableStreamDataSourceMetadata\n\n* Add missing javadocs\n\n* Add missing comments and unit test\n\n* fix SeekableStreamStartSequenceNumbers.plus and add comments\n\n* remove extra exclusivePartitions in KafkaIOConfig and fix downgrade issue\n\n* Add javadocs\n\n* fix compilation\n\n* fix test\n\n* remove unused variable\n","date":"2019-03-22 04:12:22","modifiedFileCount":"30","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-05-24 00:25:35","codes":[{"authorDate":"2019-03-22 04:12:22","commitOrder":9,"curCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","date":"2019-03-22 04:12:22","endLine":1858,"groupId":"2275","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testRestore","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/eb/ef3fc4596a43b59efefcaf094105a8ae34ab5f.src","preCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1765,"status":"N"},{"authorDate":"2019-05-24 00:25:35","commitOrder":9,"curCode":"  public void testRestoreAfterPersistingSequences() throws Exception\n  {\n    records = generateSinglePartitionRecords(topic);\n    maxRowsPerSegment = 2;\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 5L),\n        ImmutableSet.of(0)\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (task1.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task1.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    \r\n    task1.getRunner().setEndOffsets(ImmutableMap.of(0, 5L), false);\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n    \r\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(5, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(4, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2008/P1D\", 1);\n    SegmentDescriptor desc3 = sd(task1, \"2009/P1D\", 0);\n    SegmentDescriptor desc4 = sd(task1, \"2009/P1D\", 1);\n    SegmentDescriptor desc5 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc6 = sd(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc7 = sd(task1, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","date":"2019-05-24 00:25:35","endLine":1938,"groupId":"11127","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testRestoreAfterPersistingSequences","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/78/3af47ca8bb71623d67c9415e3f7a2ce8d26cb8.src","preCode":"  public void testRestoreAfterPersistingSequences() throws Exception\n  {\n    if (!isIncrementalHandoffSupported) {\n      return;\n    }\n\n    records = generateSinglePartitionRecords(topic);\n    maxRowsPerSegment = 2;\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 5L),\n        ImmutableSet.of(0)\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (task1.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task1.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    \r\n    task1.getRunner().setEndOffsets(ImmutableMap.of(0, 5L), false);\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n    \r\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(5, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(4, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2008/P1D\", 1);\n    SegmentDescriptor desc3 = sd(task1, \"2009/P1D\", 0);\n    SegmentDescriptor desc4 = sd(task1, \"2009/P1D\", 1);\n    SegmentDescriptor desc5 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc6 = sd(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc7 = sd(task1, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1831,"status":"M"}],"commitId":"eff2be4f8f9d7aad0f01516f5425f3ebccaa006c","commitMessage":"@@@Remove LegacyKafkaIndexTaskRunner (#7735)\n\n","date":"2019-05-24 00:25:35","modifiedFileCount":"4","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-07-25 08:35:46","codes":[{"authorDate":"2019-07-25 08:35:46","commitOrder":10,"curCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(\"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(\"2011/P1D\", 0);\n    assertEqualsExceptVersion(ImmutableList.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    final List<SegmentDescriptor> publishedDescriptors = publishedDescriptors();\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(0)));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(1)));\n  }\n","date":"2019-07-25 08:35:46","endLine":1862,"groupId":"2275","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testRestore","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/87/f522eae18ccef1c23a5fd9ae035dbbe3410dfc.src","preCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2011/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", desc1));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", desc2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1768,"status":"M"},{"authorDate":"2019-07-25 08:35:46","commitOrder":10,"curCode":"  public void testRestoreAfterPersistingSequences() throws Exception\n  {\n    records = generateSinglePartitionRecords(topic);\n    maxRowsPerSegment = 2;\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 5L),\n        ImmutableSet.of(0)\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (task1.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task1.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    \r\n    task1.getRunner().setEndOffsets(ImmutableMap.of(0, 5L), false);\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n    \r\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(5, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(4, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(\"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(\"2008/P1D\", 1);\n    SegmentDescriptor desc3 = sd(\"2009/P1D\", 0);\n    SegmentDescriptor desc4 = sd(\"2009/P1D\", 1);\n    SegmentDescriptor desc5 = sd(\"2010/P1D\", 0);\n    SegmentDescriptor desc6 = sd(\"2011/P1D\", 0);\n    SegmentDescriptor desc7 = sd(\"2012/P1D\", 0);\n    assertEqualsExceptVersion(ImmutableList.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","date":"2019-07-25 08:35:46","endLine":1972,"groupId":"11127","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testRestoreAfterPersistingSequences","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/87/f522eae18ccef1c23a5fd9ae035dbbe3410dfc.src","preCode":"  public void testRestoreAfterPersistingSequences() throws Exception\n  {\n    records = generateSinglePartitionRecords(topic);\n    maxRowsPerSegment = 2;\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 5L),\n        ImmutableSet.of(0)\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (task1.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task1.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    \r\n    task1.getRunner().setEndOffsets(ImmutableMap.of(0, 5L), false);\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n    \r\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(5, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(4, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(task1, \"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(task1, \"2008/P1D\", 1);\n    SegmentDescriptor desc3 = sd(task1, \"2009/P1D\", 0);\n    SegmentDescriptor desc4 = sd(task1, \"2009/P1D\", 1);\n    SegmentDescriptor desc5 = sd(task1, \"2010/P1D\", 0);\n    SegmentDescriptor desc6 = sd(task1, \"2011/P1D\", 0);\n    SegmentDescriptor desc7 = sd(task1, \"2012/P1D\", 0);\n    Assert.assertEquals(ImmutableSet.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1865,"status":"M"}],"commitId":"db149462073d59e7563f0d3834e69d44a2bb4011","commitMessage":"@@@Add support minor compaction with segment locking (#7547)\n\n* Segment locking\n\n* Allow both timeChunk and segment lock in the same gruop\n\n* fix it test\n\n* Fix adding same chunk to atomicUpdateGroup\n\n* resolving todos\n\n* Fix segments to lock\n\n* fix segments to lock\n\n* fix kill task\n\n* resolving todos\n\n* resolving todos\n\n* fix teamcity\n\n* remove unused class\n\n* fix single map\n\n* resolving todos\n\n* fix build\n\n* fix SQLMetadataSegmentManager\n\n* fix findInputSegments\n\n* adding more tests\n\n* fixing task lock checks\n\n* add SegmentTransactionalOverwriteAction\n\n* changing publisher\n\n* fixing something\n\n* fix for perfect rollup\n\n* fix test\n\n* adjust package-lock.json\n\n* fix test\n\n* fix style\n\n* adding javadocs\n\n* remove unused classes\n\n* add more javadocs\n\n* unused import\n\n* fix test\n\n* fix test\n\n* Support forceTimeChunk context and force timeChunk lock for parallel index task if intervals are missing\n\n* fix travis\n\n* fix travis\n\n* unused import\n\n* spotbug\n\n* revert getMaxVersion\n\n* address comments\n\n* fix tc\n\n* add missing error handling\n\n* fix backward compatibility\n\n* unused import\n\n* Fix perf of versionedIntervalTimeline\n\n* fix timeline\n\n* fix tc\n\n* remove remaining todos\n\n* add comment for parallel index\n\n* fix javadoc and typos\n\n* typo\n\n* address comments\n","date":"2019-07-25 08:35:46","modifiedFileCount":"130","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-11-07 03:07:04","codes":[{"authorDate":"2019-11-07 03:07:04","commitOrder":11,"curCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","date":"2019-11-07 03:07:04","endLine":1769,"groupId":"2275","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testRestore","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c8/35bb95038567e2b9b3112cb98971f6d2418766.src","preCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(\"2010/P1D\", 0);\n    SegmentDescriptor desc2 = sd(\"2011/P1D\", 0);\n    assertEqualsExceptVersion(ImmutableList.of(desc1, desc2), publishedDescriptors());\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n\n    \r\n    final List<SegmentDescriptor> publishedDescriptors = publishedDescriptors();\n    Assert.assertEquals(ImmutableList.of(\"c\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(0)));\n    Assert.assertEquals(ImmutableList.of(\"d\", \"e\"), readSegmentColumn(\"dim1\", publishedDescriptors.get(1)));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1676,"status":"M"},{"authorDate":"2019-11-07 03:07:04","commitOrder":11,"curCode":"  public void testRestoreAfterPersistingSequences() throws Exception\n  {\n    records = generateSinglePartitionRecords(topic);\n    maxRowsPerSegment = 2;\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 5L),\n        ImmutableSet.of(0)\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (task1.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task1.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    \r\n    task1.getRunner().setEndOffsets(ImmutableMap.of(0, 5L), false);\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n    \r\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(5, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(4, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2008/P1D\", 0),\n            sdd(\"2008/P1D\", 1),\n            sdd(\"2009/P1D\", 0),\n            sdd(\"2009/P1D\", 1),\n            sdd(\"2010/P1D\", 0),\n            sdd(\"2011/P1D\", 0),\n            sdd(\"2012/P1D\", 0)\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","date":"2019-11-07 03:07:04","endLine":1883,"groupId":"11127","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testRestoreAfterPersistingSequences","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c8/35bb95038567e2b9b3112cb98971f6d2418766.src","preCode":"  public void testRestoreAfterPersistingSequences() throws Exception\n  {\n    records = generateSinglePartitionRecords(topic);\n    maxRowsPerSegment = 2;\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 5L),\n        ImmutableSet.of(0)\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (task1.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task1.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    \r\n    task1.getRunner().setEndOffsets(ImmutableMap.of(0, 5L), false);\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n    \r\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(5, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(4, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    SegmentDescriptor desc1 = sd(\"2008/P1D\", 0);\n    SegmentDescriptor desc2 = sd(\"2008/P1D\", 1);\n    SegmentDescriptor desc3 = sd(\"2009/P1D\", 0);\n    SegmentDescriptor desc4 = sd(\"2009/P1D\", 1);\n    SegmentDescriptor desc5 = sd(\"2010/P1D\", 0);\n    SegmentDescriptor desc6 = sd(\"2011/P1D\", 0);\n    SegmentDescriptor desc7 = sd(\"2012/P1D\", 0);\n    assertEqualsExceptVersion(\n        ImmutableList.of(desc1, desc2, desc3, desc4, desc5, desc6, desc7),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1772,"status":"M"}],"commitId":"5c0fc0a13ab4d259b430bf50b322f631504c4529","commitMessage":"@@@Fix ambiguity about IndexerSQLMetadataStorageCoordinator.getUsedSegmentsForInterval() returning only non-overshadowed or all used segments (#8564)\n\n* IndexerSQLMetadataStorageCoordinator.getTimelineForIntervalsWithHandle() don't fetch abutting intervals; simplify getUsedSegmentsForIntervals()\n\n* Add VersionedIntervalTimeline.findNonOvershadowedObjectsInInterval() method; Propagate the decision about whether only visible segmetns or visible and overshadowed segments should be returned from IndexerMetadataStorageCoordinator's methods to the user logic; Rename SegmentListUsedAction to RetrieveUsedSegmentsAction.  SegmetnListUnusedAction to RetrieveUnusedSegmentsAction.  and UsedSegmentLister to UsedSegmentsRetriever\n\n* Fix tests\n\n* More fixes\n\n* Add javadoc notes about returning Collection instead of Set. Add JacksonUtils.readValue() to reduce boilerplate code\n\n* Fix KinesisIndexTaskTest.  factor out common parts from KinesisIndexTaskTest and KafkaIndexTaskTest into SeekableStreamIndexTaskTestBase\n\n* More test fixes\n\n* More test fixes\n\n* Add a comment to VersionedIntervalTimelineTestBase\n\n* Fix tests\n\n* Set DataSegment.size(0) in more tests\n\n* Specify DataSegment.size(0) in more places in tests\n\n* Fix more tests\n\n* Fix DruidSchemaTest\n\n* Set DataSegment's size in more tests and benchmarks\n\n* Fix HdfsDataSegmentPusherTest\n\n* Doc changes addressing comments\n\n* Extended doc for visibility\n\n* Typo\n\n* Typo 2\n\n* Address comment\n","date":"2019-11-07 03:07:04","modifiedFileCount":"88","status":"M","submitter":"Roman Leventov"},{"authorTime":"2019-11-21 06:51:25","codes":[{"authorDate":"2019-11-21 06:51:25","commitOrder":12,"curCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(NEW_DATA_SCHEMA.getDataSource())\n    );\n  }\n","date":"2019-11-21 06:51:25","endLine":1844,"groupId":"17081","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testRestore","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e8/bde11469c6f5c09cd43b1c5d562e5b78e0f593.src","preCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1749,"status":"M"},{"authorDate":"2019-11-21 06:51:25","commitOrder":12,"curCode":"  public void testRestoreAfterPersistingSequences() throws Exception\n  {\n    records = generateSinglePartitionRecords(topic);\n    maxRowsPerSegment = 2;\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 5L),\n        ImmutableSet.of(0)\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (task1.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task1.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    \r\n    task1.getRunner().setEndOffsets(ImmutableMap.of(0, 5L), false);\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n    \r\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(5, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(4, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2008/P1D\", 0),\n            sdd(\"2008/P1D\", 1),\n            sdd(\"2009/P1D\", 0),\n            sdd(\"2009/P1D\", 1),\n            sdd(\"2010/P1D\", 0),\n            sdd(\"2011/P1D\", 0),\n            sdd(\"2012/P1D\", 0)\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L))),\n        metadataStorageCoordinator.getDataSourceMetadata(NEW_DATA_SCHEMA.getDataSource())\n    );\n  }\n","date":"2019-11-21 06:51:25","endLine":1960,"groupId":"16546","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testRestoreAfterPersistingSequences","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e8/bde11469c6f5c09cd43b1c5d562e5b78e0f593.src","preCode":"  public void testRestoreAfterPersistingSequences() throws Exception\n  {\n    records = generateSinglePartitionRecords(topic);\n    maxRowsPerSegment = 2;\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 5L),\n        ImmutableSet.of(0)\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (task1.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task1.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    \r\n    task1.getRunner().setEndOffsets(ImmutableMap.of(0, 5L), false);\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n    \r\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(5, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(4, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2008/P1D\", 0),\n            sdd(\"2008/P1D\", 1),\n            sdd(\"2009/P1D\", 0),\n            sdd(\"2009/P1D\", 1),\n            sdd(\"2010/P1D\", 0),\n            sdd(\"2011/P1D\", 0),\n            sdd(\"2012/P1D\", 0)\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L))),\n        metadataStorageCoordinator.getDataSourceMetadata(DATA_SCHEMA.getDataSource())\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1847,"status":"M"}],"commitId":"ac6d703814ccb5b258c586b63e0bc33d669e0f57","commitMessage":"@@@Support inputFormat and inputSource for sampler (#8901)\n\n* Support inputFormat and inputSource for sampler\n\n* Cleanup javadocs and names\n\n* fix style\n\n* fix timed shutoff input source reader\n\n* fix timed shutoff input source reader again\n\n* tidy up timed shutoff reader\n\n* unused imports\n\n* fix tc\n","date":"2019-11-21 06:51:25","modifiedFileCount":"66","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-01-28 03:24:29","codes":[{"authorDate":"2020-01-28 03:24:29","commitOrder":13,"curCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        newDataSchemaMetadata()\n    );\n  }\n","date":"2020-01-28 03:24:29","endLine":1847,"groupId":"102530","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testRestore","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d4/468bd39e032386fe654402d0006ecea542747e.src","preCode":"  public void testRestore() throws Exception\n  {\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (countEvents(task1) != 2) {\n      Thread.sleep(25);\n    }\n\n    Assert.assertEquals(2, countEvents(task1));\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 2L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L)),\n            kafkaServer.consumerProperties(),\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 4)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(2, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(1, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2010/P1D\", 0, ImmutableList.of(\"c\")),\n            sdd(\"2011/P1D\", 0, ImmutableList.of(\"d\", \"e\"))\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 6L))),\n        metadataStorageCoordinator.getDataSourceMetadata(NEW_DATA_SCHEMA.getDataSource())\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1752,"status":"M"},{"authorDate":"2020-01-28 03:24:29","commitOrder":13,"curCode":"  public void testRestoreAfterPersistingSequences() throws Exception\n  {\n    records = generateSinglePartitionRecords(topic);\n    maxRowsPerSegment = 2;\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 5L),\n        ImmutableSet.of(0)\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (task1.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task1.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    \r\n    task1.getRunner().setEndOffsets(ImmutableMap.of(0, 5L), false);\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n    \r\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(5, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(4, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2008/P1D\", 0),\n            sdd(\"2008/P1D\", 1),\n            sdd(\"2009/P1D\", 0),\n            sdd(\"2009/P1D\", 1),\n            sdd(\"2010/P1D\", 0),\n            sdd(\"2011/P1D\", 0),\n            sdd(\"2012/P1D\", 0)\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L))),\n        newDataSchemaMetadata()\n    );\n  }\n","date":"2020-01-28 03:24:29","endLine":1963,"groupId":"102530","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testRestoreAfterPersistingSequences","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d4/468bd39e032386fe654402d0006ecea542747e.src","preCode":"  public void testRestoreAfterPersistingSequences() throws Exception\n  {\n    records = generateSinglePartitionRecords(topic);\n    maxRowsPerSegment = 2;\n    Map<String, Object> consumerProps = kafkaServer.consumerProperties();\n    consumerProps.put(\"max.poll.records\", \"1\");\n\n    final KafkaIndexTask task1 = createTask(\n        null,\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final SeekableStreamStartSequenceNumbers<Integer, Long> checkpoint = new SeekableStreamStartSequenceNumbers<>(\n        topic,\n        ImmutableMap.of(0, 5L),\n        ImmutableSet.of(0)\n    );\n\n    final ListenableFuture<TaskStatus> future1 = runTask(task1);\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.limit(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    while (task1.getRunner().getStatus() != Status.PAUSED) {\n      Thread.sleep(10);\n    }\n    final Map<Integer, Long> currentOffsets = ImmutableMap.copyOf(task1.getRunner().getCurrentOffsets());\n    Assert.assertEquals(checkpoint.getPartitionSequenceNumberMap(), currentOffsets);\n    \r\n    task1.getRunner().setEndOffsets(ImmutableMap.of(0, 5L), false);\n\n    \r\n    task1.stopGracefully(toolboxFactory.build(task1).getConfig());\n    unlockAppenderatorBasePersistDirForTask(task1);\n\n    Assert.assertEquals(TaskState.SUCCESS, future1.get().getStatusCode());\n\n    \r\n    final KafkaIndexTask task2 = createTask(\n        task1.getId(),\n        new KafkaIndexTaskIOConfig(\n            0,\n            \"sequence0\",\n            new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L), ImmutableSet.of()),\n            new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L)),\n            consumerProps,\n            KafkaSupervisorIOConfig.DEFAULT_POLL_TIMEOUT_MILLIS,\n            true,\n            null,\n            null,\n            INPUT_FORMAT\n        )\n    );\n\n    final ListenableFuture<TaskStatus> future2 = runTask(task2);\n    \r\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : Iterables.skip(records, 5)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    \r\n    Assert.assertEquals(TaskState.SUCCESS, future2.get().getStatusCode());\n\n    \r\n    Assert.assertEquals(5, task1.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task1.getRunner().getRowIngestionMeters().getThrownAway());\n    Assert.assertEquals(4, task2.getRunner().getRowIngestionMeters().getProcessed());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getUnparseable());\n    Assert.assertEquals(0, task2.getRunner().getRowIngestionMeters().getThrownAway());\n\n    \r\n    assertEqualsExceptVersion(\n        ImmutableList.of(\n            sdd(\"2008/P1D\", 0),\n            sdd(\"2008/P1D\", 1),\n            sdd(\"2009/P1D\", 0),\n            sdd(\"2009/P1D\", 1),\n            sdd(\"2010/P1D\", 0),\n            sdd(\"2011/P1D\", 0),\n            sdd(\"2012/P1D\", 0)\n        ),\n        publishedDescriptors()\n    );\n    Assert.assertEquals(\n        new KafkaDataSourceMetadata(new SeekableStreamEndSequenceNumbers<>(topic, ImmutableMap.of(0, 10L))),\n        metadataStorageCoordinator.getDataSourceMetadata(NEW_DATA_SCHEMA.getDataSource())\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1850,"status":"M"}],"commitId":"b9186f8f9ff2ff52aceda42bc5f24ffd47a7d17e","commitMessage":"@@@Reconcile terminology and method naming to 'used/unused segments'; Rename MetadataSegmentManager to MetadataSegmentsManager (#7306)\n\n* Reconcile terminology and method naming to 'used/unused segments'; Don't use terms 'enable/disable data source'; Rename MetadataSegmentManager to MetadataSegments; Make REST API methods which mark segments as used/unused to return server error instead of an empty response in case of error\n\n* Fix brace\n\n* Import order\n\n* Rename withKillDataSourceWhitelist to withSpecificDataSourcesToKill\n\n* Fix tests\n\n* Fix tests by adding proper methods without interval parameters to IndexerMetadataStorageCoordinator instead of hacking with Intervals.ETERNITY\n\n* More aligned names of DruidCoordinatorHelpers.  rename several CoordinatorDynamicConfig parameters\n\n* Rename ClientCompactTaskQuery to ClientCompactionTaskQuery for consistency with CompactionTask; ClientCompactQueryTuningConfig to ClientCompactionTaskQueryTuningConfig\n\n* More variable and method renames\n\n* Rename MetadataSegments to SegmentsMetadata\n\n* Javadoc update\n\n* Simplify SegmentsMetadata.getUnusedSegmentIntervals().  more javadocs\n\n* Update Javadoc of VersionedIntervalTimeline.iterateAllObjects()\n\n* Reorder imports\n\n* Rename SegmentsMetadata.tryMark... methods to mark... and make them to return boolean and the numbers of segments changed and relay exceptions to callers\n\n* Complete merge\n\n* Add CollectionUtils.newTreeSet(); Refactor DruidCoordinatorRuntimeParams creation in tests\n\n* Remove MetadataSegmentManager\n\n* Rename millisLagSinceCoordinatorBecomesLeaderBeforeCanMarkAsUnusedOvershadowedSegments to leadingTimeMillisBeforeCanMarkAsUnusedOvershadowedSegments\n\n* Fix tests.  refactor DruidCluster creation in tests into DruidClusterBuilder\n\n* Fix inspections\n\n* Fix SQLMetadataSegmentManagerEmptyTest and rename it to SqlSegmentsMetadataEmptyTest\n\n* Rename SegmentsAndMetadata to SegmentsAndCommitMetadata to reduce the similarity with SegmentsMetadata; Rename some methods\n\n* Rename DruidCoordinatorHelper to CoordinatorDuty.  refactor DruidCoordinator\n\n* Unused import\n\n* Optimize imports\n\n* Rename IndexerSQLMetadataStorageCoordinator.getDataSourceMetadata() to retrieveDataSourceMetadata()\n\n* Unused import\n\n* Update terminology in datasource-view.tsx\n\n* Fix label in datasource-view.spec.tsx.snap\n\n* Fix lint errors in datasource-view.tsx\n\n* Doc improvements\n\n* Another attempt to please TSLint\n\n* Another attempt to please TSLint\n\n* Style fixes\n\n* Fix IndexerSQLMetadataStorageCoordinator.createUsedSegmentsSqlQueryForIntervals() (wrong merge)\n\n* Try to fix docs build issue\n\n* Javadoc and spelling fixes\n\n* Rename SegmentsMetadata to SegmentsMetadataManager.  address other comments\n\n* Address more comments\n","date":"2020-01-28 03:24:29","modifiedFileCount":"127","status":"M","submitter":"Roman Leventov"}]
