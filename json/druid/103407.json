[{"authorTime":"2019-03-15 05:28:33","codes":[{"authorDate":"2019-11-29 04:45:24","commitOrder":4,"curCode":"  public void configure(Binder binder)\n  {\n    \r\n    \r\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).toInstance(conf);\n  }\n","date":"2019-11-29 04:45:24","endLine":93,"groupId":"15630","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"configure","params":"(Binderbinder)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e5/7995bd26f4fa4b905461d32f537261b6160e2d.src","preCode":"  public void configure(Binder binder)\n  {\n    \r\n    \r\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).toInstance(conf);\n  }\n","realPath":"extensions-core/orc-extensions/src/main/java/org/apache/druid/data/input/orc/OrcExtensionsModule.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":60,"status":"B"},{"authorDate":"2019-03-15 05:28:33","commitOrder":4,"curCode":"  public void configure(Binder binder)\n  {\n    MapBinder.newMapBinder(binder, String.class, SearchableVersionedDataFinder.class)\n             .addBinding(SCHEME)\n             .to(HdfsFileTimestampVersionFinder.class)\n             .in(LazySingleton.class);\n\n    Binders.dataSegmentPusherBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentPusher.class).in(LazySingleton.class);\n    Binders.dataSegmentKillerBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentKiller.class).in(LazySingleton.class);\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).toInstance(conf);\n    JsonConfigProvider.bind(binder, \"druid.storage\", HdfsDataSegmentPusherConfig.class);\n\n    Binders.taskLogsBinder(binder).addBinding(\"hdfs\").to(HdfsTaskLogs.class);\n    JsonConfigProvider.bind(binder, \"druid.indexer.logs\", HdfsTaskLogsConfig.class);\n    binder.bind(HdfsTaskLogs.class).in(LazySingleton.class);\n    JsonConfigProvider.bind(binder, \"druid.hadoop.security.kerberos\", HdfsKerberosConfig.class);\n    binder.bind(HdfsStorageAuthentication.class).in(ManageLifecycle.class);\n    LifecycleModule.register(binder, HdfsStorageAuthentication.class);\n\n  }\n","date":"2019-03-15 05:28:33","endLine":132,"groupId":"15630","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"configure","params":"(Binderbinder)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/95/cd12f7a819e80c3a8c3307d0f631377816e6d8.src","preCode":"  public void configure(Binder binder)\n  {\n    MapBinder.newMapBinder(binder, String.class, SearchableVersionedDataFinder.class)\n             .addBinding(SCHEME)\n             .to(HdfsFileTimestampVersionFinder.class)\n             .in(LazySingleton.class);\n\n    Binders.dataSegmentPusherBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentPusher.class).in(LazySingleton.class);\n    Binders.dataSegmentKillerBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentKiller.class).in(LazySingleton.class);\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).toInstance(conf);\n    JsonConfigProvider.bind(binder, \"druid.storage\", HdfsDataSegmentPusherConfig.class);\n\n    Binders.taskLogsBinder(binder).addBinding(\"hdfs\").to(HdfsTaskLogs.class);\n    JsonConfigProvider.bind(binder, \"druid.indexer.logs\", HdfsTaskLogsConfig.class);\n    binder.bind(HdfsTaskLogs.class).in(LazySingleton.class);\n    JsonConfigProvider.bind(binder, \"druid.hadoop.security.kerberos\", HdfsKerberosConfig.class);\n    binder.bind(HdfsStorageAuthentication.class).in(ManageLifecycle.class);\n    LifecycleModule.register(binder, HdfsStorageAuthentication.class);\n\n  }\n","realPath":"extensions-core/hdfs-storage/src/main/java/org/apache/druid/storage/hdfs/HdfsStorageDruidModule.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":85,"status":"NB"}],"commitId":"86e8903523fc3d2d177963d2f95fd56441605102","commitMessage":"@@@Support orc format for native batch ingestion (#8950)\n\n* Support orc format for native batch ingestion\n\n* fix pom and remove wrong comment\n\n* fix unnecessary condition check\n\n* use flatMap back to handle exception properly\n\n* move exceptionThrowingIterator to intermediateRowParsingReader\n\n* runtime\n","date":"2019-11-29 04:45:24","modifiedFileCount":"5","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-12-12 09:30:44","codes":[{"authorDate":"2019-12-12 09:30:44","commitOrder":5,"curCode":"  public void configure(Binder binder)\n  {\n    \r\n    \r\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).annotatedWith(Orc.class).toInstance(conf);\n  }\n","date":"2019-12-12 09:30:44","endLine":94,"groupId":"15630","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"configure","params":"(Binderbinder)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/78/082cba5a82b28f407a3d4bea9224cfd1bfa76a.src","preCode":"  public void configure(Binder binder)\n  {\n    \r\n    \r\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).toInstance(conf);\n  }\n","realPath":"extensions-core/orc-extensions/src/main/java/org/apache/druid/data/input/orc/OrcExtensionsModule.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":61,"status":"M"},{"authorDate":"2019-12-12 09:30:44","commitOrder":5,"curCode":"  public void configure(Binder binder)\n  {\n    MapBinder.newMapBinder(binder, String.class, SearchableVersionedDataFinder.class)\n             .addBinding(SCHEME)\n             .to(HdfsFileTimestampVersionFinder.class)\n             .in(LazySingleton.class);\n\n    Binders.dataSegmentPusherBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentPusher.class).in(LazySingleton.class);\n    Binders.dataSegmentKillerBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentKiller.class).in(LazySingleton.class);\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).annotatedWith(Hdfs.class).toInstance(conf);\n    JsonConfigProvider.bind(binder, \"druid.storage\", HdfsDataSegmentPusherConfig.class);\n\n    Binders.taskLogsBinder(binder).addBinding(\"hdfs\").to(HdfsTaskLogs.class);\n    JsonConfigProvider.bind(binder, \"druid.indexer.logs\", HdfsTaskLogsConfig.class);\n    binder.bind(HdfsTaskLogs.class).in(LazySingleton.class);\n    JsonConfigProvider.bind(binder, \"druid.hadoop.security.kerberos\", HdfsKerberosConfig.class);\n    binder.bind(HdfsStorageAuthentication.class).in(ManageLifecycle.class);\n    LifecycleModule.register(binder, HdfsStorageAuthentication.class);\n  }\n","date":"2019-12-12 09:30:44","endLine":121,"groupId":"15630","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"configure","params":"(Binderbinder)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d4/242bcddcfc2345ec36f617231bea6134327d8b.src","preCode":"  public void configure(Binder binder)\n  {\n    MapBinder.newMapBinder(binder, String.class, SearchableVersionedDataFinder.class)\n             .addBinding(SCHEME)\n             .to(HdfsFileTimestampVersionFinder.class)\n             .in(LazySingleton.class);\n\n    Binders.dataSegmentPusherBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentPusher.class).in(LazySingleton.class);\n    Binders.dataSegmentKillerBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentKiller.class).in(LazySingleton.class);\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).toInstance(conf);\n    JsonConfigProvider.bind(binder, \"druid.storage\", HdfsDataSegmentPusherConfig.class);\n\n    Binders.taskLogsBinder(binder).addBinding(\"hdfs\").to(HdfsTaskLogs.class);\n    JsonConfigProvider.bind(binder, \"druid.indexer.logs\", HdfsTaskLogsConfig.class);\n    binder.bind(HdfsTaskLogs.class).in(LazySingleton.class);\n    JsonConfigProvider.bind(binder, \"druid.hadoop.security.kerberos\", HdfsKerberosConfig.class);\n    binder.bind(HdfsStorageAuthentication.class).in(ManageLifecycle.class);\n    LifecycleModule.register(binder, HdfsStorageAuthentication.class);\n\n  }\n","realPath":"extensions-core/hdfs-storage/src/main/java/org/apache/druid/storage/hdfs/HdfsStorageDruidModule.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":75,"status":"M"}],"commitId":"66056b282620e82fa464d79b0f76b1687b92530e","commitMessage":"@@@Using annotation to distinguish Hadoop Configuration in each module (#9013)\n\n* Multibinding for NodeRole\n\n* Fix endpoints\n\n* fix doc\n\n* fix test\n\n* Using annotation to distinguish Hadoop Configuration in each module\n","date":"2019-12-12 09:30:44","modifiedFileCount":"15","status":"M","submitter":"Jihoon Son"},{"authorTime":"2021-03-07 03:43:00","codes":[{"authorDate":"2019-12-12 09:30:44","commitOrder":6,"curCode":"  public void configure(Binder binder)\n  {\n    \r\n    \r\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).annotatedWith(Orc.class).toInstance(conf);\n  }\n","date":"2019-12-12 09:30:44","endLine":94,"groupId":"103407","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"configure","params":"(Binderbinder)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/78/082cba5a82b28f407a3d4bea9224cfd1bfa76a.src","preCode":"  public void configure(Binder binder)\n  {\n    \r\n    \r\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).annotatedWith(Orc.class).toInstance(conf);\n  }\n","realPath":"extensions-core/orc-extensions/src/main/java/org/apache/druid/data/input/orc/OrcExtensionsModule.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":61,"status":"N"},{"authorDate":"2021-03-07 03:43:00","commitOrder":6,"curCode":"  public void configure(Binder binder)\n  {\n    MapBinder.newMapBinder(binder, String.class, SearchableVersionedDataFinder.class)\n             .addBinding(SCHEME)\n             .to(HdfsFileTimestampVersionFinder.class)\n             .in(LazySingleton.class);\n\n    Binders.dataSegmentPusherBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentPusher.class).in(LazySingleton.class);\n    Binders.dataSegmentKillerBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentKiller.class).in(LazySingleton.class);\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).annotatedWith(Hdfs.class).toInstance(conf);\n    JsonConfigProvider.bind(binder, \"druid.storage\", HdfsDataSegmentPusherConfig.class);\n\n    Binders.taskLogsBinder(binder).addBinding(\"hdfs\").to(HdfsTaskLogs.class);\n    JsonConfigProvider.bind(binder, \"druid.indexer.logs\", HdfsTaskLogsConfig.class);\n    binder.bind(HdfsTaskLogs.class).in(LazySingleton.class);\n    JsonConfigProvider.bind(binder, \"druid.hadoop.security.kerberos\", HdfsKerberosConfig.class);\n    binder.bind(HdfsStorageAuthentication.class).in(ManageLifecycle.class);\n    LifecycleModule.register(binder, HdfsStorageAuthentication.class);\n\n    JsonConfigProvider.bind(binder, \"druid.ingestion.hdfs\", HdfsInputSourceConfig.class);\n  }\n","date":"2021-03-07 03:43:00","endLine":124,"groupId":"103407","id":6,"instanceNumber":2,"isCurCommit":1,"methodName":"configure","params":"(Binderbinder)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3c/a8e23535e1de5041a408c4bdfe489d103962af.src","preCode":"  public void configure(Binder binder)\n  {\n    MapBinder.newMapBinder(binder, String.class, SearchableVersionedDataFinder.class)\n             .addBinding(SCHEME)\n             .to(HdfsFileTimestampVersionFinder.class)\n             .in(LazySingleton.class);\n\n    Binders.dataSegmentPusherBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentPusher.class).in(LazySingleton.class);\n    Binders.dataSegmentKillerBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentKiller.class).in(LazySingleton.class);\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).annotatedWith(Hdfs.class).toInstance(conf);\n    JsonConfigProvider.bind(binder, \"druid.storage\", HdfsDataSegmentPusherConfig.class);\n\n    Binders.taskLogsBinder(binder).addBinding(\"hdfs\").to(HdfsTaskLogs.class);\n    JsonConfigProvider.bind(binder, \"druid.indexer.logs\", HdfsTaskLogsConfig.class);\n    binder.bind(HdfsTaskLogs.class).in(LazySingleton.class);\n    JsonConfigProvider.bind(binder, \"druid.hadoop.security.kerberos\", HdfsKerberosConfig.class);\n    binder.bind(HdfsStorageAuthentication.class).in(ManageLifecycle.class);\n    LifecycleModule.register(binder, HdfsStorageAuthentication.class);\n  }\n","realPath":"extensions-core/hdfs-storage/src/main/java/org/apache/druid/storage/hdfs/HdfsStorageDruidModule.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":76,"status":"M"}],"commitId":"9946306d4b2c16a7fc8bac97c9f4815ed4b46570","commitMessage":"@@@Add configurations for allowed protocols for HTTP and HDFS inputSources/firehoses (#10830)\n\n* Allow only HTTP and HTTPS protocols for the HTTP inputSource\n\n* rename\n\n* Update core/src/main/java/org/apache/druid/data/input/impl/HttpInputSource.java\n\nCo-authored-by: Abhishek Agarwal <1477457+abhishekagarwal87@users.noreply.github.com>\n\n* fix http firehose and update doc\n\n* HDFS inputSource\n\n* add configs for allowed protocols\n\n* fix checkstyle and doc\n\n* more checkstyle\n\n* remove stale doc\n\n* remove more doc\n\n* Apply doc suggestions from code review\n\nCo-authored-by: Charles Smith <38529548+techdocsmith@users.noreply.github.com>\n\n* update hdfs address in docs\n\n* fix test\n\nCo-authored-by: Abhishek Agarwal <1477457+abhishekagarwal87@users.noreply.github.com>\nCo-authored-by: Charles Smith <38529548+techdocsmith@users.noreply.github.com>","date":"2021-03-07 03:43:00","modifiedFileCount":"11","status":"M","submitter":"Jihoon Son"}]
