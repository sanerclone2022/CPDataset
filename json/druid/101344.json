[{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public Map<String, Object> getStats()\n  {\n    if (groupByJob == null) {\n      return null;\n    }\n\n    try {\n      Counters jobCounters = groupByJob.getCounters();\n\n      Map<String, Object> metrics = TaskMetricsUtils.makeIngestionRowMetrics(\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_PROCESSED_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_PROCESSED_WITH_ERRORS_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_UNPARSEABLE_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_THROWN_AWAY_COUNTER).getValue()\n      );\n\n      return metrics;\n    }\n    catch (IllegalStateException ise) {\n      log.debug(\"Couldn't get counters due to job state\");\n      return null;\n    }\n    catch (Exception e) {\n      log.debug(e, \"Encountered exception in getStats().\");\n      return null;\n    }\n  }\n","date":"2018-08-31 00:56:26","endLine":292,"groupId":"8054","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"getStats","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ec/1039dde39d149f472d5b81d34fe7b69ab06523.src","preCode":"  public Map<String, Object> getStats()\n  {\n    if (groupByJob == null) {\n      return null;\n    }\n\n    try {\n      Counters jobCounters = groupByJob.getCounters();\n\n      Map<String, Object> metrics = TaskMetricsUtils.makeIngestionRowMetrics(\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_PROCESSED_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_PROCESSED_WITH_ERRORS_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_UNPARSEABLE_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_THROWN_AWAY_COUNTER).getValue()\n      );\n\n      return metrics;\n    }\n    catch (IllegalStateException ise) {\n      log.debug(\"Couldn't get counters due to job state\");\n      return null;\n    }\n    catch (Exception e) {\n      log.debug(e, \"Encountered exception in getStats().\");\n      return null;\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":266,"status":"B"},{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public Map<String, Object> getStats()\n  {\n    if (job == null) {\n      return null;\n    }\n\n    try {\n      Counters jobCounters = job.getCounters();\n\n      Map<String, Object> metrics = TaskMetricsUtils.makeIngestionRowMetrics(\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_PROCESSED_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_PROCESSED_WITH_ERRORS_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_UNPARSEABLE_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_THROWN_AWAY_COUNTER).getValue()\n      );\n\n      return metrics;\n    }\n    catch (IllegalStateException ise) {\n      log.debug(\"Couldn't get counters due to job state\");\n      return null;\n    }\n    catch (Exception e) {\n      log.debug(e, \"Encountered exception in getStats().\");\n      return null;\n    }\n  }\n","date":"2018-08-31 00:56:26","endLine":257,"groupId":"8054","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"getStats","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ac/34c755ae95eafc859b6279045a110476e53493.src","preCode":"  public Map<String, Object> getStats()\n  {\n    if (job == null) {\n      return null;\n    }\n\n    try {\n      Counters jobCounters = job.getCounters();\n\n      Map<String, Object> metrics = TaskMetricsUtils.makeIngestionRowMetrics(\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_PROCESSED_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_PROCESSED_WITH_ERRORS_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_UNPARSEABLE_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_THROWN_AWAY_COUNTER).getValue()\n      );\n\n      return metrics;\n    }\n    catch (IllegalStateException ise) {\n      log.debug(\"Couldn't get counters due to job state\");\n      return null;\n    }\n    catch (Exception e) {\n      log.debug(e, \"Encountered exception in getStats().\");\n      return null;\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/IndexGeneratorJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":231,"status":"B"}],"commitId":"431d3d8497f9079857c3baa7ae7ab4cb44a22355","commitMessage":"@@@Rename io.druid to org.apache.druid. (#6266)\n\n* Rename io.druid to org.apache.druid.\n\n* Fix META-INF files and remove some benchmark results.\n\n* MonitorsConfig update for metrics package migration.\n\n* Reorder some dimensions in inner queries for some reason.\n\n* Fix protobuf tests.\n","date":"2018-08-31 00:56:26","modifiedFileCount":"5","status":"B","submitter":"Gian Merlino"},{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2019-09-21 04:59:18","commitOrder":2,"curCode":"  public Map<String, Object> getStats()\n  {\n    if (groupByJob == null) {\n      return null;\n    }\n\n    try {\n      Counters jobCounters = groupByJob.getCounters();\n\n      return TaskMetricsUtils.makeIngestionRowMetrics(\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_PROCESSED_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_PROCESSED_WITH_ERRORS_COUNTER)\n                     .getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_UNPARSEABLE_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_THROWN_AWAY_COUNTER).getValue()\n      );\n    }\n    catch (IllegalStateException ise) {\n      log.debug(\"Couldn't get counters due to job state\");\n      return null;\n    }\n    catch (Exception e) {\n      log.debug(e, \"Encountered exception in getStats().\");\n      return null;\n    }\n  }\n","date":"2019-09-21 04:59:18","endLine":317,"groupId":"101344","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"getStats","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/59/245a1f5a234fb64eca1cfad90221b9e18f3eb0.src","preCode":"  public Map<String, Object> getStats()\n  {\n    if (groupByJob == null) {\n      return null;\n    }\n\n    try {\n      Counters jobCounters = groupByJob.getCounters();\n\n      Map<String, Object> metrics = TaskMetricsUtils.makeIngestionRowMetrics(\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_PROCESSED_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_PROCESSED_WITH_ERRORS_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_UNPARSEABLE_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_THROWN_AWAY_COUNTER).getValue()\n      );\n\n      return metrics;\n    }\n    catch (IllegalStateException ise) {\n      log.debug(\"Couldn't get counters due to job state\");\n      return null;\n    }\n    catch (Exception e) {\n      log.debug(e, \"Encountered exception in getStats().\");\n      return null;\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":292,"status":"M"},{"authorDate":"2018-08-31 00:56:26","commitOrder":2,"curCode":"  public Map<String, Object> getStats()\n  {\n    if (job == null) {\n      return null;\n    }\n\n    try {\n      Counters jobCounters = job.getCounters();\n\n      Map<String, Object> metrics = TaskMetricsUtils.makeIngestionRowMetrics(\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_PROCESSED_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_PROCESSED_WITH_ERRORS_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_UNPARSEABLE_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_THROWN_AWAY_COUNTER).getValue()\n      );\n\n      return metrics;\n    }\n    catch (IllegalStateException ise) {\n      log.debug(\"Couldn't get counters due to job state\");\n      return null;\n    }\n    catch (Exception e) {\n      log.debug(e, \"Encountered exception in getStats().\");\n      return null;\n    }\n  }\n","date":"2018-08-31 00:56:26","endLine":257,"groupId":"101344","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"getStats","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ac/34c755ae95eafc859b6279045a110476e53493.src","preCode":"  public Map<String, Object> getStats()\n  {\n    if (job == null) {\n      return null;\n    }\n\n    try {\n      Counters jobCounters = job.getCounters();\n\n      Map<String, Object> metrics = TaskMetricsUtils.makeIngestionRowMetrics(\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_PROCESSED_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_PROCESSED_WITH_ERRORS_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_UNPARSEABLE_COUNTER).getValue(),\n          jobCounters.findCounter(HadoopDruidIndexerConfig.IndexJobCounters.ROWS_THROWN_AWAY_COUNTER).getValue()\n      );\n\n      return metrics;\n    }\n    catch (IllegalStateException ise) {\n      log.debug(\"Couldn't get counters due to job state\");\n      return null;\n    }\n    catch (Exception e) {\n      log.debug(e, \"Encountered exception in getStats().\");\n      return null;\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/IndexGeneratorJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":231,"status":"N"}],"commitId":"99b6eedab59fb15664567cb9a7a40afe6ed93a8a","commitMessage":"@@@Rename partition spec fields (#8507)\n\n* Rename partition spec fields\n\nRename partition spec fields to be consistent across the various types\n(hashed.  single_dim.  dynamic). Specifically.  use targetNumRowsPerSegment\nand maxRowsPerSegment in favor of targetPartitionSize and\nmaxSegmentSize. Consistent and clearer names are easier for users to\nunderstand and use.\n\nAlso fix various IntelliJ inspection warnings and doc spelling mistakes.\n\n* Fix test\n\n* Improve docs\n\n* Add targetRowsPerSegment to HashedPartitionsSpec\n","date":"2019-09-21 04:59:18","modifiedFileCount":"9","status":"M","submitter":"Chi Cao Minh"}]
