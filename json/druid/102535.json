[{"authorTime":"2019-02-19 03:50:08","codes":[{"authorDate":"2019-02-19 03:50:08","commitOrder":2,"curCode":"  private void insertData() throws ExecutionException, InterruptedException\n  {\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n  }\n","date":"2019-02-19 03:50:08","endLine":2218,"groupId":"9874","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"insertData","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3d/6308cb0a96b01947db7e17b51cab010dd61129.src","preCode":"  private void insertData() throws ExecutionException, InterruptedException\n  {\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2208,"status":"B"},{"authorDate":"2019-02-19 03:50:08","commitOrder":2,"curCode":"  public void testPollAfterMoreDataAdded() throws InterruptedException, ExecutionException\n  {\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : records.subList(0, 13)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), objectMapper);\n\n    recordSupplier.assign(partitions);\n    recordSupplier.seekToEarliest(partitions);\n\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n    for (int i = 0; polledRecords.size() != 13 && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : records.subList(13, 15)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n\n    for (int i = 0; polledRecords.size() != records.size() && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n    List<OrderedPartitionableRecord<Integer, Long>> initialRecords = createOrderedPartitionableRecords();\n\n    Assert.assertEquals(records.size(), polledRecords.size());\n    Assert.assertEquals(partitions, recordSupplier.getAssignment());\n\n    final int initialRecordsPartition0Size = initialRecords.stream()\n                                                           .filter(r -> r.getPartitionId().equals(0))\n                                                           .collect(Collectors.toSet())\n                                                           .size();\n    final int initialRecordsPartition1Size = initialRecords.stream()\n                                                           .filter(r -> r.getPartitionId().equals(1))\n                                                           .collect(Collectors.toSet())\n                                                           .size();\n\n    final int polledRecordsPartition0Size = polledRecords.stream()\n                                                         .filter(r -> r.getPartitionId().equals(0))\n                                                         .collect(Collectors.toSet())\n                                                         .size();\n    final int polledRecordsPartition1Size = polledRecords.stream()\n                                                         .filter(r -> r.getPartitionId().equals(1))\n                                                         .collect(Collectors.toSet())\n                                                         .size();\n\n    Assert.assertEquals(initialRecordsPartition0Size, polledRecordsPartition0Size);\n    Assert.assertEquals(initialRecordsPartition1Size, polledRecordsPartition1Size);\n\n    recordSupplier.close();\n  }\n","date":"2019-02-19 03:50:08","endLine":297,"groupId":"9874","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testPollAfterMoreDataAdded","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f9/44bf04610b33cb5d4e910b174cc1d9d834f984.src","preCode":"  public void testPollAfterMoreDataAdded() throws InterruptedException, ExecutionException\n  {\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : records.subList(0, 13)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), objectMapper);\n\n    recordSupplier.assign(partitions);\n    recordSupplier.seekToEarliest(partitions);\n\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n    for (int i = 0; polledRecords.size() != 13 && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : records.subList(13, 15)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n\n    for (int i = 0; polledRecords.size() != records.size() && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n    List<OrderedPartitionableRecord<Integer, Long>> initialRecords = createOrderedPartitionableRecords();\n\n    Assert.assertEquals(records.size(), polledRecords.size());\n    Assert.assertEquals(partitions, recordSupplier.getAssignment());\n\n    final int initialRecordsPartition0Size = initialRecords.stream()\n                                                           .filter(r -> r.getPartitionId().equals(0))\n                                                           .collect(Collectors.toSet())\n                                                           .size();\n    final int initialRecordsPartition1Size = initialRecords.stream()\n                                                           .filter(r -> r.getPartitionId().equals(1))\n                                                           .collect(Collectors.toSet())\n                                                           .size();\n\n    final int polledRecordsPartition0Size = polledRecords.stream()\n                                                         .filter(r -> r.getPartitionId().equals(0))\n                                                         .collect(Collectors.toSet())\n                                                         .size();\n    final int polledRecordsPartition1Size = polledRecords.stream()\n                                                         .filter(r -> r.getPartitionId().equals(1))\n                                                         .collect(Collectors.toSet())\n                                                         .size();\n\n    Assert.assertEquals(initialRecordsPartition0Size, polledRecordsPartition0Size);\n    Assert.assertEquals(initialRecordsPartition1Size, polledRecordsPartition1Size);\n\n    recordSupplier.close();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaRecordSupplierTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":224,"status":"MB"}],"commitId":"80a2ef7be46c4fc807ac2a74507b8ba8b6a44049","commitMessage":"@@@Support kafka transactional topics (#5404) (#6496)\n\n* Support kafka transactional topics\n\n* update kafka to version 2.0.0\n* Remove the skipOffsetGaps option since it's not used anymore\n* Adjust kafka consumer to use transactional semantics\n* Update tests\n\n* Remove unused import from test\n\n* Fix compilation\n\n* Invoke transaction api to fix a unit test\n\n* temporary modification of travis.yml for debugging\n\n* another attempt to get travis tasklogs\n\n* update kafka to 2.0.1 at all places\n\n* Remove druid-kafka-eight dependency from integration-tests.  remove the kafka firehose test and deprecate kafka-eight classes\n\n* Add deprecated in docs for kafka-eight and kafka-simple extensions\n\n* Remove skipOffsetGaps and code changes for transaction support\n\n* Fix indentation\n\n* remove skipOffsetGaps from kinesis\n\n* Add transaction api to KafkaRecordSupplierTest\n\n* Fix indent\n\n* Fix test\n\n* update kafka version to 2.1.0\n","date":"2019-02-19 03:50:08","modifiedFileCount":"25","status":"M","submitter":"Surekha"},{"authorTime":"2019-08-23 18:13:54","codes":[{"authorDate":"2019-02-19 03:50:08","commitOrder":3,"curCode":"  private void insertData() throws ExecutionException, InterruptedException\n  {\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n  }\n","date":"2019-02-19 03:50:08","endLine":2218,"groupId":"9874","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"insertData","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3d/6308cb0a96b01947db7e17b51cab010dd61129.src","preCode":"  private void insertData() throws ExecutionException, InterruptedException\n  {\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2208,"status":"N"},{"authorDate":"2019-08-23 18:13:54","commitOrder":3,"curCode":"  public void testPollAfterMoreDataAdded() throws InterruptedException, ExecutionException\n  {\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : records.subList(0, 13)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), OBJECT_MAPPER);\n\n    recordSupplier.assign(partitions);\n    recordSupplier.seekToEarliest(partitions);\n\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n    for (int i = 0; polledRecords.size() != 13 && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : records.subList(13, 15)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n\n    for (int i = 0; polledRecords.size() != records.size() && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n    List<OrderedPartitionableRecord<Integer, Long>> initialRecords = createOrderedPartitionableRecords();\n\n    Assert.assertEquals(records.size(), polledRecords.size());\n    Assert.assertEquals(partitions, recordSupplier.getAssignment());\n\n    final int initialRecordsPartition0Size = initialRecords.stream()\n                                                           .filter(r -> r.getPartitionId().equals(0))\n                                                           .collect(Collectors.toSet())\n                                                           .size();\n    final int initialRecordsPartition1Size = initialRecords.stream()\n                                                           .filter(r -> r.getPartitionId().equals(1))\n                                                           .collect(Collectors.toSet())\n                                                           .size();\n\n    final int polledRecordsPartition0Size = polledRecords.stream()\n                                                         .filter(r -> r.getPartitionId().equals(0))\n                                                         .collect(Collectors.toSet())\n                                                         .size();\n    final int polledRecordsPartition1Size = polledRecords.stream()\n                                                         .filter(r -> r.getPartitionId().equals(1))\n                                                         .collect(Collectors.toSet())\n                                                         .size();\n\n    Assert.assertEquals(initialRecordsPartition0Size, polledRecordsPartition0Size);\n    Assert.assertEquals(initialRecordsPartition1Size, polledRecordsPartition1Size);\n\n    recordSupplier.close();\n  }\n","date":"2019-08-23 18:13:54","endLine":387,"groupId":"9874","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testPollAfterMoreDataAdded","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c4/739abc4deada30f0fd7e54706b1b200f2a2135.src","preCode":"  public void testPollAfterMoreDataAdded() throws InterruptedException, ExecutionException\n  {\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : records.subList(0, 13)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), objectMapper);\n\n    recordSupplier.assign(partitions);\n    recordSupplier.seekToEarliest(partitions);\n\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n    for (int i = 0; polledRecords.size() != 13 && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : records.subList(13, 15)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n\n    for (int i = 0; polledRecords.size() != records.size() && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n    List<OrderedPartitionableRecord<Integer, Long>> initialRecords = createOrderedPartitionableRecords();\n\n    Assert.assertEquals(records.size(), polledRecords.size());\n    Assert.assertEquals(partitions, recordSupplier.getAssignment());\n\n    final int initialRecordsPartition0Size = initialRecords.stream()\n                                                           .filter(r -> r.getPartitionId().equals(0))\n                                                           .collect(Collectors.toSet())\n                                                           .size();\n    final int initialRecordsPartition1Size = initialRecords.stream()\n                                                           .filter(r -> r.getPartitionId().equals(1))\n                                                           .collect(Collectors.toSet())\n                                                           .size();\n\n    final int polledRecordsPartition0Size = polledRecords.stream()\n                                                         .filter(r -> r.getPartitionId().equals(0))\n                                                         .collect(Collectors.toSet())\n                                                         .size();\n    final int polledRecordsPartition1Size = polledRecords.stream()\n                                                         .filter(r -> r.getPartitionId().equals(1))\n                                                         .collect(Collectors.toSet())\n                                                         .size();\n\n    Assert.assertEquals(initialRecordsPartition0Size, polledRecordsPartition0Size);\n    Assert.assertEquals(initialRecordsPartition1Size, polledRecordsPartition1Size);\n\n    recordSupplier.close();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaRecordSupplierTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":314,"status":"M"}],"commitId":"33f0753a70361e7d345a488034f76a889f7c3682","commitMessage":"@@@Add Checkstyle for constant name static final (#8060)\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* merging with upstream\n\n* review-1\n\n* unknow changes\n\n* unknow changes\n\n* review-2\n\n* merging with master\n\n* review-2 1 changes\n\n* review changes-2 2\n\n* bug fix\n","date":"2019-08-23 18:13:54","modifiedFileCount":"298","status":"M","submitter":"SandishKumarHN"},{"authorTime":"2021-01-09 08:04:37","codes":[{"authorDate":"2021-01-09 08:04:37","commitOrder":4,"curCode":"  private void insertData(Iterable<ProducerRecord<byte[], byte[]>> records) throws ExecutionException, InterruptedException\n  {\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n  }\n","date":"2021-01-09 08:04:37","endLine":2637,"groupId":"102535","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"insertData","params":"(Iterable<ProducerRecord<byte[]@byte[]>>records)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3c/a1c0ba4a6726ee443d344ac7a3d28f439bac51.src","preCode":"  private void insertData() throws ExecutionException, InterruptedException\n  {\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2627,"status":"M"},{"authorDate":"2021-01-09 08:04:37","commitOrder":4,"curCode":"  public void testPollAfterMoreDataAdded() throws InterruptedException, ExecutionException\n  {\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : records.subList(0, 13)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), OBJECT_MAPPER);\n\n    recordSupplier.assign(partitions);\n    recordSupplier.seekToEarliest(partitions);\n\n    List<OrderedPartitionableRecord<Integer, Long, KafkaRecordEntity>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n    for (int i = 0; polledRecords.size() != 13 && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : records.subList(13, 15)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n\n    for (int i = 0; polledRecords.size() != records.size() && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n    List<OrderedPartitionableRecord<Integer, Long, KafkaRecordEntity>> initialRecords = createOrderedPartitionableRecords();\n\n    Assert.assertEquals(records.size(), polledRecords.size());\n    Assert.assertEquals(partitions, recordSupplier.getAssignment());\n\n    final int initialRecordsPartition0Size = initialRecords.stream()\n                                                           .filter(r -> r.getPartitionId().equals(0))\n                                                           .collect(Collectors.toSet())\n                                                           .size();\n    final int initialRecordsPartition1Size = initialRecords.stream()\n                                                           .filter(r -> r.getPartitionId().equals(1))\n                                                           .collect(Collectors.toSet())\n                                                           .size();\n\n    final int polledRecordsPartition0Size = polledRecords.stream()\n                                                         .filter(r -> r.getPartitionId().equals(0))\n                                                         .collect(Collectors.toSet())\n                                                         .size();\n    final int polledRecordsPartition1Size = polledRecords.stream()\n                                                         .filter(r -> r.getPartitionId().equals(1))\n                                                         .collect(Collectors.toSet())\n                                                         .size();\n\n    Assert.assertEquals(initialRecordsPartition0Size, polledRecordsPartition0Size);\n    Assert.assertEquals(initialRecordsPartition1Size, polledRecordsPartition1Size);\n\n    recordSupplier.close();\n  }\n","date":"2021-01-09 08:04:37","endLine":395,"groupId":"102535","id":6,"instanceNumber":2,"isCurCommit":1,"methodName":"testPollAfterMoreDataAdded","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/31/a3ac09044cbda1a7c927011350ba2556a751ad.src","preCode":"  public void testPollAfterMoreDataAdded() throws InterruptedException, ExecutionException\n  {\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : records.subList(0, 13)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), OBJECT_MAPPER);\n\n    recordSupplier.assign(partitions);\n    recordSupplier.seekToEarliest(partitions);\n\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n    for (int i = 0; polledRecords.size() != 13 && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      kafkaProducer.initTransactions();\n      kafkaProducer.beginTransaction();\n      for (ProducerRecord<byte[], byte[]> record : records.subList(13, 15)) {\n        kafkaProducer.send(record).get();\n      }\n      kafkaProducer.commitTransaction();\n    }\n\n\n    for (int i = 0; polledRecords.size() != records.size() && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n    List<OrderedPartitionableRecord<Integer, Long>> initialRecords = createOrderedPartitionableRecords();\n\n    Assert.assertEquals(records.size(), polledRecords.size());\n    Assert.assertEquals(partitions, recordSupplier.getAssignment());\n\n    final int initialRecordsPartition0Size = initialRecords.stream()\n                                                           .filter(r -> r.getPartitionId().equals(0))\n                                                           .collect(Collectors.toSet())\n                                                           .size();\n    final int initialRecordsPartition1Size = initialRecords.stream()\n                                                           .filter(r -> r.getPartitionId().equals(1))\n                                                           .collect(Collectors.toSet())\n                                                           .size();\n\n    final int polledRecordsPartition0Size = polledRecords.stream()\n                                                         .filter(r -> r.getPartitionId().equals(0))\n                                                         .collect(Collectors.toSet())\n                                                         .size();\n    final int polledRecordsPartition1Size = polledRecords.stream()\n                                                         .filter(r -> r.getPartitionId().equals(1))\n                                                         .collect(Collectors.toSet())\n                                                         .size();\n\n    Assert.assertEquals(initialRecordsPartition0Size, polledRecordsPartition0Size);\n    Assert.assertEquals(initialRecordsPartition1Size, polledRecordsPartition1Size);\n\n    recordSupplier.close();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaRecordSupplierTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":322,"status":"M"}],"commitId":"118b50195e5c2989e04e0f5290aa72cae114db39","commitMessage":"@@@Introduce KafkaRecordEntity to support Kafka headers in InputFormats (#10730)\n\nToday Kafka message support in streaming indexing tasks is limited to\nmessage values.  and does not provide a way to expose Kafka headers. \ntimestamps.  or keys.  which may be of interest to more specialized\nDruid input formats. For instance.  Kafka headers may be used to indicate\npayload format/encoding or additional metadata.  and timestamps are often\nomitted from values in Kafka streams applications.  since they are\nincluded in the record.\n\nThis change proposes to introduce KafkaRecordEntity as InputEntity. \nwhich would give input formats full access to the underlying Kafka record. \nincluding headers.  key.  timestamps. It would also open access to low-level\ninformation such as topic.  partition.  offset if needed.\n\nKafkaEntity is a subclass of ByteEntity for backwards compatibility with\nexisting input formats.  and to avoid introducing unnecessary complexity\nfor Kinesis indexing tasks.","date":"2021-01-09 08:04:37","modifiedFileCount":"30","status":"M","submitter":"Xavier L?aut?"}]
