[{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      final String workingPath = args[1];\n      final String segmentOutputPath = args[2];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withIOConfig(theSchema.getIOConfig().withSegmentOutputPath(segmentOutputPath))\n              .withTuningConfig(theSchema.getTuningConfig().withWorkingPath(workingPath))\n      );\n\n      job = new HadoopDruidDetermineConfigurationJob(config);\n\n      log.info(\"Starting a hadoop determine configuration job...\");\n      if (job.run()) {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(config.getSchema(), job.getStats(), null)\n        );\n      } else {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(null, job.getStats(), job.getErrorMessage())\n        );\n      }\n    }\n","date":"2018-08-31 00:56:26","endLine":567,"groupId":"11519","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"runTask","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/96/861a94c2adeda1d13155783a0f1fdf97c985d3.src","preCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      final String workingPath = args[1];\n      final String segmentOutputPath = args[2];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withIOConfig(theSchema.getIOConfig().withSegmentOutputPath(segmentOutputPath))\n              .withTuningConfig(theSchema.getTuningConfig().withWorkingPath(workingPath))\n      );\n\n      job = new HadoopDruidDetermineConfigurationJob(config);\n\n      log.info(\"Starting a hadoop determine configuration job...\");\n      if (job.run()) {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(config.getSchema(), job.getStats(), null)\n        );\n      } else {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(null, job.getStats(), job.getErrorMessage())\n        );\n      }\n    }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/HadoopIndexTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":538,"status":"B"},{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      String version = args[1];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withTuningConfig(theSchema.getTuningConfig().withVersion(version))\n      );\n\n      \r\n      \r\n      \r\n      final MetadataStorageUpdaterJobHandler maybeHandler;\n      if (config.isUpdaterJobSpecSet()) {\n        maybeHandler = injector.getInstance(MetadataStorageUpdaterJobHandler.class);\n      } else {\n        maybeHandler = null;\n      }\n      job = new HadoopDruidIndexerJob(config, maybeHandler);\n\n      log.info(\"Starting a hadoop index generator job...\");\n      try {\n        if (job.run()) {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  job.getPublishedSegments(),\n                  job.getStats(),\n                  null\n              )\n          );\n        } else {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  null,\n                  job.getStats(),\n                  job.getErrorMessage()\n              )\n          );\n        }\n      }\n      catch (Exception e) {\n        log.error(e, \"Encountered exception in HadoopIndexGeneratorInnerProcessing.\");\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopIndexGeneratorInnerProcessingStatus(\n                null,\n                job.getStats(),\n                e.getMessage()\n            )\n        );\n      }\n    }\n","date":"2018-08-31 00:56:26","endLine":640,"groupId":"17394","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"runTask","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/96/861a94c2adeda1d13155783a0f1fdf97c985d3.src","preCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      String version = args[1];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withTuningConfig(theSchema.getTuningConfig().withVersion(version))\n      );\n\n      \r\n      \r\n      \r\n      final MetadataStorageUpdaterJobHandler maybeHandler;\n      if (config.isUpdaterJobSpecSet()) {\n        maybeHandler = injector.getInstance(MetadataStorageUpdaterJobHandler.class);\n      } else {\n        maybeHandler = null;\n      }\n      job = new HadoopDruidIndexerJob(config, maybeHandler);\n\n      log.info(\"Starting a hadoop index generator job...\");\n      try {\n        if (job.run()) {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  job.getPublishedSegments(),\n                  job.getStats(),\n                  null\n              )\n          );\n        } else {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  null,\n                  job.getStats(),\n                  job.getErrorMessage()\n              )\n          );\n        }\n      }\n      catch (Exception e) {\n        log.error(e, \"Encountered exception in HadoopIndexGeneratorInnerProcessing.\");\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopIndexGeneratorInnerProcessingStatus(\n                null,\n                job.getStats(),\n                e.getMessage()\n            )\n        );\n      }\n    }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/HadoopIndexTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":584,"status":"B"}],"commitId":"431d3d8497f9079857c3baa7ae7ab4cb44a22355","commitMessage":"@@@Rename io.druid to org.apache.druid. (#6266)\n\n* Rename io.druid to org.apache.druid.\n\n* Fix META-INF files and remove some benchmark results.\n\n* MonitorsConfig update for metrics package migration.\n\n* Reorder some dimensions in inner queries for some reason.\n\n* Fix protobuf tests.\n","date":"2018-08-31 00:56:26","modifiedFileCount":"5","status":"B","submitter":"Gian Merlino"},{"authorTime":"2019-01-26 07:43:06","codes":[{"authorDate":"2019-01-26 07:43:06","commitOrder":2,"curCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      final String workingPath = args[1];\n      final String segmentOutputPath = args[2];\n      final String hadoopJobIdFile = args[3];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withIOConfig(theSchema.getIOConfig().withSegmentOutputPath(segmentOutputPath))\n              .withTuningConfig(theSchema.getTuningConfig().withWorkingPath(workingPath))\n      );\n\n      job = new HadoopDruidDetermineConfigurationJob(config);\n      job.setHadoopJobIdFile(hadoopJobIdFile);\n\n      log.info(\"Starting a hadoop determine configuration job...\");\n      if (job.run()) {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(config.getSchema(), job.getStats(), null)\n        );\n      } else {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(null, job.getStats(), job.getErrorMessage())\n        );\n      }\n    }\n","date":"2019-01-26 07:43:06","endLine":635,"groupId":"11519","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"runTask","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/39/8ed96a2f396ab2ddf1e9460cdc7ae6022bf1fb.src","preCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      final String workingPath = args[1];\n      final String segmentOutputPath = args[2];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withIOConfig(theSchema.getIOConfig().withSegmentOutputPath(segmentOutputPath))\n              .withTuningConfig(theSchema.getTuningConfig().withWorkingPath(workingPath))\n      );\n\n      job = new HadoopDruidDetermineConfigurationJob(config);\n\n      log.info(\"Starting a hadoop determine configuration job...\");\n      if (job.run()) {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(config.getSchema(), job.getStats(), null)\n        );\n      } else {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(null, job.getStats(), job.getErrorMessage())\n        );\n      }\n    }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/HadoopIndexTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":604,"status":"M"},{"authorDate":"2019-01-26 07:43:06","commitOrder":2,"curCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      String version = args[1];\n      final String hadoopJobIdFile = args[2];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withTuningConfig(theSchema.getTuningConfig().withVersion(version))\n      );\n\n      \r\n      \r\n      \r\n      final MetadataStorageUpdaterJobHandler maybeHandler;\n      if (config.isUpdaterJobSpecSet()) {\n        maybeHandler = injector.getInstance(MetadataStorageUpdaterJobHandler.class);\n      } else {\n        maybeHandler = null;\n      }\n      job = new HadoopDruidIndexerJob(config, maybeHandler);\n      job.setHadoopJobIdFile(hadoopJobIdFile);\n\n      log.info(\"Starting a hadoop index generator job...\");\n      try {\n        if (job.run()) {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  job.getPublishedSegments(),\n                  job.getStats(),\n                  null\n              )\n          );\n        } else {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  null,\n                  job.getStats(),\n                  job.getErrorMessage()\n              )\n          );\n        }\n      }\n      catch (Exception e) {\n        log.error(e, \"Encountered exception in HadoopIndexGeneratorInnerProcessing.\");\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopIndexGeneratorInnerProcessingStatus(\n                null,\n                job.getStats(),\n                e.getMessage()\n            )\n        );\n      }\n    }\n","date":"2019-01-26 07:43:06","endLine":710,"groupId":"2618","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"runTask","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/39/8ed96a2f396ab2ddf1e9460cdc7ae6022bf1fb.src","preCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      String version = args[1];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withTuningConfig(theSchema.getTuningConfig().withVersion(version))\n      );\n\n      \r\n      \r\n      \r\n      final MetadataStorageUpdaterJobHandler maybeHandler;\n      if (config.isUpdaterJobSpecSet()) {\n        maybeHandler = injector.getInstance(MetadataStorageUpdaterJobHandler.class);\n      } else {\n        maybeHandler = null;\n      }\n      job = new HadoopDruidIndexerJob(config, maybeHandler);\n\n      log.info(\"Starting a hadoop index generator job...\");\n      try {\n        if (job.run()) {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  job.getPublishedSegments(),\n                  job.getStats(),\n                  null\n              )\n          );\n        } else {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  null,\n                  job.getStats(),\n                  job.getErrorMessage()\n              )\n          );\n        }\n      }\n      catch (Exception e) {\n        log.error(e, \"Encountered exception in HadoopIndexGeneratorInnerProcessing.\");\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopIndexGeneratorInnerProcessingStatus(\n                null,\n                job.getStats(),\n                e.getMessage()\n            )\n        );\n      }\n    }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/HadoopIndexTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":652,"status":"M"}],"commitId":"8492d94f599da1f7851add2a0e7500515abd881d","commitMessage":"@@@Kill Hadoop MR task on kill of Hadoop ingestion task  (#6828)\n\n* KillTask from overlord UI now makes sure that it terminates the underlying MR job.  thus saving unnecessary compute\n\nRun in jobby is now split into 2\n 1. submitAndGetHadoopJobId followed by 2. run\n  submitAndGetHadoopJobId is responsible for submitting the job and returning the jobId as a string.  run monitors this job for completion\n\nJobHelper writes this jobId in the path provided by HadoopIndexTask which in turn is provided by the ForkingTaskRunner\n\nHadoopIndexTask reads this path when kill task is clicked to get hte jobId and fire the kill command via the yarn api. This is taken care in the stopGracefully method which is called in SingleTaskBackgroundRunner. Have enabled `canRestore` method to return `true` for HadoopIndexTask in order for the stopGracefully method to be called\n\nHadoop*Job files have been changed to incorporate the changes to jobby\n\n* Addressing PR comments\n\n* Addressing PR comments - Fix taskDir\n\n* Addressing PR comments - For changing the contract of Task.stopGracefully()\n`SingleTaskBackgroundRunner` calls stopGracefully in stop() and then checks for canRestore condition to return the status of the task\n\n* Addressing PR comments\n 1. Formatting\n 2. Removing `submitAndGetHadoopJobId` from `Jobby` and calling writeJobIdToFile in the job itself\n\n* Addressing PR comments\n 1. POM change. Moving hadoop dependency to indexing-hadoop\n\n* Addressing PR comments\n 1. stopGracefully now accepts TaskConfig as a param\n     Handling isRestoreOnRestart in stopGracefully for `AppenderatorDriverRealtimeIndexTask.  RealtimeIndexTask.  SeekableStreamIndexTask`\n     Changing tests to make TaskConfig param isRestoreOnRestart to true\n","date":"2019-01-26 07:43:06","modifiedFileCount":"20","status":"M","submitter":"Ankit Kothari"},{"authorTime":"2019-08-23 18:13:54","codes":[{"authorDate":"2019-01-26 07:43:06","commitOrder":3,"curCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      final String workingPath = args[1];\n      final String segmentOutputPath = args[2];\n      final String hadoopJobIdFile = args[3];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withIOConfig(theSchema.getIOConfig().withSegmentOutputPath(segmentOutputPath))\n              .withTuningConfig(theSchema.getTuningConfig().withWorkingPath(workingPath))\n      );\n\n      job = new HadoopDruidDetermineConfigurationJob(config);\n      job.setHadoopJobIdFile(hadoopJobIdFile);\n\n      log.info(\"Starting a hadoop determine configuration job...\");\n      if (job.run()) {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(config.getSchema(), job.getStats(), null)\n        );\n      } else {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(null, job.getStats(), job.getErrorMessage())\n        );\n      }\n    }\n","date":"2019-01-26 07:43:06","endLine":635,"groupId":"11519","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"runTask","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/39/8ed96a2f396ab2ddf1e9460cdc7ae6022bf1fb.src","preCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      final String workingPath = args[1];\n      final String segmentOutputPath = args[2];\n      final String hadoopJobIdFile = args[3];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withIOConfig(theSchema.getIOConfig().withSegmentOutputPath(segmentOutputPath))\n              .withTuningConfig(theSchema.getTuningConfig().withWorkingPath(workingPath))\n      );\n\n      job = new HadoopDruidDetermineConfigurationJob(config);\n      job.setHadoopJobIdFile(hadoopJobIdFile);\n\n      log.info(\"Starting a hadoop determine configuration job...\");\n      if (job.run()) {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(config.getSchema(), job.getStats(), null)\n        );\n      } else {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(null, job.getStats(), job.getErrorMessage())\n        );\n      }\n    }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/HadoopIndexTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":604,"status":"N"},{"authorDate":"2019-08-23 18:13:54","commitOrder":3,"curCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      String version = args[1];\n      final String hadoopJobIdFile = args[2];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withTuningConfig(theSchema.getTuningConfig().withVersion(version))\n      );\n\n      \r\n      \r\n      \r\n      final MetadataStorageUpdaterJobHandler maybeHandler;\n      if (config.isUpdaterJobSpecSet()) {\n        maybeHandler = INJECTOR.getInstance(MetadataStorageUpdaterJobHandler.class);\n      } else {\n        maybeHandler = null;\n      }\n      job = new HadoopDruidIndexerJob(config, maybeHandler);\n      job.setHadoopJobIdFile(hadoopJobIdFile);\n\n      log.info(\"Starting a hadoop index generator job...\");\n      try {\n        if (job.run()) {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  job.getPublishedSegments(),\n                  job.getStats(),\n                  null\n              )\n          );\n        } else {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  null,\n                  job.getStats(),\n                  job.getErrorMessage()\n              )\n          );\n        }\n      }\n      catch (Exception e) {\n        log.error(e, \"Encountered exception in HadoopIndexGeneratorInnerProcessing.\");\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopIndexGeneratorInnerProcessingStatus(\n                null,\n                job.getStats(),\n                e.getMessage()\n            )\n        );\n      }\n    }\n","date":"2019-08-23 18:13:54","endLine":743,"groupId":"2618","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"runTask","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/6c/b02888eaadc19187c70d005439570675721cfc.src","preCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      String version = args[1];\n      final String hadoopJobIdFile = args[2];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withTuningConfig(theSchema.getTuningConfig().withVersion(version))\n      );\n\n      \r\n      \r\n      \r\n      final MetadataStorageUpdaterJobHandler maybeHandler;\n      if (config.isUpdaterJobSpecSet()) {\n        maybeHandler = injector.getInstance(MetadataStorageUpdaterJobHandler.class);\n      } else {\n        maybeHandler = null;\n      }\n      job = new HadoopDruidIndexerJob(config, maybeHandler);\n      job.setHadoopJobIdFile(hadoopJobIdFile);\n\n      log.info(\"Starting a hadoop index generator job...\");\n      try {\n        if (job.run()) {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  job.getPublishedSegments(),\n                  job.getStats(),\n                  null\n              )\n          );\n        } else {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  null,\n                  job.getStats(),\n                  job.getErrorMessage()\n              )\n          );\n        }\n      }\n      catch (Exception e) {\n        log.error(e, \"Encountered exception in HadoopIndexGeneratorInnerProcessing.\");\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopIndexGeneratorInnerProcessingStatus(\n                null,\n                job.getStats(),\n                e.getMessage()\n            )\n        );\n      }\n    }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/HadoopIndexTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":685,"status":"M"}],"commitId":"33f0753a70361e7d345a488034f76a889f7c3682","commitMessage":"@@@Add Checkstyle for constant name static final (#8060)\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* merging with upstream\n\n* review-1\n\n* unknow changes\n\n* unknow changes\n\n* review-2\n\n* merging with master\n\n* review-2 1 changes\n\n* review changes-2 2\n\n* bug fix\n","date":"2019-08-23 18:13:54","modifiedFileCount":"298","status":"M","submitter":"SandishKumarHN"},{"authorTime":"2021-04-22 03:24:31","codes":[{"authorDate":"2019-01-26 07:43:06","commitOrder":4,"curCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      final String workingPath = args[1];\n      final String segmentOutputPath = args[2];\n      final String hadoopJobIdFile = args[3];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withIOConfig(theSchema.getIOConfig().withSegmentOutputPath(segmentOutputPath))\n              .withTuningConfig(theSchema.getTuningConfig().withWorkingPath(workingPath))\n      );\n\n      job = new HadoopDruidDetermineConfigurationJob(config);\n      job.setHadoopJobIdFile(hadoopJobIdFile);\n\n      log.info(\"Starting a hadoop determine configuration job...\");\n      if (job.run()) {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(config.getSchema(), job.getStats(), null)\n        );\n      } else {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(null, job.getStats(), job.getErrorMessage())\n        );\n      }\n    }\n","date":"2019-01-26 07:43:06","endLine":635,"groupId":"11519","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"runTask","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/39/8ed96a2f396ab2ddf1e9460cdc7ae6022bf1fb.src","preCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      final String workingPath = args[1];\n      final String segmentOutputPath = args[2];\n      final String hadoopJobIdFile = args[3];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withIOConfig(theSchema.getIOConfig().withSegmentOutputPath(segmentOutputPath))\n              .withTuningConfig(theSchema.getTuningConfig().withWorkingPath(workingPath))\n      );\n\n      job = new HadoopDruidDetermineConfigurationJob(config);\n      job.setHadoopJobIdFile(hadoopJobIdFile);\n\n      log.info(\"Starting a hadoop determine configuration job...\");\n      if (job.run()) {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(config.getSchema(), job.getStats(), null)\n        );\n      } else {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(null, job.getStats(), job.getErrorMessage())\n        );\n      }\n    }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/HadoopIndexTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":604,"status":"N"},{"authorDate":"2021-04-22 03:24:31","commitOrder":4,"curCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      String version = args[1];\n      final String hadoopJobIdFile = args[2];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withTuningConfig(theSchema.getTuningConfig().withVersion(version))\n      );\n\n      \r\n      \r\n      \r\n      final MetadataStorageUpdaterJobHandler maybeHandler;\n      if (config.isUpdaterJobSpecSet()) {\n        maybeHandler = INJECTOR.getInstance(MetadataStorageUpdaterJobHandler.class);\n      } else {\n        maybeHandler = null;\n      }\n      job = new HadoopDruidIndexerJob(config, maybeHandler);\n      job.setHadoopJobIdFile(hadoopJobIdFile);\n\n      log.info(\"Starting a hadoop index generator job...\");\n      try {\n        if (job.run()) {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  job.getPublishedSegmentAndIndexZipFilePaths(),\n                  job.getStats(),\n                  null\n              )\n          );\n        } else {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  null,\n                  job.getStats(),\n                  job.getErrorMessage()\n              )\n          );\n        }\n      }\n      catch (Exception e) {\n        log.error(e, \"Encountered exception in HadoopIndexGeneratorInnerProcessing.\");\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopIndexGeneratorInnerProcessingStatus(\n                null,\n                job.getStats(),\n                e.getMessage()\n            )\n        );\n      }\n    }\n","date":"2021-04-22 03:24:31","endLine":871,"groupId":"9413","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"runTask","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/37/ffb4cd7905d336bec4d2c9bf184ea0220a4153.src","preCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      String version = args[1];\n      final String hadoopJobIdFile = args[2];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withTuningConfig(theSchema.getTuningConfig().withVersion(version))\n      );\n\n      \r\n      \r\n      \r\n      final MetadataStorageUpdaterJobHandler maybeHandler;\n      if (config.isUpdaterJobSpecSet()) {\n        maybeHandler = INJECTOR.getInstance(MetadataStorageUpdaterJobHandler.class);\n      } else {\n        maybeHandler = null;\n      }\n      job = new HadoopDruidIndexerJob(config, maybeHandler);\n      job.setHadoopJobIdFile(hadoopJobIdFile);\n\n      log.info(\"Starting a hadoop index generator job...\");\n      try {\n        if (job.run()) {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  job.getPublishedSegments(),\n                  job.getStats(),\n                  null\n              )\n          );\n        } else {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  null,\n                  job.getStats(),\n                  job.getErrorMessage()\n              )\n          );\n        }\n      }\n      catch (Exception e) {\n        log.error(e, \"Encountered exception in HadoopIndexGeneratorInnerProcessing.\");\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopIndexGeneratorInnerProcessingStatus(\n                null,\n                job.getStats(),\n                e.getMessage()\n            )\n        );\n      }\n    }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/HadoopIndexTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":813,"status":"M"}],"commitId":"a2892d9c40793027ba8a8977c85b3de4a949e11c","commitMessage":"@@@Adjust HadoopIndexTask temp segment renaming to avoid potential race conditions (#11075)\n\n* Do stuff\n\n* Do more stuff\n\n* * Do more stuff\n\n* * Do more stuff\n\n* * working\n\n* * cleanup\n\n* * more cleanup\n\n* * more cleanup\n\n* * add license header\n\n* * Add unit tests\n\n* * add java docs\n\n* * add more unit tests\n\n* * Cleanup test\n\n* * Move removing of workingPath to index task rather than in hadoop job.\n\n* * Address review comments\n\n* * remove unused import\n\n* * Address review comments\n\n* Do not overwrite segment descriptor for segment if it already exists.\n\n* * add comments to FileSystemHelper class\n\n* * fix local hadoop integration test","date":"2021-04-22 03:24:31","modifiedFileCount":"9","status":"M","submitter":"zachjsh"},{"authorTime":"2021-04-23 06:33:27","codes":[{"authorDate":"2019-01-26 07:43:06","commitOrder":5,"curCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      final String workingPath = args[1];\n      final String segmentOutputPath = args[2];\n      final String hadoopJobIdFile = args[3];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withIOConfig(theSchema.getIOConfig().withSegmentOutputPath(segmentOutputPath))\n              .withTuningConfig(theSchema.getTuningConfig().withWorkingPath(workingPath))\n      );\n\n      job = new HadoopDruidDetermineConfigurationJob(config);\n      job.setHadoopJobIdFile(hadoopJobIdFile);\n\n      log.info(\"Starting a hadoop determine configuration job...\");\n      if (job.run()) {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(config.getSchema(), job.getStats(), null)\n        );\n      } else {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(null, job.getStats(), job.getErrorMessage())\n        );\n      }\n    }\n","date":"2019-01-26 07:43:06","endLine":635,"groupId":"11519","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"runTask","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/39/8ed96a2f396ab2ddf1e9460cdc7ae6022bf1fb.src","preCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      final String workingPath = args[1];\n      final String segmentOutputPath = args[2];\n      final String hadoopJobIdFile = args[3];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withIOConfig(theSchema.getIOConfig().withSegmentOutputPath(segmentOutputPath))\n              .withTuningConfig(theSchema.getTuningConfig().withWorkingPath(workingPath))\n      );\n\n      job = new HadoopDruidDetermineConfigurationJob(config);\n      job.setHadoopJobIdFile(hadoopJobIdFile);\n\n      log.info(\"Starting a hadoop determine configuration job...\");\n      if (job.run()) {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(config.getSchema(), job.getStats(), null)\n        );\n      } else {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(null, job.getStats(), job.getErrorMessage())\n        );\n      }\n    }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/HadoopIndexTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":604,"status":"N"},{"authorDate":"2021-04-23 06:33:27","commitOrder":5,"curCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      String version = args[1];\n      final String hadoopJobIdFile = args[2];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withTuningConfig(theSchema.getTuningConfig().withVersion(version))\n      );\n\n      \r\n      \r\n      \r\n      final MetadataStorageUpdaterJobHandler maybeHandler;\n      if (config.isUpdaterJobSpecSet()) {\n        maybeHandler = INJECTOR.getInstance(MetadataStorageUpdaterJobHandler.class);\n      } else {\n        maybeHandler = null;\n      }\n      job = new HadoopDruidIndexerJob(config, maybeHandler);\n      job.setHadoopJobIdFile(hadoopJobIdFile);\n\n      log.info(\"Starting a hadoop index generator job...\");\n      try {\n        if (job.run()) {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  job.getPublishedSegments(),\n                  job.getStats(),\n                  null\n              )\n          );\n        } else {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  null,\n                  job.getStats(),\n                  job.getErrorMessage()\n              )\n          );\n        }\n      }\n      catch (Exception e) {\n        log.error(e, \"Encountered exception in HadoopIndexGeneratorInnerProcessing.\");\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopIndexGeneratorInnerProcessingStatus(\n                null,\n                job.getStats(),\n                e.getMessage()\n            )\n        );\n      }\n    }\n","date":"2021-04-23 06:33:27","endLine":750,"groupId":"2618","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"runTask","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b6/6ae471136c894ec22089517976186ca36c17b8.src","preCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      String version = args[1];\n      final String hadoopJobIdFile = args[2];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withTuningConfig(theSchema.getTuningConfig().withVersion(version))\n      );\n\n      \r\n      \r\n      \r\n      final MetadataStorageUpdaterJobHandler maybeHandler;\n      if (config.isUpdaterJobSpecSet()) {\n        maybeHandler = INJECTOR.getInstance(MetadataStorageUpdaterJobHandler.class);\n      } else {\n        maybeHandler = null;\n      }\n      job = new HadoopDruidIndexerJob(config, maybeHandler);\n      job.setHadoopJobIdFile(hadoopJobIdFile);\n\n      log.info(\"Starting a hadoop index generator job...\");\n      try {\n        if (job.run()) {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  job.getPublishedSegmentAndIndexZipFilePaths(),\n                  job.getStats(),\n                  null\n              )\n          );\n        } else {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  null,\n                  job.getStats(),\n                  job.getErrorMessage()\n              )\n          );\n        }\n      }\n      catch (Exception e) {\n        log.error(e, \"Encountered exception in HadoopIndexGeneratorInnerProcessing.\");\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopIndexGeneratorInnerProcessingStatus(\n                null,\n                job.getStats(),\n                e.getMessage()\n            )\n        );\n      }\n    }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/HadoopIndexTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":692,"status":"M"}],"commitId":"49a9c3ffb7b2da3401696d583bc2cd52e83f77bf","commitMessage":"@@@Revert \"Adjust HadoopIndexTask temp segment renaming to avoid potential race conditions (#11075)\" (#11151)\n\nThis reverts commit a2892d9c40793027ba8a8977c85b3de4a949e11c.","date":"2021-04-23 06:33:27","modifiedFileCount":"9","status":"M","submitter":"Jonathan Wei"},{"authorTime":"2021-05-05 08:22:18","codes":[{"authorDate":"2019-01-26 07:43:06","commitOrder":6,"curCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      final String workingPath = args[1];\n      final String segmentOutputPath = args[2];\n      final String hadoopJobIdFile = args[3];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withIOConfig(theSchema.getIOConfig().withSegmentOutputPath(segmentOutputPath))\n              .withTuningConfig(theSchema.getTuningConfig().withWorkingPath(workingPath))\n      );\n\n      job = new HadoopDruidDetermineConfigurationJob(config);\n      job.setHadoopJobIdFile(hadoopJobIdFile);\n\n      log.info(\"Starting a hadoop determine configuration job...\");\n      if (job.run()) {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(config.getSchema(), job.getStats(), null)\n        );\n      } else {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(null, job.getStats(), job.getErrorMessage())\n        );\n      }\n    }\n","date":"2019-01-26 07:43:06","endLine":635,"groupId":"114810","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"runTask","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/39/8ed96a2f396ab2ddf1e9460cdc7ae6022bf1fb.src","preCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      final String workingPath = args[1];\n      final String segmentOutputPath = args[2];\n      final String hadoopJobIdFile = args[3];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withIOConfig(theSchema.getIOConfig().withSegmentOutputPath(segmentOutputPath))\n              .withTuningConfig(theSchema.getTuningConfig().withWorkingPath(workingPath))\n      );\n\n      job = new HadoopDruidDetermineConfigurationJob(config);\n      job.setHadoopJobIdFile(hadoopJobIdFile);\n\n      log.info(\"Starting a hadoop determine configuration job...\");\n      if (job.run()) {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(config.getSchema(), job.getStats(), null)\n        );\n      } else {\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopDetermineConfigInnerProcessingStatus(null, job.getStats(), job.getErrorMessage())\n        );\n      }\n    }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/HadoopIndexTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":604,"status":"N"},{"authorDate":"2021-05-05 08:22:18","commitOrder":6,"curCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      String version = args[1];\n      final String hadoopJobIdFile = args[2];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withTuningConfig(theSchema.getTuningConfig().withVersion(version))\n      );\n\n      \r\n      \r\n      \r\n      final MetadataStorageUpdaterJobHandler maybeHandler;\n      if (config.isUpdaterJobSpecSet()) {\n        maybeHandler = INJECTOR.getInstance(MetadataStorageUpdaterJobHandler.class);\n      } else {\n        maybeHandler = null;\n      }\n      job = new HadoopDruidIndexerJob(config, maybeHandler);\n      job.setHadoopJobIdFile(hadoopJobIdFile);\n\n      log.info(\"Starting a hadoop index generator job...\");\n      try {\n        if (job.run()) {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  job.getPublishedSegmentAndIndexZipFilePaths(),\n                  job.getStats(),\n                  null\n              )\n          );\n        } else {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  null,\n                  job.getStats(),\n                  job.getErrorMessage()\n              )\n          );\n        }\n      }\n      catch (Exception e) {\n        log.error(e, \"Encountered exception in HadoopIndexGeneratorInnerProcessing.\");\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopIndexGeneratorInnerProcessingStatus(\n                null,\n                job.getStats(),\n                e.getMessage()\n            )\n        );\n      }\n    }\n","date":"2021-05-05 08:22:18","endLine":871,"groupId":"114810","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"runTask","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/37/ffb4cd7905d336bec4d2c9bf184ea0220a4153.src","preCode":"    public String runTask(String[] args) throws Exception\n    {\n      final String schema = args[0];\n      String version = args[1];\n      final String hadoopJobIdFile = args[2];\n\n      final HadoopIngestionSpec theSchema = HadoopDruidIndexerConfig.JSON_MAPPER\n          .readValue(\n              schema,\n              HadoopIngestionSpec.class\n          );\n      final HadoopDruidIndexerConfig config = HadoopDruidIndexerConfig.fromSpec(\n          theSchema\n              .withTuningConfig(theSchema.getTuningConfig().withVersion(version))\n      );\n\n      \r\n      \r\n      \r\n      final MetadataStorageUpdaterJobHandler maybeHandler;\n      if (config.isUpdaterJobSpecSet()) {\n        maybeHandler = INJECTOR.getInstance(MetadataStorageUpdaterJobHandler.class);\n      } else {\n        maybeHandler = null;\n      }\n      job = new HadoopDruidIndexerJob(config, maybeHandler);\n      job.setHadoopJobIdFile(hadoopJobIdFile);\n\n      log.info(\"Starting a hadoop index generator job...\");\n      try {\n        if (job.run()) {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  job.getPublishedSegments(),\n                  job.getStats(),\n                  null\n              )\n          );\n        } else {\n          return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n              new HadoopIndexGeneratorInnerProcessingStatus(\n                  null,\n                  job.getStats(),\n                  job.getErrorMessage()\n              )\n          );\n        }\n      }\n      catch (Exception e) {\n        log.error(e, \"Encountered exception in HadoopIndexGeneratorInnerProcessing.\");\n        return HadoopDruidIndexerConfig.JSON_MAPPER.writeValueAsString(\n            new HadoopIndexGeneratorInnerProcessingStatus(\n                null,\n                job.getStats(),\n                e.getMessage()\n            )\n        );\n      }\n    }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/HadoopIndexTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":813,"status":"M"}],"commitId":"99f39c720261923a4f908ad2ae33b03b372989b0","commitMessage":"@@@Hadoop segment index file rename (#11194)\n\n* Do stuff\n\n* Do more stuff\n\n* * Do more stuff\n\n* * Do more stuff\n\n* * working\n\n* * cleanup\n\n* * more cleanup\n\n* * more cleanup\n\n* * add license header\n\n* * Add unit tests\n\n* * add java docs\n\n* * add more unit tests\n\n* * Cleanup test\n\n* * Move removing of workingPath to index task rather than in hadoop job.\n\n* * Address review comments\n\n* * remove unused import\n\n* * Address review comments\n\n* Do not overwrite segment descriptor for segment if it already exists.\n\n* * add comments to FileSystemHelper class\n\n* * fix local hadoop integration test\n\n* * Fix failing test failures when running with java11\n\n* Revert \"Revert \"Adjust HadoopIndexTask temp segment renaming to avoid potential race conditions (#11075)\" (#11151)\"\n\nThis reverts commit 49a9c3ffb7b2da3401696d583bc2cd52e83f77bf.\n\n* * remove JobHelperPowerMockTest\n\n* * remove FileSystemHelper class","date":"2021-05-05 08:22:18","modifiedFileCount":"9","status":"M","submitter":"zachjsh"}]
