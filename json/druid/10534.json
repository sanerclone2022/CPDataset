[{"authorTime":"2020-03-11 04:22:19","codes":[{"authorDate":"2020-03-20 02:43:33","commitOrder":3,"curCode":"    private String compactSegments(\n        VersionedIntervalTimeline<String, DataSegment> timeline,\n        List<DataSegment> segments,\n        ClientCompactionTaskQueryTuningConfig tuningConfig\n    )\n    {\n      Preconditions.checkArgument(segments.size() > 1);\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      Interval compactInterval = new Interval(minStart, maxEnd);\n      segments.forEach(\n          segment -> timeline.remove(\n              segment.getInterval(),\n              segment.getVersion(),\n              segment.getShardSpec().createChunk(segment)\n          )\n      );\n      final String version = \"newVersion_\" + compactVersionSuffix++;\n      final long segmentSize = segments.stream().mapToLong(DataSegment::getSize).sum() / 2;\n      for (int i = 0; i < 2; i++) {\n        DataSegment compactSegment = new DataSegment(\n            segments.get(0).getDataSource(),\n            compactInterval,\n            version,\n            null,\n            segments.get(0).getDimensions(),\n            segments.get(0).getMetrics(),\n            new NumberedShardSpec(i, 0),\n            new CompactionState(\n                new DynamicPartitionsSpec(\n                    tuningConfig.getMaxRowsPerSegment(),\n                    tuningConfig.getMaxTotalRowsOr(Long.MAX_VALUE)\n                ),\n                ImmutableMap.of(\n                    \"bitmap\",\n                    ImmutableMap.of(\"type\", \"concise\"),\n                    \"dimensionCompression\",\n                    \"lz4\",\n                    \"metricCompression\",\n                    \"lz4\",\n                    \"longEncoding\",\n                    \"longs\"\n                )\n            ),\n            1,\n            segmentSize\n        );\n\n        timeline.add(\n            compactInterval,\n            compactSegment.getVersion(),\n            compactSegment.getShardSpec().createChunk(compactSegment)\n        );\n      }\n\n      return \"task_\" + idSuffix++;\n    }\n","date":"2020-03-20 02:43:33","endLine":508,"groupId":"15937","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"compactSegments","params":"(VersionedIntervalTimeline<String@DataSegment>timeline@List<DataSegment>segments@ClientCompactionTaskQueryTuningConfigtuningConfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/6b/ac4945e5774ac58f0ef0786efc7ff1979e93e0.src","preCode":"    private String compactSegments(\n        VersionedIntervalTimeline<String, DataSegment> timeline,\n        List<DataSegment> segments,\n        ClientCompactionTaskQueryTuningConfig tuningConfig\n    )\n    {\n      Preconditions.checkArgument(segments.size() > 1);\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      Interval compactInterval = new Interval(minStart, maxEnd);\n      segments.forEach(\n          segment -> timeline.remove(\n              segment.getInterval(),\n              segment.getVersion(),\n              segment.getShardSpec().createChunk(segment)\n          )\n      );\n      final String version = \"newVersion_\" + compactVersionSuffix++;\n      final long segmentSize = segments.stream().mapToLong(DataSegment::getSize).sum() / 2;\n      for (int i = 0; i < 2; i++) {\n        DataSegment compactSegment = new DataSegment(\n            segments.get(0).getDataSource(),\n            compactInterval,\n            version,\n            null,\n            segments.get(0).getDimensions(),\n            segments.get(0).getMetrics(),\n            new NumberedShardSpec(i, 0),\n            new CompactionState(\n                new DynamicPartitionsSpec(\n                    tuningConfig.getMaxRowsPerSegment(),\n                    tuningConfig.getMaxTotalRowsOr(Long.MAX_VALUE)\n                ),\n                ImmutableMap.of(\n                    \"bitmap\",\n                    ImmutableMap.of(\"type\", \"concise\"),\n                    \"dimensionCompression\",\n                    \"lz4\",\n                    \"metricCompression\",\n                    \"lz4\",\n                    \"longEncoding\",\n                    \"longs\"\n                )\n            ),\n            1,\n            segmentSize\n        );\n\n        timeline.add(\n            compactInterval,\n            compactSegment.getVersion(),\n            compactSegment.getShardSpec().createChunk(compactSegment)\n        );\n      }\n\n      return \"task_\" + idSuffix++;\n    }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":445,"status":"B"},{"authorDate":"2020-03-11 04:22:19","commitOrder":3,"curCode":"    private QueueEntry(List<DataSegment> segments)\n    {\n      Preconditions.checkArgument(segments != null && !segments.isEmpty());\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      this.interval = new Interval(minStart, maxEnd);\n      this.segments = segments;\n    }\n","date":"2020-03-11 04:22:19","endLine":524,"groupId":"2293","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"QueueEntry","params":"(List<DataSegment>segments)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/78/ab09ce216f12d5ad361b5e7ac0c8abbca2dd5b.src","preCode":"    private QueueEntry(List<DataSegment> segments)\n    {\n      Preconditions.checkArgument(segments != null && !segments.isEmpty());\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      this.interval = new Interval(minStart, maxEnd);\n      this.segments = segments;\n    }\n","realPath":"server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":510,"status":"NB"}],"commitId":"1e667362ebabd6f4348e9f46cf613a2f18cb70ba","commitMessage":"@@@Do not use UnmodifiableList in auto compaction (#9535)\n\n","date":"2020-03-20 02:43:33","modifiedFileCount":"3","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-03-11 04:22:19","codes":[{"authorDate":"2020-03-24 09:15:57","commitOrder":4,"curCode":"    private String compactSegments(\n        VersionedIntervalTimeline<String, DataSegment> timeline,\n        List<DataSegment> segments,\n        ClientCompactionTaskQueryTuningConfig tuningConfig\n    )\n    {\n      Preconditions.checkArgument(segments.size() > 1);\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      Interval compactInterval = new Interval(minStart, maxEnd);\n      segments.forEach(\n          segment -> timeline.remove(\n              segment.getInterval(),\n              segment.getVersion(),\n              segment.getShardSpec().createChunk(segment)\n          )\n      );\n      final String version = \"newVersion_\" + compactVersionSuffix++;\n      final long segmentSize = segments.stream().mapToLong(DataSegment::getSize).sum() / 2;\n      for (int i = 0; i < 2; i++) {\n        DataSegment compactSegment = new DataSegment(\n            segments.get(0).getDataSource(),\n            compactInterval,\n            version,\n            null,\n            segments.get(0).getDimensions(),\n            segments.get(0).getMetrics(),\n            new NumberedShardSpec(i, 0),\n            new CompactionState(\n                new DynamicPartitionsSpec(\n                    tuningConfig.getMaxRowsPerSegment(),\n                    tuningConfig.getMaxTotalRowsOr(Long.MAX_VALUE)\n                ),\n                ImmutableMap.of(\n                    \"bitmap\",\n                    ImmutableMap.of(\"type\", \"roaring\", \"compressRunOnSerialization\", true),\n                    \"dimensionCompression\",\n                    \"lz4\",\n                    \"metricCompression\",\n                    \"lz4\",\n                    \"longEncoding\",\n                    \"longs\"\n                )\n            ),\n            1,\n            segmentSize\n        );\n\n        timeline.add(\n            compactInterval,\n            compactSegment.getVersion(),\n            compactSegment.getShardSpec().createChunk(compactSegment)\n        );\n      }\n\n      return \"task_\" + idSuffix++;\n    }\n","date":"2020-03-24 09:15:57","endLine":508,"groupId":"15937","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"compactSegments","params":"(VersionedIntervalTimeline<String@DataSegment>timeline@List<DataSegment>segments@ClientCompactionTaskQueryTuningConfigtuningConfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/68/3e776fc51fce9e3bbefdb81d87fc407f18c6f9.src","preCode":"    private String compactSegments(\n        VersionedIntervalTimeline<String, DataSegment> timeline,\n        List<DataSegment> segments,\n        ClientCompactionTaskQueryTuningConfig tuningConfig\n    )\n    {\n      Preconditions.checkArgument(segments.size() > 1);\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      Interval compactInterval = new Interval(minStart, maxEnd);\n      segments.forEach(\n          segment -> timeline.remove(\n              segment.getInterval(),\n              segment.getVersion(),\n              segment.getShardSpec().createChunk(segment)\n          )\n      );\n      final String version = \"newVersion_\" + compactVersionSuffix++;\n      final long segmentSize = segments.stream().mapToLong(DataSegment::getSize).sum() / 2;\n      for (int i = 0; i < 2; i++) {\n        DataSegment compactSegment = new DataSegment(\n            segments.get(0).getDataSource(),\n            compactInterval,\n            version,\n            null,\n            segments.get(0).getDimensions(),\n            segments.get(0).getMetrics(),\n            new NumberedShardSpec(i, 0),\n            new CompactionState(\n                new DynamicPartitionsSpec(\n                    tuningConfig.getMaxRowsPerSegment(),\n                    tuningConfig.getMaxTotalRowsOr(Long.MAX_VALUE)\n                ),\n                ImmutableMap.of(\n                    \"bitmap\",\n                    ImmutableMap.of(\"type\", \"concise\"),\n                    \"dimensionCompression\",\n                    \"lz4\",\n                    \"metricCompression\",\n                    \"lz4\",\n                    \"longEncoding\",\n                    \"longs\"\n                )\n            ),\n            1,\n            segmentSize\n        );\n\n        timeline.add(\n            compactInterval,\n            compactSegment.getVersion(),\n            compactSegment.getShardSpec().createChunk(compactSegment)\n        );\n      }\n\n      return \"task_\" + idSuffix++;\n    }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":445,"status":"M"},{"authorDate":"2020-03-11 04:22:19","commitOrder":4,"curCode":"    private QueueEntry(List<DataSegment> segments)\n    {\n      Preconditions.checkArgument(segments != null && !segments.isEmpty());\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      this.interval = new Interval(minStart, maxEnd);\n      this.segments = segments;\n    }\n","date":"2020-03-11 04:22:19","endLine":524,"groupId":"2293","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"QueueEntry","params":"(List<DataSegment>segments)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/78/ab09ce216f12d5ad361b5e7ac0c8abbca2dd5b.src","preCode":"    private QueueEntry(List<DataSegment> segments)\n    {\n      Preconditions.checkArgument(segments != null && !segments.isEmpty());\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      this.interval = new Interval(minStart, maxEnd);\n      this.segments = segments;\n    }\n","realPath":"server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":510,"status":"N"}],"commitId":"bf85ea19b283ffc6e26c6aef28d264233a1509db","commitMessage":"@@@roaring bitmaps by default (#9548)\n\n* it is finally time\n\n* fix it\n\n* more docs\n\n* fix doc","date":"2020-03-24 09:15:57","modifiedFileCount":"6","status":"M","submitter":"Clint Wylie"},{"authorTime":"2020-03-11 04:22:19","codes":[{"authorDate":"2020-08-19 02:03:13","commitOrder":5,"curCode":"    private void compactSegments(\n        VersionedIntervalTimeline<String, DataSegment> timeline,\n        List<DataSegment> segments,\n        ClientCompactionTaskQueryTuningConfig tuningConfig\n    )\n    {\n      Preconditions.checkArgument(segments.size() > 1);\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      Interval compactInterval = new Interval(minStart, maxEnd);\n      segments.forEach(\n          segment -> timeline.remove(\n              segment.getInterval(),\n              segment.getVersion(),\n              segment.getShardSpec().createChunk(segment)\n          )\n      );\n      final String version = \"newVersion_\" + compactVersionSuffix++;\n      final long segmentSize = segments.stream().mapToLong(DataSegment::getSize).sum() / 2;\n      for (int i = 0; i < 2; i++) {\n        DataSegment compactSegment = new DataSegment(\n            segments.get(0).getDataSource(),\n            compactInterval,\n            version,\n            null,\n            segments.get(0).getDimensions(),\n            segments.get(0).getMetrics(),\n            new NumberedShardSpec(i, 0),\n            new CompactionState(\n                new DynamicPartitionsSpec(\n                    tuningConfig.getMaxRowsPerSegment(),\n                    tuningConfig.getMaxTotalRowsOr(Long.MAX_VALUE)\n                ),\n                ImmutableMap.of(\n                    \"bitmap\",\n                    ImmutableMap.of(\"type\", \"roaring\", \"compressRunOnSerialization\", true),\n                    \"dimensionCompression\",\n                    \"lz4\",\n                    \"metricCompression\",\n                    \"lz4\",\n                    \"longEncoding\",\n                    \"longs\"\n                )\n            ),\n            1,\n            segmentSize\n        );\n\n        timeline.add(\n            compactInterval,\n            compactSegment.getVersion(),\n            compactSegment.getShardSpec().createChunk(compactSegment)\n        );\n      }\n    }\n","date":"2020-08-19 02:03:13","endLine":505,"groupId":"15937","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"compactSegments","params":"(VersionedIntervalTimeline<String@DataSegment>timeline@List<DataSegment>segments@ClientCompactionTaskQueryTuningConfigtuningConfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/75/5e9159d20f78bf41f1a588569118a4e421c2d6.src","preCode":"    private String compactSegments(\n        VersionedIntervalTimeline<String, DataSegment> timeline,\n        List<DataSegment> segments,\n        ClientCompactionTaskQueryTuningConfig tuningConfig\n    )\n    {\n      Preconditions.checkArgument(segments.size() > 1);\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      Interval compactInterval = new Interval(minStart, maxEnd);\n      segments.forEach(\n          segment -> timeline.remove(\n              segment.getInterval(),\n              segment.getVersion(),\n              segment.getShardSpec().createChunk(segment)\n          )\n      );\n      final String version = \"newVersion_\" + compactVersionSuffix++;\n      final long segmentSize = segments.stream().mapToLong(DataSegment::getSize).sum() / 2;\n      for (int i = 0; i < 2; i++) {\n        DataSegment compactSegment = new DataSegment(\n            segments.get(0).getDataSource(),\n            compactInterval,\n            version,\n            null,\n            segments.get(0).getDimensions(),\n            segments.get(0).getMetrics(),\n            new NumberedShardSpec(i, 0),\n            new CompactionState(\n                new DynamicPartitionsSpec(\n                    tuningConfig.getMaxRowsPerSegment(),\n                    tuningConfig.getMaxTotalRowsOr(Long.MAX_VALUE)\n                ),\n                ImmutableMap.of(\n                    \"bitmap\",\n                    ImmutableMap.of(\"type\", \"roaring\", \"compressRunOnSerialization\", true),\n                    \"dimensionCompression\",\n                    \"lz4\",\n                    \"metricCompression\",\n                    \"lz4\",\n                    \"longEncoding\",\n                    \"longs\"\n                )\n            ),\n            1,\n            segmentSize\n        );\n\n        timeline.add(\n            compactInterval,\n            compactSegment.getVersion(),\n            compactSegment.getShardSpec().createChunk(compactSegment)\n        );\n      }\n\n      return \"task_\" + idSuffix++;\n    }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":444,"status":"M"},{"authorDate":"2020-03-11 04:22:19","commitOrder":5,"curCode":"    private QueueEntry(List<DataSegment> segments)\n    {\n      Preconditions.checkArgument(segments != null && !segments.isEmpty());\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      this.interval = new Interval(minStart, maxEnd);\n      this.segments = segments;\n    }\n","date":"2020-03-11 04:22:19","endLine":524,"groupId":"2293","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"QueueEntry","params":"(List<DataSegment>segments)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/78/ab09ce216f12d5ad361b5e7ac0c8abbca2dd5b.src","preCode":"    private QueueEntry(List<DataSegment> segments)\n    {\n      Preconditions.checkArgument(segments != null && !segments.isEmpty());\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      this.interval = new Interval(minStart, maxEnd);\n      this.segments = segments;\n    }\n","realPath":"server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":510,"status":"N"}],"commitId":"9a81740281d8ee8eafafbf71ccfdb90cb87e34d6","commitMessage":"@@@Don't log the entire task spec (#10278)\n\n* Don't log the entire task spec\n\n* fix lgtm\n\n* fix serde\n\n* address comments and add tests\n\n* fix tests\n\n* remove unnecessary codes","date":"2020-08-19 02:03:13","modifiedFileCount":"24","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-03-11 04:22:19","codes":[{"authorDate":"2020-08-27 04:19:18","commitOrder":6,"curCode":"    private void compactSegments(\n        VersionedIntervalTimeline<String, DataSegment> timeline,\n        List<DataSegment> segments,\n        ClientCompactionTaskQueryTuningConfig tuningConfig\n    )\n    {\n      Preconditions.checkArgument(segments.size() > 1);\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      Interval compactInterval = new Interval(minStart, maxEnd);\n      segments.forEach(\n          segment -> timeline.remove(\n              segment.getInterval(),\n              segment.getVersion(),\n              segment.getShardSpec().createChunk(segment)\n          )\n      );\n      final String version = \"newVersion_\" + compactVersionSuffix++;\n      final long segmentSize = segments.stream().mapToLong(DataSegment::getSize).sum() / 2;\n      final PartitionsSpec compactionPartitionsSpec;\n      if (tuningConfig.getPartitionsSpec() instanceof DynamicPartitionsSpec) {\n        compactionPartitionsSpec = new DynamicPartitionsSpec(\n            tuningConfig.getPartitionsSpec().getMaxRowsPerSegment(),\n            ((DynamicPartitionsSpec) tuningConfig.getPartitionsSpec()).getMaxTotalRowsOr(Long.MAX_VALUE)\n        );\n      } else {\n        compactionPartitionsSpec = tuningConfig.getPartitionsSpec();\n      }\n\n      for (int i = 0; i < 2; i++) {\n        DataSegment compactSegment = new DataSegment(\n            segments.get(0).getDataSource(),\n            compactInterval,\n            version,\n            null,\n            segments.get(0).getDimensions(),\n            segments.get(0).getMetrics(),\n            shardSpecFactory.apply(i, 2),\n            new CompactionState(\n                compactionPartitionsSpec,\n                ImmutableMap.of(\n                    \"bitmap\",\n                    ImmutableMap.of(\"type\", \"roaring\", \"compressRunOnSerialization\", true),\n                    \"dimensionCompression\",\n                    \"lz4\",\n                    \"metricCompression\",\n                    \"lz4\",\n                    \"longEncoding\",\n                    \"longs\"\n                )\n            ),\n            1,\n            segmentSize\n        );\n\n        timeline.add(\n            compactInterval,\n            compactSegment.getVersion(),\n            compactSegment.getShardSpec().createChunk(compactSegment)\n        );\n      }\n    }\n","date":"2020-08-27 04:19:18","endLine":584,"groupId":"15822","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"compactSegments","params":"(VersionedIntervalTimeline<String@DataSegment>timeline@List<DataSegment>segments@ClientCompactionTaskQueryTuningConfigtuningConfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/91/aebdc5526d5cc856d46c43851d421c72c5933b.src","preCode":"    private void compactSegments(\n        VersionedIntervalTimeline<String, DataSegment> timeline,\n        List<DataSegment> segments,\n        ClientCompactionTaskQueryTuningConfig tuningConfig\n    )\n    {\n      Preconditions.checkArgument(segments.size() > 1);\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      Interval compactInterval = new Interval(minStart, maxEnd);\n      segments.forEach(\n          segment -> timeline.remove(\n              segment.getInterval(),\n              segment.getVersion(),\n              segment.getShardSpec().createChunk(segment)\n          )\n      );\n      final String version = \"newVersion_\" + compactVersionSuffix++;\n      final long segmentSize = segments.stream().mapToLong(DataSegment::getSize).sum() / 2;\n      for (int i = 0; i < 2; i++) {\n        DataSegment compactSegment = new DataSegment(\n            segments.get(0).getDataSource(),\n            compactInterval,\n            version,\n            null,\n            segments.get(0).getDimensions(),\n            segments.get(0).getMetrics(),\n            new NumberedShardSpec(i, 0),\n            new CompactionState(\n                new DynamicPartitionsSpec(\n                    tuningConfig.getMaxRowsPerSegment(),\n                    tuningConfig.getMaxTotalRowsOr(Long.MAX_VALUE)\n                ),\n                ImmutableMap.of(\n                    \"bitmap\",\n                    ImmutableMap.of(\"type\", \"roaring\", \"compressRunOnSerialization\", true),\n                    \"dimensionCompression\",\n                    \"lz4\",\n                    \"metricCompression\",\n                    \"lz4\",\n                    \"longEncoding\",\n                    \"longs\"\n                )\n            ),\n            1,\n            segmentSize\n        );\n\n        timeline.add(\n            compactInterval,\n            compactSegment.getVersion(),\n            compactSegment.getShardSpec().createChunk(compactSegment)\n        );\n      }\n    }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":516,"status":"M"},{"authorDate":"2020-03-11 04:22:19","commitOrder":6,"curCode":"    private QueueEntry(List<DataSegment> segments)\n    {\n      Preconditions.checkArgument(segments != null && !segments.isEmpty());\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      this.interval = new Interval(minStart, maxEnd);\n      this.segments = segments;\n    }\n","date":"2020-03-11 04:22:19","endLine":524,"groupId":"2293","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"QueueEntry","params":"(List<DataSegment>segments)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/78/ab09ce216f12d5ad361b5e7ac0c8abbca2dd5b.src","preCode":"    private QueueEntry(List<DataSegment> segments)\n    {\n      Preconditions.checkArgument(segments != null && !segments.isEmpty());\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      this.interval = new Interval(minStart, maxEnd);\n      this.segments = segments;\n    }\n","realPath":"server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":510,"status":"N"}],"commitId":"b9ff3483ac353c4c7098d5caa1802befa5943ef0","commitMessage":"@@@Add support for all partitioing schemes for auto compaction  (#10307)\n\n* Add support for all partitioing schemes for auto compaction\n\n* annotate last compaction state for multi phase parallel indexing\n\n* fix build and tests\n\n* test\n\n* better home","date":"2020-08-27 04:19:18","modifiedFileCount":"37","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-03-11 04:22:19","codes":[{"authorDate":"2021-02-12 19:03:20","commitOrder":7,"curCode":"    private void compactSegments(\n        VersionedIntervalTimeline<String, DataSegment> timeline,\n        List<DataSegment> segments,\n        ClientCompactionTaskQueryTuningConfig tuningConfig\n    )\n    {\n      Preconditions.checkArgument(segments.size() > 1);\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      Interval compactInterval = new Interval(minStart, maxEnd);\n      segments.forEach(\n          segment -> timeline.remove(\n              segment.getInterval(),\n              segment.getVersion(),\n              segment.getShardSpec().createChunk(segment)\n          )\n      );\n      final String version = \"newVersion_\" + compactVersionSuffix++;\n      final long segmentSize = segments.stream().mapToLong(DataSegment::getSize).sum() / 2;\n      final PartitionsSpec compactionPartitionsSpec;\n      if (tuningConfig.getPartitionsSpec() instanceof DynamicPartitionsSpec) {\n        compactionPartitionsSpec = new DynamicPartitionsSpec(\n            tuningConfig.getPartitionsSpec().getMaxRowsPerSegment(),\n            ((DynamicPartitionsSpec) tuningConfig.getPartitionsSpec()).getMaxTotalRowsOr(Long.MAX_VALUE)\n        );\n      } else {\n        compactionPartitionsSpec = tuningConfig.getPartitionsSpec();\n      }\n\n      for (int i = 0; i < 2; i++) {\n        DataSegment compactSegment = new DataSegment(\n            segments.get(0).getDataSource(),\n            compactInterval,\n            version,\n            null,\n            segments.get(0).getDimensions(),\n            segments.get(0).getMetrics(),\n            shardSpecFactory.apply(i, 2),\n            new CompactionState(\n                compactionPartitionsSpec,\n                ImmutableMap.of(\n                    \"bitmap\",\n                    ImmutableMap.of(\"type\", \"roaring\", \"compressRunOnSerialization\", true),\n                    \"dimensionCompression\",\n                    \"lz4\",\n                    \"metricCompression\",\n                    \"lz4\",\n                    \"longEncoding\",\n                    \"longs\"\n                ),\n                ImmutableMap.of()\n            ),\n            1,\n            segmentSize\n        );\n\n        timeline.add(\n            compactInterval,\n            compactSegment.getVersion(),\n            compactSegment.getShardSpec().createChunk(compactSegment)\n        );\n      }\n    }\n","date":"2021-02-12 19:03:20","endLine":1220,"groupId":"10534","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"compactSegments","params":"(VersionedIntervalTimeline<String@DataSegment>timeline@List<DataSegment>segments@ClientCompactionTaskQueryTuningConfigtuningConfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c8/928103c09569938a64b3dda423ebd903ba11a2.src","preCode":"    private void compactSegments(\n        VersionedIntervalTimeline<String, DataSegment> timeline,\n        List<DataSegment> segments,\n        ClientCompactionTaskQueryTuningConfig tuningConfig\n    )\n    {\n      Preconditions.checkArgument(segments.size() > 1);\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      Interval compactInterval = new Interval(minStart, maxEnd);\n      segments.forEach(\n          segment -> timeline.remove(\n              segment.getInterval(),\n              segment.getVersion(),\n              segment.getShardSpec().createChunk(segment)\n          )\n      );\n      final String version = \"newVersion_\" + compactVersionSuffix++;\n      final long segmentSize = segments.stream().mapToLong(DataSegment::getSize).sum() / 2;\n      final PartitionsSpec compactionPartitionsSpec;\n      if (tuningConfig.getPartitionsSpec() instanceof DynamicPartitionsSpec) {\n        compactionPartitionsSpec = new DynamicPartitionsSpec(\n            tuningConfig.getPartitionsSpec().getMaxRowsPerSegment(),\n            ((DynamicPartitionsSpec) tuningConfig.getPartitionsSpec()).getMaxTotalRowsOr(Long.MAX_VALUE)\n        );\n      } else {\n        compactionPartitionsSpec = tuningConfig.getPartitionsSpec();\n      }\n\n      for (int i = 0; i < 2; i++) {\n        DataSegment compactSegment = new DataSegment(\n            segments.get(0).getDataSource(),\n            compactInterval,\n            version,\n            null,\n            segments.get(0).getDimensions(),\n            segments.get(0).getMetrics(),\n            shardSpecFactory.apply(i, 2),\n            new CompactionState(\n                compactionPartitionsSpec,\n                ImmutableMap.of(\n                    \"bitmap\",\n                    ImmutableMap.of(\"type\", \"roaring\", \"compressRunOnSerialization\", true),\n                    \"dimensionCompression\",\n                    \"lz4\",\n                    \"metricCompression\",\n                    \"lz4\",\n                    \"longEncoding\",\n                    \"longs\"\n                )\n            ),\n            1,\n            segmentSize\n        );\n\n        timeline.add(\n            compactInterval,\n            compactSegment.getVersion(),\n            compactSegment.getShardSpec().createChunk(compactSegment)\n        );\n      }\n    }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1151,"status":"M"},{"authorDate":"2020-03-11 04:22:19","commitOrder":7,"curCode":"    private QueueEntry(List<DataSegment> segments)\n    {\n      Preconditions.checkArgument(segments != null && !segments.isEmpty());\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      this.interval = new Interval(minStart, maxEnd);\n      this.segments = segments;\n    }\n","date":"2020-03-11 04:22:19","endLine":524,"groupId":"10534","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"QueueEntry","params":"(List<DataSegment>segments)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/78/ab09ce216f12d5ad361b5e7ac0c8abbca2dd5b.src","preCode":"    private QueueEntry(List<DataSegment> segments)\n    {\n      Preconditions.checkArgument(segments != null && !segments.isEmpty());\n      DateTime minStart = DateTimes.MAX, maxEnd = DateTimes.MIN;\n      for (DataSegment segment : segments) {\n        if (segment.getInterval().getStart().compareTo(minStart) < 0) {\n          minStart = segment.getInterval().getStart();\n        }\n        if (segment.getInterval().getEnd().compareTo(maxEnd) > 0) {\n          maxEnd = segment.getInterval().getEnd();\n        }\n      }\n      this.interval = new Interval(minStart, maxEnd);\n      this.segments = segments;\n    }\n","realPath":"server/src/main/java/org/apache/druid/server/coordinator/duty/NewestSegmentFirstIterator.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":510,"status":"N"}],"commitId":"6541178c21839530a42af4b4675a9bc680bffca6","commitMessage":"@@@Support segmentGranularity for auto-compaction (#10843)\n\n* Support segmentGranularity for auto-compaction\n\n* Support segmentGranularity for auto-compaction\n\n* Support segmentGranularity for auto-compaction\n\n* Support segmentGranularity for auto-compaction\n\n* resolve conflict\n\n* Support segmentGranularity for auto-compaction\n\n* Support segmentGranularity for auto-compaction\n\n* fix tests\n\n* fix more tests\n\n* fix checkstyle\n\n* add unit tests\n\n* fix checkstyle\n\n* fix checkstyle\n\n* fix checkstyle\n\n* add unit tests\n\n* add integration tests\n\n* fix checkstyle\n\n* fix checkstyle\n\n* fix failing tests\n\n* address comments\n\n* address comments\n\n* fix tests\n\n* fix tests\n\n* fix test\n\n* fix test\n\n* fix test\n\n* fix test\n\n* fix test\n\n* fix test\n\n* fix test\n\n* fix test","date":"2021-02-12 19:03:20","modifiedFileCount":"32","status":"M","submitter":"Maytas Monsereenusorn"}]
