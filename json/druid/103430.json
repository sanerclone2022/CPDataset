[{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  private void testUsingSchemeForMultipleSegments(final String scheme, final int numberOfSegments) throws Exception\n  {\n    Configuration conf = new Configuration(true);\n    DataSegment[] segments = new DataSegment[numberOfSegments];\n\n    \r\n    File segmentDir = tempFolder.newFolder();\n    File tmp = new File(segmentDir, \"version.bin\");\n\n    final byte[] data = new byte[]{0x0, 0x0, 0x0, 0x1};\n    Files.write(data, tmp);\n    final long size = data.length;\n\n    HdfsDataSegmentPusherConfig config = new HdfsDataSegmentPusherConfig();\n    final File storageDirectory = tempFolder.newFolder();\n\n    config.setStorageDirectory(\n        scheme != null\n        ? StringUtils.format(\"%s://%s\", scheme, storageDirectory.getAbsolutePath())\n        : storageDirectory.getAbsolutePath()\n    );\n    HdfsDataSegmentPusher pusher = new HdfsDataSegmentPusher(config, conf, new DefaultObjectMapper());\n\n    for (int i = 0; i < numberOfSegments; i++) {\n      segments[i] = new DataSegment(\n          \"foo\",\n          Intervals.of(\"2015/2016\"),\n          \"0\",\n          Maps.newHashMap(),\n          Lists.newArrayList(),\n          Lists.newArrayList(),\n          new NumberedShardSpec(i, i),\n          0,\n          size\n      );\n    }\n\n    for (int i = 0; i < numberOfSegments; i++) {\n      final DataSegment pushedSegment = pusher.push(segmentDir, segments[i], false);\n\n      String indexUri = StringUtils.format(\n          \"%s/%s/%d_index.zip\",\n          FileSystem.newInstance(conf).makeQualified(new Path(config.getStorageDirectory())).toUri().toString(),\n          pusher.getStorageDir(segments[i], false),\n          segments[i].getShardSpec().getPartitionNum()\n      );\n\n      Assert.assertEquals(segments[i].getSize(), pushedSegment.getSize());\n      Assert.assertEquals(segments[i], pushedSegment);\n      Assert.assertEquals(ImmutableMap.of(\n          \"type\",\n          \"hdfs\",\n          \"path\",\n          indexUri\n      ), pushedSegment.getLoadSpec());\n      \r\n      String segmentPath = pusher.getStorageDir(pushedSegment, false);\n\n      File indexFile = new File(StringUtils.format(\n          \"%s/%s/%d_index.zip\",\n          storageDirectory,\n          segmentPath,\n          pushedSegment.getShardSpec().getPartitionNum()\n      ));\n      Assert.assertTrue(indexFile.exists());\n      File descriptorFile = new File(StringUtils.format(\n          \"%s/%s/%d_descriptor.json\",\n          storageDirectory,\n          segmentPath,\n          pushedSegment.getShardSpec().getPartitionNum()\n      ));\n      Assert.assertTrue(descriptorFile.exists());\n\n      \r\n      DataSegment fromDescriptorFileDataSegment = objectMapper.readValue(descriptorFile, DataSegment.class);\n\n      Assert.assertEquals(segments[i].getSize(), pushedSegment.getSize());\n      Assert.assertEquals(segments[i], pushedSegment);\n      Assert.assertEquals(ImmutableMap.of(\n          \"type\",\n          \"hdfs\",\n          \"path\",\n          indexUri\n      ), fromDescriptorFileDataSegment.getLoadSpec());\n      \r\n      segmentPath = pusher.getStorageDir(fromDescriptorFileDataSegment, false);\n\n      indexFile = new File(StringUtils.format(\n          \"%s/%s/%d_index.zip\",\n          storageDirectory,\n          segmentPath,\n          fromDescriptorFileDataSegment.getShardSpec().getPartitionNum()\n      ));\n      Assert.assertTrue(indexFile.exists());\n\n\n      \r\n      File outDir = new File(StringUtils.format(\"%s/%s\", config.getStorageDirectory(), segmentPath));\n      outDir.setReadOnly();\n      try {\n        pusher.push(segmentDir, segments[i], false);\n      }\n      catch (IOException e) {\n        Assert.fail(\"should not throw exception\");\n      }\n    }\n  }\n","date":"2018-08-31 00:56:26","endLine":276,"groupId":"20743","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testUsingSchemeForMultipleSegments","params":"(finalStringscheme@finalintnumberOfSegments)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/8b/0684d173eeaa02908e1a4beca489e5ebbcdf20.src","preCode":"  private void testUsingSchemeForMultipleSegments(final String scheme, final int numberOfSegments) throws Exception\n  {\n    Configuration conf = new Configuration(true);\n    DataSegment[] segments = new DataSegment[numberOfSegments];\n\n    \r\n    File segmentDir = tempFolder.newFolder();\n    File tmp = new File(segmentDir, \"version.bin\");\n\n    final byte[] data = new byte[]{0x0, 0x0, 0x0, 0x1};\n    Files.write(data, tmp);\n    final long size = data.length;\n\n    HdfsDataSegmentPusherConfig config = new HdfsDataSegmentPusherConfig();\n    final File storageDirectory = tempFolder.newFolder();\n\n    config.setStorageDirectory(\n        scheme != null\n        ? StringUtils.format(\"%s://%s\", scheme, storageDirectory.getAbsolutePath())\n        : storageDirectory.getAbsolutePath()\n    );\n    HdfsDataSegmentPusher pusher = new HdfsDataSegmentPusher(config, conf, new DefaultObjectMapper());\n\n    for (int i = 0; i < numberOfSegments; i++) {\n      segments[i] = new DataSegment(\n          \"foo\",\n          Intervals.of(\"2015/2016\"),\n          \"0\",\n          Maps.newHashMap(),\n          Lists.newArrayList(),\n          Lists.newArrayList(),\n          new NumberedShardSpec(i, i),\n          0,\n          size\n      );\n    }\n\n    for (int i = 0; i < numberOfSegments; i++) {\n      final DataSegment pushedSegment = pusher.push(segmentDir, segments[i], false);\n\n      String indexUri = StringUtils.format(\n          \"%s/%s/%d_index.zip\",\n          FileSystem.newInstance(conf).makeQualified(new Path(config.getStorageDirectory())).toUri().toString(),\n          pusher.getStorageDir(segments[i], false),\n          segments[i].getShardSpec().getPartitionNum()\n      );\n\n      Assert.assertEquals(segments[i].getSize(), pushedSegment.getSize());\n      Assert.assertEquals(segments[i], pushedSegment);\n      Assert.assertEquals(ImmutableMap.of(\n          \"type\",\n          \"hdfs\",\n          \"path\",\n          indexUri\n      ), pushedSegment.getLoadSpec());\n      \r\n      String segmentPath = pusher.getStorageDir(pushedSegment, false);\n\n      File indexFile = new File(StringUtils.format(\n          \"%s/%s/%d_index.zip\",\n          storageDirectory,\n          segmentPath,\n          pushedSegment.getShardSpec().getPartitionNum()\n      ));\n      Assert.assertTrue(indexFile.exists());\n      File descriptorFile = new File(StringUtils.format(\n          \"%s/%s/%d_descriptor.json\",\n          storageDirectory,\n          segmentPath,\n          pushedSegment.getShardSpec().getPartitionNum()\n      ));\n      Assert.assertTrue(descriptorFile.exists());\n\n      \r\n      DataSegment fromDescriptorFileDataSegment = objectMapper.readValue(descriptorFile, DataSegment.class);\n\n      Assert.assertEquals(segments[i].getSize(), pushedSegment.getSize());\n      Assert.assertEquals(segments[i], pushedSegment);\n      Assert.assertEquals(ImmutableMap.of(\n          \"type\",\n          \"hdfs\",\n          \"path\",\n          indexUri\n      ), fromDescriptorFileDataSegment.getLoadSpec());\n      \r\n      segmentPath = pusher.getStorageDir(fromDescriptorFileDataSegment, false);\n\n      indexFile = new File(StringUtils.format(\n          \"%s/%s/%d_index.zip\",\n          storageDirectory,\n          segmentPath,\n          fromDescriptorFileDataSegment.getShardSpec().getPartitionNum()\n      ));\n      Assert.assertTrue(indexFile.exists());\n\n\n      \r\n      File outDir = new File(StringUtils.format(\"%s/%s\", config.getStorageDirectory(), segmentPath));\n      outDir.setReadOnly();\n      try {\n        pusher.push(segmentDir, segments[i], false);\n      }\n      catch (IOException e) {\n        Assert.fail(\"should not throw exception\");\n      }\n    }\n  }\n","realPath":"extensions-core/hdfs-storage/src/test/java/org/apache/druid/storage/hdfs/HdfsDataSegmentPusherTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":170,"status":"B"},{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  private void testUsingScheme(final String scheme) throws Exception\n  {\n    Configuration conf = new Configuration(true);\n\n    \r\n    File segmentDir = tempFolder.newFolder();\n    File tmp = new File(segmentDir, \"version.bin\");\n\n    final byte[] data = new byte[]{0x0, 0x0, 0x0, 0x1};\n    Files.write(data, tmp);\n    final long size = data.length;\n\n    HdfsDataSegmentPusherConfig config = new HdfsDataSegmentPusherConfig();\n    final File storageDirectory = tempFolder.newFolder();\n\n    config.setStorageDirectory(\n        scheme != null\n        ? StringUtils.format(\"%s://%s\", scheme, storageDirectory.getAbsolutePath())\n        : storageDirectory.getAbsolutePath()\n    );\n    HdfsDataSegmentPusher pusher = new HdfsDataSegmentPusher(config, conf, new DefaultObjectMapper());\n\n    DataSegment segmentToPush = new DataSegment(\n        \"foo\",\n        Intervals.of(\"2015/2016\"),\n        \"0\",\n        Maps.newHashMap(),\n        Lists.newArrayList(),\n        Lists.newArrayList(),\n        NoneShardSpec.instance(),\n        0,\n        size\n    );\n\n    DataSegment segment = pusher.push(segmentDir, segmentToPush, false);\n\n\n    String indexUri = StringUtils.format(\n        \"%s/%s/%d_index.zip\",\n        FileSystem.newInstance(conf).makeQualified(new Path(config.getStorageDirectory())).toUri().toString(),\n        pusher.getStorageDir(segmentToPush, false),\n        segmentToPush.getShardSpec().getPartitionNum()\n    );\n\n    Assert.assertEquals(segmentToPush.getSize(), segment.getSize());\n    Assert.assertEquals(segmentToPush, segment);\n    Assert.assertEquals(ImmutableMap.of(\n        \"type\",\n        \"hdfs\",\n        \"path\",\n        indexUri\n    ), segment.getLoadSpec());\n    \r\n    final String segmentPath = pusher.getStorageDir(segment, false);\n\n    File indexFile = new File(StringUtils.format(\n        \"%s/%s/%d_index.zip\",\n        storageDirectory,\n        segmentPath,\n        segment.getShardSpec().getPartitionNum()\n    ));\n    Assert.assertTrue(indexFile.exists());\n    File descriptorFile = new File(StringUtils.format(\n        \"%s/%s/%d_descriptor.json\",\n        storageDirectory,\n        segmentPath,\n        segment.getShardSpec().getPartitionNum()\n    ));\n    Assert.assertTrue(descriptorFile.exists());\n\n    \r\n    File outDir = new File(StringUtils.format(\"%s/%s\", config.getStorageDirectory(), segmentPath));\n    outDir.setReadOnly();\n    try {\n      pusher.push(segmentDir, segmentToPush, false);\n    }\n    catch (IOException e) {\n      Assert.fail(\"should not throw exception\");\n    }\n  }\n","date":"2018-08-31 00:56:26","endLine":357,"groupId":"19477","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testUsingScheme","params":"(finalStringscheme)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/8b/0684d173eeaa02908e1a4beca489e5ebbcdf20.src","preCode":"  private void testUsingScheme(final String scheme) throws Exception\n  {\n    Configuration conf = new Configuration(true);\n\n    \r\n    File segmentDir = tempFolder.newFolder();\n    File tmp = new File(segmentDir, \"version.bin\");\n\n    final byte[] data = new byte[]{0x0, 0x0, 0x0, 0x1};\n    Files.write(data, tmp);\n    final long size = data.length;\n\n    HdfsDataSegmentPusherConfig config = new HdfsDataSegmentPusherConfig();\n    final File storageDirectory = tempFolder.newFolder();\n\n    config.setStorageDirectory(\n        scheme != null\n        ? StringUtils.format(\"%s://%s\", scheme, storageDirectory.getAbsolutePath())\n        : storageDirectory.getAbsolutePath()\n    );\n    HdfsDataSegmentPusher pusher = new HdfsDataSegmentPusher(config, conf, new DefaultObjectMapper());\n\n    DataSegment segmentToPush = new DataSegment(\n        \"foo\",\n        Intervals.of(\"2015/2016\"),\n        \"0\",\n        Maps.newHashMap(),\n        Lists.newArrayList(),\n        Lists.newArrayList(),\n        NoneShardSpec.instance(),\n        0,\n        size\n    );\n\n    DataSegment segment = pusher.push(segmentDir, segmentToPush, false);\n\n\n    String indexUri = StringUtils.format(\n        \"%s/%s/%d_index.zip\",\n        FileSystem.newInstance(conf).makeQualified(new Path(config.getStorageDirectory())).toUri().toString(),\n        pusher.getStorageDir(segmentToPush, false),\n        segmentToPush.getShardSpec().getPartitionNum()\n    );\n\n    Assert.assertEquals(segmentToPush.getSize(), segment.getSize());\n    Assert.assertEquals(segmentToPush, segment);\n    Assert.assertEquals(ImmutableMap.of(\n        \"type\",\n        \"hdfs\",\n        \"path\",\n        indexUri\n    ), segment.getLoadSpec());\n    \r\n    final String segmentPath = pusher.getStorageDir(segment, false);\n\n    File indexFile = new File(StringUtils.format(\n        \"%s/%s/%d_index.zip\",\n        storageDirectory,\n        segmentPath,\n        segment.getShardSpec().getPartitionNum()\n    ));\n    Assert.assertTrue(indexFile.exists());\n    File descriptorFile = new File(StringUtils.format(\n        \"%s/%s/%d_descriptor.json\",\n        storageDirectory,\n        segmentPath,\n        segment.getShardSpec().getPartitionNum()\n    ));\n    Assert.assertTrue(descriptorFile.exists());\n\n    \r\n    File outDir = new File(StringUtils.format(\"%s/%s\", config.getStorageDirectory(), segmentPath));\n    outDir.setReadOnly();\n    try {\n      pusher.push(segmentDir, segmentToPush, false);\n    }\n    catch (IOException e) {\n      Assert.fail(\"should not throw exception\");\n    }\n  }\n","realPath":"extensions-core/hdfs-storage/src/test/java/org/apache/druid/storage/hdfs/HdfsDataSegmentPusherTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":278,"status":"B"}],"commitId":"431d3d8497f9079857c3baa7ae7ab4cb44a22355","commitMessage":"@@@Rename io.druid to org.apache.druid. (#6266)\n\n* Rename io.druid to org.apache.druid.\n\n* Fix META-INF files and remove some benchmark results.\n\n* MonitorsConfig update for metrics package migration.\n\n* Reorder some dimensions in inner queries for some reason.\n\n* Fix protobuf tests.\n","date":"2018-08-31 00:56:26","modifiedFileCount":"5","status":"B","submitter":"Gian Merlino"},{"authorTime":"2018-10-29 20:02:43","codes":[{"authorDate":"2018-10-29 20:02:43","commitOrder":2,"curCode":"  private void testUsingSchemeForMultipleSegments(final String scheme, final int numberOfSegments) throws Exception\n  {\n    Configuration conf = new Configuration(true);\n    DataSegment[] segments = new DataSegment[numberOfSegments];\n\n    \r\n    File segmentDir = tempFolder.newFolder();\n    File tmp = new File(segmentDir, \"version.bin\");\n\n    final byte[] data = new byte[]{0x0, 0x0, 0x0, 0x1};\n    Files.write(data, tmp);\n    final long size = data.length;\n\n    HdfsDataSegmentPusherConfig config = new HdfsDataSegmentPusherConfig();\n    final File storageDirectory = tempFolder.newFolder();\n\n    config.setStorageDirectory(\n        scheme != null\n        ? StringUtils.format(\"%s://%s\", scheme, storageDirectory.getAbsolutePath())\n        : storageDirectory.getAbsolutePath()\n    );\n    HdfsDataSegmentPusher pusher = new HdfsDataSegmentPusher(config, conf, new DefaultObjectMapper());\n\n    for (int i = 0; i < numberOfSegments; i++) {\n      segments[i] = new DataSegment(\n          \"foo\",\n          Intervals.of(\"2015/2016\"),\n          \"0\",\n          new HashMap<>(),\n          new ArrayList<>(),\n          new ArrayList<>(),\n          new NumberedShardSpec(i, i),\n          0,\n          size\n      );\n    }\n\n    for (int i = 0; i < numberOfSegments; i++) {\n      final DataSegment pushedSegment = pusher.push(segmentDir, segments[i], false);\n\n      String indexUri = StringUtils.format(\n          \"%s/%s/%d_index.zip\",\n          FileSystem.newInstance(conf).makeQualified(new Path(config.getStorageDirectory())).toUri().toString(),\n          pusher.getStorageDir(segments[i], false),\n          segments[i].getShardSpec().getPartitionNum()\n      );\n\n      Assert.assertEquals(segments[i].getSize(), pushedSegment.getSize());\n      Assert.assertEquals(segments[i], pushedSegment);\n      Assert.assertEquals(ImmutableMap.of(\n          \"type\",\n          \"hdfs\",\n          \"path\",\n          indexUri\n      ), pushedSegment.getLoadSpec());\n      \r\n      String segmentPath = pusher.getStorageDir(pushedSegment, false);\n\n      File indexFile = new File(StringUtils.format(\n          \"%s/%s/%d_index.zip\",\n          storageDirectory,\n          segmentPath,\n          pushedSegment.getShardSpec().getPartitionNum()\n      ));\n      Assert.assertTrue(indexFile.exists());\n      File descriptorFile = new File(StringUtils.format(\n          \"%s/%s/%d_descriptor.json\",\n          storageDirectory,\n          segmentPath,\n          pushedSegment.getShardSpec().getPartitionNum()\n      ));\n      Assert.assertTrue(descriptorFile.exists());\n\n      \r\n      DataSegment fromDescriptorFileDataSegment = objectMapper.readValue(descriptorFile, DataSegment.class);\n\n      Assert.assertEquals(segments[i].getSize(), pushedSegment.getSize());\n      Assert.assertEquals(segments[i], pushedSegment);\n      Assert.assertEquals(ImmutableMap.of(\n          \"type\",\n          \"hdfs\",\n          \"path\",\n          indexUri\n      ), fromDescriptorFileDataSegment.getLoadSpec());\n      \r\n      segmentPath = pusher.getStorageDir(fromDescriptorFileDataSegment, false);\n\n      indexFile = new File(StringUtils.format(\n          \"%s/%s/%d_index.zip\",\n          storageDirectory,\n          segmentPath,\n          fromDescriptorFileDataSegment.getShardSpec().getPartitionNum()\n      ));\n      Assert.assertTrue(indexFile.exists());\n\n\n      \r\n      File outDir = new File(StringUtils.format(\"%s/%s\", config.getStorageDirectory(), segmentPath));\n      outDir.setReadOnly();\n      try {\n        pusher.push(segmentDir, segments[i], false);\n      }\n      catch (IOException e) {\n        Assert.fail(\"should not throw exception\");\n      }\n    }\n  }\n","date":"2018-10-29 20:02:43","endLine":276,"groupId":"4575","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testUsingSchemeForMultipleSegments","params":"(finalStringscheme@finalintnumberOfSegments)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/63/0aa2447fd083c8a5e081340db3a24940510cf3.src","preCode":"  private void testUsingSchemeForMultipleSegments(final String scheme, final int numberOfSegments) throws Exception\n  {\n    Configuration conf = new Configuration(true);\n    DataSegment[] segments = new DataSegment[numberOfSegments];\n\n    \r\n    File segmentDir = tempFolder.newFolder();\n    File tmp = new File(segmentDir, \"version.bin\");\n\n    final byte[] data = new byte[]{0x0, 0x0, 0x0, 0x1};\n    Files.write(data, tmp);\n    final long size = data.length;\n\n    HdfsDataSegmentPusherConfig config = new HdfsDataSegmentPusherConfig();\n    final File storageDirectory = tempFolder.newFolder();\n\n    config.setStorageDirectory(\n        scheme != null\n        ? StringUtils.format(\"%s://%s\", scheme, storageDirectory.getAbsolutePath())\n        : storageDirectory.getAbsolutePath()\n    );\n    HdfsDataSegmentPusher pusher = new HdfsDataSegmentPusher(config, conf, new DefaultObjectMapper());\n\n    for (int i = 0; i < numberOfSegments; i++) {\n      segments[i] = new DataSegment(\n          \"foo\",\n          Intervals.of(\"2015/2016\"),\n          \"0\",\n          Maps.newHashMap(),\n          Lists.newArrayList(),\n          Lists.newArrayList(),\n          new NumberedShardSpec(i, i),\n          0,\n          size\n      );\n    }\n\n    for (int i = 0; i < numberOfSegments; i++) {\n      final DataSegment pushedSegment = pusher.push(segmentDir, segments[i], false);\n\n      String indexUri = StringUtils.format(\n          \"%s/%s/%d_index.zip\",\n          FileSystem.newInstance(conf).makeQualified(new Path(config.getStorageDirectory())).toUri().toString(),\n          pusher.getStorageDir(segments[i], false),\n          segments[i].getShardSpec().getPartitionNum()\n      );\n\n      Assert.assertEquals(segments[i].getSize(), pushedSegment.getSize());\n      Assert.assertEquals(segments[i], pushedSegment);\n      Assert.assertEquals(ImmutableMap.of(\n          \"type\",\n          \"hdfs\",\n          \"path\",\n          indexUri\n      ), pushedSegment.getLoadSpec());\n      \r\n      String segmentPath = pusher.getStorageDir(pushedSegment, false);\n\n      File indexFile = new File(StringUtils.format(\n          \"%s/%s/%d_index.zip\",\n          storageDirectory,\n          segmentPath,\n          pushedSegment.getShardSpec().getPartitionNum()\n      ));\n      Assert.assertTrue(indexFile.exists());\n      File descriptorFile = new File(StringUtils.format(\n          \"%s/%s/%d_descriptor.json\",\n          storageDirectory,\n          segmentPath,\n          pushedSegment.getShardSpec().getPartitionNum()\n      ));\n      Assert.assertTrue(descriptorFile.exists());\n\n      \r\n      DataSegment fromDescriptorFileDataSegment = objectMapper.readValue(descriptorFile, DataSegment.class);\n\n      Assert.assertEquals(segments[i].getSize(), pushedSegment.getSize());\n      Assert.assertEquals(segments[i], pushedSegment);\n      Assert.assertEquals(ImmutableMap.of(\n          \"type\",\n          \"hdfs\",\n          \"path\",\n          indexUri\n      ), fromDescriptorFileDataSegment.getLoadSpec());\n      \r\n      segmentPath = pusher.getStorageDir(fromDescriptorFileDataSegment, false);\n\n      indexFile = new File(StringUtils.format(\n          \"%s/%s/%d_index.zip\",\n          storageDirectory,\n          segmentPath,\n          fromDescriptorFileDataSegment.getShardSpec().getPartitionNum()\n      ));\n      Assert.assertTrue(indexFile.exists());\n\n\n      \r\n      File outDir = new File(StringUtils.format(\"%s/%s\", config.getStorageDirectory(), segmentPath));\n      outDir.setReadOnly();\n      try {\n        pusher.push(segmentDir, segments[i], false);\n      }\n      catch (IOException e) {\n        Assert.fail(\"should not throw exception\");\n      }\n    }\n  }\n","realPath":"extensions-core/hdfs-storage/src/test/java/org/apache/druid/storage/hdfs/HdfsDataSegmentPusherTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":170,"status":"M"},{"authorDate":"2018-10-29 20:02:43","commitOrder":2,"curCode":"  private void testUsingScheme(final String scheme) throws Exception\n  {\n    Configuration conf = new Configuration(true);\n\n    \r\n    File segmentDir = tempFolder.newFolder();\n    File tmp = new File(segmentDir, \"version.bin\");\n\n    final byte[] data = new byte[]{0x0, 0x0, 0x0, 0x1};\n    Files.write(data, tmp);\n    final long size = data.length;\n\n    HdfsDataSegmentPusherConfig config = new HdfsDataSegmentPusherConfig();\n    final File storageDirectory = tempFolder.newFolder();\n\n    config.setStorageDirectory(\n        scheme != null\n        ? StringUtils.format(\"%s://%s\", scheme, storageDirectory.getAbsolutePath())\n        : storageDirectory.getAbsolutePath()\n    );\n    HdfsDataSegmentPusher pusher = new HdfsDataSegmentPusher(config, conf, new DefaultObjectMapper());\n\n    DataSegment segmentToPush = new DataSegment(\n        \"foo\",\n        Intervals.of(\"2015/2016\"),\n        \"0\",\n        new HashMap<>(),\n        new ArrayList<>(),\n        new ArrayList<>(),\n        NoneShardSpec.instance(),\n        0,\n        size\n    );\n\n    DataSegment segment = pusher.push(segmentDir, segmentToPush, false);\n\n\n    String indexUri = StringUtils.format(\n        \"%s/%s/%d_index.zip\",\n        FileSystem.newInstance(conf).makeQualified(new Path(config.getStorageDirectory())).toUri().toString(),\n        pusher.getStorageDir(segmentToPush, false),\n        segmentToPush.getShardSpec().getPartitionNum()\n    );\n\n    Assert.assertEquals(segmentToPush.getSize(), segment.getSize());\n    Assert.assertEquals(segmentToPush, segment);\n    Assert.assertEquals(ImmutableMap.of(\n        \"type\",\n        \"hdfs\",\n        \"path\",\n        indexUri\n    ), segment.getLoadSpec());\n    \r\n    final String segmentPath = pusher.getStorageDir(segment, false);\n\n    File indexFile = new File(StringUtils.format(\n        \"%s/%s/%d_index.zip\",\n        storageDirectory,\n        segmentPath,\n        segment.getShardSpec().getPartitionNum()\n    ));\n    Assert.assertTrue(indexFile.exists());\n    File descriptorFile = new File(StringUtils.format(\n        \"%s/%s/%d_descriptor.json\",\n        storageDirectory,\n        segmentPath,\n        segment.getShardSpec().getPartitionNum()\n    ));\n    Assert.assertTrue(descriptorFile.exists());\n\n    \r\n    File outDir = new File(StringUtils.format(\"%s/%s\", config.getStorageDirectory(), segmentPath));\n    outDir.setReadOnly();\n    try {\n      pusher.push(segmentDir, segmentToPush, false);\n    }\n    catch (IOException e) {\n      Assert.fail(\"should not throw exception\");\n    }\n  }\n","date":"2018-10-29 20:02:43","endLine":357,"groupId":"4576","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testUsingScheme","params":"(finalStringscheme)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/63/0aa2447fd083c8a5e081340db3a24940510cf3.src","preCode":"  private void testUsingScheme(final String scheme) throws Exception\n  {\n    Configuration conf = new Configuration(true);\n\n    \r\n    File segmentDir = tempFolder.newFolder();\n    File tmp = new File(segmentDir, \"version.bin\");\n\n    final byte[] data = new byte[]{0x0, 0x0, 0x0, 0x1};\n    Files.write(data, tmp);\n    final long size = data.length;\n\n    HdfsDataSegmentPusherConfig config = new HdfsDataSegmentPusherConfig();\n    final File storageDirectory = tempFolder.newFolder();\n\n    config.setStorageDirectory(\n        scheme != null\n        ? StringUtils.format(\"%s://%s\", scheme, storageDirectory.getAbsolutePath())\n        : storageDirectory.getAbsolutePath()\n    );\n    HdfsDataSegmentPusher pusher = new HdfsDataSegmentPusher(config, conf, new DefaultObjectMapper());\n\n    DataSegment segmentToPush = new DataSegment(\n        \"foo\",\n        Intervals.of(\"2015/2016\"),\n        \"0\",\n        Maps.newHashMap(),\n        Lists.newArrayList(),\n        Lists.newArrayList(),\n        NoneShardSpec.instance(),\n        0,\n        size\n    );\n\n    DataSegment segment = pusher.push(segmentDir, segmentToPush, false);\n\n\n    String indexUri = StringUtils.format(\n        \"%s/%s/%d_index.zip\",\n        FileSystem.newInstance(conf).makeQualified(new Path(config.getStorageDirectory())).toUri().toString(),\n        pusher.getStorageDir(segmentToPush, false),\n        segmentToPush.getShardSpec().getPartitionNum()\n    );\n\n    Assert.assertEquals(segmentToPush.getSize(), segment.getSize());\n    Assert.assertEquals(segmentToPush, segment);\n    Assert.assertEquals(ImmutableMap.of(\n        \"type\",\n        \"hdfs\",\n        \"path\",\n        indexUri\n    ), segment.getLoadSpec());\n    \r\n    final String segmentPath = pusher.getStorageDir(segment, false);\n\n    File indexFile = new File(StringUtils.format(\n        \"%s/%s/%d_index.zip\",\n        storageDirectory,\n        segmentPath,\n        segment.getShardSpec().getPartitionNum()\n    ));\n    Assert.assertTrue(indexFile.exists());\n    File descriptorFile = new File(StringUtils.format(\n        \"%s/%s/%d_descriptor.json\",\n        storageDirectory,\n        segmentPath,\n        segment.getShardSpec().getPartitionNum()\n    ));\n    Assert.assertTrue(descriptorFile.exists());\n\n    \r\n    File outDir = new File(StringUtils.format(\"%s/%s\", config.getStorageDirectory(), segmentPath));\n    outDir.setReadOnly();\n    try {\n      pusher.push(segmentDir, segmentToPush, false);\n    }\n    catch (IOException e) {\n      Assert.fail(\"should not throw exception\");\n    }\n  }\n","realPath":"extensions-core/hdfs-storage/src/test/java/org/apache/druid/storage/hdfs/HdfsDataSegmentPusherTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":278,"status":"M"}],"commitId":"676f5e6d7f184101b8763e4249b18b237bbe0ec7","commitMessage":"@@@Prohibit some guava collection APIs and use JDK collection APIs directly (#6511)\n\n* Prohibit some guava collection APIs and use JDK APIs directly\n\n* reset files that changed by accident\n\n* sort codestyle/druid-forbidden-apis.txt alphabetically\n","date":"2018-10-29 20:02:43","modifiedFileCount":"427","status":"M","submitter":"QiuMM"},{"authorTime":"2019-02-21 07:10:29","codes":[{"authorDate":"2019-02-21 07:10:29","commitOrder":3,"curCode":"  private void testUsingSchemeForMultipleSegments(final String scheme, final int numberOfSegments) throws Exception\n  {\n    Configuration conf = new Configuration(true);\n    DataSegment[] segments = new DataSegment[numberOfSegments];\n\n    \r\n    File segmentDir = tempFolder.newFolder();\n    File tmp = new File(segmentDir, \"version.bin\");\n\n    final byte[] data = new byte[]{0x0, 0x0, 0x0, 0x1};\n    Files.write(data, tmp);\n    final long size = data.length;\n\n    HdfsDataSegmentPusherConfig config = new HdfsDataSegmentPusherConfig();\n    final File storageDirectory = tempFolder.newFolder();\n\n    config.setStorageDirectory(\n        scheme != null\n        ? StringUtils.format(\"%s://%s\", scheme, storageDirectory.getAbsolutePath())\n        : storageDirectory.getAbsolutePath()\n    );\n    HdfsDataSegmentPusher pusher = new HdfsDataSegmentPusher(config, conf, new DefaultObjectMapper());\n\n    for (int i = 0; i < numberOfSegments; i++) {\n      segments[i] = new DataSegment(\n          \"foo\",\n          Intervals.of(\"2015/2016\"),\n          \"0\",\n          new HashMap<>(),\n          new ArrayList<>(),\n          new ArrayList<>(),\n          new NumberedShardSpec(i, i),\n          0,\n          size\n      );\n    }\n\n    for (int i = 0; i < numberOfSegments; i++) {\n      final DataSegment pushedSegment = pusher.push(segmentDir, segments[i], false);\n\n      String indexUri = StringUtils.format(\n          \"%s/%s/%d_index.zip\",\n          FileSystem.newInstance(conf).makeQualified(new Path(config.getStorageDirectory())).toUri().toString(),\n          pusher.getStorageDir(segments[i], false),\n          segments[i].getShardSpec().getPartitionNum()\n      );\n\n      Assert.assertEquals(segments[i].getSize(), pushedSegment.getSize());\n      Assert.assertEquals(segments[i], pushedSegment);\n      Assert.assertEquals(ImmutableMap.of(\n          \"type\",\n          \"hdfs\",\n          \"path\",\n          indexUri\n      ), pushedSegment.getLoadSpec());\n      \r\n      String segmentPath = pusher.getStorageDir(pushedSegment, false);\n\n      File indexFile = new File(StringUtils.format(\n          \"%s/%s/%d_index.zip\",\n          storageDirectory,\n          segmentPath,\n          pushedSegment.getShardSpec().getPartitionNum()\n      ));\n      Assert.assertTrue(indexFile.exists());\n\n      Assert.assertEquals(segments[i].getSize(), pushedSegment.getSize());\n      Assert.assertEquals(segments[i], pushedSegment);\n\n      indexFile = new File(StringUtils.format(\n          \"%s/%s/%d_index.zip\",\n          storageDirectory,\n          segmentPath,\n          pushedSegment.getShardSpec().getPartitionNum()\n      ));\n      Assert.assertTrue(indexFile.exists());\n\n\n      \r\n      File outDir = new File(StringUtils.format(\"%s/%s\", config.getStorageDirectory(), segmentPath));\n      outDir.setReadOnly();\n      try {\n        pusher.push(segmentDir, segments[i], false);\n      }\n      catch (IOException e) {\n        Assert.fail(\"should not throw exception\");\n      }\n    }\n  }\n","date":"2019-02-21 07:10:29","endLine":261,"groupId":"103430","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testUsingSchemeForMultipleSegments","params":"(finalStringscheme@finalintnumberOfSegments)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f1/045aca9ce971e5b062b3650ff3de64fe381e64.src","preCode":"  private void testUsingSchemeForMultipleSegments(final String scheme, final int numberOfSegments) throws Exception\n  {\n    Configuration conf = new Configuration(true);\n    DataSegment[] segments = new DataSegment[numberOfSegments];\n\n    \r\n    File segmentDir = tempFolder.newFolder();\n    File tmp = new File(segmentDir, \"version.bin\");\n\n    final byte[] data = new byte[]{0x0, 0x0, 0x0, 0x1};\n    Files.write(data, tmp);\n    final long size = data.length;\n\n    HdfsDataSegmentPusherConfig config = new HdfsDataSegmentPusherConfig();\n    final File storageDirectory = tempFolder.newFolder();\n\n    config.setStorageDirectory(\n        scheme != null\n        ? StringUtils.format(\"%s://%s\", scheme, storageDirectory.getAbsolutePath())\n        : storageDirectory.getAbsolutePath()\n    );\n    HdfsDataSegmentPusher pusher = new HdfsDataSegmentPusher(config, conf, new DefaultObjectMapper());\n\n    for (int i = 0; i < numberOfSegments; i++) {\n      segments[i] = new DataSegment(\n          \"foo\",\n          Intervals.of(\"2015/2016\"),\n          \"0\",\n          new HashMap<>(),\n          new ArrayList<>(),\n          new ArrayList<>(),\n          new NumberedShardSpec(i, i),\n          0,\n          size\n      );\n    }\n\n    for (int i = 0; i < numberOfSegments; i++) {\n      final DataSegment pushedSegment = pusher.push(segmentDir, segments[i], false);\n\n      String indexUri = StringUtils.format(\n          \"%s/%s/%d_index.zip\",\n          FileSystem.newInstance(conf).makeQualified(new Path(config.getStorageDirectory())).toUri().toString(),\n          pusher.getStorageDir(segments[i], false),\n          segments[i].getShardSpec().getPartitionNum()\n      );\n\n      Assert.assertEquals(segments[i].getSize(), pushedSegment.getSize());\n      Assert.assertEquals(segments[i], pushedSegment);\n      Assert.assertEquals(ImmutableMap.of(\n          \"type\",\n          \"hdfs\",\n          \"path\",\n          indexUri\n      ), pushedSegment.getLoadSpec());\n      \r\n      String segmentPath = pusher.getStorageDir(pushedSegment, false);\n\n      File indexFile = new File(StringUtils.format(\n          \"%s/%s/%d_index.zip\",\n          storageDirectory,\n          segmentPath,\n          pushedSegment.getShardSpec().getPartitionNum()\n      ));\n      Assert.assertTrue(indexFile.exists());\n      File descriptorFile = new File(StringUtils.format(\n          \"%s/%s/%d_descriptor.json\",\n          storageDirectory,\n          segmentPath,\n          pushedSegment.getShardSpec().getPartitionNum()\n      ));\n      Assert.assertTrue(descriptorFile.exists());\n\n      \r\n      DataSegment fromDescriptorFileDataSegment = objectMapper.readValue(descriptorFile, DataSegment.class);\n\n      Assert.assertEquals(segments[i].getSize(), pushedSegment.getSize());\n      Assert.assertEquals(segments[i], pushedSegment);\n      Assert.assertEquals(ImmutableMap.of(\n          \"type\",\n          \"hdfs\",\n          \"path\",\n          indexUri\n      ), fromDescriptorFileDataSegment.getLoadSpec());\n      \r\n      segmentPath = pusher.getStorageDir(fromDescriptorFileDataSegment, false);\n\n      indexFile = new File(StringUtils.format(\n          \"%s/%s/%d_index.zip\",\n          storageDirectory,\n          segmentPath,\n          fromDescriptorFileDataSegment.getShardSpec().getPartitionNum()\n      ));\n      Assert.assertTrue(indexFile.exists());\n\n\n      \r\n      File outDir = new File(StringUtils.format(\"%s/%s\", config.getStorageDirectory(), segmentPath));\n      outDir.setReadOnly();\n      try {\n        pusher.push(segmentDir, segments[i], false);\n      }\n      catch (IOException e) {\n        Assert.fail(\"should not throw exception\");\n      }\n    }\n  }\n","realPath":"extensions-core/hdfs-storage/src/test/java/org/apache/druid/storage/hdfs/HdfsDataSegmentPusherTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":173,"status":"M"},{"authorDate":"2019-02-21 07:10:29","commitOrder":3,"curCode":"  private void testUsingScheme(final String scheme) throws Exception\n  {\n    Configuration conf = new Configuration(true);\n\n    \r\n    File segmentDir = tempFolder.newFolder();\n    File tmp = new File(segmentDir, \"version.bin\");\n\n    final byte[] data = new byte[]{0x0, 0x0, 0x0, 0x1};\n    Files.write(data, tmp);\n    final long size = data.length;\n\n    HdfsDataSegmentPusherConfig config = new HdfsDataSegmentPusherConfig();\n    final File storageDirectory = tempFolder.newFolder();\n\n    config.setStorageDirectory(\n        scheme != null\n        ? StringUtils.format(\"%s://%s\", scheme, storageDirectory.getAbsolutePath())\n        : storageDirectory.getAbsolutePath()\n    );\n    HdfsDataSegmentPusher pusher = new HdfsDataSegmentPusher(config, conf, new DefaultObjectMapper());\n\n    DataSegment segmentToPush = new DataSegment(\n        \"foo\",\n        Intervals.of(\"2015/2016\"),\n        \"0\",\n        new HashMap<>(),\n        new ArrayList<>(),\n        new ArrayList<>(),\n        NoneShardSpec.instance(),\n        0,\n        size\n    );\n\n    DataSegment segment = pusher.push(segmentDir, segmentToPush, false);\n\n\n    String indexUri = StringUtils.format(\n        \"%s/%s/%d_index.zip\",\n        FileSystem.newInstance(conf).makeQualified(new Path(config.getStorageDirectory())).toUri().toString(),\n        pusher.getStorageDir(segmentToPush, false),\n        segmentToPush.getShardSpec().getPartitionNum()\n    );\n\n    Assert.assertEquals(segmentToPush.getSize(), segment.getSize());\n    Assert.assertEquals(segmentToPush, segment);\n    Assert.assertEquals(ImmutableMap.of(\n        \"type\",\n        \"hdfs\",\n        \"path\",\n        indexUri\n    ), segment.getLoadSpec());\n    \r\n    final String segmentPath = pusher.getStorageDir(segment, false);\n\n    File indexFile = new File(StringUtils.format(\n        \"%s/%s/%d_index.zip\",\n        storageDirectory,\n        segmentPath,\n        segment.getShardSpec().getPartitionNum()\n    ));\n    Assert.assertTrue(indexFile.exists());\n\n    \r\n    File outDir = new File(StringUtils.format(\"%s/%s\", config.getStorageDirectory(), segmentPath));\n    outDir.setReadOnly();\n    try {\n      pusher.push(segmentDir, segmentToPush, false);\n    }\n    catch (IOException e) {\n      Assert.fail(\"should not throw exception\");\n    }\n  }\n","date":"2019-02-21 07:10:29","endLine":335,"groupId":"103430","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testUsingScheme","params":"(finalStringscheme)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f1/045aca9ce971e5b062b3650ff3de64fe381e64.src","preCode":"  private void testUsingScheme(final String scheme) throws Exception\n  {\n    Configuration conf = new Configuration(true);\n\n    \r\n    File segmentDir = tempFolder.newFolder();\n    File tmp = new File(segmentDir, \"version.bin\");\n\n    final byte[] data = new byte[]{0x0, 0x0, 0x0, 0x1};\n    Files.write(data, tmp);\n    final long size = data.length;\n\n    HdfsDataSegmentPusherConfig config = new HdfsDataSegmentPusherConfig();\n    final File storageDirectory = tempFolder.newFolder();\n\n    config.setStorageDirectory(\n        scheme != null\n        ? StringUtils.format(\"%s://%s\", scheme, storageDirectory.getAbsolutePath())\n        : storageDirectory.getAbsolutePath()\n    );\n    HdfsDataSegmentPusher pusher = new HdfsDataSegmentPusher(config, conf, new DefaultObjectMapper());\n\n    DataSegment segmentToPush = new DataSegment(\n        \"foo\",\n        Intervals.of(\"2015/2016\"),\n        \"0\",\n        new HashMap<>(),\n        new ArrayList<>(),\n        new ArrayList<>(),\n        NoneShardSpec.instance(),\n        0,\n        size\n    );\n\n    DataSegment segment = pusher.push(segmentDir, segmentToPush, false);\n\n\n    String indexUri = StringUtils.format(\n        \"%s/%s/%d_index.zip\",\n        FileSystem.newInstance(conf).makeQualified(new Path(config.getStorageDirectory())).toUri().toString(),\n        pusher.getStorageDir(segmentToPush, false),\n        segmentToPush.getShardSpec().getPartitionNum()\n    );\n\n    Assert.assertEquals(segmentToPush.getSize(), segment.getSize());\n    Assert.assertEquals(segmentToPush, segment);\n    Assert.assertEquals(ImmutableMap.of(\n        \"type\",\n        \"hdfs\",\n        \"path\",\n        indexUri\n    ), segment.getLoadSpec());\n    \r\n    final String segmentPath = pusher.getStorageDir(segment, false);\n\n    File indexFile = new File(StringUtils.format(\n        \"%s/%s/%d_index.zip\",\n        storageDirectory,\n        segmentPath,\n        segment.getShardSpec().getPartitionNum()\n    ));\n    Assert.assertTrue(indexFile.exists());\n    File descriptorFile = new File(StringUtils.format(\n        \"%s/%s/%d_descriptor.json\",\n        storageDirectory,\n        segmentPath,\n        segment.getShardSpec().getPartitionNum()\n    ));\n    Assert.assertTrue(descriptorFile.exists());\n\n    \r\n    File outDir = new File(StringUtils.format(\"%s/%s\", config.getStorageDirectory(), segmentPath));\n    outDir.setReadOnly();\n    try {\n      pusher.push(segmentDir, segmentToPush, false);\n    }\n    catch (IOException e) {\n      Assert.fail(\"should not throw exception\");\n    }\n  }\n","realPath":"extensions-core/hdfs-storage/src/test/java/org/apache/druid/storage/hdfs/HdfsDataSegmentPusherTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":263,"status":"M"}],"commitId":"4e2b0852012ab4aed20189878a202e3fe590710b","commitMessage":"@@@Remove DataSegmentFinder.  InsertSegmentToDb.  and descriptor.json file in deep storage (#6911)\n\n* Remove DataSegmentFinder.  InsertSegmentToDb.  and descriptor.json file\n\n* delete descriptor.file when killing segments\n\n* fix test\n\n* Add doc for ha\n\n* improve warning\n","date":"2019-02-21 07:10:29","modifiedFileCount":"37","status":"M","submitter":"Jihoon Son"}]
