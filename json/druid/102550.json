[{"authorTime":"2018-12-22 03:49:24","codes":[{"authorDate":"2018-12-22 03:49:24","commitOrder":1,"curCode":"  public void testSeek() throws InterruptedException, ExecutionException\n  {\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), objectMapper);\n\n    recordSupplier.assign(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seek(partition0, 2L);\n    recordSupplier.seek(partition1, 2L);\n\n    List<OrderedPartitionableRecord<Integer, Long>> initialRecords = createOrderedPartitionableRecords();\n\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n    for (int i = 0; polledRecords.size() != 11 && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n\n    Assert.assertEquals(11, polledRecords.size());\n    Assert.assertTrue(initialRecords.containsAll(polledRecords));\n\n\n    recordSupplier.close();\n\n  }\n","date":"2018-12-22 03:49:24","endLine":323,"groupId":"19857","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testSeek","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/2f/445aa994319a7249a367bdb57844d4987601b6.src","preCode":"  public void testSeek() throws InterruptedException, ExecutionException\n  {\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), objectMapper);\n\n    recordSupplier.assign(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seek(partition0, 2L);\n    recordSupplier.seek(partition1, 2L);\n\n    List<OrderedPartitionableRecord<Integer, Long>> initialRecords = createOrderedPartitionableRecords();\n\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n    for (int i = 0; polledRecords.size() != 11 && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n\n    Assert.assertEquals(11, polledRecords.size());\n    Assert.assertTrue(initialRecords.containsAll(polledRecords));\n\n\n    recordSupplier.close();\n\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaRecordSupplierTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":280,"status":"B"},{"authorDate":"2018-12-22 03:49:24","commitOrder":1,"curCode":"  public void testSeekToLatest() throws InterruptedException, ExecutionException\n  {\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), objectMapper);\n\n    recordSupplier.assign(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seekToLatest(partitions);\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n\n    Assert.assertEquals(Collections.emptyList(), polledRecords);\n    recordSupplier.close();\n  }\n","date":"2018-12-22 03:49:24","endLine":356,"groupId":"19857","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testSeekToLatest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/2f/445aa994319a7249a367bdb57844d4987601b6.src","preCode":"  public void testSeekToLatest() throws InterruptedException, ExecutionException\n  {\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), objectMapper);\n\n    recordSupplier.assign(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seekToLatest(partitions);\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n\n    Assert.assertEquals(Collections.emptyList(), polledRecords);\n    recordSupplier.close();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaRecordSupplierTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":326,"status":"B"}],"commitId":"7c7997e8a1183a7bffad731ca94e8b4c381e8665","commitMessage":"@@@Add Kinesis Indexing Service to core Druid (#6431)\n\n* created seekablestream classes\n\n* created seekablestreamsupervisor class\n\n* first attempt to integrate kafa indexing service to use SeekableStream\n\n* seekablestream bug fixes\n\n* kafkarecordsupplier\n\n* integrated kafka indexing service with seekablestream\n\n* implemented resume/suspend and refactored some package names\n\n* moved kinesis indexing service into core druid extensions\n\n* merged some changes from kafka supervisor race condition\n\n* integrated kinesis-indexing-service with seekablestream\n\n* unite tests for kinesis-indexing-service\n\n* various bug fixes for kinesis-indexing-service\n\n* refactored kinesisindexingtask\n\n* finished up more kinesis unit tests\n\n* more bug fixes for kinesis-indexing-service\n\n* finsihed refactoring kinesis unit tests\n\n* removed KinesisParititons and KafkaPartitions to use SeekableStreamPartitions\n\n* kinesis-indexing-service code cleanup and docs\n\n* merge #6291\n\nmerge #6337\n\nmerge #6383\n\n* added more docs and reordered methods\n\n* fixd kinesis tests after merging master and added docs in seekablestream\n\n* fix various things from pr comment\n\n* improve recordsupplier and add unit tests\n\n* migrated to aws-java-sdk-kinesis\n\n* merge changes from master\n\n* fix pom files and forbiddenapi checks\n\n* checkpoint JavaType bug fix\n\n* fix pom and stuff\n\n* disable checkpointing in kinesis\n\n* fix kinesis sequence number null in closed shard\n\n* merge changes from master\n\n* fixes for kinesis tasks\n\n* capitalized <partitionType.  sequenceType>\n\n* removed abstract class loggers\n\n* conform to guava api restrictions\n\n* add docker for travis other modules test\n\n* address comments\n\n* improve RecordSupplier to supply records in batch\n\n* fix strict compile issue\n\n* add test scope for localstack dependency\n\n* kinesis indexing task refactoring\n\n* comments\n\n* github comments\n\n* minor fix\n\n* removed unneeded readme\n\n* fix deserialization bug\n\n* fix various bugs\n\n* KinesisRecordSupplier unable to catch up to earliest position in stream bug fix\n\n* minor changes to kinesis\n\n* implement deaggregate for kinesis\n\n* Merge remote-tracking branch 'upstream/master' into seekablestream\n\n* fix kinesis offset discrepancy with kafka\n\n* kinesis record supplier disable getPosition\n\n* pr comments\n\n* mock for kinesis tests and remove docker dependency for unit tests\n\n* PR comments\n\n* avg lag in kafkasupervisor #6587\n\n* refacotred SequenceMetadata in taskRunners\n\n* small fix\n\n* more small fix\n\n* recordsupplier resource leak\n\n* revert .travis.yml formatting\n\n* fix style\n\n* kinesis docs\n\n* doc part2\n\n* more docs\n\n* comments\n\n* comments*2\n\n* revert string replace changes\n\n* comments\n\n* teamcity\n\n* comments part 1\n\n* comments part 2\n\n* comments part 3\n\n* merge #6754\n\n* fix injection binding\n\n* comments\n\n* KinesisRegion refactor\n\n* comments part idk lol\n\n* can't think of a commit msg anymore\n\n* remove possiblyResetDataSourceMetadata() for IncrementalPublishingTaskRunner\n\n* commmmmmmmmmments\n\n* extra error handling in KinesisRecordSupplier getRecords\n\n* comments\n\n* quickfix\n\n* typo\n\n* oof\n","date":"2018-12-22 03:49:24","modifiedFileCount":"22","status":"B","submitter":"Joshua Sun"},{"authorTime":"2019-02-19 03:50:08","codes":[{"authorDate":"2019-02-19 03:50:08","commitOrder":2,"curCode":"  public void testSeek() throws InterruptedException, ExecutionException\n  {\n    \r\n    insertData();\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), objectMapper);\n\n    recordSupplier.assign(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seek(partition0, 2L);\n    recordSupplier.seek(partition1, 2L);\n\n    List<OrderedPartitionableRecord<Integer, Long>> initialRecords = createOrderedPartitionableRecords();\n\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n    for (int i = 0; polledRecords.size() != 11 && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n\n    Assert.assertEquals(11, polledRecords.size());\n    Assert.assertTrue(initialRecords.containsAll(polledRecords));\n\n\n    recordSupplier.close();\n\n  }\n","date":"2019-02-19 03:50:08","endLine":339,"groupId":"19857","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testSeek","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f9/44bf04610b33cb5d4e910b174cc1d9d834f984.src","preCode":"  public void testSeek() throws InterruptedException, ExecutionException\n  {\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), objectMapper);\n\n    recordSupplier.assign(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seek(partition0, 2L);\n    recordSupplier.seek(partition1, 2L);\n\n    List<OrderedPartitionableRecord<Integer, Long>> initialRecords = createOrderedPartitionableRecords();\n\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n    for (int i = 0; polledRecords.size() != 11 && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n\n    Assert.assertEquals(11, polledRecords.size());\n    Assert.assertTrue(initialRecords.containsAll(polledRecords));\n\n\n    recordSupplier.close();\n\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaRecordSupplierTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":300,"status":"M"},{"authorDate":"2019-02-19 03:50:08","commitOrder":2,"curCode":"  public void testSeekToLatest() throws InterruptedException, ExecutionException\n  {\n    \r\n    insertData();\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), objectMapper);\n\n    recordSupplier.assign(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seekToLatest(partitions);\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n\n    Assert.assertEquals(Collections.emptyList(), polledRecords);\n    recordSupplier.close();\n  }\n","date":"2019-02-19 03:50:08","endLine":368,"groupId":"19857","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testSeekToLatest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f9/44bf04610b33cb5d4e910b174cc1d9d834f984.src","preCode":"  public void testSeekToLatest() throws InterruptedException, ExecutionException\n  {\n    \r\n    try (final KafkaProducer<byte[], byte[]> kafkaProducer = kafkaServer.newProducer()) {\n      for (ProducerRecord<byte[], byte[]> record : records) {\n        kafkaProducer.send(record).get();\n      }\n    }\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), objectMapper);\n\n    recordSupplier.assign(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seekToLatest(partitions);\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n\n    Assert.assertEquals(Collections.emptyList(), polledRecords);\n    recordSupplier.close();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaRecordSupplierTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":342,"status":"M"}],"commitId":"80a2ef7be46c4fc807ac2a74507b8ba8b6a44049","commitMessage":"@@@Support kafka transactional topics (#5404) (#6496)\n\n* Support kafka transactional topics\n\n* update kafka to version 2.0.0\n* Remove the skipOffsetGaps option since it's not used anymore\n* Adjust kafka consumer to use transactional semantics\n* Update tests\n\n* Remove unused import from test\n\n* Fix compilation\n\n* Invoke transaction api to fix a unit test\n\n* temporary modification of travis.yml for debugging\n\n* another attempt to get travis tasklogs\n\n* update kafka to 2.0.1 at all places\n\n* Remove druid-kafka-eight dependency from integration-tests.  remove the kafka firehose test and deprecate kafka-eight classes\n\n* Add deprecated in docs for kafka-eight and kafka-simple extensions\n\n* Remove skipOffsetGaps and code changes for transaction support\n\n* Fix indentation\n\n* remove skipOffsetGaps from kinesis\n\n* Add transaction api to KafkaRecordSupplierTest\n\n* Fix indent\n\n* Fix test\n\n* update kafka version to 2.1.0\n","date":"2019-02-19 03:50:08","modifiedFileCount":"25","status":"M","submitter":"Surekha"},{"authorTime":"2019-03-22 02:19:49","codes":[{"authorDate":"2019-03-22 02:19:49","commitOrder":3,"curCode":"  public void testSeek() throws InterruptedException, ExecutionException\n  {\n    \r\n    insertData();\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), objectMapper);\n\n    recordSupplier.assign(partitions);\n    recordSupplier.seekToEarliest(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seek(partition0, 2L);\n    recordSupplier.seek(partition1, 2L);\n\n    List<OrderedPartitionableRecord<Integer, Long>> initialRecords = createOrderedPartitionableRecords();\n\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n    for (int i = 0; polledRecords.size() != 11 && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n\n    Assert.assertEquals(11, polledRecords.size());\n    Assert.assertTrue(initialRecords.containsAll(polledRecords));\n\n\n    recordSupplier.close();\n\n  }\n","date":"2019-03-22 02:19:49","endLine":338,"groupId":"16377","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testSeek","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b5/0588406162b213f7aa2b2632278d4fb0bb6c6e.src","preCode":"  public void testSeek() throws InterruptedException, ExecutionException\n  {\n    \r\n    insertData();\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), objectMapper);\n\n    recordSupplier.assign(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seek(partition0, 2L);\n    recordSupplier.seek(partition1, 2L);\n\n    List<OrderedPartitionableRecord<Integer, Long>> initialRecords = createOrderedPartitionableRecords();\n\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n    for (int i = 0; polledRecords.size() != 11 && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n\n    Assert.assertEquals(11, polledRecords.size());\n    Assert.assertTrue(initialRecords.containsAll(polledRecords));\n\n\n    recordSupplier.close();\n\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaRecordSupplierTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":298,"status":"M"},{"authorDate":"2019-03-22 02:19:49","commitOrder":3,"curCode":"  public void testSeekToLatest() throws InterruptedException, ExecutionException\n  {\n    \r\n    insertData();\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), objectMapper);\n\n    recordSupplier.assign(partitions);\n    recordSupplier.seekToEarliest(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seekToLatest(partitions);\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n\n    Assert.assertEquals(Collections.emptyList(), polledRecords);\n    recordSupplier.close();\n  }\n","date":"2019-03-22 02:19:49","endLine":368,"groupId":"16377","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testSeekToLatest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b5/0588406162b213f7aa2b2632278d4fb0bb6c6e.src","preCode":"  public void testSeekToLatest() throws InterruptedException, ExecutionException\n  {\n    \r\n    insertData();\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), objectMapper);\n\n    recordSupplier.assign(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seekToLatest(partitions);\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n\n    Assert.assertEquals(Collections.emptyList(), polledRecords);\n    recordSupplier.close();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaRecordSupplierTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":341,"status":"M"}],"commitId":"e17020387601cdebb7db6a0a8a3c3052aaf66064","commitMessage":"@@@Consolidate kafka consumer configs (#7249)\n\n* Consolidate kafka consumer configs\n\n* change the order of adding properties\n\n* Add consumer properties to fix test\n\nit seems kafka consumer does not reveive any message without these configs\n\n* Use KafkaConsumerConfigs in integration test\n\n* Update zookeeper and kafka versions in the setup.sh for the base druid image\n\n*  use version 0.2 of base druid image\n\n* Try to fix tests in KafkaRecordSupplierTest\n\n* unused import\n\n* Fix tests in KafkaSupervisorTest\n","date":"2019-03-22 02:19:49","modifiedFileCount":"8","status":"M","submitter":"Surekha"},{"authorTime":"2019-08-23 18:13:54","codes":[{"authorDate":"2019-08-23 18:13:54","commitOrder":4,"curCode":"  public void testSeek() throws InterruptedException, ExecutionException\n  {\n    \r\n    insertData();\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), OBJECT_MAPPER);\n\n    recordSupplier.assign(partitions);\n    recordSupplier.seekToEarliest(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seek(partition0, 2L);\n    recordSupplier.seek(partition1, 2L);\n\n    List<OrderedPartitionableRecord<Integer, Long>> initialRecords = createOrderedPartitionableRecords();\n\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n    for (int i = 0; polledRecords.size() != 11 && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n\n    Assert.assertEquals(11, polledRecords.size());\n    Assert.assertTrue(initialRecords.containsAll(polledRecords));\n\n\n    recordSupplier.close();\n\n  }\n","date":"2019-08-23 18:13:54","endLine":430,"groupId":"16377","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testSeek","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c4/739abc4deada30f0fd7e54706b1b200f2a2135.src","preCode":"  public void testSeek() throws InterruptedException, ExecutionException\n  {\n    \r\n    insertData();\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), objectMapper);\n\n    recordSupplier.assign(partitions);\n    recordSupplier.seekToEarliest(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seek(partition0, 2L);\n    recordSupplier.seek(partition1, 2L);\n\n    List<OrderedPartitionableRecord<Integer, Long>> initialRecords = createOrderedPartitionableRecords();\n\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n    for (int i = 0; polledRecords.size() != 11 && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n\n    Assert.assertEquals(11, polledRecords.size());\n    Assert.assertTrue(initialRecords.containsAll(polledRecords));\n\n\n    recordSupplier.close();\n\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaRecordSupplierTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":390,"status":"M"},{"authorDate":"2019-08-23 18:13:54","commitOrder":4,"curCode":"  public void testSeekToLatest() throws InterruptedException, ExecutionException\n  {\n    \r\n    insertData();\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), OBJECT_MAPPER);\n\n    recordSupplier.assign(partitions);\n    recordSupplier.seekToEarliest(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seekToLatest(partitions);\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n\n    Assert.assertEquals(Collections.emptyList(), polledRecords);\n    recordSupplier.close();\n  }\n","date":"2019-08-23 18:13:54","endLine":460,"groupId":"16377","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testSeekToLatest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c4/739abc4deada30f0fd7e54706b1b200f2a2135.src","preCode":"  public void testSeekToLatest() throws InterruptedException, ExecutionException\n  {\n    \r\n    insertData();\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), objectMapper);\n\n    recordSupplier.assign(partitions);\n    recordSupplier.seekToEarliest(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seekToLatest(partitions);\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n\n    Assert.assertEquals(Collections.emptyList(), polledRecords);\n    recordSupplier.close();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaRecordSupplierTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":433,"status":"M"}],"commitId":"33f0753a70361e7d345a488034f76a889f7c3682","commitMessage":"@@@Add Checkstyle for constant name static final (#8060)\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* merging with upstream\n\n* review-1\n\n* unknow changes\n\n* unknow changes\n\n* review-2\n\n* merging with master\n\n* review-2 1 changes\n\n* review changes-2 2\n\n* bug fix\n","date":"2019-08-23 18:13:54","modifiedFileCount":"298","status":"M","submitter":"SandishKumarHN"},{"authorTime":"2021-01-09 08:04:37","codes":[{"authorDate":"2021-01-09 08:04:37","commitOrder":5,"curCode":"  public void testSeek() throws InterruptedException, ExecutionException\n  {\n    \r\n    insertData();\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), OBJECT_MAPPER);\n\n    recordSupplier.assign(partitions);\n    recordSupplier.seekToEarliest(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seek(partition0, 2L);\n    recordSupplier.seek(partition1, 2L);\n\n    List<OrderedPartitionableRecord<Integer, Long, KafkaRecordEntity>> initialRecords = createOrderedPartitionableRecords();\n\n    List<OrderedPartitionableRecord<Integer, Long, KafkaRecordEntity>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n    for (int i = 0; polledRecords.size() != 11 && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n\n    Assert.assertEquals(11, polledRecords.size());\n    Assert.assertTrue(initialRecords.containsAll(polledRecords));\n\n\n    recordSupplier.close();\n\n  }\n","date":"2021-01-09 08:04:37","endLine":438,"groupId":"102550","id":9,"instanceNumber":1,"isCurCommit":1,"methodName":"testSeek","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/31/a3ac09044cbda1a7c927011350ba2556a751ad.src","preCode":"  public void testSeek() throws InterruptedException, ExecutionException\n  {\n    \r\n    insertData();\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), OBJECT_MAPPER);\n\n    recordSupplier.assign(partitions);\n    recordSupplier.seekToEarliest(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seek(partition0, 2L);\n    recordSupplier.seek(partition1, 2L);\n\n    List<OrderedPartitionableRecord<Integer, Long>> initialRecords = createOrderedPartitionableRecords();\n\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n    for (int i = 0; polledRecords.size() != 11 && i < pollRetry; i++) {\n      polledRecords.addAll(recordSupplier.poll(poll_timeout_millis));\n      Thread.sleep(200);\n    }\n\n\n    Assert.assertEquals(11, polledRecords.size());\n    Assert.assertTrue(initialRecords.containsAll(polledRecords));\n\n\n    recordSupplier.close();\n\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaRecordSupplierTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":398,"status":"M"},{"authorDate":"2021-01-09 08:04:37","commitOrder":5,"curCode":"  public void testSeekToLatest() throws InterruptedException, ExecutionException\n  {\n    \r\n    insertData();\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), OBJECT_MAPPER);\n\n    recordSupplier.assign(partitions);\n    recordSupplier.seekToEarliest(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seekToLatest(partitions);\n    List<OrderedPartitionableRecord<Integer, Long, KafkaRecordEntity>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n\n    Assert.assertEquals(Collections.emptyList(), polledRecords);\n    recordSupplier.close();\n  }\n","date":"2021-01-09 08:04:37","endLine":468,"groupId":"102550","id":10,"instanceNumber":2,"isCurCommit":1,"methodName":"testSeekToLatest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/31/a3ac09044cbda1a7c927011350ba2556a751ad.src","preCode":"  public void testSeekToLatest() throws InterruptedException, ExecutionException\n  {\n    \r\n    insertData();\n\n    StreamPartition<Integer> partition0 = StreamPartition.of(topic, 0);\n    StreamPartition<Integer> partition1 = StreamPartition.of(topic, 1);\n\n    Set<StreamPartition<Integer>> partitions = ImmutableSet.of(\n        StreamPartition.of(topic, 0),\n        StreamPartition.of(topic, 1)\n    );\n\n    KafkaRecordSupplier recordSupplier = new KafkaRecordSupplier(\n        kafkaServer.consumerProperties(), OBJECT_MAPPER);\n\n    recordSupplier.assign(partitions);\n    recordSupplier.seekToEarliest(partitions);\n\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition0));\n    Assert.assertEquals(0L, (long) recordSupplier.getEarliestSequenceNumber(partition1));\n\n    recordSupplier.seekToLatest(partitions);\n    List<OrderedPartitionableRecord<Integer, Long>> polledRecords = recordSupplier.poll(poll_timeout_millis);\n\n    Assert.assertEquals(Collections.emptyList(), polledRecords);\n    recordSupplier.close();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaRecordSupplierTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":441,"status":"M"}],"commitId":"118b50195e5c2989e04e0f5290aa72cae114db39","commitMessage":"@@@Introduce KafkaRecordEntity to support Kafka headers in InputFormats (#10730)\n\nToday Kafka message support in streaming indexing tasks is limited to\nmessage values.  and does not provide a way to expose Kafka headers. \ntimestamps.  or keys.  which may be of interest to more specialized\nDruid input formats. For instance.  Kafka headers may be used to indicate\npayload format/encoding or additional metadata.  and timestamps are often\nomitted from values in Kafka streams applications.  since they are\nincluded in the record.\n\nThis change proposes to introduce KafkaRecordEntity as InputEntity. \nwhich would give input formats full access to the underlying Kafka record. \nincluding headers.  key.  timestamps. It would also open access to low-level\ninformation such as topic.  partition.  offset if needed.\n\nKafkaEntity is a subclass of ByteEntity for backwards compatibility with\nexisting input formats.  and to avoid introducing unnecessary complexity\nfor Kinesis indexing tasks.","date":"2021-01-09 08:04:37","modifiedFileCount":"30","status":"M","submitter":"Xavier L?aut?"}]
