[{"authorTime":"2020-09-19 07:37:58","codes":[{"authorDate":"2020-09-19 07:37:58","commitOrder":1,"curCode":"  public void testMakeStatsForDataSourceWithCompactedIntervalBetweenNonCompactedIntervals()\n  {\n    \r\n    \r\n    String dataSourceName = DATA_SOURCE_PREFIX + 1;\n    List<DataSegment> segments = new ArrayList<>();\n    for (int j : new int[]{0, 1, 2, 3, 7, 8}) {\n      for (int k = 0; k < 4; k++) {\n        DataSegment beforeNoon = createSegment(dataSourceName, j, true, k);\n        DataSegment afterNoon = createSegment(dataSourceName, j, false, k);\n        if (j == 3) {\n          \r\n          beforeNoon = beforeNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of()));\n          afterNoon = afterNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of()));\n        }\n        if (j == 1) {\n          \r\n          afterNoon = afterNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of()));\n        }\n        segments.add(beforeNoon);\n        segments.add(afterNoon);\n      }\n    }\n\n    dataSources = DataSourcesSnapshot\n        .fromUsedSegments(segments, ImmutableMap.of())\n        .getUsedSegmentsTimelinesPerDataSource();\n\n\n    final TestDruidLeaderClient leaderClient = new TestDruidLeaderClient(JSON_MAPPER);\n    leaderClient.start();\n    final HttpIndexingServiceClient indexingServiceClient = new HttpIndexingServiceClient(JSON_MAPPER, leaderClient);\n    final CompactSegments compactSegments = new CompactSegments(JSON_MAPPER, indexingServiceClient);\n\n    \r\n    Map<String, AutoCompactionSnapshot> autoCompactionSnapshots = compactSegments.getAutoCompactionSnapshot();\n    Assert.assertEquals(0, autoCompactionSnapshots.size());\n\n    \r\n    for (int compaction_run_count = 0; compaction_run_count < 8; compaction_run_count++) {\n      \r\n      final CoordinatorStats stats = doCompactSegments(compactSegments);\n      Assert.assertEquals(\n          1,\n          stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n      );\n\n      verifySnapshot(\n          compactSegments,\n          AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n          dataSourceName,\n          TOTAL_BYTE_PER_DATASOURCE - 120 - 40 * (compaction_run_count + 1),\n          120 + 40 * (compaction_run_count + 1),\n          0,\n          TOTAL_INTERVAL_PER_DATASOURCE - 3 - (compaction_run_count + 1),\n          3 + (compaction_run_count + 1),\n          0,\n          TOTAL_SEGMENT_PER_DATASOURCE - 12 - 4 * (compaction_run_count + 1),\n          \r\n          \r\n          \r\n          12 + 4 + 2 * (compaction_run_count),\n          0\n      );\n    }\n\n    \r\n    final CoordinatorStats stats = doCompactSegments(compactSegments);\n    Assert.assertEquals(\n        0,\n        stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n    );\n    verifySnapshot(\n        compactSegments,\n        AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n        dataSourceName,\n        0,\n        TOTAL_BYTE_PER_DATASOURCE,\n        0,\n        0,\n        TOTAL_INTERVAL_PER_DATASOURCE,\n        0,\n        0,\n        \r\n        \r\n        12 + 16,\n        0\n    );\n  }\n","date":"2020-09-19 07:37:58","endLine":432,"groupId":"14019","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testMakeStatsForDataSourceWithCompactedIntervalBetweenNonCompactedIntervals","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/20/512316840bdcff534541e930b5e57f17cc55b0.src","preCode":"  public void testMakeStatsForDataSourceWithCompactedIntervalBetweenNonCompactedIntervals()\n  {\n    \r\n    \r\n    String dataSourceName = DATA_SOURCE_PREFIX + 1;\n    List<DataSegment> segments = new ArrayList<>();\n    for (int j : new int[]{0, 1, 2, 3, 7, 8}) {\n      for (int k = 0; k < 4; k++) {\n        DataSegment beforeNoon = createSegment(dataSourceName, j, true, k);\n        DataSegment afterNoon = createSegment(dataSourceName, j, false, k);\n        if (j == 3) {\n          \r\n          beforeNoon = beforeNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of()));\n          afterNoon = afterNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of()));\n        }\n        if (j == 1) {\n          \r\n          afterNoon = afterNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of()));\n        }\n        segments.add(beforeNoon);\n        segments.add(afterNoon);\n      }\n    }\n\n    dataSources = DataSourcesSnapshot\n        .fromUsedSegments(segments, ImmutableMap.of())\n        .getUsedSegmentsTimelinesPerDataSource();\n\n\n    final TestDruidLeaderClient leaderClient = new TestDruidLeaderClient(JSON_MAPPER);\n    leaderClient.start();\n    final HttpIndexingServiceClient indexingServiceClient = new HttpIndexingServiceClient(JSON_MAPPER, leaderClient);\n    final CompactSegments compactSegments = new CompactSegments(JSON_MAPPER, indexingServiceClient);\n\n    \r\n    Map<String, AutoCompactionSnapshot> autoCompactionSnapshots = compactSegments.getAutoCompactionSnapshot();\n    Assert.assertEquals(0, autoCompactionSnapshots.size());\n\n    \r\n    for (int compaction_run_count = 0; compaction_run_count < 8; compaction_run_count++) {\n      \r\n      final CoordinatorStats stats = doCompactSegments(compactSegments);\n      Assert.assertEquals(\n          1,\n          stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n      );\n\n      verifySnapshot(\n          compactSegments,\n          AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n          dataSourceName,\n          TOTAL_BYTE_PER_DATASOURCE - 120 - 40 * (compaction_run_count + 1),\n          120 + 40 * (compaction_run_count + 1),\n          0,\n          TOTAL_INTERVAL_PER_DATASOURCE - 3 - (compaction_run_count + 1),\n          3 + (compaction_run_count + 1),\n          0,\n          TOTAL_SEGMENT_PER_DATASOURCE - 12 - 4 * (compaction_run_count + 1),\n          \r\n          \r\n          \r\n          12 + 4 + 2 * (compaction_run_count),\n          0\n      );\n    }\n\n    \r\n    final CoordinatorStats stats = doCompactSegments(compactSegments);\n    Assert.assertEquals(\n        0,\n        stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n    );\n    verifySnapshot(\n        compactSegments,\n        AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n        dataSourceName,\n        0,\n        TOTAL_BYTE_PER_DATASOURCE,\n        0,\n        0,\n        TOTAL_INTERVAL_PER_DATASOURCE,\n        0,\n        0,\n        \r\n        \r\n        12 + 16,\n        0\n    );\n  }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":344,"status":"B"},{"authorDate":"2020-09-19 07:37:58","commitOrder":1,"curCode":"  public void testMakeStatsForDataSourceWithSkipped()\n  {\n    \r\n    \r\n    \r\n    \r\n    String dataSourceName = DATA_SOURCE_PREFIX + 1;\n    List<DataSegment> segments = new ArrayList<>();\n    for (int j : new int[]{0, 1, 2, 3, 7, 8}) {\n      for (int k = 0; k < 4; k++) {\n        DataSegment beforeNoon = createSegment(dataSourceName, j, true, k);\n        DataSegment afterNoon = createSegment(dataSourceName, j, false, k);\n        if (j == 3) {\n          \r\n          beforeNoon = beforeNoon.withSize(100);\n          afterNoon = afterNoon.withSize(100);\n        }\n        if (j == 1) {\n          \r\n          afterNoon = afterNoon.withSize(100);\n        }\n        segments.add(beforeNoon);\n        segments.add(afterNoon);\n      }\n    }\n\n    dataSources = DataSourcesSnapshot\n        .fromUsedSegments(segments, ImmutableMap.of())\n        .getUsedSegmentsTimelinesPerDataSource();\n\n\n    final TestDruidLeaderClient leaderClient = new TestDruidLeaderClient(JSON_MAPPER);\n    leaderClient.start();\n    final HttpIndexingServiceClient indexingServiceClient = new HttpIndexingServiceClient(JSON_MAPPER, leaderClient);\n    final CompactSegments compactSegments = new CompactSegments(JSON_MAPPER, indexingServiceClient);\n\n    \r\n    Map<String, AutoCompactionSnapshot> autoCompactionSnapshots = compactSegments.getAutoCompactionSnapshot();\n    Assert.assertEquals(0, autoCompactionSnapshots.size());\n\n    \r\n    for (int compaction_run_count = 0; compaction_run_count < 8; compaction_run_count++) {\n      \r\n      final CoordinatorStats stats = doCompactSegments(compactSegments);\n      Assert.assertEquals(\n          1,\n          stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n      );\n\n      verifySnapshot(\n          compactSegments,\n          AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n          dataSourceName,\n          \r\n          TOTAL_BYTE_PER_DATASOURCE - 120 - 40 * (compaction_run_count + 1),\n          40 * (compaction_run_count + 1),\n          1200,\n          TOTAL_INTERVAL_PER_DATASOURCE - 3 - (compaction_run_count + 1),\n          (compaction_run_count + 1),\n          3,\n          TOTAL_SEGMENT_PER_DATASOURCE - 12 - 4 * (compaction_run_count + 1),\n          4 + 2 * (compaction_run_count),\n          12\n      );\n    }\n\n    \r\n    final CoordinatorStats stats = doCompactSegments(compactSegments);\n    Assert.assertEquals(\n        0,\n        stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n    );\n    verifySnapshot(\n        compactSegments,\n        AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n        dataSourceName,\n        0,\n        \r\n        TOTAL_BYTE_PER_DATASOURCE - 120,\n        1200,\n        0,\n        TOTAL_INTERVAL_PER_DATASOURCE - 3,\n        3,\n        0,\n        16,\n        12\n    );\n  }\n","date":"2020-09-19 07:37:58","endLine":522,"groupId":"14018","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testMakeStatsForDataSourceWithSkipped","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/20/512316840bdcff534541e930b5e57f17cc55b0.src","preCode":"  public void testMakeStatsForDataSourceWithSkipped()\n  {\n    \r\n    \r\n    \r\n    \r\n    String dataSourceName = DATA_SOURCE_PREFIX + 1;\n    List<DataSegment> segments = new ArrayList<>();\n    for (int j : new int[]{0, 1, 2, 3, 7, 8}) {\n      for (int k = 0; k < 4; k++) {\n        DataSegment beforeNoon = createSegment(dataSourceName, j, true, k);\n        DataSegment afterNoon = createSegment(dataSourceName, j, false, k);\n        if (j == 3) {\n          \r\n          beforeNoon = beforeNoon.withSize(100);\n          afterNoon = afterNoon.withSize(100);\n        }\n        if (j == 1) {\n          \r\n          afterNoon = afterNoon.withSize(100);\n        }\n        segments.add(beforeNoon);\n        segments.add(afterNoon);\n      }\n    }\n\n    dataSources = DataSourcesSnapshot\n        .fromUsedSegments(segments, ImmutableMap.of())\n        .getUsedSegmentsTimelinesPerDataSource();\n\n\n    final TestDruidLeaderClient leaderClient = new TestDruidLeaderClient(JSON_MAPPER);\n    leaderClient.start();\n    final HttpIndexingServiceClient indexingServiceClient = new HttpIndexingServiceClient(JSON_MAPPER, leaderClient);\n    final CompactSegments compactSegments = new CompactSegments(JSON_MAPPER, indexingServiceClient);\n\n    \r\n    Map<String, AutoCompactionSnapshot> autoCompactionSnapshots = compactSegments.getAutoCompactionSnapshot();\n    Assert.assertEquals(0, autoCompactionSnapshots.size());\n\n    \r\n    for (int compaction_run_count = 0; compaction_run_count < 8; compaction_run_count++) {\n      \r\n      final CoordinatorStats stats = doCompactSegments(compactSegments);\n      Assert.assertEquals(\n          1,\n          stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n      );\n\n      verifySnapshot(\n          compactSegments,\n          AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n          dataSourceName,\n          \r\n          TOTAL_BYTE_PER_DATASOURCE - 120 - 40 * (compaction_run_count + 1),\n          40 * (compaction_run_count + 1),\n          1200,\n          TOTAL_INTERVAL_PER_DATASOURCE - 3 - (compaction_run_count + 1),\n          (compaction_run_count + 1),\n          3,\n          TOTAL_SEGMENT_PER_DATASOURCE - 12 - 4 * (compaction_run_count + 1),\n          4 + 2 * (compaction_run_count),\n          12\n      );\n    }\n\n    \r\n    final CoordinatorStats stats = doCompactSegments(compactSegments);\n    Assert.assertEquals(\n        0,\n        stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n    );\n    verifySnapshot(\n        compactSegments,\n        AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n        dataSourceName,\n        0,\n        \r\n        TOTAL_BYTE_PER_DATASOURCE - 120,\n        1200,\n        0,\n        TOTAL_INTERVAL_PER_DATASOURCE - 3,\n        3,\n        0,\n        16,\n        12\n    );\n  }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":435,"status":"B"}],"commitId":"e78d7862a8ae60b8504355cbd969a6f49cda6065","commitMessage":"@@@Auto-compaction snapshot status API (#10371)\n\n* Auto-compaction snapshot API\n\n* Auto-compaction snapshot API\n\n* Auto-compaction snapshot API\n\n* Auto-compaction snapshot API\n\n* Auto-compaction snapshot API\n\n* Auto-compaction snapshot API\n\n* Auto-compaction snapshot API\n\n* fix when not all compacted segments are iterated\n\n* add unit tests\n\n* add unit tests\n\n* add unit tests\n\n* add unit tests\n\n* add unit tests\n\n* add unit tests\n\n* add some tests to make code cov happy\n\n* address comments\n\n* address comments\n\n* address comments\n\n* address comments\n\n* make code coverage happy\n\n* address comments\n\n* address comments\n\n* address comments\n\n* address comments","date":"2020-09-19 07:37:58","modifiedFileCount":"7","status":"B","submitter":"Maytas Monsereenusorn"},{"authorTime":"2020-10-07 12:56:03","codes":[{"authorDate":"2020-10-07 12:56:03","commitOrder":2,"curCode":"  public void testMakeStatsForDataSourceWithCompactedIntervalBetweenNonCompactedIntervals()\n  {\n    \r\n    \r\n    String dataSourceName = DATA_SOURCE_PREFIX + 1;\n    List<DataSegment> segments = new ArrayList<>();\n    for (int j : new int[]{0, 1, 2, 3, 7, 8}) {\n      for (int k = 0; k < 4; k++) {\n        DataSegment beforeNoon = createSegment(dataSourceName, j, true, k);\n        DataSegment afterNoon = createSegment(dataSourceName, j, false, k);\n        if (j == 3) {\n          \r\n          beforeNoon = beforeNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of()));\n          afterNoon = afterNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of()));\n        }\n        if (j == 1) {\n          \r\n          afterNoon = afterNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of()));\n        }\n        segments.add(beforeNoon);\n        segments.add(afterNoon);\n      }\n    }\n\n    dataSources = DataSourcesSnapshot\n        .fromUsedSegments(segments, ImmutableMap.of())\n        .getUsedSegmentsTimelinesPerDataSource();\n\n\n    final TestDruidLeaderClient leaderClient = new TestDruidLeaderClient(JSON_MAPPER);\n    leaderClient.start();\n    final HttpIndexingServiceClient indexingServiceClient = new HttpIndexingServiceClient(JSON_MAPPER, leaderClient);\n    final CompactSegments compactSegments = new CompactSegments(JSON_MAPPER, indexingServiceClient);\n\n    \r\n    Map<String, AutoCompactionSnapshot> autoCompactionSnapshots = compactSegments.getAutoCompactionSnapshot();\n    Assert.assertEquals(0, autoCompactionSnapshots.size());\n\n    \r\n    for (int compactionRunCount = 0; compactionRunCount < 8; compactionRunCount++) {\n      \r\n      final CoordinatorStats stats = doCompactSegments(compactSegments);\n      Assert.assertEquals(\n          1,\n          stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n      );\n\n      verifySnapshot(\n          compactSegments,\n          AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n          dataSourceName,\n          TOTAL_BYTE_PER_DATASOURCE - 120 - 40 * (compactionRunCount + 1),\n          120 + 40 * (compactionRunCount + 1),\n          0,\n          TOTAL_INTERVAL_PER_DATASOURCE - 3 - (compactionRunCount + 1),\n          3 + (compactionRunCount + 1),\n          0,\n          TOTAL_SEGMENT_PER_DATASOURCE - 12 - 4 * (compactionRunCount + 1),\n          \r\n          \r\n          \r\n          12 + 4 + 2 * (compactionRunCount),\n          0\n      );\n    }\n\n    \r\n    final CoordinatorStats stats = doCompactSegments(compactSegments);\n    Assert.assertEquals(\n        0,\n        stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n    );\n    verifySnapshot(\n        compactSegments,\n        AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n        dataSourceName,\n        0,\n        TOTAL_BYTE_PER_DATASOURCE,\n        0,\n        0,\n        TOTAL_INTERVAL_PER_DATASOURCE,\n        0,\n        0,\n        \r\n        \r\n        12 + 16,\n        0\n    );\n  }\n","date":"2020-10-07 12:56:03","endLine":435,"groupId":"14019","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testMakeStatsForDataSourceWithCompactedIntervalBetweenNonCompactedIntervals","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0c/17cf9436ae812c1bf2183bf7afedd62e5720bf.src","preCode":"  public void testMakeStatsForDataSourceWithCompactedIntervalBetweenNonCompactedIntervals()\n  {\n    \r\n    \r\n    String dataSourceName = DATA_SOURCE_PREFIX + 1;\n    List<DataSegment> segments = new ArrayList<>();\n    for (int j : new int[]{0, 1, 2, 3, 7, 8}) {\n      for (int k = 0; k < 4; k++) {\n        DataSegment beforeNoon = createSegment(dataSourceName, j, true, k);\n        DataSegment afterNoon = createSegment(dataSourceName, j, false, k);\n        if (j == 3) {\n          \r\n          beforeNoon = beforeNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of()));\n          afterNoon = afterNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of()));\n        }\n        if (j == 1) {\n          \r\n          afterNoon = afterNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of()));\n        }\n        segments.add(beforeNoon);\n        segments.add(afterNoon);\n      }\n    }\n\n    dataSources = DataSourcesSnapshot\n        .fromUsedSegments(segments, ImmutableMap.of())\n        .getUsedSegmentsTimelinesPerDataSource();\n\n\n    final TestDruidLeaderClient leaderClient = new TestDruidLeaderClient(JSON_MAPPER);\n    leaderClient.start();\n    final HttpIndexingServiceClient indexingServiceClient = new HttpIndexingServiceClient(JSON_MAPPER, leaderClient);\n    final CompactSegments compactSegments = new CompactSegments(JSON_MAPPER, indexingServiceClient);\n\n    \r\n    Map<String, AutoCompactionSnapshot> autoCompactionSnapshots = compactSegments.getAutoCompactionSnapshot();\n    Assert.assertEquals(0, autoCompactionSnapshots.size());\n\n    \r\n    for (int compaction_run_count = 0; compaction_run_count < 8; compaction_run_count++) {\n      \r\n      final CoordinatorStats stats = doCompactSegments(compactSegments);\n      Assert.assertEquals(\n          1,\n          stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n      );\n\n      verifySnapshot(\n          compactSegments,\n          AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n          dataSourceName,\n          TOTAL_BYTE_PER_DATASOURCE - 120 - 40 * (compaction_run_count + 1),\n          120 + 40 * (compaction_run_count + 1),\n          0,\n          TOTAL_INTERVAL_PER_DATASOURCE - 3 - (compaction_run_count + 1),\n          3 + (compaction_run_count + 1),\n          0,\n          TOTAL_SEGMENT_PER_DATASOURCE - 12 - 4 * (compaction_run_count + 1),\n          \r\n          \r\n          \r\n          12 + 4 + 2 * (compaction_run_count),\n          0\n      );\n    }\n\n    \r\n    final CoordinatorStats stats = doCompactSegments(compactSegments);\n    Assert.assertEquals(\n        0,\n        stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n    );\n    verifySnapshot(\n        compactSegments,\n        AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n        dataSourceName,\n        0,\n        TOTAL_BYTE_PER_DATASOURCE,\n        0,\n        0,\n        TOTAL_INTERVAL_PER_DATASOURCE,\n        0,\n        0,\n        \r\n        \r\n        12 + 16,\n        0\n    );\n  }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":347,"status":"M"},{"authorDate":"2020-10-07 12:56:03","commitOrder":2,"curCode":"  public void testMakeStatsForDataSourceWithSkipped()\n  {\n    \r\n    \r\n    \r\n    \r\n    String dataSourceName = DATA_SOURCE_PREFIX + 1;\n    List<DataSegment> segments = new ArrayList<>();\n    for (int j : new int[]{0, 1, 2, 3, 7, 8}) {\n      for (int k = 0; k < 4; k++) {\n        DataSegment beforeNoon = createSegment(dataSourceName, j, true, k);\n        DataSegment afterNoon = createSegment(dataSourceName, j, false, k);\n        if (j == 3) {\n          \r\n          beforeNoon = beforeNoon.withSize(100);\n          afterNoon = afterNoon.withSize(100);\n        }\n        if (j == 1) {\n          \r\n          afterNoon = afterNoon.withSize(100);\n        }\n        segments.add(beforeNoon);\n        segments.add(afterNoon);\n      }\n    }\n\n    dataSources = DataSourcesSnapshot\n        .fromUsedSegments(segments, ImmutableMap.of())\n        .getUsedSegmentsTimelinesPerDataSource();\n\n\n    final TestDruidLeaderClient leaderClient = new TestDruidLeaderClient(JSON_MAPPER);\n    leaderClient.start();\n    final HttpIndexingServiceClient indexingServiceClient = new HttpIndexingServiceClient(JSON_MAPPER, leaderClient);\n    final CompactSegments compactSegments = new CompactSegments(JSON_MAPPER, indexingServiceClient);\n\n    \r\n    Map<String, AutoCompactionSnapshot> autoCompactionSnapshots = compactSegments.getAutoCompactionSnapshot();\n    Assert.assertEquals(0, autoCompactionSnapshots.size());\n\n    \r\n    for (int compactionRunCount = 0; compactionRunCount < 8; compactionRunCount++) {\n      \r\n      final CoordinatorStats stats = doCompactSegments(compactSegments);\n      Assert.assertEquals(\n          1,\n          stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n      );\n\n      verifySnapshot(\n          compactSegments,\n          AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n          dataSourceName,\n          \r\n          TOTAL_BYTE_PER_DATASOURCE - 120 - 40 * (compactionRunCount + 1),\n          40 * (compactionRunCount + 1),\n          1200,\n          TOTAL_INTERVAL_PER_DATASOURCE - 3 - (compactionRunCount + 1),\n          (compactionRunCount + 1),\n          3,\n          TOTAL_SEGMENT_PER_DATASOURCE - 12 - 4 * (compactionRunCount + 1),\n          4 + 2 * (compactionRunCount),\n          12\n      );\n    }\n\n    \r\n    final CoordinatorStats stats = doCompactSegments(compactSegments);\n    Assert.assertEquals(\n        0,\n        stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n    );\n    verifySnapshot(\n        compactSegments,\n        AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n        dataSourceName,\n        0,\n        \r\n        TOTAL_BYTE_PER_DATASOURCE - 120,\n        1200,\n        0,\n        TOTAL_INTERVAL_PER_DATASOURCE - 3,\n        3,\n        0,\n        16,\n        12\n    );\n  }\n","date":"2020-10-07 12:56:03","endLine":525,"groupId":"14018","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testMakeStatsForDataSourceWithSkipped","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0c/17cf9436ae812c1bf2183bf7afedd62e5720bf.src","preCode":"  public void testMakeStatsForDataSourceWithSkipped()\n  {\n    \r\n    \r\n    \r\n    \r\n    String dataSourceName = DATA_SOURCE_PREFIX + 1;\n    List<DataSegment> segments = new ArrayList<>();\n    for (int j : new int[]{0, 1, 2, 3, 7, 8}) {\n      for (int k = 0; k < 4; k++) {\n        DataSegment beforeNoon = createSegment(dataSourceName, j, true, k);\n        DataSegment afterNoon = createSegment(dataSourceName, j, false, k);\n        if (j == 3) {\n          \r\n          beforeNoon = beforeNoon.withSize(100);\n          afterNoon = afterNoon.withSize(100);\n        }\n        if (j == 1) {\n          \r\n          afterNoon = afterNoon.withSize(100);\n        }\n        segments.add(beforeNoon);\n        segments.add(afterNoon);\n      }\n    }\n\n    dataSources = DataSourcesSnapshot\n        .fromUsedSegments(segments, ImmutableMap.of())\n        .getUsedSegmentsTimelinesPerDataSource();\n\n\n    final TestDruidLeaderClient leaderClient = new TestDruidLeaderClient(JSON_MAPPER);\n    leaderClient.start();\n    final HttpIndexingServiceClient indexingServiceClient = new HttpIndexingServiceClient(JSON_MAPPER, leaderClient);\n    final CompactSegments compactSegments = new CompactSegments(JSON_MAPPER, indexingServiceClient);\n\n    \r\n    Map<String, AutoCompactionSnapshot> autoCompactionSnapshots = compactSegments.getAutoCompactionSnapshot();\n    Assert.assertEquals(0, autoCompactionSnapshots.size());\n\n    \r\n    for (int compaction_run_count = 0; compaction_run_count < 8; compaction_run_count++) {\n      \r\n      final CoordinatorStats stats = doCompactSegments(compactSegments);\n      Assert.assertEquals(\n          1,\n          stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n      );\n\n      verifySnapshot(\n          compactSegments,\n          AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n          dataSourceName,\n          \r\n          TOTAL_BYTE_PER_DATASOURCE - 120 - 40 * (compaction_run_count + 1),\n          40 * (compaction_run_count + 1),\n          1200,\n          TOTAL_INTERVAL_PER_DATASOURCE - 3 - (compaction_run_count + 1),\n          (compaction_run_count + 1),\n          3,\n          TOTAL_SEGMENT_PER_DATASOURCE - 12 - 4 * (compaction_run_count + 1),\n          4 + 2 * (compaction_run_count),\n          12\n      );\n    }\n\n    \r\n    final CoordinatorStats stats = doCompactSegments(compactSegments);\n    Assert.assertEquals(\n        0,\n        stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n    );\n    verifySnapshot(\n        compactSegments,\n        AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n        dataSourceName,\n        0,\n        \r\n        TOTAL_BYTE_PER_DATASOURCE - 120,\n        1200,\n        0,\n        TOTAL_INTERVAL_PER_DATASOURCE - 3,\n        3,\n        0,\n        16,\n        12\n    );\n  }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":438,"status":"M"}],"commitId":"e9e7d82714e48de48bcf3c159a8e2d04ab7dc35e","commitMessage":"@@@Fix compaction task slot computation in auto compaction (#10479)\n\n* Fix compaction task slot computation in auto compaction\n\n* add tests for task counting","date":"2020-10-07 12:56:03","modifiedFileCount":"4","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-10-07 12:56:03","codes":[{"authorDate":"2021-02-12 19:03:20","commitOrder":3,"curCode":"  public void testMakeStatsForDataSourceWithCompactedIntervalBetweenNonCompactedIntervals()\n  {\n    \r\n    \r\n    String dataSourceName = DATA_SOURCE_PREFIX + 1;\n    List<DataSegment> segments = new ArrayList<>();\n    for (int j : new int[]{0, 1, 2, 3, 7, 8}) {\n      for (int k = 0; k < PARTITION_PER_TIME_INTERVAL; k++) {\n        DataSegment beforeNoon = createSegment(dataSourceName, j, true, k);\n        DataSegment afterNoon = createSegment(dataSourceName, j, false, k);\n        if (j == 3) {\n          \r\n          beforeNoon = beforeNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of(), ImmutableMap.of()));\n          afterNoon = afterNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of(), ImmutableMap.of()));\n        }\n        if (j == 1) {\n          \r\n          afterNoon = afterNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of(), ImmutableMap.of()));\n        }\n        segments.add(beforeNoon);\n        segments.add(afterNoon);\n      }\n    }\n\n    dataSources = DataSourcesSnapshot\n        .fromUsedSegments(segments, ImmutableMap.of())\n        .getUsedSegmentsTimelinesPerDataSource();\n\n\n    final TestDruidLeaderClient leaderClient = new TestDruidLeaderClient(JSON_MAPPER);\n    leaderClient.start();\n    final HttpIndexingServiceClient indexingServiceClient = new HttpIndexingServiceClient(JSON_MAPPER, leaderClient);\n    final CompactSegments compactSegments = new CompactSegments(JSON_MAPPER, indexingServiceClient);\n\n    \r\n    Map<String, AutoCompactionSnapshot> autoCompactionSnapshots = compactSegments.getAutoCompactionSnapshot();\n    Assert.assertEquals(0, autoCompactionSnapshots.size());\n\n    \r\n    for (int compactionRunCount = 0; compactionRunCount < 8; compactionRunCount++) {\n      \r\n      final CoordinatorStats stats = doCompactSegments(compactSegments);\n      Assert.assertEquals(\n          1,\n          stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n      );\n\n      verifySnapshot(\n          compactSegments,\n          AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n          dataSourceName,\n          TOTAL_BYTE_PER_DATASOURCE - 120 - 40 * (compactionRunCount + 1),\n          120 + 40 * (compactionRunCount + 1),\n          0,\n          TOTAL_INTERVAL_PER_DATASOURCE - 3 - (compactionRunCount + 1),\n          3 + (compactionRunCount + 1),\n          0,\n          TOTAL_SEGMENT_PER_DATASOURCE - 12 - 4 * (compactionRunCount + 1),\n          \r\n          \r\n          \r\n          12 + 4 + 2 * (compactionRunCount),\n          0\n      );\n    }\n\n    \r\n    final CoordinatorStats stats = doCompactSegments(compactSegments);\n    Assert.assertEquals(\n        0,\n        stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n    );\n    verifySnapshot(\n        compactSegments,\n        AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n        dataSourceName,\n        0,\n        TOTAL_BYTE_PER_DATASOURCE,\n        0,\n        0,\n        TOTAL_INTERVAL_PER_DATASOURCE,\n        0,\n        0,\n        \r\n        \r\n        12 + 16,\n        0\n    );\n  }\n","date":"2021-02-12 19:03:20","endLine":455,"groupId":"11829","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testMakeStatsForDataSourceWithCompactedIntervalBetweenNonCompactedIntervals","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c8/928103c09569938a64b3dda423ebd903ba11a2.src","preCode":"  public void testMakeStatsForDataSourceWithCompactedIntervalBetweenNonCompactedIntervals()\n  {\n    \r\n    \r\n    String dataSourceName = DATA_SOURCE_PREFIX + 1;\n    List<DataSegment> segments = new ArrayList<>();\n    for (int j : new int[]{0, 1, 2, 3, 7, 8}) {\n      for (int k = 0; k < 4; k++) {\n        DataSegment beforeNoon = createSegment(dataSourceName, j, true, k);\n        DataSegment afterNoon = createSegment(dataSourceName, j, false, k);\n        if (j == 3) {\n          \r\n          beforeNoon = beforeNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of()));\n          afterNoon = afterNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of()));\n        }\n        if (j == 1) {\n          \r\n          afterNoon = afterNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of()));\n        }\n        segments.add(beforeNoon);\n        segments.add(afterNoon);\n      }\n    }\n\n    dataSources = DataSourcesSnapshot\n        .fromUsedSegments(segments, ImmutableMap.of())\n        .getUsedSegmentsTimelinesPerDataSource();\n\n\n    final TestDruidLeaderClient leaderClient = new TestDruidLeaderClient(JSON_MAPPER);\n    leaderClient.start();\n    final HttpIndexingServiceClient indexingServiceClient = new HttpIndexingServiceClient(JSON_MAPPER, leaderClient);\n    final CompactSegments compactSegments = new CompactSegments(JSON_MAPPER, indexingServiceClient);\n\n    \r\n    Map<String, AutoCompactionSnapshot> autoCompactionSnapshots = compactSegments.getAutoCompactionSnapshot();\n    Assert.assertEquals(0, autoCompactionSnapshots.size());\n\n    \r\n    for (int compactionRunCount = 0; compactionRunCount < 8; compactionRunCount++) {\n      \r\n      final CoordinatorStats stats = doCompactSegments(compactSegments);\n      Assert.assertEquals(\n          1,\n          stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n      );\n\n      verifySnapshot(\n          compactSegments,\n          AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n          dataSourceName,\n          TOTAL_BYTE_PER_DATASOURCE - 120 - 40 * (compactionRunCount + 1),\n          120 + 40 * (compactionRunCount + 1),\n          0,\n          TOTAL_INTERVAL_PER_DATASOURCE - 3 - (compactionRunCount + 1),\n          3 + (compactionRunCount + 1),\n          0,\n          TOTAL_SEGMENT_PER_DATASOURCE - 12 - 4 * (compactionRunCount + 1),\n          \r\n          \r\n          \r\n          12 + 4 + 2 * (compactionRunCount),\n          0\n      );\n    }\n\n    \r\n    final CoordinatorStats stats = doCompactSegments(compactSegments);\n    Assert.assertEquals(\n        0,\n        stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n    );\n    verifySnapshot(\n        compactSegments,\n        AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n        dataSourceName,\n        0,\n        TOTAL_BYTE_PER_DATASOURCE,\n        0,\n        0,\n        TOTAL_INTERVAL_PER_DATASOURCE,\n        0,\n        0,\n        \r\n        \r\n        12 + 16,\n        0\n    );\n  }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":367,"status":"M"},{"authorDate":"2020-10-07 12:56:03","commitOrder":3,"curCode":"  public void testMakeStatsForDataSourceWithSkipped()\n  {\n    \r\n    \r\n    \r\n    \r\n    String dataSourceName = DATA_SOURCE_PREFIX + 1;\n    List<DataSegment> segments = new ArrayList<>();\n    for (int j : new int[]{0, 1, 2, 3, 7, 8}) {\n      for (int k = 0; k < 4; k++) {\n        DataSegment beforeNoon = createSegment(dataSourceName, j, true, k);\n        DataSegment afterNoon = createSegment(dataSourceName, j, false, k);\n        if (j == 3) {\n          \r\n          beforeNoon = beforeNoon.withSize(100);\n          afterNoon = afterNoon.withSize(100);\n        }\n        if (j == 1) {\n          \r\n          afterNoon = afterNoon.withSize(100);\n        }\n        segments.add(beforeNoon);\n        segments.add(afterNoon);\n      }\n    }\n\n    dataSources = DataSourcesSnapshot\n        .fromUsedSegments(segments, ImmutableMap.of())\n        .getUsedSegmentsTimelinesPerDataSource();\n\n\n    final TestDruidLeaderClient leaderClient = new TestDruidLeaderClient(JSON_MAPPER);\n    leaderClient.start();\n    final HttpIndexingServiceClient indexingServiceClient = new HttpIndexingServiceClient(JSON_MAPPER, leaderClient);\n    final CompactSegments compactSegments = new CompactSegments(JSON_MAPPER, indexingServiceClient);\n\n    \r\n    Map<String, AutoCompactionSnapshot> autoCompactionSnapshots = compactSegments.getAutoCompactionSnapshot();\n    Assert.assertEquals(0, autoCompactionSnapshots.size());\n\n    \r\n    for (int compactionRunCount = 0; compactionRunCount < 8; compactionRunCount++) {\n      \r\n      final CoordinatorStats stats = doCompactSegments(compactSegments);\n      Assert.assertEquals(\n          1,\n          stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n      );\n\n      verifySnapshot(\n          compactSegments,\n          AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n          dataSourceName,\n          \r\n          TOTAL_BYTE_PER_DATASOURCE - 120 - 40 * (compactionRunCount + 1),\n          40 * (compactionRunCount + 1),\n          1200,\n          TOTAL_INTERVAL_PER_DATASOURCE - 3 - (compactionRunCount + 1),\n          (compactionRunCount + 1),\n          3,\n          TOTAL_SEGMENT_PER_DATASOURCE - 12 - 4 * (compactionRunCount + 1),\n          4 + 2 * (compactionRunCount),\n          12\n      );\n    }\n\n    \r\n    final CoordinatorStats stats = doCompactSegments(compactSegments);\n    Assert.assertEquals(\n        0,\n        stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n    );\n    verifySnapshot(\n        compactSegments,\n        AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n        dataSourceName,\n        0,\n        \r\n        TOTAL_BYTE_PER_DATASOURCE - 120,\n        1200,\n        0,\n        TOTAL_INTERVAL_PER_DATASOURCE - 3,\n        3,\n        0,\n        16,\n        12\n    );\n  }\n","date":"2020-10-07 12:56:03","endLine":525,"groupId":"14018","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testMakeStatsForDataSourceWithSkipped","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0c/17cf9436ae812c1bf2183bf7afedd62e5720bf.src","preCode":"  public void testMakeStatsForDataSourceWithSkipped()\n  {\n    \r\n    \r\n    \r\n    \r\n    String dataSourceName = DATA_SOURCE_PREFIX + 1;\n    List<DataSegment> segments = new ArrayList<>();\n    for (int j : new int[]{0, 1, 2, 3, 7, 8}) {\n      for (int k = 0; k < 4; k++) {\n        DataSegment beforeNoon = createSegment(dataSourceName, j, true, k);\n        DataSegment afterNoon = createSegment(dataSourceName, j, false, k);\n        if (j == 3) {\n          \r\n          beforeNoon = beforeNoon.withSize(100);\n          afterNoon = afterNoon.withSize(100);\n        }\n        if (j == 1) {\n          \r\n          afterNoon = afterNoon.withSize(100);\n        }\n        segments.add(beforeNoon);\n        segments.add(afterNoon);\n      }\n    }\n\n    dataSources = DataSourcesSnapshot\n        .fromUsedSegments(segments, ImmutableMap.of())\n        .getUsedSegmentsTimelinesPerDataSource();\n\n\n    final TestDruidLeaderClient leaderClient = new TestDruidLeaderClient(JSON_MAPPER);\n    leaderClient.start();\n    final HttpIndexingServiceClient indexingServiceClient = new HttpIndexingServiceClient(JSON_MAPPER, leaderClient);\n    final CompactSegments compactSegments = new CompactSegments(JSON_MAPPER, indexingServiceClient);\n\n    \r\n    Map<String, AutoCompactionSnapshot> autoCompactionSnapshots = compactSegments.getAutoCompactionSnapshot();\n    Assert.assertEquals(0, autoCompactionSnapshots.size());\n\n    \r\n    for (int compactionRunCount = 0; compactionRunCount < 8; compactionRunCount++) {\n      \r\n      final CoordinatorStats stats = doCompactSegments(compactSegments);\n      Assert.assertEquals(\n          1,\n          stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n      );\n\n      verifySnapshot(\n          compactSegments,\n          AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n          dataSourceName,\n          \r\n          TOTAL_BYTE_PER_DATASOURCE - 120 - 40 * (compactionRunCount + 1),\n          40 * (compactionRunCount + 1),\n          1200,\n          TOTAL_INTERVAL_PER_DATASOURCE - 3 - (compactionRunCount + 1),\n          (compactionRunCount + 1),\n          3,\n          TOTAL_SEGMENT_PER_DATASOURCE - 12 - 4 * (compactionRunCount + 1),\n          4 + 2 * (compactionRunCount),\n          12\n      );\n    }\n\n    \r\n    final CoordinatorStats stats = doCompactSegments(compactSegments);\n    Assert.assertEquals(\n        0,\n        stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n    );\n    verifySnapshot(\n        compactSegments,\n        AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n        dataSourceName,\n        0,\n        \r\n        TOTAL_BYTE_PER_DATASOURCE - 120,\n        1200,\n        0,\n        TOTAL_INTERVAL_PER_DATASOURCE - 3,\n        3,\n        0,\n        16,\n        12\n    );\n  }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":438,"status":"N"}],"commitId":"6541178c21839530a42af4b4675a9bc680bffca6","commitMessage":"@@@Support segmentGranularity for auto-compaction (#10843)\n\n* Support segmentGranularity for auto-compaction\n\n* Support segmentGranularity for auto-compaction\n\n* Support segmentGranularity for auto-compaction\n\n* Support segmentGranularity for auto-compaction\n\n* resolve conflict\n\n* Support segmentGranularity for auto-compaction\n\n* Support segmentGranularity for auto-compaction\n\n* fix tests\n\n* fix more tests\n\n* fix checkstyle\n\n* add unit tests\n\n* fix checkstyle\n\n* fix checkstyle\n\n* fix checkstyle\n\n* add unit tests\n\n* add integration tests\n\n* fix checkstyle\n\n* fix checkstyle\n\n* fix failing tests\n\n* address comments\n\n* address comments\n\n* fix tests\n\n* fix tests\n\n* fix test\n\n* fix test\n\n* fix test\n\n* fix test\n\n* fix test\n\n* fix test\n\n* fix test\n\n* fix test","date":"2021-02-12 19:03:20","modifiedFileCount":"32","status":"M","submitter":"Maytas Monsereenusorn"},{"authorTime":"2021-06-21 08:21:59","codes":[{"authorDate":"2021-06-21 08:21:59","commitOrder":4,"curCode":"  public void testMakeStatsForDataSourceWithCompactedIntervalBetweenNonCompactedIntervals()\n  {\n    \r\n    \r\n    String dataSourceName = DATA_SOURCE_PREFIX + 1;\n    List<DataSegment> segments = new ArrayList<>();\n    for (int j : new int[]{0, 1, 2, 3, 7, 8}) {\n      for (int k = 0; k < PARTITION_PER_TIME_INTERVAL; k++) {\n        DataSegment beforeNoon = createSegment(dataSourceName, j, true, k);\n        DataSegment afterNoon = createSegment(dataSourceName, j, false, k);\n        if (j == 3) {\n          \r\n          beforeNoon = beforeNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of(), ImmutableMap.of()));\n          afterNoon = afterNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of(), ImmutableMap.of()));\n        }\n        if (j == 1) {\n          \r\n          afterNoon = afterNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of(), ImmutableMap.of()));\n        }\n        segments.add(beforeNoon);\n        segments.add(afterNoon);\n      }\n    }\n\n    dataSources = DataSourcesSnapshot\n        .fromUsedSegments(segments, ImmutableMap.of())\n        .getUsedSegmentsTimelinesPerDataSource();\n\n\n    final TestDruidLeaderClient leaderClient = new TestDruidLeaderClient(JSON_MAPPER);\n    leaderClient.start();\n    final HttpIndexingServiceClient indexingServiceClient = new HttpIndexingServiceClient(JSON_MAPPER, leaderClient);\n    final CompactSegments compactSegments = new CompactSegments(COORDINATOR_CONFIG, JSON_MAPPER, indexingServiceClient);\n\n    \r\n    Map<String, AutoCompactionSnapshot> autoCompactionSnapshots = compactSegments.getAutoCompactionSnapshot();\n    Assert.assertEquals(0, autoCompactionSnapshots.size());\n\n    \r\n    for (int compactionRunCount = 0; compactionRunCount < 8; compactionRunCount++) {\n      \r\n      final CoordinatorStats stats = doCompactSegments(compactSegments);\n      Assert.assertEquals(\n          1,\n          stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n      );\n\n      verifySnapshot(\n          compactSegments,\n          AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n          dataSourceName,\n          TOTAL_BYTE_PER_DATASOURCE - 120 - 40 * (compactionRunCount + 1),\n          120 + 40 * (compactionRunCount + 1),\n          0,\n          TOTAL_INTERVAL_PER_DATASOURCE - 3 - (compactionRunCount + 1),\n          3 + (compactionRunCount + 1),\n          0,\n          TOTAL_SEGMENT_PER_DATASOURCE - 12 - 4 * (compactionRunCount + 1),\n          \r\n          \r\n          \r\n          12 + 4 + 2 * (compactionRunCount),\n          0\n      );\n    }\n\n    \r\n    final CoordinatorStats stats = doCompactSegments(compactSegments);\n    Assert.assertEquals(\n        0,\n        stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n    );\n    verifySnapshot(\n        compactSegments,\n        AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n        dataSourceName,\n        0,\n        TOTAL_BYTE_PER_DATASOURCE,\n        0,\n        0,\n        TOTAL_INTERVAL_PER_DATASOURCE,\n        0,\n        0,\n        \r\n        \r\n        12 + 16,\n        0\n    );\n  }\n","date":"2021-06-21 08:21:59","endLine":459,"groupId":"10528","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testMakeStatsForDataSourceWithCompactedIntervalBetweenNonCompactedIntervals","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/a1/932dd1f4c60c16e6d0bbc3f4f96f080a5a5022.src","preCode":"  public void testMakeStatsForDataSourceWithCompactedIntervalBetweenNonCompactedIntervals()\n  {\n    \r\n    \r\n    String dataSourceName = DATA_SOURCE_PREFIX + 1;\n    List<DataSegment> segments = new ArrayList<>();\n    for (int j : new int[]{0, 1, 2, 3, 7, 8}) {\n      for (int k = 0; k < PARTITION_PER_TIME_INTERVAL; k++) {\n        DataSegment beforeNoon = createSegment(dataSourceName, j, true, k);\n        DataSegment afterNoon = createSegment(dataSourceName, j, false, k);\n        if (j == 3) {\n          \r\n          beforeNoon = beforeNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of(), ImmutableMap.of()));\n          afterNoon = afterNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of(), ImmutableMap.of()));\n        }\n        if (j == 1) {\n          \r\n          afterNoon = afterNoon.withLastCompactionState(new CompactionState(partitionsSpec, ImmutableMap.of(), ImmutableMap.of()));\n        }\n        segments.add(beforeNoon);\n        segments.add(afterNoon);\n      }\n    }\n\n    dataSources = DataSourcesSnapshot\n        .fromUsedSegments(segments, ImmutableMap.of())\n        .getUsedSegmentsTimelinesPerDataSource();\n\n\n    final TestDruidLeaderClient leaderClient = new TestDruidLeaderClient(JSON_MAPPER);\n    leaderClient.start();\n    final HttpIndexingServiceClient indexingServiceClient = new HttpIndexingServiceClient(JSON_MAPPER, leaderClient);\n    final CompactSegments compactSegments = new CompactSegments(JSON_MAPPER, indexingServiceClient);\n\n    \r\n    Map<String, AutoCompactionSnapshot> autoCompactionSnapshots = compactSegments.getAutoCompactionSnapshot();\n    Assert.assertEquals(0, autoCompactionSnapshots.size());\n\n    \r\n    for (int compactionRunCount = 0; compactionRunCount < 8; compactionRunCount++) {\n      \r\n      final CoordinatorStats stats = doCompactSegments(compactSegments);\n      Assert.assertEquals(\n          1,\n          stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n      );\n\n      verifySnapshot(\n          compactSegments,\n          AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n          dataSourceName,\n          TOTAL_BYTE_PER_DATASOURCE - 120 - 40 * (compactionRunCount + 1),\n          120 + 40 * (compactionRunCount + 1),\n          0,\n          TOTAL_INTERVAL_PER_DATASOURCE - 3 - (compactionRunCount + 1),\n          3 + (compactionRunCount + 1),\n          0,\n          TOTAL_SEGMENT_PER_DATASOURCE - 12 - 4 * (compactionRunCount + 1),\n          \r\n          \r\n          \r\n          12 + 4 + 2 * (compactionRunCount),\n          0\n      );\n    }\n\n    \r\n    final CoordinatorStats stats = doCompactSegments(compactSegments);\n    Assert.assertEquals(\n        0,\n        stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n    );\n    verifySnapshot(\n        compactSegments,\n        AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n        dataSourceName,\n        0,\n        TOTAL_BYTE_PER_DATASOURCE,\n        0,\n        0,\n        TOTAL_INTERVAL_PER_DATASOURCE,\n        0,\n        0,\n        \r\n        \r\n        12 + 16,\n        0\n    );\n  }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":371,"status":"M"},{"authorDate":"2021-06-21 08:21:59","commitOrder":4,"curCode":"  public void testMakeStatsForDataSourceWithSkipped()\n  {\n    \r\n    \r\n    \r\n    \r\n    String dataSourceName = DATA_SOURCE_PREFIX + 1;\n    List<DataSegment> segments = new ArrayList<>();\n    for (int j : new int[]{0, 1, 2, 3, 7, 8}) {\n      for (int k = 0; k < 4; k++) {\n        DataSegment beforeNoon = createSegment(dataSourceName, j, true, k);\n        DataSegment afterNoon = createSegment(dataSourceName, j, false, k);\n        if (j == 3) {\n          \r\n          beforeNoon = beforeNoon.withSize(100);\n          afterNoon = afterNoon.withSize(100);\n        }\n        if (j == 1) {\n          \r\n          afterNoon = afterNoon.withSize(100);\n        }\n        segments.add(beforeNoon);\n        segments.add(afterNoon);\n      }\n    }\n\n    dataSources = DataSourcesSnapshot\n        .fromUsedSegments(segments, ImmutableMap.of())\n        .getUsedSegmentsTimelinesPerDataSource();\n\n\n    final TestDruidLeaderClient leaderClient = new TestDruidLeaderClient(JSON_MAPPER);\n    leaderClient.start();\n    final HttpIndexingServiceClient indexingServiceClient = new HttpIndexingServiceClient(JSON_MAPPER, leaderClient);\n    final CompactSegments compactSegments = new CompactSegments(COORDINATOR_CONFIG, JSON_MAPPER, indexingServiceClient);\n\n    \r\n    Map<String, AutoCompactionSnapshot> autoCompactionSnapshots = compactSegments.getAutoCompactionSnapshot();\n    Assert.assertEquals(0, autoCompactionSnapshots.size());\n\n    \r\n    for (int compactionRunCount = 0; compactionRunCount < 8; compactionRunCount++) {\n      \r\n      final CoordinatorStats stats = doCompactSegments(compactSegments);\n      Assert.assertEquals(\n          1,\n          stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n      );\n\n      verifySnapshot(\n          compactSegments,\n          AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n          dataSourceName,\n          \r\n          TOTAL_BYTE_PER_DATASOURCE - 120 - 40 * (compactionRunCount + 1),\n          40 * (compactionRunCount + 1),\n          1200,\n          TOTAL_INTERVAL_PER_DATASOURCE - 3 - (compactionRunCount + 1),\n          (compactionRunCount + 1),\n          3,\n          TOTAL_SEGMENT_PER_DATASOURCE - 12 - 4 * (compactionRunCount + 1),\n          4 + 2 * (compactionRunCount),\n          12\n      );\n    }\n\n    \r\n    final CoordinatorStats stats = doCompactSegments(compactSegments);\n    Assert.assertEquals(\n        0,\n        stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n    );\n    verifySnapshot(\n        compactSegments,\n        AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n        dataSourceName,\n        0,\n        \r\n        TOTAL_BYTE_PER_DATASOURCE - 120,\n        1200,\n        0,\n        TOTAL_INTERVAL_PER_DATASOURCE - 3,\n        3,\n        0,\n        16,\n        12\n    );\n  }\n","date":"2021-06-21 08:21:59","endLine":549,"groupId":"10528","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testMakeStatsForDataSourceWithSkipped","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/a1/932dd1f4c60c16e6d0bbc3f4f96f080a5a5022.src","preCode":"  public void testMakeStatsForDataSourceWithSkipped()\n  {\n    \r\n    \r\n    \r\n    \r\n    String dataSourceName = DATA_SOURCE_PREFIX + 1;\n    List<DataSegment> segments = new ArrayList<>();\n    for (int j : new int[]{0, 1, 2, 3, 7, 8}) {\n      for (int k = 0; k < 4; k++) {\n        DataSegment beforeNoon = createSegment(dataSourceName, j, true, k);\n        DataSegment afterNoon = createSegment(dataSourceName, j, false, k);\n        if (j == 3) {\n          \r\n          beforeNoon = beforeNoon.withSize(100);\n          afterNoon = afterNoon.withSize(100);\n        }\n        if (j == 1) {\n          \r\n          afterNoon = afterNoon.withSize(100);\n        }\n        segments.add(beforeNoon);\n        segments.add(afterNoon);\n      }\n    }\n\n    dataSources = DataSourcesSnapshot\n        .fromUsedSegments(segments, ImmutableMap.of())\n        .getUsedSegmentsTimelinesPerDataSource();\n\n\n    final TestDruidLeaderClient leaderClient = new TestDruidLeaderClient(JSON_MAPPER);\n    leaderClient.start();\n    final HttpIndexingServiceClient indexingServiceClient = new HttpIndexingServiceClient(JSON_MAPPER, leaderClient);\n    final CompactSegments compactSegments = new CompactSegments(JSON_MAPPER, indexingServiceClient);\n\n    \r\n    Map<String, AutoCompactionSnapshot> autoCompactionSnapshots = compactSegments.getAutoCompactionSnapshot();\n    Assert.assertEquals(0, autoCompactionSnapshots.size());\n\n    \r\n    for (int compactionRunCount = 0; compactionRunCount < 8; compactionRunCount++) {\n      \r\n      final CoordinatorStats stats = doCompactSegments(compactSegments);\n      Assert.assertEquals(\n          1,\n          stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n      );\n\n      verifySnapshot(\n          compactSegments,\n          AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n          dataSourceName,\n          \r\n          TOTAL_BYTE_PER_DATASOURCE - 120 - 40 * (compactionRunCount + 1),\n          40 * (compactionRunCount + 1),\n          1200,\n          TOTAL_INTERVAL_PER_DATASOURCE - 3 - (compactionRunCount + 1),\n          (compactionRunCount + 1),\n          3,\n          TOTAL_SEGMENT_PER_DATASOURCE - 12 - 4 * (compactionRunCount + 1),\n          4 + 2 * (compactionRunCount),\n          12\n      );\n    }\n\n    \r\n    final CoordinatorStats stats = doCompactSegments(compactSegments);\n    Assert.assertEquals(\n        0,\n        stats.getGlobalStat(CompactSegments.COMPACTION_TASK_COUNT)\n    );\n    verifySnapshot(\n        compactSegments,\n        AutoCompactionSnapshot.AutoCompactionScheduleStatus.RUNNING,\n        dataSourceName,\n        0,\n        \r\n        TOTAL_BYTE_PER_DATASOURCE - 120,\n        1200,\n        0,\n        TOTAL_INTERVAL_PER_DATASOURCE - 3,\n        3,\n        0,\n        16,\n        12\n    );\n  }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/duty/CompactSegmentsTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":462,"status":"M"}],"commitId":"f0b105ec63657643c85d82686ae1dea3e588a5f5","commitMessage":"@@@Temporarily skip compaction for locked intervals (#11190)\n\n* Add overlord API /lockedIntervals. Skip compaction for locked intervals\n\n* Revert formatting changes\n\n* Add license info\n\n* Fix checkstyle\n\n* Remove invalid API invocation\n\n* Fix checkstyle\n\n* Add DatasourceIntervalsTest\n\n* Fix checkstyle\n\n* Remove LockedIntervalsResponse\n\n* Add integration tests for lockedIntervals\n\n* Add ITAutoCompactionLockContentionTest\n\n* Add config druid.coordinator.compaction.skipLockedIntervals\n\n* Add test for TaskQueue","date":"2021-06-21 08:21:59","modifiedFileCount":"19","status":"M","submitter":"Kashif Faraz"}]
