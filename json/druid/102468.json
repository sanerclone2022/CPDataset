[{"authorTime":"2018-10-04 10:08:20","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":2,"curCode":"  public void testSkipOffsetGaps() throws Exception\n  {\n    supervisor = getSupervisor(1, 1, true, \"PT1H\", null, null, true);\n    addSomeEvents(1);\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    KafkaIOConfig taskConfig = task.getIOConfig();\n\n    Assert.assertTrue(\"skipOffsetGaps\", taskConfig.isSkipOffsetGaps());\n  }\n","date":"2018-08-31 00:56:26","endLine":314,"groupId":"4225","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testSkipOffsetGaps","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5d/773db661d0c8e87703ae229b128551316998e5.src","preCode":"  public void testSkipOffsetGaps() throws Exception\n  {\n    supervisor = getSupervisor(1, 1, true, \"PT1H\", null, null, true);\n    addSomeEvents(1);\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    KafkaIOConfig taskConfig = task.getIOConfig();\n\n    Assert.assertTrue(\"skipOffsetGaps\", taskConfig.isSkipOffsetGaps());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":288,"status":"NB"},{"authorDate":"2018-10-04 10:08:20","commitOrder":2,"curCode":"  public void testFailedInitializationAndRecovery() throws Exception\n  {\n    \r\n    supervisor = getSupervisor(\n        1,\n        1,\n        true,\n        \"PT1H\",\n        null,\n        null,\n        false,\n        false,\n        StringUtils.format(\"badhostname:%d\", kafkaServer.getPort())\n    );\n    addSomeEvents(1);\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n\n    replayAll();\n\n    supervisor.start();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertFalse(supervisor.isStarted());\n\n    verifyAll();\n\n    while (supervisor.getInitRetryCounter() < 3) {\n      Thread.sleep(1000);\n    }\n\n    \r\n    resetAll();\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    \r\n    \r\n    \r\n    supervisor.getIoConfig().getConsumerProperties().put(\"bootstrap.servers\", kafkaHost);\n    supervisor.tryInit();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertTrue(supervisor.isStarted());\n\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    Assert.assertEquals(dataSchema, task.getDataSchema());\n    Assert.assertEquals(KafkaTuningConfig.copyOf(tuningConfig), task.getTuningConfig());\n\n    KafkaIOConfig taskConfig = task.getIOConfig();\n    Assert.assertEquals(kafkaHost, taskConfig.getConsumerProperties().get(\"bootstrap.servers\"));\n    Assert.assertEquals(\"myCustomValue\", taskConfig.getConsumerProperties().get(\"myCustomKey\"));\n    Assert.assertEquals(\"sequenceName-0\", taskConfig.getBaseSequenceName());\n    Assert.assertTrue(\"isUseTransaction\", taskConfig.isUseTransaction());\n    Assert.assertFalse(\"minimumMessageTime\", taskConfig.getMinimumMessageTime().isPresent());\n    Assert.assertFalse(\"maximumMessageTime\", taskConfig.getMaximumMessageTime().isPresent());\n    Assert.assertFalse(\"skipOffsetGaps\", taskConfig.isSkipOffsetGaps());\n\n    Assert.assertEquals(topic, taskConfig.getStartPartitions().getTopic());\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionOffsetMap().get(0));\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionOffsetMap().get(1));\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionOffsetMap().get(2));\n\n    Assert.assertEquals(topic, taskConfig.getEndPartitions().getTopic());\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionOffsetMap().get(0));\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionOffsetMap().get(1));\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionOffsetMap().get(2));\n  }\n","date":"2018-10-04 10:08:20","endLine":2530,"groupId":"4225","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testFailedInitializationAndRecovery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d5/b048a239c6a2fa79c36e7b2dea6584a1db1668.src","preCode":"  public void testFailedInitializationAndRecovery() throws Exception\n  {\n    \r\n    supervisor = getSupervisor(\n        1,\n        1,\n        true,\n        \"PT1H\",\n        null,\n        null,\n        false,\n        false,\n        StringUtils.format(\"badhostname:%d\", kafkaServer.getPort())\n    );\n    addSomeEvents(1);\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n\n    replayAll();\n\n    supervisor.start();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertFalse(supervisor.isStarted());\n\n    verifyAll();\n\n    while (supervisor.getInitRetryCounter() < 3) {\n      Thread.sleep(1000);\n    }\n\n    \r\n    resetAll();\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    \r\n    \r\n    \r\n    supervisor.getIoConfig().getConsumerProperties().put(\"bootstrap.servers\", kafkaHost);\n    supervisor.tryInit();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertTrue(supervisor.isStarted());\n\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    Assert.assertEquals(dataSchema, task.getDataSchema());\n    Assert.assertEquals(KafkaTuningConfig.copyOf(tuningConfig), task.getTuningConfig());\n\n    KafkaIOConfig taskConfig = task.getIOConfig();\n    Assert.assertEquals(kafkaHost, taskConfig.getConsumerProperties().get(\"bootstrap.servers\"));\n    Assert.assertEquals(\"myCustomValue\", taskConfig.getConsumerProperties().get(\"myCustomKey\"));\n    Assert.assertEquals(\"sequenceName-0\", taskConfig.getBaseSequenceName());\n    Assert.assertTrue(\"isUseTransaction\", taskConfig.isUseTransaction());\n    Assert.assertFalse(\"minimumMessageTime\", taskConfig.getMinimumMessageTime().isPresent());\n    Assert.assertFalse(\"maximumMessageTime\", taskConfig.getMaximumMessageTime().isPresent());\n    Assert.assertFalse(\"skipOffsetGaps\", taskConfig.isSkipOffsetGaps());\n\n    Assert.assertEquals(topic, taskConfig.getStartPartitions().getTopic());\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionOffsetMap().get(0));\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionOffsetMap().get(1));\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionOffsetMap().get(2));\n\n    Assert.assertEquals(topic, taskConfig.getEndPartitions().getTopic());\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionOffsetMap().get(0));\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionOffsetMap().get(1));\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionOffsetMap().get(2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2442,"status":"B"}],"commitId":"c7ac8785a11bfb4d50237abefac93a8553216174","commitMessage":"@@@Prevent failed KafkaConsumer creation from blocking overlord startup (#6383)\n\n* Prevent failed KafkaConsumer creation from blocking overlord startup\n\n* PR comments\n\n* Fix random task ID length\n\n* Adjust test timer\n\n* Use Integer.SIZE\n","date":"2018-10-04 10:08:20","modifiedFileCount":"3","status":"M","submitter":"Jonathan Wei"},{"authorTime":"2018-12-22 03:49:24","codes":[{"authorDate":"2018-12-22 03:49:24","commitOrder":3,"curCode":"  public void testSkipOffsetGaps() throws Exception\n  {\n    supervisor = getSupervisor(1, 1, true, \"PT1H\", null, null, true);\n    addSomeEvents(1);\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n\n    Assert.assertTrue(\"skipOffsetGaps\", taskConfig.isSkipOffsetGaps());\n  }\n","date":"2018-12-22 03:49:24","endLine":332,"groupId":"4225","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testSkipOffsetGaps","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/42/7b9d1742dec9346159f08cf020ea57960091eb.src","preCode":"  public void testSkipOffsetGaps() throws Exception\n  {\n    supervisor = getSupervisor(1, 1, true, \"PT1H\", null, null, true);\n    addSomeEvents(1);\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    KafkaIOConfig taskConfig = task.getIOConfig();\n\n    Assert.assertTrue(\"skipOffsetGaps\", taskConfig.isSkipOffsetGaps());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":306,"status":"M"},{"authorDate":"2018-12-22 03:49:24","commitOrder":3,"curCode":"  public void testFailedInitializationAndRecovery() throws Exception\n  {\n    \r\n    supervisor = getSupervisor(\n        1,\n        1,\n        true,\n        \"PT1H\",\n        null,\n        null,\n        false,\n        false,\n        StringUtils.format(\"badhostname:%d\", kafkaServer.getPort())\n    );\n    addSomeEvents(1);\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n\n    replayAll();\n\n    supervisor.start();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertFalse(supervisor.isStarted());\n\n    verifyAll();\n\n    while (supervisor.getInitRetryCounter() < 3) {\n      Thread.sleep(1000);\n    }\n\n    \r\n    resetAll();\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    \r\n    \r\n    \r\n    supervisor.getIoConfig().getConsumerProperties().put(\"bootstrap.servers\", kafkaHost);\n    supervisor.tryInit();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertTrue(supervisor.isStarted());\n\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    Assert.assertEquals(dataSchema, task.getDataSchema());\n    Assert.assertEquals(tuningConfig.convertToTaskTuningConfig(), task.getTuningConfig());\n\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n    Assert.assertEquals(kafkaHost, taskConfig.getConsumerProperties().get(\"bootstrap.servers\"));\n    Assert.assertEquals(\"myCustomValue\", taskConfig.getConsumerProperties().get(\"myCustomKey\"));\n    Assert.assertEquals(\"sequenceName-0\", taskConfig.getBaseSequenceName());\n    Assert.assertTrue(\"isUseTransaction\", taskConfig.isUseTransaction());\n    Assert.assertFalse(\"minimumMessageTime\", taskConfig.getMinimumMessageTime().isPresent());\n    Assert.assertFalse(\"maximumMessageTime\", taskConfig.getMaximumMessageTime().isPresent());\n    Assert.assertFalse(\"skipOffsetGaps\", taskConfig.isSkipOffsetGaps());\n\n    Assert.assertEquals(topic, taskConfig.getStartPartitions().getStream());\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionSequenceNumberMap().get(0));\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionSequenceNumberMap().get(1));\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionSequenceNumberMap().get(2));\n\n    Assert.assertEquals(topic, taskConfig.getEndPartitions().getStream());\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionSequenceNumberMap().get(0));\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionSequenceNumberMap().get(1));\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionSequenceNumberMap().get(2));\n  }\n","date":"2018-12-22 03:49:24","endLine":2684,"groupId":"4225","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testFailedInitializationAndRecovery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/42/7b9d1742dec9346159f08cf020ea57960091eb.src","preCode":"  public void testFailedInitializationAndRecovery() throws Exception\n  {\n    \r\n    supervisor = getSupervisor(\n        1,\n        1,\n        true,\n        \"PT1H\",\n        null,\n        null,\n        false,\n        false,\n        StringUtils.format(\"badhostname:%d\", kafkaServer.getPort())\n    );\n    addSomeEvents(1);\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n\n    replayAll();\n\n    supervisor.start();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertFalse(supervisor.isStarted());\n\n    verifyAll();\n\n    while (supervisor.getInitRetryCounter() < 3) {\n      Thread.sleep(1000);\n    }\n\n    \r\n    resetAll();\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    \r\n    \r\n    \r\n    supervisor.getIoConfig().getConsumerProperties().put(\"bootstrap.servers\", kafkaHost);\n    supervisor.tryInit();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertTrue(supervisor.isStarted());\n\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    Assert.assertEquals(dataSchema, task.getDataSchema());\n    Assert.assertEquals(KafkaTuningConfig.copyOf(tuningConfig), task.getTuningConfig());\n\n    KafkaIOConfig taskConfig = task.getIOConfig();\n    Assert.assertEquals(kafkaHost, taskConfig.getConsumerProperties().get(\"bootstrap.servers\"));\n    Assert.assertEquals(\"myCustomValue\", taskConfig.getConsumerProperties().get(\"myCustomKey\"));\n    Assert.assertEquals(\"sequenceName-0\", taskConfig.getBaseSequenceName());\n    Assert.assertTrue(\"isUseTransaction\", taskConfig.isUseTransaction());\n    Assert.assertFalse(\"minimumMessageTime\", taskConfig.getMinimumMessageTime().isPresent());\n    Assert.assertFalse(\"maximumMessageTime\", taskConfig.getMaximumMessageTime().isPresent());\n    Assert.assertFalse(\"skipOffsetGaps\", taskConfig.isSkipOffsetGaps());\n\n    Assert.assertEquals(topic, taskConfig.getStartPartitions().getTopic());\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionOffsetMap().get(0));\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionOffsetMap().get(1));\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionOffsetMap().get(2));\n\n    Assert.assertEquals(topic, taskConfig.getEndPartitions().getTopic());\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionOffsetMap().get(0));\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionOffsetMap().get(1));\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionOffsetMap().get(2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2596,"status":"M"}],"commitId":"7c7997e8a1183a7bffad731ca94e8b4c381e8665","commitMessage":"@@@Add Kinesis Indexing Service to core Druid (#6431)\n\n* created seekablestream classes\n\n* created seekablestreamsupervisor class\n\n* first attempt to integrate kafa indexing service to use SeekableStream\n\n* seekablestream bug fixes\n\n* kafkarecordsupplier\n\n* integrated kafka indexing service with seekablestream\n\n* implemented resume/suspend and refactored some package names\n\n* moved kinesis indexing service into core druid extensions\n\n* merged some changes from kafka supervisor race condition\n\n* integrated kinesis-indexing-service with seekablestream\n\n* unite tests for kinesis-indexing-service\n\n* various bug fixes for kinesis-indexing-service\n\n* refactored kinesisindexingtask\n\n* finished up more kinesis unit tests\n\n* more bug fixes for kinesis-indexing-service\n\n* finsihed refactoring kinesis unit tests\n\n* removed KinesisParititons and KafkaPartitions to use SeekableStreamPartitions\n\n* kinesis-indexing-service code cleanup and docs\n\n* merge #6291\n\nmerge #6337\n\nmerge #6383\n\n* added more docs and reordered methods\n\n* fixd kinesis tests after merging master and added docs in seekablestream\n\n* fix various things from pr comment\n\n* improve recordsupplier and add unit tests\n\n* migrated to aws-java-sdk-kinesis\n\n* merge changes from master\n\n* fix pom files and forbiddenapi checks\n\n* checkpoint JavaType bug fix\n\n* fix pom and stuff\n\n* disable checkpointing in kinesis\n\n* fix kinesis sequence number null in closed shard\n\n* merge changes from master\n\n* fixes for kinesis tasks\n\n* capitalized <partitionType.  sequenceType>\n\n* removed abstract class loggers\n\n* conform to guava api restrictions\n\n* add docker for travis other modules test\n\n* address comments\n\n* improve RecordSupplier to supply records in batch\n\n* fix strict compile issue\n\n* add test scope for localstack dependency\n\n* kinesis indexing task refactoring\n\n* comments\n\n* github comments\n\n* minor fix\n\n* removed unneeded readme\n\n* fix deserialization bug\n\n* fix various bugs\n\n* KinesisRecordSupplier unable to catch up to earliest position in stream bug fix\n\n* minor changes to kinesis\n\n* implement deaggregate for kinesis\n\n* Merge remote-tracking branch 'upstream/master' into seekablestream\n\n* fix kinesis offset discrepancy with kafka\n\n* kinesis record supplier disable getPosition\n\n* pr comments\n\n* mock for kinesis tests and remove docker dependency for unit tests\n\n* PR comments\n\n* avg lag in kafkasupervisor #6587\n\n* refacotred SequenceMetadata in taskRunners\n\n* small fix\n\n* more small fix\n\n* recordsupplier resource leak\n\n* revert .travis.yml formatting\n\n* fix style\n\n* kinesis docs\n\n* doc part2\n\n* more docs\n\n* comments\n\n* comments*2\n\n* revert string replace changes\n\n* comments\n\n* teamcity\n\n* comments part 1\n\n* comments part 2\n\n* comments part 3\n\n* merge #6754\n\n* fix injection binding\n\n* comments\n\n* KinesisRegion refactor\n\n* comments part idk lol\n\n* can't think of a commit msg anymore\n\n* remove possiblyResetDataSourceMetadata() for IncrementalPublishingTaskRunner\n\n* commmmmmmmmmments\n\n* extra error handling in KinesisRecordSupplier getRecords\n\n* comments\n\n* quickfix\n\n* typo\n\n* oof\n","date":"2018-12-22 03:49:24","modifiedFileCount":"22","status":"M","submitter":"Joshua Sun"},{"authorTime":"2019-02-19 03:50:08","codes":[{"authorDate":"2019-02-19 03:50:08","commitOrder":4,"curCode":"  public void testSkipOffsetGaps() throws Exception\n  {\n    supervisor = getSupervisor(1, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n\n  }\n","date":"2019-02-19 03:50:08","endLine":330,"groupId":"4225","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testSkipOffsetGaps","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f2/db280c51eb85380271ce661d1c415120afa0e0.src","preCode":"  public void testSkipOffsetGaps() throws Exception\n  {\n    supervisor = getSupervisor(1, 1, true, \"PT1H\", null, null, true);\n    addSomeEvents(1);\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n\n    Assert.assertTrue(\"skipOffsetGaps\", taskConfig.isSkipOffsetGaps());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":305,"status":"M"},{"authorDate":"2019-02-19 03:50:08","commitOrder":4,"curCode":"  public void testFailedInitializationAndRecovery() throws Exception\n  {\n    \r\n    supervisor = getSupervisor(\n        1,\n        1,\n        true,\n        \"PT1H\",\n        null,\n        null,\n        false,\n        StringUtils.format(\"badhostname:%d\", kafkaServer.getPort())\n    );\n    addSomeEvents(1);\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n\n    replayAll();\n\n    supervisor.start();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertFalse(supervisor.isStarted());\n\n    verifyAll();\n\n    while (supervisor.getInitRetryCounter() < 3) {\n      Thread.sleep(1000);\n    }\n\n    \r\n    resetAll();\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    \r\n    \r\n    \r\n    supervisor.getIoConfig().getConsumerProperties().put(\"bootstrap.servers\", kafkaHost);\n    supervisor.tryInit();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertTrue(supervisor.isStarted());\n\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    Assert.assertEquals(dataSchema, task.getDataSchema());\n    Assert.assertEquals(tuningConfig.convertToTaskTuningConfig(), task.getTuningConfig());\n\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n    Assert.assertEquals(kafkaHost, taskConfig.getConsumerProperties().get(\"bootstrap.servers\"));\n    Assert.assertEquals(\"myCustomValue\", taskConfig.getConsumerProperties().get(\"myCustomKey\"));\n    Assert.assertEquals(\"sequenceName-0\", taskConfig.getBaseSequenceName());\n    Assert.assertTrue(\"isUseTransaction\", taskConfig.isUseTransaction());\n    Assert.assertFalse(\"minimumMessageTime\", taskConfig.getMinimumMessageTime().isPresent());\n    Assert.assertFalse(\"maximumMessageTime\", taskConfig.getMaximumMessageTime().isPresent());\n\n    Assert.assertEquals(topic, taskConfig.getStartPartitions().getStream());\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionSequenceNumberMap().get(0));\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionSequenceNumberMap().get(1));\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionSequenceNumberMap().get(2));\n\n    Assert.assertEquals(topic, taskConfig.getEndPartitions().getStream());\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionSequenceNumberMap().get(0));\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionSequenceNumberMap().get(1));\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionSequenceNumberMap().get(2));\n  }\n","date":"2019-02-19 03:50:08","endLine":2680,"groupId":"4225","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testFailedInitializationAndRecovery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f2/db280c51eb85380271ce661d1c415120afa0e0.src","preCode":"  public void testFailedInitializationAndRecovery() throws Exception\n  {\n    \r\n    supervisor = getSupervisor(\n        1,\n        1,\n        true,\n        \"PT1H\",\n        null,\n        null,\n        false,\n        false,\n        StringUtils.format(\"badhostname:%d\", kafkaServer.getPort())\n    );\n    addSomeEvents(1);\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n\n    replayAll();\n\n    supervisor.start();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertFalse(supervisor.isStarted());\n\n    verifyAll();\n\n    while (supervisor.getInitRetryCounter() < 3) {\n      Thread.sleep(1000);\n    }\n\n    \r\n    resetAll();\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    \r\n    \r\n    \r\n    supervisor.getIoConfig().getConsumerProperties().put(\"bootstrap.servers\", kafkaHost);\n    supervisor.tryInit();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertTrue(supervisor.isStarted());\n\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    Assert.assertEquals(dataSchema, task.getDataSchema());\n    Assert.assertEquals(tuningConfig.convertToTaskTuningConfig(), task.getTuningConfig());\n\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n    Assert.assertEquals(kafkaHost, taskConfig.getConsumerProperties().get(\"bootstrap.servers\"));\n    Assert.assertEquals(\"myCustomValue\", taskConfig.getConsumerProperties().get(\"myCustomKey\"));\n    Assert.assertEquals(\"sequenceName-0\", taskConfig.getBaseSequenceName());\n    Assert.assertTrue(\"isUseTransaction\", taskConfig.isUseTransaction());\n    Assert.assertFalse(\"minimumMessageTime\", taskConfig.getMinimumMessageTime().isPresent());\n    Assert.assertFalse(\"maximumMessageTime\", taskConfig.getMaximumMessageTime().isPresent());\n    Assert.assertFalse(\"skipOffsetGaps\", taskConfig.isSkipOffsetGaps());\n\n    Assert.assertEquals(topic, taskConfig.getStartPartitions().getStream());\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionSequenceNumberMap().get(0));\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionSequenceNumberMap().get(1));\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionSequenceNumberMap().get(2));\n\n    Assert.assertEquals(topic, taskConfig.getEndPartitions().getStream());\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionSequenceNumberMap().get(0));\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionSequenceNumberMap().get(1));\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionSequenceNumberMap().get(2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2594,"status":"M"}],"commitId":"80a2ef7be46c4fc807ac2a74507b8ba8b6a44049","commitMessage":"@@@Support kafka transactional topics (#5404) (#6496)\n\n* Support kafka transactional topics\n\n* update kafka to version 2.0.0\n* Remove the skipOffsetGaps option since it's not used anymore\n* Adjust kafka consumer to use transactional semantics\n* Update tests\n\n* Remove unused import from test\n\n* Fix compilation\n\n* Invoke transaction api to fix a unit test\n\n* temporary modification of travis.yml for debugging\n\n* another attempt to get travis tasklogs\n\n* update kafka to 2.0.1 at all places\n\n* Remove druid-kafka-eight dependency from integration-tests.  remove the kafka firehose test and deprecate kafka-eight classes\n\n* Add deprecated in docs for kafka-eight and kafka-simple extensions\n\n* Remove skipOffsetGaps and code changes for transaction support\n\n* Fix indentation\n\n* remove skipOffsetGaps from kinesis\n\n* Add transaction api to KafkaRecordSupplierTest\n\n* Fix indent\n\n* Fix test\n\n* update kafka version to 2.1.0\n","date":"2019-02-19 03:50:08","modifiedFileCount":"25","status":"M","submitter":"Surekha"},{"authorTime":"2019-03-22 04:12:22","codes":[{"authorDate":"2019-02-19 03:50:08","commitOrder":5,"curCode":"  public void testSkipOffsetGaps() throws Exception\n  {\n    supervisor = getSupervisor(1, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n\n  }\n","date":"2019-02-19 03:50:08","endLine":330,"groupId":"4225","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testSkipOffsetGaps","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f2/db280c51eb85380271ce661d1c415120afa0e0.src","preCode":"  public void testSkipOffsetGaps() throws Exception\n  {\n    supervisor = getSupervisor(1, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":305,"status":"N"},{"authorDate":"2019-03-22 04:12:22","commitOrder":5,"curCode":"  public void testFailedInitializationAndRecovery() throws Exception\n  {\n    \r\n    supervisor = getSupervisor(\n        1,\n        1,\n        true,\n        \"PT1H\",\n        null,\n        null,\n        false,\n        StringUtils.format(\"badhostname:%d\", kafkaServer.getPort())\n    );\n    addSomeEvents(1);\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n\n    replayAll();\n\n    supervisor.start();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertFalse(supervisor.isStarted());\n\n    verifyAll();\n\n    while (supervisor.getInitRetryCounter() < 3) {\n      Thread.sleep(1000);\n    }\n\n    \r\n    resetAll();\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    \r\n    \r\n    \r\n    supervisor.getIoConfig().getConsumerProperties().put(\"bootstrap.servers\", kafkaHost);\n    supervisor.tryInit();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertTrue(supervisor.isStarted());\n\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    Assert.assertEquals(dataSchema, task.getDataSchema());\n    Assert.assertEquals(tuningConfig.convertToTaskTuningConfig(), task.getTuningConfig());\n\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n    Assert.assertEquals(kafkaHost, taskConfig.getConsumerProperties().get(\"bootstrap.servers\"));\n    Assert.assertEquals(\"myCustomValue\", taskConfig.getConsumerProperties().get(\"myCustomKey\"));\n    Assert.assertEquals(\"sequenceName-0\", taskConfig.getBaseSequenceName());\n    Assert.assertTrue(\"isUseTransaction\", taskConfig.isUseTransaction());\n    Assert.assertFalse(\"minimumMessageTime\", taskConfig.getMinimumMessageTime().isPresent());\n    Assert.assertFalse(\"maximumMessageTime\", taskConfig.getMaximumMessageTime().isPresent());\n\n    Assert.assertEquals(topic, taskConfig.getStartSequenceNumbers().getStream());\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n\n    Assert.assertEquals(topic, taskConfig.getEndSequenceNumbers().getStream());\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n  }\n","date":"2019-03-22 04:12:22","endLine":2791,"groupId":"4225","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testFailedInitializationAndRecovery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b3/0b75ee7886fa6f4c326a81770066d6778e2033.src","preCode":"  public void testFailedInitializationAndRecovery() throws Exception\n  {\n    \r\n    supervisor = getSupervisor(\n        1,\n        1,\n        true,\n        \"PT1H\",\n        null,\n        null,\n        false,\n        StringUtils.format(\"badhostname:%d\", kafkaServer.getPort())\n    );\n    addSomeEvents(1);\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n\n    replayAll();\n\n    supervisor.start();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertFalse(supervisor.isStarted());\n\n    verifyAll();\n\n    while (supervisor.getInitRetryCounter() < 3) {\n      Thread.sleep(1000);\n    }\n\n    \r\n    resetAll();\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    \r\n    \r\n    \r\n    supervisor.getIoConfig().getConsumerProperties().put(\"bootstrap.servers\", kafkaHost);\n    supervisor.tryInit();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertTrue(supervisor.isStarted());\n\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    Assert.assertEquals(dataSchema, task.getDataSchema());\n    Assert.assertEquals(tuningConfig.convertToTaskTuningConfig(), task.getTuningConfig());\n\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n    Assert.assertEquals(kafkaHost, taskConfig.getConsumerProperties().get(\"bootstrap.servers\"));\n    Assert.assertEquals(\"myCustomValue\", taskConfig.getConsumerProperties().get(\"myCustomKey\"));\n    Assert.assertEquals(\"sequenceName-0\", taskConfig.getBaseSequenceName());\n    Assert.assertTrue(\"isUseTransaction\", taskConfig.isUseTransaction());\n    Assert.assertFalse(\"minimumMessageTime\", taskConfig.getMinimumMessageTime().isPresent());\n    Assert.assertFalse(\"maximumMessageTime\", taskConfig.getMaximumMessageTime().isPresent());\n\n    Assert.assertEquals(topic, taskConfig.getStartPartitions().getStream());\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionSequenceNumberMap().get(0));\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionSequenceNumberMap().get(1));\n    Assert.assertEquals(0L, (long) taskConfig.getStartPartitions().getPartitionSequenceNumberMap().get(2));\n\n    Assert.assertEquals(topic, taskConfig.getEndPartitions().getStream());\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionSequenceNumberMap().get(0));\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionSequenceNumberMap().get(1));\n    Assert.assertEquals(Long.MAX_VALUE, (long) taskConfig.getEndPartitions().getPartitionSequenceNumberMap().get(2));\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2687,"status":"M"}],"commitId":"0c5dcf5586e33607849e397209f3eb0b10661f1e","commitMessage":"@@@Fix exclusivity for start offset in kinesis indexing service & check exclusivity properly in IndexerSQLMetadataStorageCoordinator (#7291)\n\n* Fix exclusivity for start offset in kinesis indexing service\n\n* some adjustment\n\n* Fix SeekableStreamDataSourceMetadata\n\n* Add missing javadocs\n\n* Add missing comments and unit test\n\n* fix SeekableStreamStartSequenceNumbers.plus and add comments\n\n* remove extra exclusivePartitions in KafkaIOConfig and fix downgrade issue\n\n* Add javadocs\n\n* fix compilation\n\n* fix test\n\n* remove unused variable\n","date":"2019-03-22 04:12:22","modifiedFileCount":"30","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-04-11 09:16:38","codes":[{"authorDate":"2019-04-11 09:16:38","commitOrder":6,"curCode":"  public void testSkipOffsetGaps() throws Exception\n  {\n    supervisor = getTestableSupervisor(1, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n\n  }\n","date":"2019-04-11 09:16:38","endLine":342,"groupId":"4225","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testSkipOffsetGaps","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b6/e7b3406771642225b6ce082b4b2bc68e735a67.src","preCode":"  public void testSkipOffsetGaps() throws Exception\n  {\n    supervisor = getSupervisor(1, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":317,"status":"M"},{"authorDate":"2019-04-11 09:16:38","commitOrder":6,"curCode":"  public void testFailedInitializationAndRecovery() throws Exception\n  {\n    \r\n    supervisor = getTestableSupervisor(\n        1,\n        1,\n        true,\n        \"PT1H\",\n        null,\n        null,\n        false,\n        StringUtils.format(\"badhostname:%d\", kafkaServer.getPort())\n    );\n    addSomeEvents(1);\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n\n    replayAll();\n\n    supervisor.start();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertFalse(supervisor.isStarted());\n\n    verifyAll();\n\n    while (supervisor.getInitRetryCounter() < 3) {\n      Thread.sleep(1000);\n    }\n\n    \r\n    resetAll();\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    \r\n    \r\n    \r\n    supervisor.getIoConfig().getConsumerProperties().put(\"bootstrap.servers\", kafkaHost);\n    supervisor.tryInit();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertTrue(supervisor.isStarted());\n\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    Assert.assertEquals(dataSchema, task.getDataSchema());\n    Assert.assertEquals(tuningConfig.convertToTaskTuningConfig(), task.getTuningConfig());\n\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n    Assert.assertEquals(kafkaHost, taskConfig.getConsumerProperties().get(\"bootstrap.servers\"));\n    Assert.assertEquals(\"myCustomValue\", taskConfig.getConsumerProperties().get(\"myCustomKey\"));\n    Assert.assertEquals(\"sequenceName-0\", taskConfig.getBaseSequenceName());\n    Assert.assertTrue(\"isUseTransaction\", taskConfig.isUseTransaction());\n    Assert.assertFalse(\"minimumMessageTime\", taskConfig.getMinimumMessageTime().isPresent());\n    Assert.assertFalse(\"maximumMessageTime\", taskConfig.getMaximumMessageTime().isPresent());\n\n    Assert.assertEquals(topic, taskConfig.getStartSequenceNumbers().getStream());\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n\n    Assert.assertEquals(topic, taskConfig.getEndSequenceNumbers().getStream());\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n  }\n","date":"2019-04-11 09:16:38","endLine":2779,"groupId":"4225","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testFailedInitializationAndRecovery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b6/e7b3406771642225b6ce082b4b2bc68e735a67.src","preCode":"  public void testFailedInitializationAndRecovery() throws Exception\n  {\n    \r\n    supervisor = getSupervisor(\n        1,\n        1,\n        true,\n        \"PT1H\",\n        null,\n        null,\n        false,\n        StringUtils.format(\"badhostname:%d\", kafkaServer.getPort())\n    );\n    addSomeEvents(1);\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n\n    replayAll();\n\n    supervisor.start();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertFalse(supervisor.isStarted());\n\n    verifyAll();\n\n    while (supervisor.getInitRetryCounter() < 3) {\n      Thread.sleep(1000);\n    }\n\n    \r\n    resetAll();\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    \r\n    \r\n    \r\n    supervisor.getIoConfig().getConsumerProperties().put(\"bootstrap.servers\", kafkaHost);\n    supervisor.tryInit();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertTrue(supervisor.isStarted());\n\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    Assert.assertEquals(dataSchema, task.getDataSchema());\n    Assert.assertEquals(tuningConfig.convertToTaskTuningConfig(), task.getTuningConfig());\n\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n    Assert.assertEquals(kafkaHost, taskConfig.getConsumerProperties().get(\"bootstrap.servers\"));\n    Assert.assertEquals(\"myCustomValue\", taskConfig.getConsumerProperties().get(\"myCustomKey\"));\n    Assert.assertEquals(\"sequenceName-0\", taskConfig.getBaseSequenceName());\n    Assert.assertTrue(\"isUseTransaction\", taskConfig.isUseTransaction());\n    Assert.assertFalse(\"minimumMessageTime\", taskConfig.getMinimumMessageTime().isPresent());\n    Assert.assertFalse(\"maximumMessageTime\", taskConfig.getMaximumMessageTime().isPresent());\n\n    Assert.assertEquals(topic, taskConfig.getStartSequenceNumbers().getStream());\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n\n    Assert.assertEquals(topic, taskConfig.getEndSequenceNumbers().getStream());\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2675,"status":"M"}],"commitId":"2771ed50b0f07b0ee519da72ed9f4877466f8be4","commitMessage":"@@@Support Kafka supervisor adopting running tasks between versions  (#7212)\n\n* Recompute hash in isTaskCurrent() and added tests\n\n* Fixed checkstyle stuff\n\n* Fixed failing tests\n\n* Make TestableKafkaSupervisorWithCustomIsTaskCurrent static\n\n* Add doc\n\n* baseSequenceName change\n\n* Added comment\n\n* WIP\n\n* Fixed imports\n\n* Undid lambda change for diff sake\n\n* Cleanup\n\n* Added comment\n\n* Reinsert Kafka tests\n\n* Readded kinesis test\n\n* Readd bad partition assignment in kinesis supervisor test\n\n* Nit\n\n* Misnamed var\n","date":"2019-04-11 09:16:38","modifiedFileCount":"6","status":"M","submitter":"Justin Borromeo"},{"authorTime":"2019-07-07 00:33:12","codes":[{"authorDate":"2019-07-07 00:33:12","commitOrder":7,"curCode":"  public void testSkipOffsetGaps() throws Exception\n  {\n    supervisor = getTestableSupervisor(1, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskQueue.add(EasyMock.capture(captured))).andReturn(true);\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","date":"2019-07-07 00:33:12","endLine":335,"groupId":"4225","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testSkipOffsetGaps","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/af/f5639cd0e0e7c6a18b14f694e3cf2c6cb73dec.src","preCode":"  public void testSkipOffsetGaps() throws Exception\n  {\n    supervisor = getTestableSupervisor(1, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":314,"status":"M"},{"authorDate":"2019-07-07 00:33:12","commitOrder":7,"curCode":"  public void testFailedInitializationAndRecovery() throws Exception\n  {\n    \r\n    supervisor = getTestableSupervisor(\n        1,\n        1,\n        true,\n        \"PT1H\",\n        null,\n        null,\n        false,\n        StringUtils.format(\"badhostname:%d\", kafkaServer.getPort())\n    );\n    addSomeEvents(1);\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n\n    replayAll();\n\n    supervisor.start();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertFalse(supervisor.isStarted());\n\n    verifyAll();\n\n    while (supervisor.getInitRetryCounter() < 3) {\n      Thread.sleep(1000);\n    }\n\n    \r\n    resetAll();\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskQueue.add(EasyMock.capture(captured))).andReturn(true);\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    \r\n    \r\n    \r\n    supervisor.getIoConfig().getConsumerProperties().put(\"bootstrap.servers\", kafkaHost);\n    supervisor.tryInit();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertTrue(supervisor.isStarted());\n\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    Assert.assertEquals(dataSchema, task.getDataSchema());\n    Assert.assertEquals(tuningConfig.convertToTaskTuningConfig(), task.getTuningConfig());\n\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n    Assert.assertEquals(kafkaHost, taskConfig.getConsumerProperties().get(\"bootstrap.servers\"));\n    Assert.assertEquals(\"myCustomValue\", taskConfig.getConsumerProperties().get(\"myCustomKey\"));\n    Assert.assertEquals(\"sequenceName-0\", taskConfig.getBaseSequenceName());\n    Assert.assertTrue(\"isUseTransaction\", taskConfig.isUseTransaction());\n    Assert.assertFalse(\"minimumMessageTime\", taskConfig.getMinimumMessageTime().isPresent());\n    Assert.assertFalse(\"maximumMessageTime\", taskConfig.getMaximumMessageTime().isPresent());\n\n    Assert.assertEquals(topic, taskConfig.getStartSequenceNumbers().getStream());\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n\n    Assert.assertEquals(topic, taskConfig.getEndSequenceNumbers().getStream());\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n  }\n","date":"2019-07-07 00:33:12","endLine":2789,"groupId":"4225","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testFailedInitializationAndRecovery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/af/f5639cd0e0e7c6a18b14f694e3cf2c6cb73dec.src","preCode":"  public void testFailedInitializationAndRecovery() throws Exception\n  {\n    \r\n    supervisor = getTestableSupervisor(\n        1,\n        1,\n        true,\n        \"PT1H\",\n        null,\n        null,\n        false,\n        StringUtils.format(\"badhostname:%d\", kafkaServer.getPort())\n    );\n    addSomeEvents(1);\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n\n    replayAll();\n\n    supervisor.start();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertFalse(supervisor.isStarted());\n\n    verifyAll();\n\n    while (supervisor.getInitRetryCounter() < 3) {\n      Thread.sleep(1000);\n    }\n\n    \r\n    resetAll();\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskQueue.add(capture(captured))).andReturn(true);\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    \r\n    \r\n    \r\n    supervisor.getIoConfig().getConsumerProperties().put(\"bootstrap.servers\", kafkaHost);\n    supervisor.tryInit();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertTrue(supervisor.isStarted());\n\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    Assert.assertEquals(dataSchema, task.getDataSchema());\n    Assert.assertEquals(tuningConfig.convertToTaskTuningConfig(), task.getTuningConfig());\n\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n    Assert.assertEquals(kafkaHost, taskConfig.getConsumerProperties().get(\"bootstrap.servers\"));\n    Assert.assertEquals(\"myCustomValue\", taskConfig.getConsumerProperties().get(\"myCustomKey\"));\n    Assert.assertEquals(\"sequenceName-0\", taskConfig.getBaseSequenceName());\n    Assert.assertTrue(\"isUseTransaction\", taskConfig.isUseTransaction());\n    Assert.assertFalse(\"minimumMessageTime\", taskConfig.getMinimumMessageTime().isPresent());\n    Assert.assertFalse(\"maximumMessageTime\", taskConfig.getMaximumMessageTime().isPresent());\n\n    Assert.assertEquals(topic, taskConfig.getStartSequenceNumbers().getStream());\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n\n    Assert.assertEquals(topic, taskConfig.getEndSequenceNumbers().getStream());\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2685,"status":"M"}],"commitId":"1166bbcb75d432817715fdd429737f86730b5591","commitMessage":"@@@Remove static imports from tests (#8036)\n\nMake static imports forbidden in tests and remove all occurrences to be\nconsistent with the non-test code.\n\nAlso.  various changes to files affected by above:\n- Reformat to adhere to druid style guide\n- Fix various IntelliJ warnings\n- Fix various SonarLint warnings (e.g..  the expected/actual args to\n  Assert.assertEquals() were flipped)","date":"2019-07-07 00:33:12","modifiedFileCount":"98","status":"M","submitter":"Chi Cao Minh"},{"authorTime":"2019-08-23 05:51:25","codes":[{"authorDate":"2019-07-07 00:33:12","commitOrder":8,"curCode":"  public void testSkipOffsetGaps() throws Exception\n  {\n    supervisor = getTestableSupervisor(1, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskQueue.add(EasyMock.capture(captured))).andReturn(true);\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","date":"2019-07-07 00:33:12","endLine":335,"groupId":"4225","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testSkipOffsetGaps","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/af/f5639cd0e0e7c6a18b14f694e3cf2c6cb73dec.src","preCode":"  public void testSkipOffsetGaps() throws Exception\n  {\n    supervisor = getTestableSupervisor(1, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskQueue.add(EasyMock.capture(captured))).andReturn(true);\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":314,"status":"N"},{"authorDate":"2019-08-23 05:51:25","commitOrder":8,"curCode":"  public void testFailedInitializationAndRecovery() throws Exception\n  {\n    \r\n    supervisor = getTestableSupervisor(\n        1,\n        1,\n        true,\n        \"PT1H\",\n        null,\n        null,\n        false,\n        StringUtils.format(\"badhostname:%d\", kafkaServer.getPort())\n    );\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n\n    replayAll();\n\n    supervisor.start();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertFalse(supervisor.isStarted());\n\n    verifyAll();\n\n    while (supervisor.getInitRetryCounter() < 3) {\n      Thread.sleep(1000);\n    }\n\n    \r\n    resetAll();\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskQueue.add(EasyMock.capture(captured))).andReturn(true);\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    \r\n    \r\n    \r\n    supervisor.getIoConfig().getConsumerProperties().put(\"bootstrap.servers\", kafkaHost);\n    supervisor.tryInit();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertTrue(supervisor.isStarted());\n\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    Assert.assertEquals(dataSchema, task.getDataSchema());\n    Assert.assertEquals(tuningConfig.convertToTaskTuningConfig(), task.getTuningConfig());\n\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n    Assert.assertEquals(kafkaHost, taskConfig.getConsumerProperties().get(\"bootstrap.servers\"));\n    Assert.assertEquals(\"myCustomValue\", taskConfig.getConsumerProperties().get(\"myCustomKey\"));\n    Assert.assertEquals(\"sequenceName-0\", taskConfig.getBaseSequenceName());\n    Assert.assertTrue(\"isUseTransaction\", taskConfig.isUseTransaction());\n    Assert.assertFalse(\"minimumMessageTime\", taskConfig.getMinimumMessageTime().isPresent());\n    Assert.assertFalse(\"maximumMessageTime\", taskConfig.getMaximumMessageTime().isPresent());\n\n    Assert.assertEquals(topic, taskConfig.getStartSequenceNumbers().getStream());\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n\n    Assert.assertEquals(topic, taskConfig.getEndSequenceNumbers().getStream());\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n  }\n","date":"2019-08-23 05:51:25","endLine":2865,"groupId":"4225","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testFailedInitializationAndRecovery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0d/255b66d24d58735694d9fe14274069e9ba5ee9.src","preCode":"  public void testFailedInitializationAndRecovery() throws Exception\n  {\n    \r\n    supervisor = getTestableSupervisor(\n        1,\n        1,\n        true,\n        \"PT1H\",\n        null,\n        null,\n        false,\n        StringUtils.format(\"badhostname:%d\", kafkaServer.getPort())\n    );\n    addSomeEvents(1);\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n\n    replayAll();\n\n    supervisor.start();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertFalse(supervisor.isStarted());\n\n    verifyAll();\n\n    while (supervisor.getInitRetryCounter() < 3) {\n      Thread.sleep(1000);\n    }\n\n    \r\n    resetAll();\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskQueue.add(EasyMock.capture(captured))).andReturn(true);\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    \r\n    \r\n    \r\n    supervisor.getIoConfig().getConsumerProperties().put(\"bootstrap.servers\", kafkaHost);\n    supervisor.tryInit();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertTrue(supervisor.isStarted());\n\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    Assert.assertEquals(dataSchema, task.getDataSchema());\n    Assert.assertEquals(tuningConfig.convertToTaskTuningConfig(), task.getTuningConfig());\n\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n    Assert.assertEquals(kafkaHost, taskConfig.getConsumerProperties().get(\"bootstrap.servers\"));\n    Assert.assertEquals(\"myCustomValue\", taskConfig.getConsumerProperties().get(\"myCustomKey\"));\n    Assert.assertEquals(\"sequenceName-0\", taskConfig.getBaseSequenceName());\n    Assert.assertTrue(\"isUseTransaction\", taskConfig.isUseTransaction());\n    Assert.assertFalse(\"minimumMessageTime\", taskConfig.getMinimumMessageTime().isPresent());\n    Assert.assertFalse(\"maximumMessageTime\", taskConfig.getMaximumMessageTime().isPresent());\n\n    Assert.assertEquals(topic, taskConfig.getStartSequenceNumbers().getStream());\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n\n    Assert.assertEquals(topic, taskConfig.getEndSequenceNumbers().getStream());\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2760,"status":"M"}],"commitId":"fba92ae469b512cca6cdf86ffc1c1a2090808453","commitMessage":"@@@Fix to always use end sequenceNumber for reset (#8305)\n\n* Fix to always use end sequenceNumber for reset\n\n* fix checkstyle\n\n* fix style and add log\n","date":"2019-08-23 05:51:25","modifiedFileCount":"8","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-09-27 07:15:24","codes":[{"authorDate":"2019-09-27 07:15:24","commitOrder":9,"curCode":"  public void testSkipOffsetGaps() throws Exception\n  {\n    supervisor = getTestableSupervisor(1, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE)).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskQueue.add(EasyMock.capture(captured))).andReturn(true);\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","date":"2019-09-27 07:15:24","endLine":358,"groupId":"4225","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testSkipOffsetGaps","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/75/9182feac2928302ae8f3579d872a1883a232c5.src","preCode":"  public void testSkipOffsetGaps() throws Exception\n  {\n    supervisor = getTestableSupervisor(1, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskQueue.add(EasyMock.capture(captured))).andReturn(true);\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":337,"status":"M"},{"authorDate":"2019-09-27 07:15:24","commitOrder":9,"curCode":"  public void testFailedInitializationAndRecovery() throws Exception\n  {\n    \r\n    supervisor = getTestableSupervisor(\n        1,\n        1,\n        true,\n        \"PT1H\",\n        null,\n        null,\n        false,\n        StringUtils.format(\"badhostname:%d\", kafkaServer.getPort())\n    );\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE)).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n\n    replayAll();\n\n    supervisor.start();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertFalse(supervisor.isStarted());\n\n    verifyAll();\n\n    while (supervisor.getInitRetryCounter() < 3) {\n      Thread.sleep(1000);\n    }\n\n    \r\n    resetAll();\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE)).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskQueue.add(EasyMock.capture(captured))).andReturn(true);\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    \r\n    \r\n    \r\n    supervisor.getIoConfig().getConsumerProperties().put(\"bootstrap.servers\", kafkaHost);\n    supervisor.tryInit();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertTrue(supervisor.isStarted());\n\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    Assert.assertEquals(dataSchema, task.getDataSchema());\n    Assert.assertEquals(tuningConfig.convertToTaskTuningConfig(), task.getTuningConfig());\n\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n    Assert.assertEquals(kafkaHost, taskConfig.getConsumerProperties().get(\"bootstrap.servers\"));\n    Assert.assertEquals(\"myCustomValue\", taskConfig.getConsumerProperties().get(\"myCustomKey\"));\n    Assert.assertEquals(\"sequenceName-0\", taskConfig.getBaseSequenceName());\n    Assert.assertTrue(\"isUseTransaction\", taskConfig.isUseTransaction());\n    Assert.assertFalse(\"minimumMessageTime\", taskConfig.getMinimumMessageTime().isPresent());\n    Assert.assertFalse(\"maximumMessageTime\", taskConfig.getMaximumMessageTime().isPresent());\n\n    Assert.assertEquals(topic, taskConfig.getStartSequenceNumbers().getStream());\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n\n    Assert.assertEquals(topic, taskConfig.getEndSequenceNumbers().getStream());\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n  }\n","date":"2019-09-27 07:15:24","endLine":2853,"groupId":"4225","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testFailedInitializationAndRecovery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/75/9182feac2928302ae8f3579d872a1883a232c5.src","preCode":"  public void testFailedInitializationAndRecovery() throws Exception\n  {\n    \r\n    supervisor = getTestableSupervisor(\n        1,\n        1,\n        true,\n        \"PT1H\",\n        null,\n        null,\n        false,\n        StringUtils.format(\"badhostname:%d\", kafkaServer.getPort())\n    );\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n\n    replayAll();\n\n    supervisor.start();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertFalse(supervisor.isStarted());\n\n    verifyAll();\n\n    while (supervisor.getInitRetryCounter() < 3) {\n      Thread.sleep(1000);\n    }\n\n    \r\n    resetAll();\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskQueue.add(EasyMock.capture(captured))).andReturn(true);\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    \r\n    \r\n    \r\n    supervisor.getIoConfig().getConsumerProperties().put(\"bootstrap.servers\", kafkaHost);\n    supervisor.tryInit();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertTrue(supervisor.isStarted());\n\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    Assert.assertEquals(dataSchema, task.getDataSchema());\n    Assert.assertEquals(tuningConfig.convertToTaskTuningConfig(), task.getTuningConfig());\n\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n    Assert.assertEquals(kafkaHost, taskConfig.getConsumerProperties().get(\"bootstrap.servers\"));\n    Assert.assertEquals(\"myCustomValue\", taskConfig.getConsumerProperties().get(\"myCustomKey\"));\n    Assert.assertEquals(\"sequenceName-0\", taskConfig.getBaseSequenceName());\n    Assert.assertTrue(\"isUseTransaction\", taskConfig.isUseTransaction());\n    Assert.assertFalse(\"minimumMessageTime\", taskConfig.getMinimumMessageTime().isPresent());\n    Assert.assertFalse(\"maximumMessageTime\", taskConfig.getMaximumMessageTime().isPresent());\n\n    Assert.assertEquals(topic, taskConfig.getStartSequenceNumbers().getStream());\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n\n    Assert.assertEquals(topic, taskConfig.getEndSequenceNumbers().getStream());\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2748,"status":"M"}],"commitId":"7f2b6577ef19f18523e8353336ad496e8dc4a270","commitMessage":"@@@get active task by datasource when supervisor discover tasks (#8450)\n\n* get active task by datasource when supervisor discover tasks\n\n* fix ut\n\n* fix ut\n\n* fix ut\n\n* remove unnecessary condition check\n\n* fix ut\n\n* remove stream in hot loop\n","date":"2019-09-27 07:15:24","modifiedFileCount":"7","status":"M","submitter":"elloooooo"},{"authorTime":"2020-01-28 03:24:29","codes":[{"authorDate":"2020-01-28 03:24:29","commitOrder":10,"curCode":"  public void testSkipOffsetGaps() throws Exception\n  {\n    supervisor = getTestableSupervisor(1, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE)).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.retrieveDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskQueue.add(EasyMock.capture(captured))).andReturn(true);\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","date":"2020-01-28 03:24:29","endLine":370,"groupId":"102468","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"testSkipOffsetGaps","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0b/29b65a628bb6f0503d980831a84ce2d009150d.src","preCode":"  public void testSkipOffsetGaps() throws Exception\n  {\n    supervisor = getTestableSupervisor(1, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE)).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskQueue.add(EasyMock.capture(captured))).andReturn(true);\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":349,"status":"M"},{"authorDate":"2020-01-28 03:24:29","commitOrder":10,"curCode":"  public void testFailedInitializationAndRecovery() throws Exception\n  {\n    \r\n    supervisor = getTestableSupervisor(\n        1,\n        1,\n        true,\n        \"PT1H\",\n        null,\n        null,\n        false,\n        StringUtils.format(\"badhostname:%d\", kafkaServer.getPort())\n    );\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE)).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.retrieveDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n\n    replayAll();\n\n    supervisor.start();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertFalse(supervisor.isStarted());\n\n    verifyAll();\n\n    while (supervisor.getInitRetryCounter() < 3) {\n      Thread.sleep(1000);\n    }\n\n    \r\n    resetAll();\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE)).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.retrieveDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskQueue.add(EasyMock.capture(captured))).andReturn(true);\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    \r\n    \r\n    \r\n    supervisor.getIoConfig().getConsumerProperties().put(\"bootstrap.servers\", kafkaHost);\n    supervisor.tryInit();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertTrue(supervisor.isStarted());\n\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    Assert.assertEquals(dataSchema, task.getDataSchema());\n    Assert.assertEquals(tuningConfig.convertToTaskTuningConfig(), task.getTuningConfig());\n\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n    Assert.assertEquals(kafkaHost, taskConfig.getConsumerProperties().get(\"bootstrap.servers\"));\n    Assert.assertEquals(\"myCustomValue\", taskConfig.getConsumerProperties().get(\"myCustomKey\"));\n    Assert.assertEquals(\"sequenceName-0\", taskConfig.getBaseSequenceName());\n    Assert.assertTrue(\"isUseTransaction\", taskConfig.isUseTransaction());\n    Assert.assertFalse(\"minimumMessageTime\", taskConfig.getMinimumMessageTime().isPresent());\n    Assert.assertFalse(\"maximumMessageTime\", taskConfig.getMaximumMessageTime().isPresent());\n\n    Assert.assertEquals(topic, taskConfig.getStartSequenceNumbers().getStream());\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n\n    Assert.assertEquals(topic, taskConfig.getEndSequenceNumbers().getStream());\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n  }\n","date":"2020-01-28 03:24:29","endLine":2885,"groupId":"102468","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testFailedInitializationAndRecovery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0b/29b65a628bb6f0503d980831a84ce2d009150d.src","preCode":"  public void testFailedInitializationAndRecovery() throws Exception\n  {\n    \r\n    supervisor = getTestableSupervisor(\n        1,\n        1,\n        true,\n        \"PT1H\",\n        null,\n        null,\n        false,\n        StringUtils.format(\"badhostname:%d\", kafkaServer.getPort())\n    );\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE)).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n\n    replayAll();\n\n    supervisor.start();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertFalse(supervisor.isStarted());\n\n    verifyAll();\n\n    while (supervisor.getInitRetryCounter() < 3) {\n      Thread.sleep(1000);\n    }\n\n    \r\n    resetAll();\n\n    Capture<KafkaIndexTask> captured = Capture.newInstance();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE)).andReturn(ImmutableList.of()).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskQueue.add(EasyMock.capture(captured))).andReturn(true);\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    \r\n    \r\n    \r\n    supervisor.getIoConfig().getConsumerProperties().put(\"bootstrap.servers\", kafkaHost);\n    supervisor.tryInit();\n\n    Assert.assertTrue(supervisor.isLifecycleStarted());\n    Assert.assertTrue(supervisor.isStarted());\n\n    supervisor.runInternal();\n    verifyAll();\n\n    KafkaIndexTask task = captured.getValue();\n    Assert.assertEquals(dataSchema, task.getDataSchema());\n    Assert.assertEquals(tuningConfig.convertToTaskTuningConfig(), task.getTuningConfig());\n\n    KafkaIndexTaskIOConfig taskConfig = task.getIOConfig();\n    Assert.assertEquals(kafkaHost, taskConfig.getConsumerProperties().get(\"bootstrap.servers\"));\n    Assert.assertEquals(\"myCustomValue\", taskConfig.getConsumerProperties().get(\"myCustomKey\"));\n    Assert.assertEquals(\"sequenceName-0\", taskConfig.getBaseSequenceName());\n    Assert.assertTrue(\"isUseTransaction\", taskConfig.isUseTransaction());\n    Assert.assertFalse(\"minimumMessageTime\", taskConfig.getMinimumMessageTime().isPresent());\n    Assert.assertFalse(\"maximumMessageTime\", taskConfig.getMaximumMessageTime().isPresent());\n\n    Assert.assertEquals(topic, taskConfig.getStartSequenceNumbers().getStream());\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        0L,\n        taskConfig.getStartSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n\n    Assert.assertEquals(topic, taskConfig.getEndSequenceNumbers().getStream());\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(0).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(1).longValue()\n    );\n    Assert.assertEquals(\n        Long.MAX_VALUE,\n        taskConfig.getEndSequenceNumbers().getPartitionSequenceNumberMap().get(2).longValue()\n    );\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2780,"status":"M"}],"commitId":"b9186f8f9ff2ff52aceda42bc5f24ffd47a7d17e","commitMessage":"@@@Reconcile terminology and method naming to 'used/unused segments'; Rename MetadataSegmentManager to MetadataSegmentsManager (#7306)\n\n* Reconcile terminology and method naming to 'used/unused segments'; Don't use terms 'enable/disable data source'; Rename MetadataSegmentManager to MetadataSegments; Make REST API methods which mark segments as used/unused to return server error instead of an empty response in case of error\n\n* Fix brace\n\n* Import order\n\n* Rename withKillDataSourceWhitelist to withSpecificDataSourcesToKill\n\n* Fix tests\n\n* Fix tests by adding proper methods without interval parameters to IndexerMetadataStorageCoordinator instead of hacking with Intervals.ETERNITY\n\n* More aligned names of DruidCoordinatorHelpers.  rename several CoordinatorDynamicConfig parameters\n\n* Rename ClientCompactTaskQuery to ClientCompactionTaskQuery for consistency with CompactionTask; ClientCompactQueryTuningConfig to ClientCompactionTaskQueryTuningConfig\n\n* More variable and method renames\n\n* Rename MetadataSegments to SegmentsMetadata\n\n* Javadoc update\n\n* Simplify SegmentsMetadata.getUnusedSegmentIntervals().  more javadocs\n\n* Update Javadoc of VersionedIntervalTimeline.iterateAllObjects()\n\n* Reorder imports\n\n* Rename SegmentsMetadata.tryMark... methods to mark... and make them to return boolean and the numbers of segments changed and relay exceptions to callers\n\n* Complete merge\n\n* Add CollectionUtils.newTreeSet(); Refactor DruidCoordinatorRuntimeParams creation in tests\n\n* Remove MetadataSegmentManager\n\n* Rename millisLagSinceCoordinatorBecomesLeaderBeforeCanMarkAsUnusedOvershadowedSegments to leadingTimeMillisBeforeCanMarkAsUnusedOvershadowedSegments\n\n* Fix tests.  refactor DruidCluster creation in tests into DruidClusterBuilder\n\n* Fix inspections\n\n* Fix SQLMetadataSegmentManagerEmptyTest and rename it to SqlSegmentsMetadataEmptyTest\n\n* Rename SegmentsAndMetadata to SegmentsAndCommitMetadata to reduce the similarity with SegmentsMetadata; Rename some methods\n\n* Rename DruidCoordinatorHelper to CoordinatorDuty.  refactor DruidCoordinator\n\n* Unused import\n\n* Optimize imports\n\n* Rename IndexerSQLMetadataStorageCoordinator.getDataSourceMetadata() to retrieveDataSourceMetadata()\n\n* Unused import\n\n* Update terminology in datasource-view.tsx\n\n* Fix label in datasource-view.spec.tsx.snap\n\n* Fix lint errors in datasource-view.tsx\n\n* Doc improvements\n\n* Another attempt to please TSLint\n\n* Another attempt to please TSLint\n\n* Style fixes\n\n* Fix IndexerSQLMetadataStorageCoordinator.createUsedSegmentsSqlQueryForIntervals() (wrong merge)\n\n* Try to fix docs build issue\n\n* Javadoc and spelling fixes\n\n* Rename SegmentsMetadata to SegmentsMetadataManager.  address other comments\n\n* Address more comments\n","date":"2020-01-28 03:24:29","modifiedFileCount":"127","status":"M","submitter":"Roman Leventov"}]
