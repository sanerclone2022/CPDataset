[{"authorTime":"2018-09-29 02:16:35","codes":[{"authorDate":"2018-09-29 02:16:35","commitOrder":2,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        keepSegmentGranularity,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(5, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, Collections.singletonList(COMPACTION_INTERVAL));\n    }\n  }\n","date":"2018-09-29 02:16:35","endLine":416,"groupId":"14702","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5c/d5f1fbdbda0c802521cfd0ada3c308b677af81.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        keepSegmentGranularity,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(5, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, Collections.singletonList(COMPACTION_INTERVAL));\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":389,"status":"B"},{"authorDate":"2018-09-29 02:16:35","commitOrder":2,"curCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(SEGMENTS),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        keepSegmentGranularity,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(5, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, Collections.singletonList(COMPACTION_INTERVAL));\n    }\n  }\n","date":"2018-09-29 02:16:35","endLine":690,"groupId":"14702","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithCustomSegments","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5c/d5f1fbdbda0c802521cfd0ada3c308b677af81.src","preCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(SEGMENTS),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        keepSegmentGranularity,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(5, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, Collections.singletonList(COMPACTION_INTERVAL));\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":663,"status":"MB"}],"commitId":"122caec7b187820a0a7fc89e85eb8216dd57df21","commitMessage":"@@@Add support targetCompactionSizeBytes for compactionTask (#6203)\n\n* Add support targetCompactionSizeBytes for compactionTask\n\n* fix test\n\n* fix a bug in keepSegmentGranularity\n\n* fix wrong noinspection comment\n\n* address comments\n","date":"2018-09-29 02:16:35","modifiedFileCount":"7","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-01-04 09:50:45","codes":[{"authorDate":"2019-01-04 09:50:45","commitOrder":3,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS, Granularities.MONTH);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","date":"2019-01-04 09:50:45","endLine":426,"groupId":"4596","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/86/7cdfe03c74e942da7029a636d3ee9e1e7c92b5.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        keepSegmentGranularity,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(5, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, Collections.singletonList(COMPACTION_INTERVAL));\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":393,"status":"M"},{"authorDate":"2019-01-04 09:50:45","commitOrder":3,"curCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(SEGMENTS),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS, Granularities.MONTH);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","date":"2019-01-04 09:50:45","endLine":736,"groupId":"4596","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithCustomSegments","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/86/7cdfe03c74e942da7029a636d3ee9e1e7c92b5.src","preCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(SEGMENTS),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        keepSegmentGranularity,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(5, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, Collections.singletonList(COMPACTION_INTERVAL));\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":703,"status":"M"}],"commitId":"9ad6a733a58e81ef2e0dee067b1df8477af1dab4","commitMessage":"@@@Add support segmentGranularity for CompactionTask (#6758)\n\n* Add support segmentGranularity\n\n* add doc and fix combination of options\n\n* improve doc\n","date":"2019-01-04 09:50:45","modifiedFileCount":"5","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-01-11 01:50:14","codes":[{"authorDate":"2019-01-11 01:50:14","commitOrder":4,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, AGGREGATORS, SEGMENT_INTERVALS, Granularities.MONTH);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","date":"2019-01-11 01:50:14","endLine":454,"groupId":"14702","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f7/59cc7c1c622766f0623fa48f3320ec1f3375f6.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS, Granularities.MONTH);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":419,"status":"M"},{"authorDate":"2019-01-11 01:50:14","commitOrder":4,"curCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(SEGMENTS),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, AGGREGATORS, SEGMENT_INTERVALS, Granularities.MONTH);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","date":"2019-01-11 01:50:14","endLine":833,"groupId":"14702","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithCustomSegments","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f7/59cc7c1c622766f0623fa48f3320ec1f3375f6.src","preCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(SEGMENTS),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS, Granularities.MONTH);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":798,"status":"M"}],"commitId":"c35a39d70bf705aa49c3a3c97bab87959bb80a4e","commitMessage":"@@@Add support maxRowsPerSegment for auto compaction (#6780)\n\n* Add support maxRowsPerSegment for auto compaction\n\n* fix build\n\n* fix build\n\n* fix teamcity\n\n* add test\n\n* fix test\n\n* address comment\n","date":"2019-01-11 01:50:14","modifiedFileCount":"33","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-02-24 09:02:56","codes":[{"authorDate":"2019-02-24 09:02:56","commitOrder":5,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          SEGMENT_INTERVALS,\n          Granularities.MONTH\n      );\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","date":"2019-02-24 09:02:56","endLine":482,"groupId":"14702","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/51/17c1a31c88aed24b6a09f380d4c84fb2a9340b.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, AGGREGATORS, SEGMENT_INTERVALS, Granularities.MONTH);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":438,"status":"M"},{"authorDate":"2019-02-24 09:02:56","commitOrder":5,"curCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(SEGMENTS),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          SEGMENT_INTERVALS,\n          Granularities.MONTH\n      );\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","date":"2019-02-24 09:02:56","endLine":885,"groupId":"14702","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithCustomSegments","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/51/17c1a31c88aed24b6a09f380d4c84fb2a9340b.src","preCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(SEGMENTS),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, AGGREGATORS, SEGMENT_INTERVALS, Granularities.MONTH);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":841,"status":"M"}],"commitId":"1c2753ab9033ff0c785d6e80f5f7c07dc34c3889","commitMessage":"@@@ParallelIndexSubTask: support ingestSegment in delegating factories (#7089)\n\nIndexTask had special-cased code to properly send a TaskToolbox to a\nIngestSegmentFirehoseFactory that's nested inside a CombiningFirehoseFactory. \nbut ParallelIndexSubTask didn't.\n\nThis change refactors IngestSegmentFirehoseFactory so that it doesn't need a\nTaskToolbox; it instead gets a CoordinatorClient and a SegmentLoaderFactory\ndirectly injected into it.\n\nThis also refactors SegmentLoaderFactory so it doesn't depend on\nan injectable SegmentLoaderConfig.  since its only method always\nreplaces the preconfigured SegmentLoaderConfig anyway.\nThis makes it possible to use SegmentLoaderFactory without setting\ndruid.segmentCaches.locations to some dummy value.\n\nAnother goal of this PR is to make it possible for IngestSegmentFirehoseFactory\nto list data segments outside of connect() --- specifically.  to make it a\nFiniteFirehoseFactory which can query the coordinator in order to calculate its\nsplits. See #7048.\n\nThis also adds missing datasource name URL-encoding to an API used by\nCoordinatorBasedSegmentHandoffNotifier.","date":"2019-02-24 09:02:56","modifiedFileCount":"21","status":"M","submitter":"David Glasser"},{"authorTime":"2019-06-04 03:59:15","codes":[{"authorDate":"2019-06-04 03:59:15","commitOrder":6,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-06-04 03:59:15","endLine":445,"groupId":"14702","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/69/f10f6be40030eba55eb7286babe6c6779c7cf6.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          SEGMENT_INTERVALS,\n          Granularities.MONTH\n      );\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":415,"status":"M"},{"authorDate":"2019-06-04 03:59:15","commitOrder":6,"curCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(SEGMENTS),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-06-04 03:59:15","endLine":760,"groupId":"14702","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithCustomSegments","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/69/f10f6be40030eba55eb7286babe6c6779c7cf6.src","preCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(SEGMENTS),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          SEGMENT_INTERVALS,\n          Granularities.MONTH\n      );\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":730,"status":"M"}],"commitId":"61ec521135fd8ae40110524e8857610d0830e279","commitMessage":"@@@Remove keepSegmentGranularity option for compaction  (#7747)\n\n* Remove keepSegmentGranularity option from compaction\n\n* fix it test\n\n* clean up\n\n* remove from web console\n\n* fix test\n","date":"2019-06-04 03:59:15","modifiedFileCount":"15","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-10-10 02:12:00","codes":[{"authorDate":"2019-10-10 02:12:00","commitOrder":7,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-10-10 02:12:00","endLine":457,"groupId":"14702","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/fd/201d933c67e13e31fa6cb784a9bb71bb1f5f06.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":427,"status":"M"},{"authorDate":"2019-10-10 02:12:00","commitOrder":7,"curCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, SpecificSegmentsSpec.fromSegments(SEGMENTS)),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-10-10 02:12:00","endLine":775,"groupId":"14702","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithCustomSegments","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/fd/201d933c67e13e31fa6cb784a9bb71bb1f5f06.src","preCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(SEGMENTS),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":745,"status":"M"}],"commitId":"96d8523ecbc0108e622084671172fded81065aa0","commitMessage":"@@@Use hash of Segment IDs instead of a list of explicit segments in auto compaction (#8571)\n\n* IOConfig for compaction task\n\n* add javadoc.  doc.  unit test\n\n* fix webconsole test\n\n* add spelling\n\n* address comments\n\n* fix build and test\n\n* address comments\n","date":"2019-10-10 02:12:00","modifiedFileCount":"16","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-10-16 13:57:42","codes":[{"authorDate":"2019-10-16 13:57:42","commitOrder":8,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-10-16 13:57:42","endLine":455,"groupId":"14702","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f8/fe11a53a9f9e2f6e577bc2211f5299510368f7.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":425,"status":"M"},{"authorDate":"2019-10-16 13:57:42","commitOrder":8,"curCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, SpecificSegmentsSpec.fromSegments(SEGMENTS)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-10-16 13:57:42","endLine":773,"groupId":"14702","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithCustomSegments","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f8/fe11a53a9f9e2f6e577bc2211f5299510368f7.src","preCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, SpecificSegmentsSpec.fromSegments(SEGMENTS)),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":743,"status":"M"}],"commitId":"4046c86d62192c812cea87188dd17e745fb83b04","commitMessage":"@@@Stateful auto compaction (#8573)\n\n* Stateful auto compaction\n\n* javaodc\n\n* add removed test back\n\n* fix test\n\n* adding indexSpec to compactionState\n\n* fix build\n\n* add lastCompactionState\n\n* address comments\n\n* extract CompactionState\n\n* fix doc\n\n* fix build and test\n\n* Add a task context to store compaction state; add javadoc\n\n* fix it test\n","date":"2019-10-16 13:57:42","modifiedFileCount":"69","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-10-19 04:24:14","codes":[{"authorDate":"2019-10-19 04:24:14","commitOrder":9,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-10-19 04:24:14","endLine":468,"groupId":"14702","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/32/53fdc4f6721a3c50d3358c8490177d835b99c8.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":438,"status":"M"},{"authorDate":"2019-10-19 04:24:14","commitOrder":9,"curCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, SpecificSegmentsSpec.fromSegments(SEGMENTS)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-10-19 04:24:14","endLine":804,"groupId":"14702","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithCustomSegments","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/32/53fdc4f6721a3c50d3358c8490177d835b99c8.src","preCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, SpecificSegmentsSpec.fromSegments(SEGMENTS)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":774,"status":"M"}],"commitId":"30c15900bec7f4df7a72088ec076223839da258f","commitMessage":"@@@Auto compaction based on parallel indexing (#8570)\n\n* Auto compaction based on parallel indexing\n\n* javadoc and doc\n\n* typo\n\n* update spell\n\n* addressing comments\n\n* address comments\n\n* fix log\n\n* fix build\n\n* fix test\n\n* increase default max input segment bytes per task\n\n* fix test\n","date":"2019-10-19 04:24:14","modifiedFileCount":"28","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-06-26 04:37:31","codes":[{"authorDate":"2020-06-26 04:37:31","commitOrder":10,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2020-06-26 04:37:31","endLine":610,"groupId":"14702","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/db/cfd4ad251ad89e7b104537357321fc716ef554.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":579,"status":"M"},{"authorDate":"2020-06-26 04:37:31","commitOrder":10,"curCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, SpecificSegmentsSpec.fromSegments(SEGMENTS)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2020-06-26 04:37:31","endLine":952,"groupId":"14702","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithCustomSegments","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/db/cfd4ad251ad89e7b104537357321fc716ef554.src","preCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, SpecificSegmentsSpec.fromSegments(SEGMENTS)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":921,"status":"M"}],"commitId":"aaee72c781dea0f06bd5781d085b356d5d4241d0","commitMessage":"@@@Allow append to existing datasources when dynamic partitioning is used (#10033)\n\n* Fill in the core partition set size properly for batch ingestion with\ndynamic partitioning\n\n* incomplete javadoc\n\n* Address comments\n\n* fix tests\n\n* fix json serde.  add tests\n\n* checkstyle\n\n* Set core partition set size for hash-partitioned segments properly in\nbatch ingestion\n\n* test for both parallel and single-threaded task\n\n* unused variables\n\n* fix test\n\n* unused imports\n\n* add hash/range buckets\n\n* some test adjustment and missing json serde\n\n* centralized partition id allocation in parallel and simple tasks\n\n* remove string partition chunk\n\n* revive string partition chunk\n\n* fill numCorePartitions for hadoop\n\n* clean up hash stuffs\n\n* resolved todos\n\n* javadocs\n\n* Fix tests\n\n* add more tests\n\n* doc\n\n* unused imports\n\n* Allow append to existing datasources when dynamic partitioing is used\n\n* fix test\n\n* checkstyle\n\n* checkstyle\n\n* fix test\n\n* fix test\n\n* fix other tests..\n\n* checkstyle\n\n* hansle unknown core partitions size in overlord segment allocation\n\n* fail to append when numCorePartitions is unknown\n\n* log\n\n* fix comment; rename to be more intuitive\n\n* double append test\n\n* cleanup complete(); add tests\n\n* fix build\n\n* add tests\n\n* address comments\n\n* checkstyle","date":"2020-06-26 04:37:31","modifiedFileCount":"53","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-08-27 08:08:12","codes":[{"authorDate":"2020-08-27 08:08:12","commitOrder":11,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2020-08-27 08:08:12","endLine":578,"groupId":"14702","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e6/35381327075830831d9c03e9520b7941ab304c.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":548,"status":"M"},{"authorDate":"2020-08-27 08:08:12","commitOrder":11,"curCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, SpecificSegmentsSpec.fromSegments(SEGMENTS)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2020-08-27 08:08:12","endLine":914,"groupId":"14702","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithCustomSegments","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e6/35381327075830831d9c03e9520b7941ab304c.src","preCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, SpecificSegmentsSpec.fromSegments(SEGMENTS)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":884,"status":"M"}],"commitId":"f82fd22fa7de175200b7127c34c2eb2900bf7317","commitMessage":"@@@Move tools for indexing to TaskToolbox instead of injecting them in constructor (#10308)\n\n* Move tools for indexing to TaskToolbox instead of injecting them in constructor\n\n* oops.  other changes\n\n* fix test\n\n* unnecessary new file\n\n* fix test\n\n* fix build","date":"2020-08-27 08:08:12","modifiedFileCount":"67","status":"M","submitter":"Jihoon Son"},{"authorTime":"2021-03-03 03:23:52","codes":[{"authorDate":"2021-03-03 03:23:52","commitOrder":12,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","date":"2021-03-03 03:23:52","endLine":643,"groupId":"14702","id":21,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b0/deee546550836611f13834cf7230e1115b793c.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":612,"status":"M"},{"authorDate":"2021-03-03 03:23:52","commitOrder":12,"curCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, SpecificSegmentsSpec.fromSegments(SEGMENTS)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","date":"2021-03-03 03:23:52","endLine":994,"groupId":"14702","id":22,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithCustomSegments","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b0/deee546550836611f13834cf7230e1115b793c.src","preCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, SpecificSegmentsSpec.fromSegments(SEGMENTS)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":963,"status":"M"}],"commitId":"b7b0ee83627dd7887392e8f9d6fb5cb29465c28c","commitMessage":"@@@Add query granularity to compaction task (#10900)\n\n* add query granularity to compaction task\n\n* fix checkstyle\n\n* fix checkstyle\n\n* fix test\n\n* fix test\n\n* add tests\n\n* fix test\n\n* fix test\n\n* cleanup\n\n* rename class\n\n* fix test\n\n* fix test\n\n* add test\n\n* fix test","date":"2021-03-03 03:23:52","modifiedFileCount":"15","status":"M","submitter":"Maytas Monsereenusorn"},{"authorTime":"2021-04-09 15:12:28","codes":[{"authorDate":"2021-04-09 15:12:28","commitOrder":13,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n  }\n","date":"2021-04-09 15:12:28","endLine":873,"groupId":"14702","id":23,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c4/faa5b2754ed36e4f3b71a6caa2cd64ceba3565.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":840,"status":"M"},{"authorDate":"2021-04-09 15:12:28","commitOrder":13,"curCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, SpecificSegmentsSpec.fromSegments(SEGMENTS)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n  }\n","date":"2021-04-09 15:12:28","endLine":1239,"groupId":"14702","id":24,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithCustomSegments","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c4/faa5b2754ed36e4f3b71a6caa2cd64ceba3565.src","preCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, SpecificSegmentsSpec.fromSegments(SEGMENTS)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1206,"status":"M"}],"commitId":"4576152e4a0213d17048a330e7089aa9d89f3972","commitMessage":"@@@Make dropExisting flag for Compaction configurable and add warning documentations (#11070)\n\n* Make dropExisting flag for Compaction configurable\n\n* fix checkstyle\n\n* fix checkstyle\n\n* fix test\n\n* add tests\n\n* fix spelling\n\n* fix docs\n\n* add IT\n\n* fix test\n\n* fix doc\n\n* fix doc","date":"2021-04-09 15:12:28","modifiedFileCount":"20","status":"M","submitter":"Maytas Monsereenusorn"},{"authorTime":"2021-07-21 02:44:19","codes":[{"authorDate":"2021-07-21 02:44:19","commitOrder":14,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentCacheManagerFactory,\n        RETRY_POLICY_FACTORY,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n  }\n","date":"2021-07-21 02:44:19","endLine":873,"groupId":"104503","id":25,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ac/9e3d011aef9e7655bf6426661366cead8b80e9.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":840,"status":"M"},{"authorDate":"2021-07-21 02:44:19","commitOrder":14,"curCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, SpecificSegmentsSpec.fromSegments(SEGMENTS)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentCacheManagerFactory,\n        RETRY_POLICY_FACTORY,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n  }\n","date":"2021-07-21 02:44:19","endLine":1239,"groupId":"104503","id":26,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithCustomSegments","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ac/9e3d011aef9e7655bf6426661366cead8b80e9.src","preCode":"  public void testCreateIngestionSchemaWithCustomSegments() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, SpecificSegmentsSpec.fromSegments(SEGMENTS)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1206,"status":"M"}],"commitId":"94c1671eaf7b050972602fdedcb1971cdbde692d","commitMessage":"@@@Split SegmentLoader into SegmentLoader and SegmentCacheManager (#11466)\n\nThis PR splits current SegmentLoader into SegmentLoader and SegmentCacheManager.\n\nSegmentLoader - this class is responsible for building the segment object but does not expose any methods for downloading.  cache space management.  etc. Default implementation delegates the download operations to SegmentCacheManager and only contains the logic for building segments once downloaded. . This class will be used in SegmentManager to construct Segment objects.\n\nSegmentCacheManager - this class manages the segment cache on the local disk. It fetches the segment files to the local disk.  can clean up the cache.  and in the future.  support reserve and release on cache space. [See https://github.com/Make SegmentLoader extensible and customizable #11398]. This class will be used in ingestion tasks such as compaction.  re-indexing where segment files need to be downloaded locally.","date":"2021-07-21 02:44:19","modifiedFileCount":"41","status":"M","submitter":"Abhishek Agarwal"}]
