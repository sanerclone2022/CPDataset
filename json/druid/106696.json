[{"authorTime":"2019-03-29 05:37:09","codes":[{"authorDate":"2019-03-29 05:37:09","commitOrder":1,"curCode":"  public void testBrokerOrderedScan()\n  {\n    ScanQuery query = Druids.newScanQueryBuilder()\n                            .limit(limit)\n                            .order(ScanQuery.Order.DESCENDING)\n                            .dataSource(\"some datasource\")\n                            .batchSize(batchSize)\n                            .intervals(QueryRunnerTestHelper.fullOnIntervalSpec)\n                            .resultFormat(resultFormat)\n                            .build();\n    QueryPlus<ScanResultValue> queryPlus = QueryPlus.wrap(query);\n    ScanQueryLimitRowIterator itr = new ScanQueryLimitRowIterator(\n        ((queryInput, responseContext) -> Sequences.simple(singleEventScanResultValues)),\n        queryPlus,\n        ImmutableMap.of()\n    );\n\n    int count = 0;\n    int expectedNumRows = Math.min(limit, NUM_ELEMENTS);\n    while (itr.hasNext()) {\n      ScanResultValue curr = itr.next();\n      List<Map<String, Object>> events = ScanQueryTestHelper.getEventsListResultFormat(curr);\n      if (events.size() != batchSize) {\n        if (expectedNumRows - count >= batchSize) {\n          Assert.fail(\"Batch size is incorrect\");\n        } else {\n          Assert.assertEquals(expectedNumRows - count, events.size());\n        }\n      }\n      count += events.size();\n    }\n    Assert.assertEquals(expectedNumRows, count);\n  }\n","date":"2019-03-29 05:37:09","endLine":175,"groupId":"18739","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testBrokerOrderedScan","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f0/dafbc4370c2768f8c49c159f7e13fb70bdecc7.src","preCode":"  public void testBrokerOrderedScan()\n  {\n    ScanQuery query = Druids.newScanQueryBuilder()\n                            .limit(limit)\n                            .order(ScanQuery.Order.DESCENDING)\n                            .dataSource(\"some datasource\")\n                            .batchSize(batchSize)\n                            .intervals(QueryRunnerTestHelper.fullOnIntervalSpec)\n                            .resultFormat(resultFormat)\n                            .build();\n    QueryPlus<ScanResultValue> queryPlus = QueryPlus.wrap(query);\n    ScanQueryLimitRowIterator itr = new ScanQueryLimitRowIterator(\n        ((queryInput, responseContext) -> Sequences.simple(singleEventScanResultValues)),\n        queryPlus,\n        ImmutableMap.of()\n    );\n\n    int count = 0;\n    int expectedNumRows = Math.min(limit, NUM_ELEMENTS);\n    while (itr.hasNext()) {\n      ScanResultValue curr = itr.next();\n      List<Map<String, Object>> events = ScanQueryTestHelper.getEventsListResultFormat(curr);\n      if (events.size() != batchSize) {\n        if (expectedNumRows - count >= batchSize) {\n          Assert.fail(\"Batch size is incorrect\");\n        } else {\n          Assert.assertEquals(expectedNumRows - count, events.size());\n        }\n      }\n      count += events.size();\n    }\n    Assert.assertEquals(expectedNumRows, count);\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/scan/ScanQueryLimitRowIteratorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":143,"status":"B"},{"authorDate":"2019-03-29 05:37:09","commitOrder":1,"curCode":"  public void testHistoricalOrderedScan()\n  {\n    ScanQuery query = Druids.newScanQueryBuilder()\n                            .limit(limit)\n                            .order(ScanQuery.Order.DESCENDING)\n                            .dataSource(\"some datasource\")\n                            .batchSize(batchSize)\n                            .intervals(QueryRunnerTestHelper.fullOnIntervalSpec)\n                            .resultFormat(resultFormat)\n                            .context(ImmutableMap.of(ScanQuery.CTX_KEY_OUTERMOST, false))\n                            .build();\n\n    QueryPlus<ScanResultValue> queryPlus = QueryPlus.wrap(query);\n    ScanQueryLimitRowIterator itr = new ScanQueryLimitRowIterator(\n        ((queryInput, responseContext) -> Sequences.simple(singleEventScanResultValues)),\n        queryPlus,\n        ImmutableMap.of()\n    );\n\n    int count = 0;\n    int expectedNumRows = Math.min(limit, NUM_ELEMENTS);\n    while (itr.hasNext()) {\n      ScanResultValue curr = itr.next();\n      List<Map<String, Object>> events = ScanQueryTestHelper.getEventsListResultFormat(curr);\n      Assert.assertEquals(1, events.size());\n      count += events.size();\n    }\n    Assert.assertEquals(expectedNumRows, count);\n  }\n","date":"2019-03-29 05:37:09","endLine":210,"groupId":"4906","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testHistoricalOrderedScan","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f0/dafbc4370c2768f8c49c159f7e13fb70bdecc7.src","preCode":"  public void testHistoricalOrderedScan()\n  {\n    ScanQuery query = Druids.newScanQueryBuilder()\n                            .limit(limit)\n                            .order(ScanQuery.Order.DESCENDING)\n                            .dataSource(\"some datasource\")\n                            .batchSize(batchSize)\n                            .intervals(QueryRunnerTestHelper.fullOnIntervalSpec)\n                            .resultFormat(resultFormat)\n                            .context(ImmutableMap.of(ScanQuery.CTX_KEY_OUTERMOST, false))\n                            .build();\n\n    QueryPlus<ScanResultValue> queryPlus = QueryPlus.wrap(query);\n    ScanQueryLimitRowIterator itr = new ScanQueryLimitRowIterator(\n        ((queryInput, responseContext) -> Sequences.simple(singleEventScanResultValues)),\n        queryPlus,\n        ImmutableMap.of()\n    );\n\n    int count = 0;\n    int expectedNumRows = Math.min(limit, NUM_ELEMENTS);\n    while (itr.hasNext()) {\n      ScanResultValue curr = itr.next();\n      List<Map<String, Object>> events = ScanQueryTestHelper.getEventsListResultFormat(curr);\n      Assert.assertEquals(1, events.size());\n      count += events.size();\n    }\n    Assert.assertEquals(expectedNumRows, count);\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/scan/ScanQueryLimitRowIteratorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":182,"status":"B"}],"commitId":"ad7862c58ac7f05cb28108543c017e2b186d1031","commitMessage":"@@@Time Ordering On Scans (#7133)\n\n* Moved Scan Builder to Druids class and started on Scan Benchmark setup\n\n* Need to form queries\n\n* It runs.\n\n* Stuff for time-ordered scan query\n\n* Move ScanResultValue timestamp comparator to a separate class for testing\n\n* Licensing stuff\n\n* Change benchmark\n\n* Remove todos\n\n* Added TimestampComparator tests\n\n* Change number of benchmark iterations\n\n* Added time ordering to the scan benchmark\n\n* Changed benchmark params\n\n* More param changes\n\n* Benchmark param change\n\n* Made Jon's changes and removed TODOs\n\n* Broke some long lines into two lines\n\n* nit\n\n* Decrease segment size for less memory usage\n\n* Wrote tests for heapsort scan result values and fixed bug where iterator\nwasn't returning elements in correct order\n\n* Wrote more tests for scan result value sort\n\n* Committing a param change to kick teamcity\n\n* Fixed codestyle and forbidden API errors\n\n* .\n\n* Improved conciseness\n\n* nit\n\n* Created an error message for when someone tries to time order a result\nset > threshold limit\n\n* Set to spaces over tabs\n\n* Fixing tests WIP\n\n* Fixed failing calcite tests\n\n* Kicking travis with change to benchmark param\n\n* added all query types to scan benchmark\n\n* Fixed benchmark queries\n\n* Renamed sort function\n\n* Added javadoc on ScanResultValueTimestampComparator\n\n* Unused import\n\n* Added more javadoc\n\n* improved doc\n\n* Removed unused import to satisfy PMD check\n\n* Small changes\n\n* Changes based on Gian's comments\n\n* Fixed failing test due to null resultFormat\n\n* Added config and get # of segments\n\n* Set up time ordering strategy decision tree\n\n* Refactor and pQueue works\n\n* Cleanup\n\n* Ordering is correct on n-way merge -> still need to batch events into\nScanResultValues\n\n* WIP\n\n* Sequence stuff is so dirty :(\n\n* Fixed bug introduced by replacing deque with list\n\n* Wrote docs\n\n* Multi-historical setup works\n\n* WIP\n\n* Change so batching only occurs on broker for time-ordered scans\n\nRestricted batching to broker for time-ordered queries and adjusted\ntests\n\nFormatting\n\nCleanup\n\n* Fixed mistakes in merge\n\n* Fixed failing tests\n\n* Reset config\n\n* Wrote tests and added Javadoc\n\n* Nit-change on javadoc\n\n* Checkstyle fix\n\n* Improved test and appeased TeamCity\n\n* Sorry.  checkstyle\n\n* Applied Jon's recommended changes\n\n* Checkstyle fix\n\n* Optimization\n\n* Fixed tests\n\n* Updated error message\n\n* Added error message for UOE\n\n* Renaming\n\n* Finish rename\n\n* Smarter limiting for pQueue method\n\n* Optimized n-way merge strategy\n\n* Rename segment limit -> segment partitions limit\n\n* Added a bit of docs\n\n* More comments\n\n* Fix checkstyle and test\n\n* Nit comment\n\n* Fixed failing tests -> allow usage of all types of segment spec\n\n* Fixed failing tests -> allow usage of all types of segment spec\n\n* Revert \"Fixed failing tests -> allow usage of all types of segment spec\"\n\nThis reverts commit ec470288c7b725f5310bcf69d1db9f85ff509c8d.\n\n* Revert \"Merge branch '6088-Time-Ordering-On-Scans-N-Way-Merge' of github.com:justinborromeo/incubator-druid into 6088-Time-Ordering-On-Scans-N-Way-Merge\"\n\nThis reverts commit 57033f36df6e3ef887e5f0399ad74bb091306de8.  reversing\nchanges made to 8f01d8dd16f40d10c60519ca0ec0d2e6b2dde941.\n\n* Check type of segment spec before using for time ordering\n\n* Fix bug in numRowsScanned\n\n* Fix bug messing up count of rows\n\n* Fix docs and flipped boolean in ScanQueryLimitRowIterator\n\n* Refactor n-way merge\n\n* Added test for n-way merge\n\n* Refixed regression\n\n* Checkstyle and doc update\n\n* Modified sequence limit to accept longs and added test for long limits\n\n* doc fix\n\n* Implemented Clint's recommendations\n","date":"2019-03-29 05:37:09","modifiedFileCount":"23","status":"B","submitter":"Justin Borromeo"},{"authorTime":"2019-07-24 23:29:03","codes":[{"authorDate":"2019-07-24 23:29:03","commitOrder":2,"curCode":"  public void testBrokerOrderedScan()\n  {\n    ScanQuery query = Druids.newScanQueryBuilder()\n                            .limit(limit)\n                            .order(ScanQuery.Order.DESCENDING)\n                            .dataSource(\"some datasource\")\n                            .batchSize(batchSize)\n                            .intervals(QueryRunnerTestHelper.fullOnIntervalSpec)\n                            .resultFormat(resultFormat)\n                            .build();\n    QueryPlus<ScanResultValue> queryPlus = QueryPlus.wrap(query);\n    ScanQueryLimitRowIterator itr = new ScanQueryLimitRowIterator(\n        ((queryInput, responseContext) -> Sequences.simple(singleEventScanResultValues)),\n        queryPlus,\n        ResponseContext.createEmpty()\n    );\n\n    int count = 0;\n    int expectedNumRows = Math.min(limit, NUM_ELEMENTS);\n    while (itr.hasNext()) {\n      ScanResultValue curr = itr.next();\n      List<Map<String, Object>> events = ScanQueryTestHelper.getEventsListResultFormat(curr);\n      if (events.size() != batchSize) {\n        if (expectedNumRows - count >= batchSize) {\n          Assert.fail(\"Batch size is incorrect\");\n        } else {\n          Assert.assertEquals(expectedNumRows - count, events.size());\n        }\n      }\n      count += events.size();\n    }\n    Assert.assertEquals(expectedNumRows, count);\n  }\n","date":"2019-07-24 23:29:03","endLine":176,"groupId":"18739","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testBrokerOrderedScan","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c0/3ddde9494a2ab51b3725ce547624975d7b675d.src","preCode":"  public void testBrokerOrderedScan()\n  {\n    ScanQuery query = Druids.newScanQueryBuilder()\n                            .limit(limit)\n                            .order(ScanQuery.Order.DESCENDING)\n                            .dataSource(\"some datasource\")\n                            .batchSize(batchSize)\n                            .intervals(QueryRunnerTestHelper.fullOnIntervalSpec)\n                            .resultFormat(resultFormat)\n                            .build();\n    QueryPlus<ScanResultValue> queryPlus = QueryPlus.wrap(query);\n    ScanQueryLimitRowIterator itr = new ScanQueryLimitRowIterator(\n        ((queryInput, responseContext) -> Sequences.simple(singleEventScanResultValues)),\n        queryPlus,\n        ImmutableMap.of()\n    );\n\n    int count = 0;\n    int expectedNumRows = Math.min(limit, NUM_ELEMENTS);\n    while (itr.hasNext()) {\n      ScanResultValue curr = itr.next();\n      List<Map<String, Object>> events = ScanQueryTestHelper.getEventsListResultFormat(curr);\n      if (events.size() != batchSize) {\n        if (expectedNumRows - count >= batchSize) {\n          Assert.fail(\"Batch size is incorrect\");\n        } else {\n          Assert.assertEquals(expectedNumRows - count, events.size());\n        }\n      }\n      count += events.size();\n    }\n    Assert.assertEquals(expectedNumRows, count);\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/scan/ScanQueryLimitRowIteratorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":144,"status":"M"},{"authorDate":"2019-07-24 23:29:03","commitOrder":2,"curCode":"  public void testHistoricalOrderedScan()\n  {\n    ScanQuery query = Druids.newScanQueryBuilder()\n                            .limit(limit)\n                            .order(ScanQuery.Order.DESCENDING)\n                            .dataSource(\"some datasource\")\n                            .batchSize(batchSize)\n                            .intervals(QueryRunnerTestHelper.fullOnIntervalSpec)\n                            .resultFormat(resultFormat)\n                            .context(ImmutableMap.of(ScanQuery.CTX_KEY_OUTERMOST, false))\n                            .build();\n\n    QueryPlus<ScanResultValue> queryPlus = QueryPlus.wrap(query);\n    ScanQueryLimitRowIterator itr = new ScanQueryLimitRowIterator(\n        ((queryInput, responseContext) -> Sequences.simple(singleEventScanResultValues)),\n        queryPlus,\n        ResponseContext.createEmpty()\n    );\n\n    int count = 0;\n    int expectedNumRows = Math.min(limit, NUM_ELEMENTS);\n    while (itr.hasNext()) {\n      ScanResultValue curr = itr.next();\n      List<Map<String, Object>> events = ScanQueryTestHelper.getEventsListResultFormat(curr);\n      Assert.assertEquals(1, events.size());\n      count += events.size();\n    }\n    Assert.assertEquals(expectedNumRows, count);\n  }\n","date":"2019-07-24 23:29:03","endLine":211,"groupId":"4906","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testHistoricalOrderedScan","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c0/3ddde9494a2ab51b3725ce547624975d7b675d.src","preCode":"  public void testHistoricalOrderedScan()\n  {\n    ScanQuery query = Druids.newScanQueryBuilder()\n                            .limit(limit)\n                            .order(ScanQuery.Order.DESCENDING)\n                            .dataSource(\"some datasource\")\n                            .batchSize(batchSize)\n                            .intervals(QueryRunnerTestHelper.fullOnIntervalSpec)\n                            .resultFormat(resultFormat)\n                            .context(ImmutableMap.of(ScanQuery.CTX_KEY_OUTERMOST, false))\n                            .build();\n\n    QueryPlus<ScanResultValue> queryPlus = QueryPlus.wrap(query);\n    ScanQueryLimitRowIterator itr = new ScanQueryLimitRowIterator(\n        ((queryInput, responseContext) -> Sequences.simple(singleEventScanResultValues)),\n        queryPlus,\n        ImmutableMap.of()\n    );\n\n    int count = 0;\n    int expectedNumRows = Math.min(limit, NUM_ELEMENTS);\n    while (itr.hasNext()) {\n      ScanResultValue curr = itr.next();\n      List<Map<String, Object>> events = ScanQueryTestHelper.getEventsListResultFormat(curr);\n      Assert.assertEquals(1, events.size());\n      count += events.size();\n    }\n    Assert.assertEquals(expectedNumRows, count);\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/scan/ScanQueryLimitRowIteratorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":183,"status":"M"}],"commitId":"799d20249fe6333ea86b020f6d09c91fa4d3f998","commitMessage":"@@@Response context refactoring (#8110)\n\n* Response context refactoring\n\n* Serialization/Deserialization of ResponseContext\n\n* Added java doc comments\n\n* Renamed vars related to ResponseContext\n\n* Renamed empty() methods to createEmpty()\n\n* Fixed ResponseContext usage\n\n* Renamed multiple ResponseContext static fields\n\n* Added PublicApi annotations\n\n* Renamed QueryResponseContext class to ResourceIOReaderWriter\n\n* Moved the protected method below public static constants\n\n* Added createEmpty method to ResponseContext with DefaultResponseContext creation\n\n* Fixed inspection error\n\n* Added comments to the ResponseContext length limit and ResponseContext\nhttp header name\n\n* Added a comment of possible future refactoring\n\n* Removed .gitignore file of indexing-service\n\n* Removed a never-used method\n\n* VisibleForTesting method reducing boilerplate\n\nCo-Authored-By: Clint Wylie <cjwylie@gmail.com>\n\n* Reduced boilerplate\n\n* Renamed the method serialize to serializeWith\n\n* Removed unused import\n\n* Fixed incorrectly refactored test method\n\n* Added comments for ResponseContext keys\n\n* Fixed incorrectly refactored test method\n\n* Fixed IntervalChunkingQueryRunnerTest mocks\n","date":"2019-07-24 23:29:03","modifiedFileCount":"142","status":"M","submitter":"Eugene Sevastianov"},{"authorTime":"2019-08-23 18:13:54","codes":[{"authorDate":"2019-08-23 18:13:54","commitOrder":3,"curCode":"  public void testBrokerOrderedScan()\n  {\n    ScanQuery query = Druids.newScanQueryBuilder()\n                            .limit(limit)\n                            .order(ScanQuery.Order.DESCENDING)\n                            .dataSource(\"some datasource\")\n                            .batchSize(batchSize)\n                            .intervals(QueryRunnerTestHelper.FULL_ON_INTERVAL_SPEC)\n                            .resultFormat(RESULT_FORMAT)\n                            .build();\n    QueryPlus<ScanResultValue> queryPlus = QueryPlus.wrap(query);\n    ScanQueryLimitRowIterator itr = new ScanQueryLimitRowIterator(\n        ((queryInput, responseContext) -> Sequences.simple(singleEventScanResultValues)),\n        queryPlus,\n        ResponseContext.createEmpty()\n    );\n\n    int count = 0;\n    int expectedNumRows = Math.min(limit, NUM_ELEMENTS);\n    while (itr.hasNext()) {\n      ScanResultValue curr = itr.next();\n      List<Map<String, Object>> events = ScanQueryTestHelper.getEventsListResultFormat(curr);\n      if (events.size() != batchSize) {\n        if (expectedNumRows - count >= batchSize) {\n          Assert.fail(\"Batch size is incorrect\");\n        } else {\n          Assert.assertEquals(expectedNumRows - count, events.size());\n        }\n      }\n      count += events.size();\n    }\n    Assert.assertEquals(expectedNumRows, count);\n  }\n","date":"2019-08-23 18:13:54","endLine":176,"groupId":"106696","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testBrokerOrderedScan","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/bc/653c71895680467e6d0fc3d01f478ab8039918.src","preCode":"  public void testBrokerOrderedScan()\n  {\n    ScanQuery query = Druids.newScanQueryBuilder()\n                            .limit(limit)\n                            .order(ScanQuery.Order.DESCENDING)\n                            .dataSource(\"some datasource\")\n                            .batchSize(batchSize)\n                            .intervals(QueryRunnerTestHelper.fullOnIntervalSpec)\n                            .resultFormat(resultFormat)\n                            .build();\n    QueryPlus<ScanResultValue> queryPlus = QueryPlus.wrap(query);\n    ScanQueryLimitRowIterator itr = new ScanQueryLimitRowIterator(\n        ((queryInput, responseContext) -> Sequences.simple(singleEventScanResultValues)),\n        queryPlus,\n        ResponseContext.createEmpty()\n    );\n\n    int count = 0;\n    int expectedNumRows = Math.min(limit, NUM_ELEMENTS);\n    while (itr.hasNext()) {\n      ScanResultValue curr = itr.next();\n      List<Map<String, Object>> events = ScanQueryTestHelper.getEventsListResultFormat(curr);\n      if (events.size() != batchSize) {\n        if (expectedNumRows - count >= batchSize) {\n          Assert.fail(\"Batch size is incorrect\");\n        } else {\n          Assert.assertEquals(expectedNumRows - count, events.size());\n        }\n      }\n      count += events.size();\n    }\n    Assert.assertEquals(expectedNumRows, count);\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/scan/ScanQueryLimitRowIteratorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":144,"status":"M"},{"authorDate":"2019-08-23 18:13:54","commitOrder":3,"curCode":"  public void testHistoricalOrderedScan()\n  {\n    ScanQuery query = Druids.newScanQueryBuilder()\n                            .limit(limit)\n                            .order(ScanQuery.Order.DESCENDING)\n                            .dataSource(\"some datasource\")\n                            .batchSize(batchSize)\n                            .intervals(QueryRunnerTestHelper.FULL_ON_INTERVAL_SPEC)\n                            .resultFormat(RESULT_FORMAT)\n                            .context(ImmutableMap.of(ScanQuery.CTX_KEY_OUTERMOST, false))\n                            .build();\n\n    QueryPlus<ScanResultValue> queryPlus = QueryPlus.wrap(query);\n    ScanQueryLimitRowIterator itr = new ScanQueryLimitRowIterator(\n        ((queryInput, responseContext) -> Sequences.simple(singleEventScanResultValues)),\n        queryPlus,\n        ResponseContext.createEmpty()\n    );\n\n    int count = 0;\n    int expectedNumRows = Math.min(limit, NUM_ELEMENTS);\n    while (itr.hasNext()) {\n      ScanResultValue curr = itr.next();\n      List<Map<String, Object>> events = ScanQueryTestHelper.getEventsListResultFormat(curr);\n      Assert.assertEquals(1, events.size());\n      count += events.size();\n    }\n    Assert.assertEquals(expectedNumRows, count);\n  }\n","date":"2019-08-23 18:13:54","endLine":211,"groupId":"106696","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testHistoricalOrderedScan","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/bc/653c71895680467e6d0fc3d01f478ab8039918.src","preCode":"  public void testHistoricalOrderedScan()\n  {\n    ScanQuery query = Druids.newScanQueryBuilder()\n                            .limit(limit)\n                            .order(ScanQuery.Order.DESCENDING)\n                            .dataSource(\"some datasource\")\n                            .batchSize(batchSize)\n                            .intervals(QueryRunnerTestHelper.fullOnIntervalSpec)\n                            .resultFormat(resultFormat)\n                            .context(ImmutableMap.of(ScanQuery.CTX_KEY_OUTERMOST, false))\n                            .build();\n\n    QueryPlus<ScanResultValue> queryPlus = QueryPlus.wrap(query);\n    ScanQueryLimitRowIterator itr = new ScanQueryLimitRowIterator(\n        ((queryInput, responseContext) -> Sequences.simple(singleEventScanResultValues)),\n        queryPlus,\n        ResponseContext.createEmpty()\n    );\n\n    int count = 0;\n    int expectedNumRows = Math.min(limit, NUM_ELEMENTS);\n    while (itr.hasNext()) {\n      ScanResultValue curr = itr.next();\n      List<Map<String, Object>> events = ScanQueryTestHelper.getEventsListResultFormat(curr);\n      Assert.assertEquals(1, events.size());\n      count += events.size();\n    }\n    Assert.assertEquals(expectedNumRows, count);\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/scan/ScanQueryLimitRowIteratorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":183,"status":"M"}],"commitId":"33f0753a70361e7d345a488034f76a889f7c3682","commitMessage":"@@@Add Checkstyle for constant name static final (#8060)\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* merging with upstream\n\n* review-1\n\n* unknow changes\n\n* unknow changes\n\n* review-2\n\n* merging with master\n\n* review-2 1 changes\n\n* review changes-2 2\n\n* bug fix\n","date":"2019-08-23 18:13:54","modifiedFileCount":"298","status":"M","submitter":"SandishKumarHN"}]
