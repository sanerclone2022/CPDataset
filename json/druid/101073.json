[{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  private void mergeAndPush()\n  {\n    final Granularity segmentGranularity = schema.getGranularitySpec().getSegmentGranularity();\n    final Period windowPeriod = config.getWindowPeriod();\n\n    final long windowMillis = windowPeriod.toStandardDuration().getMillis();\n    log.info(\"Starting merge and push.\");\n    DateTime minTimestampAsDate = segmentGranularity.bucketStart(\n        DateTimes.utc(Math.max(windowMillis, rejectionPolicy.getCurrMaxTime().getMillis()) - windowMillis)\n    );\n    long minTimestamp = minTimestampAsDate.getMillis();\n\n    final List<SegmentIdentifier> appenderatorSegments = appenderator.getSegments();\n    final List<SegmentIdentifier> segmentsToPush = Lists.newArrayList();\n\n    if (shuttingDown) {\n      log.info(\"Found [%,d] segments. Attempting to hand off all of them.\", appenderatorSegments.size());\n      segmentsToPush.addAll(appenderatorSegments);\n    } else {\n      log.info(\n          \"Found [%,d] segments. Attempting to hand off segments that start before [%s].\",\n          appenderatorSegments.size(),\n          minTimestampAsDate\n      );\n\n      for (SegmentIdentifier segment : appenderatorSegments) {\n        final Long intervalStart = segment.getInterval().getStartMillis();\n        if (intervalStart < minTimestamp) {\n          log.info(\"Adding entry [%s] for merge and push.\", segment);\n          segmentsToPush.add(segment);\n        } else {\n          log.info(\n              \"Skipping persist and merge for entry [%s] : Start time [%s] >= [%s] min timestamp required in this run. Segment will be picked up in a future run.\",\n              segment,\n              DateTimes.utc(intervalStart),\n              minTimestampAsDate\n          );\n        }\n      }\n    }\n\n    log.info(\"Found [%,d] segments to persist and merge\", segmentsToPush.size());\n\n    final Function<Throwable, Void> errorHandler = new Function<Throwable, Void>()\n    {\n      @Override\n      public Void apply(Throwable throwable)\n      {\n        final List<String> segmentIdentifierStrings = Lists.transform(\n            segmentsToPush,\n            new Function<SegmentIdentifier, String>()\n            {\n              @Override\n              public String apply(SegmentIdentifier input)\n              {\n                return input.getIdentifierAsString();\n              }\n            }\n        );\n\n        log.makeAlert(throwable, \"Failed to publish merged indexes[%s]\", schema.getDataSource())\n           .addData(\"segments\", segmentIdentifierStrings)\n           .emit();\n\n        if (shuttingDown) {\n          \r\n          \r\n          cleanShutdown = false;\n          for (SegmentIdentifier identifier : segmentsToPush) {\n            dropSegment(identifier);\n          }\n        }\n\n        return null;\n      }\n    };\n\n    \r\n    Futures.addCallback(\n        appenderator.push(segmentsToPush, Committers.nil(), false),\n        new FutureCallback<SegmentsAndMetadata>()\n        {\n          @Override\n          public void onSuccess(SegmentsAndMetadata result)\n          {\n            \r\n            for (DataSegment pushedSegment : result.getSegments()) {\n              try {\n                segmentPublisher.publishSegment(pushedSegment);\n              }\n              catch (Exception e) {\n                errorHandler.apply(e);\n              }\n            }\n\n            log.info(\"Published [%,d] sinks.\", segmentsToPush.size());\n          }\n\n          @Override\n          public void onFailure(Throwable e)\n          {\n            log.warn(e, \"Failed to push [%,d] segments.\", segmentsToPush.size());\n            errorHandler.apply(e);\n          }\n        }\n    );\n  }\n","date":"2018-08-31 00:56:26","endLine":492,"groupId":"6183","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"mergeAndPush","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d0/e72e3d6d1f985379cdbd64cbed93e6321016a4.src","preCode":"  private void mergeAndPush()\n  {\n    final Granularity segmentGranularity = schema.getGranularitySpec().getSegmentGranularity();\n    final Period windowPeriod = config.getWindowPeriod();\n\n    final long windowMillis = windowPeriod.toStandardDuration().getMillis();\n    log.info(\"Starting merge and push.\");\n    DateTime minTimestampAsDate = segmentGranularity.bucketStart(\n        DateTimes.utc(Math.max(windowMillis, rejectionPolicy.getCurrMaxTime().getMillis()) - windowMillis)\n    );\n    long minTimestamp = minTimestampAsDate.getMillis();\n\n    final List<SegmentIdentifier> appenderatorSegments = appenderator.getSegments();\n    final List<SegmentIdentifier> segmentsToPush = Lists.newArrayList();\n\n    if (shuttingDown) {\n      log.info(\"Found [%,d] segments. Attempting to hand off all of them.\", appenderatorSegments.size());\n      segmentsToPush.addAll(appenderatorSegments);\n    } else {\n      log.info(\n          \"Found [%,d] segments. Attempting to hand off segments that start before [%s].\",\n          appenderatorSegments.size(),\n          minTimestampAsDate\n      );\n\n      for (SegmentIdentifier segment : appenderatorSegments) {\n        final Long intervalStart = segment.getInterval().getStartMillis();\n        if (intervalStart < minTimestamp) {\n          log.info(\"Adding entry [%s] for merge and push.\", segment);\n          segmentsToPush.add(segment);\n        } else {\n          log.info(\n              \"Skipping persist and merge for entry [%s] : Start time [%s] >= [%s] min timestamp required in this run. Segment will be picked up in a future run.\",\n              segment,\n              DateTimes.utc(intervalStart),\n              minTimestampAsDate\n          );\n        }\n      }\n    }\n\n    log.info(\"Found [%,d] segments to persist and merge\", segmentsToPush.size());\n\n    final Function<Throwable, Void> errorHandler = new Function<Throwable, Void>()\n    {\n      @Override\n      public Void apply(Throwable throwable)\n      {\n        final List<String> segmentIdentifierStrings = Lists.transform(\n            segmentsToPush,\n            new Function<SegmentIdentifier, String>()\n            {\n              @Override\n              public String apply(SegmentIdentifier input)\n              {\n                return input.getIdentifierAsString();\n              }\n            }\n        );\n\n        log.makeAlert(throwable, \"Failed to publish merged indexes[%s]\", schema.getDataSource())\n           .addData(\"segments\", segmentIdentifierStrings)\n           .emit();\n\n        if (shuttingDown) {\n          \r\n          \r\n          cleanShutdown = false;\n          for (SegmentIdentifier identifier : segmentsToPush) {\n            dropSegment(identifier);\n          }\n        }\n\n        return null;\n      }\n    };\n\n    \r\n    Futures.addCallback(\n        appenderator.push(segmentsToPush, Committers.nil(), false),\n        new FutureCallback<SegmentsAndMetadata>()\n        {\n          @Override\n          public void onSuccess(SegmentsAndMetadata result)\n          {\n            \r\n            for (DataSegment pushedSegment : result.getSegments()) {\n              try {\n                segmentPublisher.publishSegment(pushedSegment);\n              }\n              catch (Exception e) {\n                errorHandler.apply(e);\n              }\n            }\n\n            log.info(\"Published [%,d] sinks.\", segmentsToPush.size());\n          }\n\n          @Override\n          public void onFailure(Throwable e)\n          {\n            log.warn(e, \"Failed to push [%,d] segments.\", segmentsToPush.size());\n            errorHandler.apply(e);\n          }\n        }\n    );\n  }\n","realPath":"server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorPlumber.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":386,"status":"B"},{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  private DateTime getAllowedMinTime()\n  {\n    final Granularity segmentGranularity = schema.getGranularitySpec().getSegmentGranularity();\n    final Period windowPeriod = config.getWindowPeriod();\n\n    final long windowMillis = windowPeriod.toStandardDuration().getMillis();\n    return segmentGranularity.bucketStart(\n        DateTimes.utc(Math.max(windowMillis, rejectionPolicy.getCurrMaxTime().getMillis()) - windowMillis)\n    );\n  }\n","date":"2018-08-31 00:56:26","endLine":842,"groupId":"14037","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"getAllowedMinTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5d/9e6222e20ceed8b9cb93e231b4eb8c6424782a.src","preCode":"  private DateTime getAllowedMinTime()\n  {\n    final Granularity segmentGranularity = schema.getGranularitySpec().getSegmentGranularity();\n    final Period windowPeriod = config.getWindowPeriod();\n\n    final long windowMillis = windowPeriod.toStandardDuration().getMillis();\n    return segmentGranularity.bucketStart(\n        DateTimes.utc(Math.max(windowMillis, rejectionPolicy.getCurrMaxTime().getMillis()) - windowMillis)\n    );\n  }\n","realPath":"server/src/main/java/org/apache/druid/segment/realtime/plumber/RealtimePlumber.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":833,"status":"B"}],"commitId":"431d3d8497f9079857c3baa7ae7ab4cb44a22355","commitMessage":"@@@Rename io.druid to org.apache.druid. (#6266)\n\n* Rename io.druid to org.apache.druid.\n\n* Fix META-INF files and remove some benchmark results.\n\n* MonitorsConfig update for metrics package migration.\n\n* Reorder some dimensions in inner queries for some reason.\n\n* Fix protobuf tests.\n","date":"2018-08-31 00:56:26","modifiedFileCount":"5","status":"B","submitter":"Gian Merlino"},{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-10-29 20:02:43","commitOrder":2,"curCode":"  private void mergeAndPush()\n  {\n    final Granularity segmentGranularity = schema.getGranularitySpec().getSegmentGranularity();\n    final Period windowPeriod = config.getWindowPeriod();\n\n    final long windowMillis = windowPeriod.toStandardDuration().getMillis();\n    log.info(\"Starting merge and push.\");\n    DateTime minTimestampAsDate = segmentGranularity.bucketStart(\n        DateTimes.utc(Math.max(windowMillis, rejectionPolicy.getCurrMaxTime().getMillis()) - windowMillis)\n    );\n    long minTimestamp = minTimestampAsDate.getMillis();\n\n    final List<SegmentIdentifier> appenderatorSegments = appenderator.getSegments();\n    final List<SegmentIdentifier> segmentsToPush = new ArrayList<>();\n\n    if (shuttingDown) {\n      log.info(\"Found [%,d] segments. Attempting to hand off all of them.\", appenderatorSegments.size());\n      segmentsToPush.addAll(appenderatorSegments);\n    } else {\n      log.info(\n          \"Found [%,d] segments. Attempting to hand off segments that start before [%s].\",\n          appenderatorSegments.size(),\n          minTimestampAsDate\n      );\n\n      for (SegmentIdentifier segment : appenderatorSegments) {\n        final Long intervalStart = segment.getInterval().getStartMillis();\n        if (intervalStart < minTimestamp) {\n          log.info(\"Adding entry [%s] for merge and push.\", segment);\n          segmentsToPush.add(segment);\n        } else {\n          log.info(\n              \"Skipping persist and merge for entry [%s] : Start time [%s] >= [%s] min timestamp required in this run. Segment will be picked up in a future run.\",\n              segment,\n              DateTimes.utc(intervalStart),\n              minTimestampAsDate\n          );\n        }\n      }\n    }\n\n    log.info(\"Found [%,d] segments to persist and merge\", segmentsToPush.size());\n\n    final Function<Throwable, Void> errorHandler = new Function<Throwable, Void>()\n    {\n      @Override\n      public Void apply(Throwable throwable)\n      {\n        final List<String> segmentIdentifierStrings = Lists.transform(\n            segmentsToPush,\n            new Function<SegmentIdentifier, String>()\n            {\n              @Override\n              public String apply(SegmentIdentifier input)\n              {\n                return input.getIdentifierAsString();\n              }\n            }\n        );\n\n        log.makeAlert(throwable, \"Failed to publish merged indexes[%s]\", schema.getDataSource())\n           .addData(\"segments\", segmentIdentifierStrings)\n           .emit();\n\n        if (shuttingDown) {\n          \r\n          \r\n          cleanShutdown = false;\n          for (SegmentIdentifier identifier : segmentsToPush) {\n            dropSegment(identifier);\n          }\n        }\n\n        return null;\n      }\n    };\n\n    \r\n    Futures.addCallback(\n        appenderator.push(segmentsToPush, Committers.nil(), false),\n        new FutureCallback<SegmentsAndMetadata>()\n        {\n          @Override\n          public void onSuccess(SegmentsAndMetadata result)\n          {\n            \r\n            for (DataSegment pushedSegment : result.getSegments()) {\n              try {\n                segmentPublisher.publishSegment(pushedSegment);\n              }\n              catch (Exception e) {\n                errorHandler.apply(e);\n              }\n            }\n\n            log.info(\"Published [%,d] sinks.\", segmentsToPush.size());\n          }\n\n          @Override\n          public void onFailure(Throwable e)\n          {\n            log.warn(e, \"Failed to push [%,d] segments.\", segmentsToPush.size());\n            errorHandler.apply(e);\n          }\n        }\n    );\n  }\n","date":"2018-10-29 20:02:43","endLine":493,"groupId":"6183","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"mergeAndPush","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/97/55e6523f9f7b666ff561c0b4582d9aa8f4f6f1.src","preCode":"  private void mergeAndPush()\n  {\n    final Granularity segmentGranularity = schema.getGranularitySpec().getSegmentGranularity();\n    final Period windowPeriod = config.getWindowPeriod();\n\n    final long windowMillis = windowPeriod.toStandardDuration().getMillis();\n    log.info(\"Starting merge and push.\");\n    DateTime minTimestampAsDate = segmentGranularity.bucketStart(\n        DateTimes.utc(Math.max(windowMillis, rejectionPolicy.getCurrMaxTime().getMillis()) - windowMillis)\n    );\n    long minTimestamp = minTimestampAsDate.getMillis();\n\n    final List<SegmentIdentifier> appenderatorSegments = appenderator.getSegments();\n    final List<SegmentIdentifier> segmentsToPush = Lists.newArrayList();\n\n    if (shuttingDown) {\n      log.info(\"Found [%,d] segments. Attempting to hand off all of them.\", appenderatorSegments.size());\n      segmentsToPush.addAll(appenderatorSegments);\n    } else {\n      log.info(\n          \"Found [%,d] segments. Attempting to hand off segments that start before [%s].\",\n          appenderatorSegments.size(),\n          minTimestampAsDate\n      );\n\n      for (SegmentIdentifier segment : appenderatorSegments) {\n        final Long intervalStart = segment.getInterval().getStartMillis();\n        if (intervalStart < minTimestamp) {\n          log.info(\"Adding entry [%s] for merge and push.\", segment);\n          segmentsToPush.add(segment);\n        } else {\n          log.info(\n              \"Skipping persist and merge for entry [%s] : Start time [%s] >= [%s] min timestamp required in this run. Segment will be picked up in a future run.\",\n              segment,\n              DateTimes.utc(intervalStart),\n              minTimestampAsDate\n          );\n        }\n      }\n    }\n\n    log.info(\"Found [%,d] segments to persist and merge\", segmentsToPush.size());\n\n    final Function<Throwable, Void> errorHandler = new Function<Throwable, Void>()\n    {\n      @Override\n      public Void apply(Throwable throwable)\n      {\n        final List<String> segmentIdentifierStrings = Lists.transform(\n            segmentsToPush,\n            new Function<SegmentIdentifier, String>()\n            {\n              @Override\n              public String apply(SegmentIdentifier input)\n              {\n                return input.getIdentifierAsString();\n              }\n            }\n        );\n\n        log.makeAlert(throwable, \"Failed to publish merged indexes[%s]\", schema.getDataSource())\n           .addData(\"segments\", segmentIdentifierStrings)\n           .emit();\n\n        if (shuttingDown) {\n          \r\n          \r\n          cleanShutdown = false;\n          for (SegmentIdentifier identifier : segmentsToPush) {\n            dropSegment(identifier);\n          }\n        }\n\n        return null;\n      }\n    };\n\n    \r\n    Futures.addCallback(\n        appenderator.push(segmentsToPush, Committers.nil(), false),\n        new FutureCallback<SegmentsAndMetadata>()\n        {\n          @Override\n          public void onSuccess(SegmentsAndMetadata result)\n          {\n            \r\n            for (DataSegment pushedSegment : result.getSegments()) {\n              try {\n                segmentPublisher.publishSegment(pushedSegment);\n              }\n              catch (Exception e) {\n                errorHandler.apply(e);\n              }\n            }\n\n            log.info(\"Published [%,d] sinks.\", segmentsToPush.size());\n          }\n\n          @Override\n          public void onFailure(Throwable e)\n          {\n            log.warn(e, \"Failed to push [%,d] segments.\", segmentsToPush.size());\n            errorHandler.apply(e);\n          }\n        }\n    );\n  }\n","realPath":"server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorPlumber.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":387,"status":"M"},{"authorDate":"2018-08-31 00:56:26","commitOrder":2,"curCode":"  private DateTime getAllowedMinTime()\n  {\n    final Granularity segmentGranularity = schema.getGranularitySpec().getSegmentGranularity();\n    final Period windowPeriod = config.getWindowPeriod();\n\n    final long windowMillis = windowPeriod.toStandardDuration().getMillis();\n    return segmentGranularity.bucketStart(\n        DateTimes.utc(Math.max(windowMillis, rejectionPolicy.getCurrMaxTime().getMillis()) - windowMillis)\n    );\n  }\n","date":"2018-08-31 00:56:26","endLine":842,"groupId":"14037","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"getAllowedMinTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5d/9e6222e20ceed8b9cb93e231b4eb8c6424782a.src","preCode":"  private DateTime getAllowedMinTime()\n  {\n    final Granularity segmentGranularity = schema.getGranularitySpec().getSegmentGranularity();\n    final Period windowPeriod = config.getWindowPeriod();\n\n    final long windowMillis = windowPeriod.toStandardDuration().getMillis();\n    return segmentGranularity.bucketStart(\n        DateTimes.utc(Math.max(windowMillis, rejectionPolicy.getCurrMaxTime().getMillis()) - windowMillis)\n    );\n  }\n","realPath":"server/src/main/java/org/apache/druid/segment/realtime/plumber/RealtimePlumber.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":833,"status":"N"}],"commitId":"676f5e6d7f184101b8763e4249b18b237bbe0ec7","commitMessage":"@@@Prohibit some guava collection APIs and use JDK collection APIs directly (#6511)\n\n* Prohibit some guava collection APIs and use JDK APIs directly\n\n* reset files that changed by accident\n\n* sort codestyle/druid-forbidden-apis.txt alphabetically\n","date":"2018-10-29 20:02:43","modifiedFileCount":"427","status":"M","submitter":"QiuMM"},{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2019-01-22 03:11:10","commitOrder":3,"curCode":"  private void mergeAndPush()\n  {\n    final Granularity segmentGranularity = schema.getGranularitySpec().getSegmentGranularity();\n    final Period windowPeriod = config.getWindowPeriod();\n\n    final long windowMillis = windowPeriod.toStandardDuration().getMillis();\n    log.info(\"Starting merge and push.\");\n    DateTime minTimestampAsDate = segmentGranularity.bucketStart(\n        DateTimes.utc(Math.max(windowMillis, rejectionPolicy.getCurrMaxTime().getMillis()) - windowMillis)\n    );\n    long minTimestamp = minTimestampAsDate.getMillis();\n\n    final List<SegmentIdWithShardSpec> appenderatorSegments = appenderator.getSegments();\n    final List<SegmentIdWithShardSpec> segmentsToPush = new ArrayList<>();\n\n    if (shuttingDown) {\n      log.info(\"Found [%,d] segments. Attempting to hand off all of them.\", appenderatorSegments.size());\n      segmentsToPush.addAll(appenderatorSegments);\n    } else {\n      log.info(\n          \"Found [%,d] segments. Attempting to hand off segments that start before [%s].\",\n          appenderatorSegments.size(),\n          minTimestampAsDate\n      );\n\n      for (SegmentIdWithShardSpec segment : appenderatorSegments) {\n        final Long intervalStart = segment.getInterval().getStartMillis();\n        if (intervalStart < minTimestamp) {\n          log.info(\"Adding entry [%s] for merge and push.\", segment);\n          segmentsToPush.add(segment);\n        } else {\n          log.info(\n              \"Skipping persist and merge for entry [%s] : Start time [%s] >= [%s] min timestamp required in this run. Segment will be picked up in a future run.\",\n              segment,\n              DateTimes.utc(intervalStart),\n              minTimestampAsDate\n          );\n        }\n      }\n    }\n\n    log.info(\"Found [%,d] segments to persist and merge\", segmentsToPush.size());\n\n    final Function<Throwable, Void> errorHandler = new Function<Throwable, Void>()\n    {\n      @Override\n      public Void apply(Throwable throwable)\n      {\n        final List<String> segmentIdentifierStrings = Lists.transform(\n            segmentsToPush,\n            SegmentIdWithShardSpec::toString\n        );\n\n        log.makeAlert(throwable, \"Failed to publish merged indexes[%s]\", schema.getDataSource())\n           .addData(\"segments\", segmentIdentifierStrings)\n           .emit();\n\n        if (shuttingDown) {\n          \r\n          \r\n          cleanShutdown = false;\n          for (SegmentIdWithShardSpec identifier : segmentsToPush) {\n            dropSegment(identifier);\n          }\n        }\n\n        return null;\n      }\n    };\n\n    \r\n    Futures.addCallback(\n        appenderator.push(segmentsToPush, Committers.nil(), false),\n        new FutureCallback<SegmentsAndMetadata>()\n        {\n          @Override\n          public void onSuccess(SegmentsAndMetadata result)\n          {\n            \r\n            for (DataSegment pushedSegment : result.getSegments()) {\n              try {\n                segmentPublisher.publishSegment(pushedSegment);\n              }\n              catch (Exception e) {\n                errorHandler.apply(e);\n              }\n            }\n\n            log.info(\"Published [%,d] sinks.\", segmentsToPush.size());\n          }\n\n          @Override\n          public void onFailure(Throwable e)\n          {\n            log.warn(e, \"Failed to push [%,d] segments.\", segmentsToPush.size());\n            errorHandler.apply(e);\n          }\n        }\n    );\n  }\n","date":"2019-01-22 03:11:10","endLine":486,"groupId":"1098","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"mergeAndPush","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ed/0b8e43e7dceb091ef08486135332eb81e62386.src","preCode":"  private void mergeAndPush()\n  {\n    final Granularity segmentGranularity = schema.getGranularitySpec().getSegmentGranularity();\n    final Period windowPeriod = config.getWindowPeriod();\n\n    final long windowMillis = windowPeriod.toStandardDuration().getMillis();\n    log.info(\"Starting merge and push.\");\n    DateTime minTimestampAsDate = segmentGranularity.bucketStart(\n        DateTimes.utc(Math.max(windowMillis, rejectionPolicy.getCurrMaxTime().getMillis()) - windowMillis)\n    );\n    long minTimestamp = minTimestampAsDate.getMillis();\n\n    final List<SegmentIdentifier> appenderatorSegments = appenderator.getSegments();\n    final List<SegmentIdentifier> segmentsToPush = new ArrayList<>();\n\n    if (shuttingDown) {\n      log.info(\"Found [%,d] segments. Attempting to hand off all of them.\", appenderatorSegments.size());\n      segmentsToPush.addAll(appenderatorSegments);\n    } else {\n      log.info(\n          \"Found [%,d] segments. Attempting to hand off segments that start before [%s].\",\n          appenderatorSegments.size(),\n          minTimestampAsDate\n      );\n\n      for (SegmentIdentifier segment : appenderatorSegments) {\n        final Long intervalStart = segment.getInterval().getStartMillis();\n        if (intervalStart < minTimestamp) {\n          log.info(\"Adding entry [%s] for merge and push.\", segment);\n          segmentsToPush.add(segment);\n        } else {\n          log.info(\n              \"Skipping persist and merge for entry [%s] : Start time [%s] >= [%s] min timestamp required in this run. Segment will be picked up in a future run.\",\n              segment,\n              DateTimes.utc(intervalStart),\n              minTimestampAsDate\n          );\n        }\n      }\n    }\n\n    log.info(\"Found [%,d] segments to persist and merge\", segmentsToPush.size());\n\n    final Function<Throwable, Void> errorHandler = new Function<Throwable, Void>()\n    {\n      @Override\n      public Void apply(Throwable throwable)\n      {\n        final List<String> segmentIdentifierStrings = Lists.transform(\n            segmentsToPush,\n            new Function<SegmentIdentifier, String>()\n            {\n              @Override\n              public String apply(SegmentIdentifier input)\n              {\n                return input.getIdentifierAsString();\n              }\n            }\n        );\n\n        log.makeAlert(throwable, \"Failed to publish merged indexes[%s]\", schema.getDataSource())\n           .addData(\"segments\", segmentIdentifierStrings)\n           .emit();\n\n        if (shuttingDown) {\n          \r\n          \r\n          cleanShutdown = false;\n          for (SegmentIdentifier identifier : segmentsToPush) {\n            dropSegment(identifier);\n          }\n        }\n\n        return null;\n      }\n    };\n\n    \r\n    Futures.addCallback(\n        appenderator.push(segmentsToPush, Committers.nil(), false),\n        new FutureCallback<SegmentsAndMetadata>()\n        {\n          @Override\n          public void onSuccess(SegmentsAndMetadata result)\n          {\n            \r\n            for (DataSegment pushedSegment : result.getSegments()) {\n              try {\n                segmentPublisher.publishSegment(pushedSegment);\n              }\n              catch (Exception e) {\n                errorHandler.apply(e);\n              }\n            }\n\n            log.info(\"Published [%,d] sinks.\", segmentsToPush.size());\n          }\n\n          @Override\n          public void onFailure(Throwable e)\n          {\n            log.warn(e, \"Failed to push [%,d] segments.\", segmentsToPush.size());\n            errorHandler.apply(e);\n          }\n        }\n    );\n  }\n","realPath":"server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorPlumber.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":387,"status":"M"},{"authorDate":"2018-08-31 00:56:26","commitOrder":3,"curCode":"  private DateTime getAllowedMinTime()\n  {\n    final Granularity segmentGranularity = schema.getGranularitySpec().getSegmentGranularity();\n    final Period windowPeriod = config.getWindowPeriod();\n\n    final long windowMillis = windowPeriod.toStandardDuration().getMillis();\n    return segmentGranularity.bucketStart(\n        DateTimes.utc(Math.max(windowMillis, rejectionPolicy.getCurrMaxTime().getMillis()) - windowMillis)\n    );\n  }\n","date":"2018-08-31 00:56:26","endLine":842,"groupId":"14037","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"getAllowedMinTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5d/9e6222e20ceed8b9cb93e231b4eb8c6424782a.src","preCode":"  private DateTime getAllowedMinTime()\n  {\n    final Granularity segmentGranularity = schema.getGranularitySpec().getSegmentGranularity();\n    final Period windowPeriod = config.getWindowPeriod();\n\n    final long windowMillis = windowPeriod.toStandardDuration().getMillis();\n    return segmentGranularity.bucketStart(\n        DateTimes.utc(Math.max(windowMillis, rejectionPolicy.getCurrMaxTime().getMillis()) - windowMillis)\n    );\n  }\n","realPath":"server/src/main/java/org/apache/druid/segment/realtime/plumber/RealtimePlumber.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":833,"status":"N"}],"commitId":"8eae26fd4e7572060d112864dd3d5f6a865b9c89","commitMessage":"@@@Introduce SegmentId class (#6370)\n\n* Introduce SegmentId class\n\n* tmp\n\n* Fix SelectQueryRunnerTest\n\n* Fix indentation\n\n* Fixes\n\n* Remove Comparators.inverse() tests\n\n* Refinements\n\n* Fix tests\n\n* Fix more tests\n\n* Remove duplicate DataSegmentTest.  fixes #6064\n\n* SegmentDescriptor doc\n\n* Fix SQLMetadataStorageUpdaterJobHandler\n\n* Fix DataSegment deserialization for ignoring id\n\n* Add comments\n\n* More comments\n\n* Address more comments\n\n* Fix compilation\n\n* Restore segment2 in SystemSchemaTest according to a comment\n\n* Fix style\n\n* fix testServerSegmentsTable\n\n* Fix compilation\n\n* Add comments about why SegmentId and SegmentIdWithShardSpec are separate classes\n\n* Fix SystemSchemaTest\n\n* Fix style\n\n* Compare SegmentDescriptor with SegmentId in Javadoc and comments rather than with DataSegment\n\n* Remove a link.  see https://youtrack.jetbrains.com/issue/IDEA-205164\n\n* Fix compilation\n","date":"2019-01-22 03:11:10","modifiedFileCount":"308","status":"M","submitter":"Roman Leventov"},{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2020-01-28 03:24:29","commitOrder":4,"curCode":"  private void mergeAndPush()\n  {\n    final Granularity segmentGranularity = schema.getGranularitySpec().getSegmentGranularity();\n    final Period windowPeriod = config.getWindowPeriod();\n\n    final long windowMillis = windowPeriod.toStandardDuration().getMillis();\n    log.info(\"Starting merge and push.\");\n    DateTime minTimestampAsDate = segmentGranularity.bucketStart(\n        DateTimes.utc(Math.max(windowMillis, rejectionPolicy.getCurrMaxTime().getMillis()) - windowMillis)\n    );\n    long minTimestamp = minTimestampAsDate.getMillis();\n\n    final List<SegmentIdWithShardSpec> appenderatorSegments = appenderator.getSegments();\n    final List<SegmentIdWithShardSpec> segmentsToPush = new ArrayList<>();\n\n    if (shuttingDown) {\n      log.info(\"Found [%,d] segments. Attempting to hand off all of them.\", appenderatorSegments.size());\n      segmentsToPush.addAll(appenderatorSegments);\n    } else {\n      log.info(\n          \"Found [%,d] segments. Attempting to hand off segments that start before [%s].\",\n          appenderatorSegments.size(),\n          minTimestampAsDate\n      );\n\n      for (SegmentIdWithShardSpec segment : appenderatorSegments) {\n        final Long intervalStart = segment.getInterval().getStartMillis();\n        if (intervalStart < minTimestamp) {\n          log.info(\"Adding entry [%s] for merge and push.\", segment);\n          segmentsToPush.add(segment);\n        } else {\n          log.info(\n              \"Skipping persist and merge for entry [%s] : Start time [%s] >= [%s] min timestamp required in this run. Segment will be picked up in a future run.\",\n              segment,\n              DateTimes.utc(intervalStart),\n              minTimestampAsDate\n          );\n        }\n      }\n    }\n\n    log.info(\"Found [%,d] segments to persist and merge\", segmentsToPush.size());\n\n    final Function<Throwable, Void> errorHandler = new Function<Throwable, Void>()\n    {\n      @Override\n      public Void apply(Throwable throwable)\n      {\n        final List<String> segmentIdentifierStrings = Lists.transform(\n            segmentsToPush,\n            SegmentIdWithShardSpec::toString\n        );\n\n        log.makeAlert(throwable, \"Failed to publish merged indexes[%s]\", schema.getDataSource())\n           .addData(\"segments\", segmentIdentifierStrings)\n           .emit();\n\n        if (shuttingDown) {\n          \r\n          \r\n          cleanShutdown = false;\n          for (SegmentIdWithShardSpec identifier : segmentsToPush) {\n            dropSegment(identifier);\n          }\n        }\n\n        return null;\n      }\n    };\n\n    \r\n    Futures.addCallback(\n        appenderator.push(segmentsToPush, Committers.nil(), false),\n        new FutureCallback<SegmentsAndCommitMetadata>()\n        {\n          @Override\n          public void onSuccess(SegmentsAndCommitMetadata result)\n          {\n            \r\n            for (DataSegment pushedSegment : result.getSegments()) {\n              try {\n                segmentPublisher.publishSegment(pushedSegment);\n              }\n              catch (Exception e) {\n                errorHandler.apply(e);\n              }\n            }\n\n            log.info(\"Published [%,d] sinks.\", segmentsToPush.size());\n          }\n\n          @Override\n          public void onFailure(Throwable e)\n          {\n            log.warn(e, \"Failed to push [%,d] segments.\", segmentsToPush.size());\n            errorHandler.apply(e);\n          }\n        }\n    );\n  }\n","date":"2020-01-28 03:24:29","endLine":488,"groupId":"101073","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"mergeAndPush","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/28/c3ac74df3612e20ada56feaaa46825e64a0700.src","preCode":"  private void mergeAndPush()\n  {\n    final Granularity segmentGranularity = schema.getGranularitySpec().getSegmentGranularity();\n    final Period windowPeriod = config.getWindowPeriod();\n\n    final long windowMillis = windowPeriod.toStandardDuration().getMillis();\n    log.info(\"Starting merge and push.\");\n    DateTime minTimestampAsDate = segmentGranularity.bucketStart(\n        DateTimes.utc(Math.max(windowMillis, rejectionPolicy.getCurrMaxTime().getMillis()) - windowMillis)\n    );\n    long minTimestamp = minTimestampAsDate.getMillis();\n\n    final List<SegmentIdWithShardSpec> appenderatorSegments = appenderator.getSegments();\n    final List<SegmentIdWithShardSpec> segmentsToPush = new ArrayList<>();\n\n    if (shuttingDown) {\n      log.info(\"Found [%,d] segments. Attempting to hand off all of them.\", appenderatorSegments.size());\n      segmentsToPush.addAll(appenderatorSegments);\n    } else {\n      log.info(\n          \"Found [%,d] segments. Attempting to hand off segments that start before [%s].\",\n          appenderatorSegments.size(),\n          minTimestampAsDate\n      );\n\n      for (SegmentIdWithShardSpec segment : appenderatorSegments) {\n        final Long intervalStart = segment.getInterval().getStartMillis();\n        if (intervalStart < minTimestamp) {\n          log.info(\"Adding entry [%s] for merge and push.\", segment);\n          segmentsToPush.add(segment);\n        } else {\n          log.info(\n              \"Skipping persist and merge for entry [%s] : Start time [%s] >= [%s] min timestamp required in this run. Segment will be picked up in a future run.\",\n              segment,\n              DateTimes.utc(intervalStart),\n              minTimestampAsDate\n          );\n        }\n      }\n    }\n\n    log.info(\"Found [%,d] segments to persist and merge\", segmentsToPush.size());\n\n    final Function<Throwable, Void> errorHandler = new Function<Throwable, Void>()\n    {\n      @Override\n      public Void apply(Throwable throwable)\n      {\n        final List<String> segmentIdentifierStrings = Lists.transform(\n            segmentsToPush,\n            SegmentIdWithShardSpec::toString\n        );\n\n        log.makeAlert(throwable, \"Failed to publish merged indexes[%s]\", schema.getDataSource())\n           .addData(\"segments\", segmentIdentifierStrings)\n           .emit();\n\n        if (shuttingDown) {\n          \r\n          \r\n          cleanShutdown = false;\n          for (SegmentIdWithShardSpec identifier : segmentsToPush) {\n            dropSegment(identifier);\n          }\n        }\n\n        return null;\n      }\n    };\n\n    \r\n    Futures.addCallback(\n        appenderator.push(segmentsToPush, Committers.nil(), false),\n        new FutureCallback<SegmentsAndMetadata>()\n        {\n          @Override\n          public void onSuccess(SegmentsAndMetadata result)\n          {\n            \r\n            for (DataSegment pushedSegment : result.getSegments()) {\n              try {\n                segmentPublisher.publishSegment(pushedSegment);\n              }\n              catch (Exception e) {\n                errorHandler.apply(e);\n              }\n            }\n\n            log.info(\"Published [%,d] sinks.\", segmentsToPush.size());\n          }\n\n          @Override\n          public void onFailure(Throwable e)\n          {\n            log.warn(e, \"Failed to push [%,d] segments.\", segmentsToPush.size());\n            errorHandler.apply(e);\n          }\n        }\n    );\n  }\n","realPath":"server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorPlumber.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":389,"status":"M"},{"authorDate":"2018-08-31 00:56:26","commitOrder":4,"curCode":"  private DateTime getAllowedMinTime()\n  {\n    final Granularity segmentGranularity = schema.getGranularitySpec().getSegmentGranularity();\n    final Period windowPeriod = config.getWindowPeriod();\n\n    final long windowMillis = windowPeriod.toStandardDuration().getMillis();\n    return segmentGranularity.bucketStart(\n        DateTimes.utc(Math.max(windowMillis, rejectionPolicy.getCurrMaxTime().getMillis()) - windowMillis)\n    );\n  }\n","date":"2018-08-31 00:56:26","endLine":842,"groupId":"101073","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"getAllowedMinTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5d/9e6222e20ceed8b9cb93e231b4eb8c6424782a.src","preCode":"  private DateTime getAllowedMinTime()\n  {\n    final Granularity segmentGranularity = schema.getGranularitySpec().getSegmentGranularity();\n    final Period windowPeriod = config.getWindowPeriod();\n\n    final long windowMillis = windowPeriod.toStandardDuration().getMillis();\n    return segmentGranularity.bucketStart(\n        DateTimes.utc(Math.max(windowMillis, rejectionPolicy.getCurrMaxTime().getMillis()) - windowMillis)\n    );\n  }\n","realPath":"server/src/main/java/org/apache/druid/segment/realtime/plumber/RealtimePlumber.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":833,"status":"N"}],"commitId":"b9186f8f9ff2ff52aceda42bc5f24ffd47a7d17e","commitMessage":"@@@Reconcile terminology and method naming to 'used/unused segments'; Rename MetadataSegmentManager to MetadataSegmentsManager (#7306)\n\n* Reconcile terminology and method naming to 'used/unused segments'; Don't use terms 'enable/disable data source'; Rename MetadataSegmentManager to MetadataSegments; Make REST API methods which mark segments as used/unused to return server error instead of an empty response in case of error\n\n* Fix brace\n\n* Import order\n\n* Rename withKillDataSourceWhitelist to withSpecificDataSourcesToKill\n\n* Fix tests\n\n* Fix tests by adding proper methods without interval parameters to IndexerMetadataStorageCoordinator instead of hacking with Intervals.ETERNITY\n\n* More aligned names of DruidCoordinatorHelpers.  rename several CoordinatorDynamicConfig parameters\n\n* Rename ClientCompactTaskQuery to ClientCompactionTaskQuery for consistency with CompactionTask; ClientCompactQueryTuningConfig to ClientCompactionTaskQueryTuningConfig\n\n* More variable and method renames\n\n* Rename MetadataSegments to SegmentsMetadata\n\n* Javadoc update\n\n* Simplify SegmentsMetadata.getUnusedSegmentIntervals().  more javadocs\n\n* Update Javadoc of VersionedIntervalTimeline.iterateAllObjects()\n\n* Reorder imports\n\n* Rename SegmentsMetadata.tryMark... methods to mark... and make them to return boolean and the numbers of segments changed and relay exceptions to callers\n\n* Complete merge\n\n* Add CollectionUtils.newTreeSet(); Refactor DruidCoordinatorRuntimeParams creation in tests\n\n* Remove MetadataSegmentManager\n\n* Rename millisLagSinceCoordinatorBecomesLeaderBeforeCanMarkAsUnusedOvershadowedSegments to leadingTimeMillisBeforeCanMarkAsUnusedOvershadowedSegments\n\n* Fix tests.  refactor DruidCluster creation in tests into DruidClusterBuilder\n\n* Fix inspections\n\n* Fix SQLMetadataSegmentManagerEmptyTest and rename it to SqlSegmentsMetadataEmptyTest\n\n* Rename SegmentsAndMetadata to SegmentsAndCommitMetadata to reduce the similarity with SegmentsMetadata; Rename some methods\n\n* Rename DruidCoordinatorHelper to CoordinatorDuty.  refactor DruidCoordinator\n\n* Unused import\n\n* Optimize imports\n\n* Rename IndexerSQLMetadataStorageCoordinator.getDataSourceMetadata() to retrieveDataSourceMetadata()\n\n* Unused import\n\n* Update terminology in datasource-view.tsx\n\n* Fix label in datasource-view.spec.tsx.snap\n\n* Fix lint errors in datasource-view.tsx\n\n* Doc improvements\n\n* Another attempt to please TSLint\n\n* Another attempt to please TSLint\n\n* Style fixes\n\n* Fix IndexerSQLMetadataStorageCoordinator.createUsedSegmentsSqlQueryForIntervals() (wrong merge)\n\n* Try to fix docs build issue\n\n* Javadoc and spelling fixes\n\n* Rename SegmentsMetadata to SegmentsMetadataManager.  address other comments\n\n* Address more comments\n","date":"2020-01-28 03:24:29","modifiedFileCount":"127","status":"M","submitter":"Roman Leventov"}]
