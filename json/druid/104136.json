[{"authorTime":"2021-03-12 14:04:58","codes":[{"authorDate":"2021-01-09 08:04:37","commitOrder":5,"curCode":"  public void testWithNullParserAndInputformatParseProperly() throws IOException\n  {\n    final JsonInputFormat inputFormat = new JsonInputFormat(JSONPathSpec.DEFAULT, Collections.emptyMap(), null);\n    final StreamChunkParser<ByteEntity> chunkParser = new StreamChunkParser<>(\n        null,\n        inputFormat,\n        new InputRowSchema(TIMESTAMP_SPEC, DimensionsSpec.EMPTY, Collections.emptyList()),\n        TransformSpec.NONE,\n        temporaryFolder.newFolder(),\n        row -> true,\n        rowIngestionMeters,\n        parseExceptionHandler\n    );\n    parseAndAssertResult(chunkParser);\n  }\n","date":"2021-01-09 08:04:37","endLine":115,"groupId":"12090","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testWithNullParserAndInputformatParseProperly","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/7e/0ece635dc181c8b9ad806c5ff83afeff4cbcef.src","preCode":"  public void testWithNullParserAndInputformatParseProperly() throws IOException\n  {\n    final JsonInputFormat inputFormat = new JsonInputFormat(JSONPathSpec.DEFAULT, Collections.emptyMap(), null);\n    final StreamChunkParser<ByteEntity> chunkParser = new StreamChunkParser<>(\n        null,\n        inputFormat,\n        new InputRowSchema(TIMESTAMP_SPEC, DimensionsSpec.EMPTY, Collections.emptyList()),\n        TransformSpec.NONE,\n        temporaryFolder.newFolder(),\n        row -> true,\n        rowIngestionMeters,\n        parseExceptionHandler\n    );\n    parseAndAssertResult(chunkParser);\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/seekablestream/StreamChunkParserTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":101,"status":"NB"},{"authorDate":"2021-03-12 14:04:58","commitOrder":5,"curCode":"  public void parseEmptyNotEndOfShard() throws IOException\n  {\n    final TrackingJsonInputFormat inputFormat = new TrackingJsonInputFormat(\n        JSONPathSpec.DEFAULT,\n        Collections.emptyMap()\n    );\n    RowIngestionMeters mockRowIngestionMeters = Mockito.mock(RowIngestionMeters.class);\n    final StreamChunkParser<ByteEntity> chunkParser = new StreamChunkParser<>(\n        null,\n        inputFormat,\n        new InputRowSchema(TIMESTAMP_SPEC, DimensionsSpec.EMPTY, Collections.emptyList()),\n        TransformSpec.NONE,\n        temporaryFolder.newFolder(),\n        row -> true,\n        mockRowIngestionMeters,\n        parseExceptionHandler\n    );\n    List<InputRow> parsedRows = chunkParser.parse(ImmutableList.of(), false);\n    Assert.assertEquals(0, parsedRows.size());\n    Mockito.verify(mockRowIngestionMeters).incrementThrownAway();\n  }\n","date":"2021-03-12 14:04:58","endLine":192,"groupId":"9059","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"parseEmptyNotEndOfShard","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/97/dc28050359bf943b560e9de6d4755a5e203532.src","preCode":"  public void parseEmptyNotEndOfShard() throws IOException\n  {\n    final TrackingJsonInputFormat inputFormat = new TrackingJsonInputFormat(\n        JSONPathSpec.DEFAULT,\n        Collections.emptyMap()\n    );\n    RowIngestionMeters mockRowIngestionMeters = Mockito.mock(RowIngestionMeters.class);\n    final StreamChunkParser<ByteEntity> chunkParser = new StreamChunkParser<>(\n        null,\n        inputFormat,\n        new InputRowSchema(TIMESTAMP_SPEC, DimensionsSpec.EMPTY, Collections.emptyList()),\n        TransformSpec.NONE,\n        temporaryFolder.newFolder(),\n        row -> true,\n        mockRowIngestionMeters,\n        parseExceptionHandler\n    );\n    List<InputRow> parsedRows = chunkParser.parse(ImmutableList.of(), false);\n    Assert.assertEquals(0, parsedRows.size());\n    Mockito.verify(mockRowIngestionMeters).incrementThrownAway();\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/seekablestream/StreamChunkParserTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":172,"status":"B"}],"commitId":"ed91a2bb384df8c70a39abdb1813cff7d3d48a17","commitMessage":"@@@Fix Kinesis should not increment throwAway count on EOS record (#10976)\n\n* fix Kinesis increament throwAway on EOS record\n\n* fix checkstyle\n\n* fix IT\n\n* fix test\n\n* fix IT\n\n* fix IT\n\n* fix IT\n\n* fix IT","date":"2021-03-12 14:04:58","modifiedFileCount":"6","status":"M","submitter":"Maytas Monsereenusorn"},{"authorTime":"2021-03-26 01:32:21","codes":[{"authorDate":"2021-03-26 01:32:21","commitOrder":6,"curCode":"  public void testWithNullParserAndInputformatParseProperly() throws IOException\n  {\n    final JsonInputFormat inputFormat = new JsonInputFormat(JSONPathSpec.DEFAULT, Collections.emptyMap(), null);\n    final StreamChunkParser<ByteEntity> chunkParser = new StreamChunkParser<>(\n        null,\n        inputFormat,\n        new InputRowSchema(TIMESTAMP_SPEC, DimensionsSpec.EMPTY, ColumnsFilter.all()),\n        TransformSpec.NONE,\n        temporaryFolder.newFolder(),\n        row -> true,\n        rowIngestionMeters,\n        parseExceptionHandler\n    );\n    parseAndAssertResult(chunkParser);\n  }\n","date":"2021-03-26 01:32:21","endLine":121,"groupId":"104136","id":3,"instanceNumber":1,"isCurCommit":1,"methodName":"testWithNullParserAndInputformatParseProperly","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/1c/ab704a2effeae8be86596b362e3208085f6b8f.src","preCode":"  public void testWithNullParserAndInputformatParseProperly() throws IOException\n  {\n    final JsonInputFormat inputFormat = new JsonInputFormat(JSONPathSpec.DEFAULT, Collections.emptyMap(), null);\n    final StreamChunkParser<ByteEntity> chunkParser = new StreamChunkParser<>(\n        null,\n        inputFormat,\n        new InputRowSchema(TIMESTAMP_SPEC, DimensionsSpec.EMPTY, Collections.emptyList()),\n        TransformSpec.NONE,\n        temporaryFolder.newFolder(),\n        row -> true,\n        rowIngestionMeters,\n        parseExceptionHandler\n    );\n    parseAndAssertResult(chunkParser);\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/seekablestream/StreamChunkParserTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":107,"status":"M"},{"authorDate":"2021-03-26 01:32:21","commitOrder":6,"curCode":"  public void parseEmptyNotEndOfShard() throws IOException\n  {\n    final TrackingJsonInputFormat inputFormat = new TrackingJsonInputFormat(\n        JSONPathSpec.DEFAULT,\n        Collections.emptyMap()\n    );\n    RowIngestionMeters mockRowIngestionMeters = Mockito.mock(RowIngestionMeters.class);\n    final StreamChunkParser<ByteEntity> chunkParser = new StreamChunkParser<>(\n        null,\n        inputFormat,\n        new InputRowSchema(TIMESTAMP_SPEC, DimensionsSpec.EMPTY, ColumnsFilter.all()),\n        TransformSpec.NONE,\n        temporaryFolder.newFolder(),\n        row -> true,\n        mockRowIngestionMeters,\n        parseExceptionHandler\n    );\n    List<InputRow> parsedRows = chunkParser.parse(ImmutableList.of(), false);\n    Assert.assertEquals(0, parsedRows.size());\n    Mockito.verify(mockRowIngestionMeters).incrementThrownAway();\n  }\n","date":"2021-03-26 01:32:21","endLine":193,"groupId":"104136","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"parseEmptyNotEndOfShard","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/1c/ab704a2effeae8be86596b362e3208085f6b8f.src","preCode":"  public void parseEmptyNotEndOfShard() throws IOException\n  {\n    final TrackingJsonInputFormat inputFormat = new TrackingJsonInputFormat(\n        JSONPathSpec.DEFAULT,\n        Collections.emptyMap()\n    );\n    RowIngestionMeters mockRowIngestionMeters = Mockito.mock(RowIngestionMeters.class);\n    final StreamChunkParser<ByteEntity> chunkParser = new StreamChunkParser<>(\n        null,\n        inputFormat,\n        new InputRowSchema(TIMESTAMP_SPEC, DimensionsSpec.EMPTY, Collections.emptyList()),\n        TransformSpec.NONE,\n        temporaryFolder.newFolder(),\n        row -> true,\n        mockRowIngestionMeters,\n        parseExceptionHandler\n    );\n    List<InputRow> parsedRows = chunkParser.parse(ImmutableList.of(), false);\n    Assert.assertEquals(0, parsedRows.size());\n    Mockito.verify(mockRowIngestionMeters).incrementThrownAway();\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/seekablestream/StreamChunkParserTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":173,"status":"M"}],"commitId":"bf20f9e9798417c9293a195690b6adcb48f44d3f","commitMessage":"@@@DruidInputSource: Fix issues in column projection.  timestamp handling. (#10267)\n\n* DruidInputSource: Fix issues in column projection.  timestamp handling.\n\nDruidInputSource.  DruidSegmentReader changes:\n\n1) Remove \"dimensions\" and \"metrics\". They are not necessary.  because we\n   can compute which columns we need to read based on what is going to\n   be used by the timestamp.  transform.  dimensions.  and metrics.\n2) Start using ColumnsFilter (see below) to decide which columns we need\n   to read.\n3) Actually respect the \"timestampSpec\". Previously.  it was ignored.  and\n   the timestamp of the returned InputRows was set to the `__time` column\n   of the input datasource.\n\n(1) and (2) together fix a bug in which the DruidInputSource would not\nproperly read columns that are used as inputs to a transformSpec.\n\n(3) fixes a bug where the timestampSpec would be ignored if you attempted\nto set the column to something other than `__time`.\n\n(1) and (3) are breaking changes.\n\nWeb console changes:\n\n1) Remove \"Dimensions\" and \"Metrics\" from the Druid input source.\n2) Set timestampSpec to `{\"column\": \"__time\".  \"format\": \"millis\"}` for\n   compatibility with the new behavior.\n\nOther changes:\n\n1) Add ColumnsFilter.  a new class that allows input readers to determine\n   which columns they need to read. Currently.  it's only used by the\n   DruidInputSource.  but it could be used by other columnar input sources\n   in the future.\n2) Add a ColumnsFilter to InputRowSchema.\n3) Remove the metric names from InputRowSchema (they were unused).\n4) Add InputRowSchemas.fromDataSchema method that computes the proper\n   ColumnsFilter for given timestamp.  dimensions.  transform.  and metrics.\n5) Add \"getRequiredColumns\" method to TransformSpec to support the above.\n\n* Various fixups.\n\n* Uncomment incorrectly commented lines.\n\n* Move TransformSpecTest to the proper module.\n\n* Add druid.indexer.task.ignoreTimestampSpecForDruidInputSource setting.\n\n* Fix.\n\n* Fix build.\n\n* Checkstyle.\n\n* Misc fixes.\n\n* Fix test.\n\n* Move config.\n\n* Fix imports.\n\n* Fixup.\n\n* Fix ShuffleResourceTest.\n\n* Add import.\n\n* Smarter exclusions.\n\n* Fixes based on tests.\n\nAlso.  add TIME_COLUMN constant in the web console.\n\n* Adjustments for tests.\n\n* Reorder test data.\n\n* Update docs.\n\n* Update docs to say Druid 0.22.0 instead of 0.21.0.\n\n* Fix test.\n\n* Fix ITAutoCompactionTest.\n\n* Changes from review & from merging.","date":"2021-03-26 01:32:21","modifiedFileCount":"60","status":"M","submitter":"Gian Merlino"}]
