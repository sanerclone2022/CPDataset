[{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"    protected void setup(Context context)\n    {\n      config = HadoopDruidIndexerConfig.fromConfiguration(context.getConfiguration());\n\n      aggregators = config.getSchema().getDataSchema().getAggregators();\n      combiningAggs = new AggregatorFactory[aggregators.length];\n      for (int i = 0; i < aggregators.length; ++i) {\n        combiningAggs[i] = aggregators[i].getCombiningFactory();\n      }\n      typeHelperMap = InputRowSerde.getTypeHelperMap(config.getSchema()\n                                                           .getDataSchema()\n                                                           .getParser()\n                                                           .getParseSpec()\n                                                           .getDimensionsSpec());\n    }\n","date":"2018-08-31 00:56:26","endLine":421,"groupId":"12623","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ac/34c755ae95eafc859b6279045a110476e53493.src","preCode":"    protected void setup(Context context)\n    {\n      config = HadoopDruidIndexerConfig.fromConfiguration(context.getConfiguration());\n\n      aggregators = config.getSchema().getDataSchema().getAggregators();\n      combiningAggs = new AggregatorFactory[aggregators.length];\n      for (int i = 0; i < aggregators.length; ++i) {\n        combiningAggs[i] = aggregators[i].getCombiningFactory();\n      }\n      typeHelperMap = InputRowSerde.getTypeHelperMap(config.getSchema()\n                                                           .getDataSchema()\n                                                           .getParser()\n                                                           .getParseSpec()\n                                                           .getDimensionsSpec());\n    }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/IndexGeneratorJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":407,"status":"B"},{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"    protected void setup(Context context)\n    {\n      config = HadoopDruidIndexerConfig.fromConfiguration(context.getConfiguration());\n\n      aggregators = config.getSchema().getDataSchema().getAggregators();\n      combiningAggs = new AggregatorFactory[aggregators.length];\n      for (int i = 0; i < aggregators.length; ++i) {\n        metricNames.add(aggregators[i].getName());\n        combiningAggs[i] = aggregators[i].getCombiningFactory();\n      }\n      typeHelperMap = InputRowSerde.getTypeHelperMap(config.getSchema()\n                                                           .getDataSchema()\n                                                           .getParser()\n                                                           .getParseSpec()\n                                                           .getDimensionsSpec());\n    }\n","date":"2018-08-31 00:56:26","endLine":625,"groupId":"12623","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"setup","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ac/34c755ae95eafc859b6279045a110476e53493.src","preCode":"    protected void setup(Context context)\n    {\n      config = HadoopDruidIndexerConfig.fromConfiguration(context.getConfiguration());\n\n      aggregators = config.getSchema().getDataSchema().getAggregators();\n      combiningAggs = new AggregatorFactory[aggregators.length];\n      for (int i = 0; i < aggregators.length; ++i) {\n        metricNames.add(aggregators[i].getName());\n        combiningAggs[i] = aggregators[i].getCombiningFactory();\n      }\n      typeHelperMap = InputRowSerde.getTypeHelperMap(config.getSchema()\n                                                           .getDataSchema()\n                                                           .getParser()\n                                                           .getParseSpec()\n                                                           .getDimensionsSpec());\n    }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/IndexGeneratorJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":610,"status":"B"}],"commitId":"431d3d8497f9079857c3baa7ae7ab4cb44a22355","commitMessage":"@@@Rename io.druid to org.apache.druid. (#6266)\n\n* Rename io.druid to org.apache.druid.\n\n* Fix META-INF files and remove some benchmark results.\n\n* MonitorsConfig update for metrics package migration.\n\n* Reorder some dimensions in inner queries for some reason.\n\n* Fix protobuf tests.\n","date":"2018-08-31 00:56:26","modifiedFileCount":"5","status":"B","submitter":"Gian Merlino"},{"authorTime":"2019-11-16 01:22:09","codes":[{"authorDate":"2019-11-16 01:22:09","commitOrder":2,"curCode":"    protected void setup(Context context)\n    {\n      config = HadoopDruidIndexerConfig.fromConfiguration(context.getConfiguration());\n\n      aggregators = config.getSchema().getDataSchema().getAggregators();\n      combiningAggs = new AggregatorFactory[aggregators.length];\n      for (int i = 0; i < aggregators.length; ++i) {\n        combiningAggs[i] = aggregators[i].getCombiningFactory();\n      }\n      typeHelperMap = InputRowSerde.getTypeHelperMap(config.getSchema().getDataSchema().getDimensionsSpec());\n    }\n","date":"2019-11-16 01:22:09","endLine":431,"groupId":"111360","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"setup","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/33/8b857c05e9cc72cd25f771ae2b6702c54680c6.src","preCode":"    protected void setup(Context context)\n    {\n      config = HadoopDruidIndexerConfig.fromConfiguration(context.getConfiguration());\n\n      aggregators = config.getSchema().getDataSchema().getAggregators();\n      combiningAggs = new AggregatorFactory[aggregators.length];\n      for (int i = 0; i < aggregators.length; ++i) {\n        combiningAggs[i] = aggregators[i].getCombiningFactory();\n      }\n      typeHelperMap = InputRowSerde.getTypeHelperMap(config.getSchema()\n                                                           .getDataSchema()\n                                                           .getParser()\n                                                           .getParseSpec()\n                                                           .getDimensionsSpec());\n    }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/IndexGeneratorJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":421,"status":"M"},{"authorDate":"2019-11-16 01:22:09","commitOrder":2,"curCode":"    protected void setup(Context context)\n    {\n      config = HadoopDruidIndexerConfig.fromConfiguration(context.getConfiguration());\n\n      aggregators = config.getSchema().getDataSchema().getAggregators();\n      combiningAggs = new AggregatorFactory[aggregators.length];\n      for (int i = 0; i < aggregators.length; ++i) {\n        metricNames.add(aggregators[i].getName());\n        combiningAggs[i] = aggregators[i].getCombiningFactory();\n      }\n      typeHelperMap = InputRowSerde.getTypeHelperMap(config.getSchema().getDataSchema().getDimensionsSpec());\n    }\n","date":"2019-11-16 01:22:09","endLine":627,"groupId":"111360","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"setup","params":"(Contextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/33/8b857c05e9cc72cd25f771ae2b6702c54680c6.src","preCode":"    protected void setup(Context context)\n    {\n      config = HadoopDruidIndexerConfig.fromConfiguration(context.getConfiguration());\n\n      aggregators = config.getSchema().getDataSchema().getAggregators();\n      combiningAggs = new AggregatorFactory[aggregators.length];\n      for (int i = 0; i < aggregators.length; ++i) {\n        metricNames.add(aggregators[i].getName());\n        combiningAggs[i] = aggregators[i].getCombiningFactory();\n      }\n      typeHelperMap = InputRowSerde.getTypeHelperMap(config.getSchema()\n                                                           .getDataSchema()\n                                                           .getParser()\n                                                           .getParseSpec()\n                                                           .getDimensionsSpec());\n    }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/IndexGeneratorJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":616,"status":"M"}],"commitId":"1611792855ad9def8b6f5b1375862d05d1acca0a","commitMessage":"@@@Add InputSource and InputFormat interfaces (#8823)\n\n* Add InputSource and InputFormat interfaces\n\n* revert orc dependency\n\n* fix dimension exclusions and failing unit tests\n\n* fix tests\n\n* fix test\n\n* fix test\n\n* fix firehose and inputSource for parallel indexing task\n\n* fix tc\n\n* fix tc: remove unused method\n\n* Formattable\n\n* add needsFormat(); renamed to ObjectSource; pass metricsName for reader\n\n* address comments\n\n* fix closing resource\n\n* fix checkstyle\n\n* fix tests\n\n* remove verify from csv\n\n* Revert \"remove verify from csv\"\n\nThis reverts commit 1ea7758489cc8c9d708bd691fd48e62085fd9455.\n\n* address comments\n\n* fix import order and javadoc\n\n* flatMap\n\n* sampleLine\n\n* Add IntermediateRowParsingReader\n\n* Address comments\n\n* move csv reader test\n\n* remove test for verify\n\n* adjust comments\n\n* Fix InputEntityIteratingReader\n\n* rename source -> entity\n\n* address comments\n","date":"2019-11-16 01:22:09","modifiedFileCount":"72","status":"M","submitter":"Jihoon Son"}]
