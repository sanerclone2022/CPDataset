[{"authorTime":"2020-07-02 13:20:53","codes":[{"authorDate":"2020-04-14 04:03:56","commitOrder":3,"curCode":"  void doTest(Pair<String, List> azureInputSource) throws Exception\n  {\n    final String indexDatasource = \"wikipedia_index_test_\" + UUID.randomUUID();\n    try (\n        final Closeable ignored1 = unloader(indexDatasource + config.getExtraDatasourceNameSuffix());\n    ) {\n      final Function<String, String> azurePropsTransform = spec -> {\n        try {\n          String inputSourceValue = jsonMapper.writeValueAsString(azureInputSource.rhs);\n          inputSourceValue = StringUtils.replace(\n              inputSourceValue,\n              \"%%BUCKET%%\",\n              config.getCloudBucket()\n          );\n          inputSourceValue = StringUtils.replace(\n              inputSourceValue,\n              \"%%PATH%%\",\n              config.getCloudPath()\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_FORMAT_TYPE%%\",\n              InputFormatDetails.JSON.getInputFormatType()\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%PARTITIONS_SPEC%%\",\n              jsonMapper.writeValueAsString(new DynamicPartitionsSpec(null, null))\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_TYPE%%\",\n              \"azure\"\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_PROPERTY_KEY%%\",\n              azureInputSource.lhs\n          );\n          return StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_PROPERTY_VALUE%%\",\n              inputSourceValue\n          );\n        }\n        catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      };\n\n      doIndexTest(\n          indexDatasource,\n          INDEX_TASK,\n          azurePropsTransform,\n          INDEX_QUERIES_RESOURCE,\n          false,\n          true,\n          true\n      );\n    }\n  }\n","date":"2020-04-14 04:03:56","endLine":131,"groupId":"22182","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"doTest","params":"(Pair<String@List>azureInputSource)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/8c/a08a0ef1e57df38c69841a3996994bf8f5c546.src","preCode":"  void doTest(Pair<String, List> azureInputSource) throws Exception\n  {\n    final String indexDatasource = \"wikipedia_index_test_\" + UUID.randomUUID();\n    try (\n        final Closeable ignored1 = unloader(indexDatasource + config.getExtraDatasourceNameSuffix());\n    ) {\n      final Function<String, String> azurePropsTransform = spec -> {\n        try {\n          String inputSourceValue = jsonMapper.writeValueAsString(azureInputSource.rhs);\n          inputSourceValue = StringUtils.replace(\n              inputSourceValue,\n              \"%%BUCKET%%\",\n              config.getCloudBucket()\n          );\n          inputSourceValue = StringUtils.replace(\n              inputSourceValue,\n              \"%%PATH%%\",\n              config.getCloudPath()\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_FORMAT_TYPE%%\",\n              InputFormatDetails.JSON.getInputFormatType()\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%PARTITIONS_SPEC%%\",\n              jsonMapper.writeValueAsString(new DynamicPartitionsSpec(null, null))\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_TYPE%%\",\n              \"azure\"\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_PROPERTY_KEY%%\",\n              azureInputSource.lhs\n          );\n          return StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_PROPERTY_VALUE%%\",\n              inputSourceValue\n          );\n        }\n        catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      };\n\n      doIndexTest(\n          indexDatasource,\n          INDEX_TASK,\n          azurePropsTransform,\n          INDEX_QUERIES_RESOURCE,\n          false,\n          true,\n          true\n      );\n    }\n  }\n","realPath":"integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractAzureInputSourceParallelIndexTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":71,"status":"NB"},{"authorDate":"2020-07-02 13:20:53","commitOrder":3,"curCode":"  void doTest(Pair<String, List> inputSource) throws Exception\n  {\n    final String indexDatasource = \"wikipedia_index_test_\" + UUID.randomUUID();\n    try (\n        final Closeable ignored1 = unloader(indexDatasource + config.getExtraDatasourceNameSuffix());\n    ) {\n      final Function<String, String> propsTransform = spec -> {\n        try {\n          String inputSourceValue = jsonMapper.writeValueAsString(inputSource.rhs);\n          inputSourceValue = StringUtils.replace(\n              inputSourceValue,\n              \"%%BUCKET%%\",\n              config.getCloudBucket()\n          );\n          inputSourceValue = StringUtils.replace(\n              inputSourceValue,\n              \"%%PATH%%\",\n              config.getCloudPath()\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_FORMAT_TYPE%%\",\n              InputFormatDetails.JSON.getInputFormatType()\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%PARTITIONS_SPEC%%\",\n              jsonMapper.writeValueAsString(new DynamicPartitionsSpec(null, null))\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_TYPE%%\",\n              \"oss\"\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_PROPERTY_KEY%%\",\n              inputSource.lhs\n          );\n          return StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_PROPERTY_VALUE%%\",\n              inputSourceValue\n          );\n        }\n        catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      };\n\n      doIndexTest(\n          indexDatasource,\n          INDEX_TASK,\n          propsTransform,\n          INDEX_QUERIES_RESOURCE,\n          false,\n          true,\n          true\n      );\n    }\n  }\n","date":"2020-07-02 13:20:53","endLine":131,"groupId":"22182","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"doTest","params":"(Pair<String@List>inputSource)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/72/b0d355b2aaa8e6815133621e009dc18c50680a.src","preCode":"  void doTest(Pair<String, List> inputSource) throws Exception\n  {\n    final String indexDatasource = \"wikipedia_index_test_\" + UUID.randomUUID();\n    try (\n        final Closeable ignored1 = unloader(indexDatasource + config.getExtraDatasourceNameSuffix());\n    ) {\n      final Function<String, String> propsTransform = spec -> {\n        try {\n          String inputSourceValue = jsonMapper.writeValueAsString(inputSource.rhs);\n          inputSourceValue = StringUtils.replace(\n              inputSourceValue,\n              \"%%BUCKET%%\",\n              config.getCloudBucket()\n          );\n          inputSourceValue = StringUtils.replace(\n              inputSourceValue,\n              \"%%PATH%%\",\n              config.getCloudPath()\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_FORMAT_TYPE%%\",\n              InputFormatDetails.JSON.getInputFormatType()\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%PARTITIONS_SPEC%%\",\n              jsonMapper.writeValueAsString(new DynamicPartitionsSpec(null, null))\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_TYPE%%\",\n              \"oss\"\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_PROPERTY_KEY%%\",\n              inputSource.lhs\n          );\n          return StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_PROPERTY_VALUE%%\",\n              inputSourceValue\n          );\n        }\n        catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      };\n\n      doIndexTest(\n          indexDatasource,\n          INDEX_TASK,\n          propsTransform,\n          INDEX_QUERIES_RESOURCE,\n          false,\n          true,\n          true\n      );\n    }\n  }\n","realPath":"integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractOssInputSourceParallelIndexTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":71,"status":"B"}],"commitId":"60c6bd5b4c44f28f5dbff48e70c6138ce35204b6","commitMessage":"@@@support Aliyun OSS service as deep storage (#9898)\n\n* init commit.  all tests passed\n\n* fix format\n\nSigned-off-by: frank chen <frank.chen021@outlook.com>\n\n* data stored successfully\n\n* modify config path\n\n* add doc\n\n* add aliyun-oss extension to project\n\n* remove descriptor deletion code to avoid warning message output by aliyun client\n\n* fix warnings reported by lgtm-com\n\n* fix ci warnings\n\nSigned-off-by: frank chen <frank.chen021@outlook.com>\n\n* fix errors reported by intellj inspection check\n\nSigned-off-by: frank chen <frank.chen021@outlook.com>\n\n* fix doc spelling check\n\nSigned-off-by: frank chen <frank.chen021@outlook.com>\n\n* fix dependency warnings reported by ci\n\nSigned-off-by: frank chen <frank.chen021@outlook.com>\n\n* fix warnings reported by CI\n\nSigned-off-by: frank chen <frank.chen021@outlook.com>\n\n* add package configuration to support showing extension info\n\nSigned-off-by: frank chen <frank.chen021@outlook.com>\n\n* add IT test cases and fix bugs\n\nSigned-off-by: frank chen <frank.chen021@outlook.com>\n\n* 1. code review comments adopted\n2. change schema from 'aliyun-oss' to 'oss'\n\nSigned-off-by: frank chen <frank.chen021@outlook.com>\n\n* add license info\n\nSigned-off-by: frank chen <frank.chen021@outlook.com>\n\n* fix doc\n\nSigned-off-by: frank chen <frank.chen021@outlook.com>\n\n* exclude execution of IT testcases of OSS extension from CI\n\nSigned-off-by: frank chen <frank.chen021@outlook.com>\n\n* put the extensions under contrib group and add to distribution\n\n* fix names in test cases\n\n* add unit test to cover OssInputSource\n\n* fix names in test cases\n\n* fix dependency problem reported by CI\n\nSigned-off-by: frank chen <frank.chen021@outlook.com>","date":"2020-07-02 13:20:53","modifiedFileCount":"1","status":"M","submitter":"frank chen"},{"authorTime":"2021-04-09 12:03:00","codes":[{"authorDate":"2021-04-09 12:03:00","commitOrder":4,"curCode":"  void doTest(\n      Pair<String, List> azureInputSource,\n      Pair<Boolean, Boolean> segmentAvailabilityConfirmationPair\n  ) throws Exception\n  {\n    final String indexDatasource = \"wikipedia_index_test_\" + UUID.randomUUID();\n    try (\n        final Closeable ignored1 = unloader(indexDatasource + config.getExtraDatasourceNameSuffix());\n    ) {\n      final Function<String, String> azurePropsTransform = spec -> {\n        try {\n          String inputSourceValue = jsonMapper.writeValueAsString(azureInputSource.rhs);\n          inputSourceValue = StringUtils.replace(\n              inputSourceValue,\n              \"%%BUCKET%%\",\n              config.getCloudBucket()\n          );\n          inputSourceValue = StringUtils.replace(\n              inputSourceValue,\n              \"%%PATH%%\",\n              config.getCloudPath()\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_FORMAT_TYPE%%\",\n              InputFormatDetails.JSON.getInputFormatType()\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%PARTITIONS_SPEC%%\",\n              jsonMapper.writeValueAsString(new DynamicPartitionsSpec(null, null))\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_TYPE%%\",\n              \"azure\"\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_PROPERTY_KEY%%\",\n              azureInputSource.lhs\n          );\n          return StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_PROPERTY_VALUE%%\",\n              inputSourceValue\n          );\n        }\n        catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      };\n\n      doIndexTest(\n          indexDatasource,\n          INDEX_TASK,\n          azurePropsTransform,\n          INDEX_QUERIES_RESOURCE,\n          false,\n          true,\n          true,\n          segmentAvailabilityConfirmationPair\n      );\n    }\n  }\n","date":"2021-04-09 12:03:00","endLine":135,"groupId":"103652","id":3,"instanceNumber":1,"isCurCommit":1,"methodName":"doTest","params":"(Pair<String@List>azureInputSource@Pair<Boolean@Boolean>segmentAvailabilityConfirmationPair)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/9c/e161f6ccd9fec014b718eee60df5904242a6b0.src","preCode":"  void doTest(Pair<String, List> azureInputSource) throws Exception\n  {\n    final String indexDatasource = \"wikipedia_index_test_\" + UUID.randomUUID();\n    try (\n        final Closeable ignored1 = unloader(indexDatasource + config.getExtraDatasourceNameSuffix());\n    ) {\n      final Function<String, String> azurePropsTransform = spec -> {\n        try {\n          String inputSourceValue = jsonMapper.writeValueAsString(azureInputSource.rhs);\n          inputSourceValue = StringUtils.replace(\n              inputSourceValue,\n              \"%%BUCKET%%\",\n              config.getCloudBucket()\n          );\n          inputSourceValue = StringUtils.replace(\n              inputSourceValue,\n              \"%%PATH%%\",\n              config.getCloudPath()\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_FORMAT_TYPE%%\",\n              InputFormatDetails.JSON.getInputFormatType()\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%PARTITIONS_SPEC%%\",\n              jsonMapper.writeValueAsString(new DynamicPartitionsSpec(null, null))\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_TYPE%%\",\n              \"azure\"\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_PROPERTY_KEY%%\",\n              azureInputSource.lhs\n          );\n          return StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_PROPERTY_VALUE%%\",\n              inputSourceValue\n          );\n        }\n        catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      };\n\n      doIndexTest(\n          indexDatasource,\n          INDEX_TASK,\n          azurePropsTransform,\n          INDEX_QUERIES_RESOURCE,\n          false,\n          true,\n          true\n      );\n    }\n  }\n","realPath":"integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractAzureInputSourceParallelIndexTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":71,"status":"M"},{"authorDate":"2021-04-09 12:03:00","commitOrder":4,"curCode":"  void doTest(\n      Pair<String, List> inputSource,\n      Pair<Boolean, Boolean> segmentAvailabilityConfirmationPair\n  ) throws Exception\n  {\n    final String indexDatasource = \"wikipedia_index_test_\" + UUID.randomUUID();\n    try (\n        final Closeable ignored1 = unloader(indexDatasource + config.getExtraDatasourceNameSuffix());\n    ) {\n      final Function<String, String> propsTransform = spec -> {\n        try {\n          String inputSourceValue = jsonMapper.writeValueAsString(inputSource.rhs);\n          inputSourceValue = StringUtils.replace(\n              inputSourceValue,\n              \"%%BUCKET%%\",\n              config.getCloudBucket()\n          );\n          inputSourceValue = StringUtils.replace(\n              inputSourceValue,\n              \"%%PATH%%\",\n              config.getCloudPath()\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_FORMAT_TYPE%%\",\n              InputFormatDetails.JSON.getInputFormatType()\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%PARTITIONS_SPEC%%\",\n              jsonMapper.writeValueAsString(new DynamicPartitionsSpec(null, null))\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_TYPE%%\",\n              \"oss\"\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_PROPERTY_KEY%%\",\n              inputSource.lhs\n          );\n          return StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_PROPERTY_VALUE%%\",\n              inputSourceValue\n          );\n        }\n        catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      };\n\n      doIndexTest(\n          indexDatasource,\n          INDEX_TASK,\n          propsTransform,\n          INDEX_QUERIES_RESOURCE,\n          false,\n          true,\n          true,\n          segmentAvailabilityConfirmationPair\n      );\n    }\n  }\n","date":"2021-04-09 12:03:00","endLine":135,"groupId":"103652","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"doTest","params":"(Pair<String@List>inputSource@Pair<Boolean@Boolean>segmentAvailabilityConfirmationPair)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/1d/91d8e6b4f8ce39552571c704d3fd16f248d270.src","preCode":"  void doTest(Pair<String, List> inputSource) throws Exception\n  {\n    final String indexDatasource = \"wikipedia_index_test_\" + UUID.randomUUID();\n    try (\n        final Closeable ignored1 = unloader(indexDatasource + config.getExtraDatasourceNameSuffix());\n    ) {\n      final Function<String, String> propsTransform = spec -> {\n        try {\n          String inputSourceValue = jsonMapper.writeValueAsString(inputSource.rhs);\n          inputSourceValue = StringUtils.replace(\n              inputSourceValue,\n              \"%%BUCKET%%\",\n              config.getCloudBucket()\n          );\n          inputSourceValue = StringUtils.replace(\n              inputSourceValue,\n              \"%%PATH%%\",\n              config.getCloudPath()\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_FORMAT_TYPE%%\",\n              InputFormatDetails.JSON.getInputFormatType()\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%PARTITIONS_SPEC%%\",\n              jsonMapper.writeValueAsString(new DynamicPartitionsSpec(null, null))\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_TYPE%%\",\n              \"oss\"\n          );\n          spec = StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_PROPERTY_KEY%%\",\n              inputSource.lhs\n          );\n          return StringUtils.replace(\n              spec,\n              \"%%INPUT_SOURCE_PROPERTY_VALUE%%\",\n              inputSourceValue\n          );\n        }\n        catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      };\n\n      doIndexTest(\n          indexDatasource,\n          INDEX_TASK,\n          propsTransform,\n          INDEX_QUERIES_RESOURCE,\n          false,\n          true,\n          true\n      );\n    }\n  }\n","realPath":"integration-tests/src/test/java/org/apache/druid/tests/indexer/AbstractOssInputSourceParallelIndexTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":71,"status":"M"}],"commitId":"8264203cee688607091232897749e959e7706010","commitMessage":"@@@Allow client to configure batch ingestion task to wait to complete until segments are confirmed to be available by other (#10676)\n\n* Add ability to wait for segment availability for batch jobs\n\n* IT updates\n\n* fix queries in legacy hadoop IT\n\n* Fix broken indexing integration tests\n\n* address an lgtm flag\n\n* spell checker still flagging for hadoop doc. adding under that file header too\n\n* fix compaction IT\n\n* Updates to wait for availability method\n\n* improve unit testing for patch\n\n* fix bad indentation\n\n* refactor waitForSegmentAvailability\n\n* Fixes based off of review comments\n\n* cleanup to get compile after merging with master\n\n* fix failing test after previous logic update\n\n* add back code that must have gotten deleted during conflict resolution\n\n* update some logging code\n\n* fixes to get compilation working after merge with master\n\n* reset interrupt flag in catch block after code review pointed it out\n\n* small changes following self-review\n\n* fixup some issues brought on by merge with master\n\n* small changes after review\n\n* cleanup a little bit after merge with master\n\n* Fix potential resource leak in AbstractBatchIndexTask\n\n* syntax fix\n\n* Add a Compcation TuningConfig type\n\n* add docs stipulating the lack of support by Compaction tasks for the new config\n\n* Fixup compilation errors after merge with master\n\n* Remove erreneous newline","date":"2021-04-09 12:03:00","modifiedFileCount":"106","status":"M","submitter":"Lucas Capistrant"}]
