[{"authorTime":"2019-08-16 08:43:35","codes":[{"authorDate":"2019-08-16 08:43:35","commitOrder":1,"curCode":"  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n  {\n    final FirehoseFactory firehoseFactory = ingestionSchema.getIOConfig().getFirehoseFactory();\n\n    final File firehoseTempDir = toolbox.getFirehoseTemporaryDir();\n    \r\n    FileUtils.forceMkdir(firehoseTempDir);\n\n    final ParallelIndexTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n\n    final List<DataSegment> segments = generateSegments(toolbox, firehoseFactory, firehoseTempDir);\n    final List<PartitionStat> partitionStats = segments\n        .stream()\n        .map(segment -> new PartitionStat(\n            toolbox.getTaskExecutorNode().getHost(),\n            toolbox.getTaskExecutorNode().getPortToUse(),\n            toolbox.getTaskExecutorNode().isEnableTlsPort(),\n            segment.getInterval(),\n            segment.getShardSpec().getPartitionNum(),\n            null, \r\n            null  \r\n        ))\n        .collect(Collectors.toList());\n    taskClient.report(supervisorTaskId, new GeneratedPartitionsReport(getId(), partitionStats));\n\n    return TaskStatus.success(getId());\n  }\n","date":"2019-08-16 08:43:35","endLine":239,"groupId":"22052","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"runTask","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d7/8e337c5dd7d1b2c12b9cabee44772b35becbc5.src","preCode":"  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n  {\n    final FirehoseFactory firehoseFactory = ingestionSchema.getIOConfig().getFirehoseFactory();\n\n    final File firehoseTempDir = toolbox.getFirehoseTemporaryDir();\n    \r\n    FileUtils.forceMkdir(firehoseTempDir);\n\n    final ParallelIndexTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n\n    final List<DataSegment> segments = generateSegments(toolbox, firehoseFactory, firehoseTempDir);\n    final List<PartitionStat> partitionStats = segments\n        .stream()\n        .map(segment -> new PartitionStat(\n            toolbox.getTaskExecutorNode().getHost(),\n            toolbox.getTaskExecutorNode().getPortToUse(),\n            toolbox.getTaskExecutorNode().isEnableTlsPort(),\n            segment.getInterval(),\n            segment.getShardSpec().getPartitionNum(),\n            null, \r\n            null  \r\n        ))\n        .collect(Collectors.toList());\n    taskClient.report(supervisorTaskId, new GeneratedPartitionsReport(getId(), partitionStats));\n\n    return TaskStatus.success(getId());\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialSegmentGenerateTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":207,"status":"B"},{"authorDate":"2019-08-16 08:43:35","commitOrder":1,"curCode":"  public TaskStatus runTask(final TaskToolbox toolbox) throws Exception\n  {\n    if (missingIntervalsInOverwriteMode) {\n      LOG.warn(\n          \"Intervals are missing in granularitySpec while this task is potentially overwriting existing segments. \"\n          + \"Forced to use timeChunk lock.\"\n      );\n    }\n    final FirehoseFactory firehoseFactory = ingestionSchema.getIOConfig().getFirehoseFactory();\n\n    final File firehoseTempDir = toolbox.getFirehoseTemporaryDir();\n    \r\n    FileUtils.forceMkdir(firehoseTempDir);\n\n    final ParallelIndexTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n    final Set<DataSegment> pushedSegments = generateAndPushSegments(\n        toolbox,\n        taskClient,\n        firehoseFactory,\n        firehoseTempDir\n    );\n\n    \r\n    final Set<DataSegment> allSegments = new HashSet<>(getSegmentLockHelper().getLockedExistingSegments());\n    allSegments.addAll(pushedSegments);\n    final VersionedIntervalTimeline<String, DataSegment> timeline = VersionedIntervalTimeline.forSegments(allSegments);\n    final Set<DataSegment> oldSegments = timeline.findFullyOvershadowed()\n                                                 .stream()\n                                                 .flatMap(holder -> holder.getObject().stream())\n                                                 .map(PartitionChunk::getObject)\n                                                 .collect(Collectors.toSet());\n    taskClient.report(supervisorTaskId, new PushedSegmentsReport(getId(), oldSegments, pushedSegments));\n\n    return TaskStatus.success(getId());\n  }\n","date":"2019-08-16 08:43:35","endLine":243,"groupId":"13980","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"runTask","params":"(finalTaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e5/5c4731fa7668544f6d7186e7fb2ea400a294fa.src","preCode":"  public TaskStatus runTask(final TaskToolbox toolbox) throws Exception\n  {\n    if (missingIntervalsInOverwriteMode) {\n      LOG.warn(\n          \"Intervals are missing in granularitySpec while this task is potentially overwriting existing segments. \"\n          + \"Forced to use timeChunk lock.\"\n      );\n    }\n    final FirehoseFactory firehoseFactory = ingestionSchema.getIOConfig().getFirehoseFactory();\n\n    final File firehoseTempDir = toolbox.getFirehoseTemporaryDir();\n    \r\n    FileUtils.forceMkdir(firehoseTempDir);\n\n    final ParallelIndexTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n    final Set<DataSegment> pushedSegments = generateAndPushSegments(\n        toolbox,\n        taskClient,\n        firehoseFactory,\n        firehoseTempDir\n    );\n\n    \r\n    final Set<DataSegment> allSegments = new HashSet<>(getSegmentLockHelper().getLockedExistingSegments());\n    allSegments.addAll(pushedSegments);\n    final VersionedIntervalTimeline<String, DataSegment> timeline = VersionedIntervalTimeline.forSegments(allSegments);\n    final Set<DataSegment> oldSegments = timeline.findFullyOvershadowed()\n                                                 .stream()\n                                                 .flatMap(holder -> holder.getObject().stream())\n                                                 .map(PartitionChunk::getObject)\n                                                 .collect(Collectors.toSet());\n    taskClient.report(supervisorTaskId, new PushedSegmentsReport(getId(), oldSegments, pushedSegments));\n\n    return TaskStatus.success(getId());\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/SinglePhaseSubTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":203,"status":"B"}],"commitId":"5dac6375f382ba485ec000802ff0e922929df95d","commitMessage":"@@@Add support for parallel native indexing with shuffle for perfect rollup (#8257)\n\n* Add TaskResourceCleaner; fix a couple of concurrency bugs in batch tasks\n\n* kill runner when it's ready\n\n* add comment\n\n* kill run thread\n\n* fix test\n\n* Take closeable out of Appenderator\n\n* add javadoc\n\n* fix test\n\n* fix test\n\n* update javadoc\n\n* add javadoc about killed task\n\n* address comment\n\n* Add support for parallel native indexing with shuffle for perfect rollup.\n\n* Add comment about volatiles\n\n* fix test\n\n* fix test\n\n* handling missing exceptions\n\n* more clear javadoc for stopGracefully\n\n* unused import\n\n* update javadoc\n\n* Add missing statement in javadoc\n\n* address comments; fix doc\n\n* add javadoc for isGuaranteedRollup\n\n* Rename confusing variable name and fix typos\n\n* fix typos; move fetch() to a better home; fix the expiration time\n\n* add support https\n","date":"2019-08-16 08:43:35","modifiedFileCount":"49","status":"B","submitter":"Jihoon Son"},{"authorTime":"2019-08-27 10:27:41","codes":[{"authorDate":"2019-08-27 10:27:41","commitOrder":2,"curCode":"  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n  {\n    final FirehoseFactory firehoseFactory = ingestionSchema.getIOConfig().getFirehoseFactory();\n\n    final File firehoseTempDir = toolbox.getFirehoseTemporaryDir();\n    \r\n    FileUtils.forceMkdir(firehoseTempDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n\n    final List<DataSegment> segments = generateSegments(toolbox, firehoseFactory, firehoseTempDir);\n    final List<PartitionStat> partitionStats = segments\n        .stream()\n        .map(segment -> new PartitionStat(\n            toolbox.getTaskExecutorNode().getHost(),\n            toolbox.getTaskExecutorNode().getPortToUse(),\n            toolbox.getTaskExecutorNode().isEnableTlsPort(),\n            segment.getInterval(),\n            segment.getShardSpec().getPartitionNum(),\n            null, \r\n            null  \r\n        ))\n        .collect(Collectors.toList());\n    taskClient.report(supervisorTaskId, new GeneratedPartitionsReport(getId(), partitionStats));\n\n    return TaskStatus.success(getId());\n  }\n","date":"2019-08-27 10:27:41","endLine":239,"groupId":"22052","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"runTask","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/34/a93599b7e0fd9fecf251d0c82f10eecbfaacb7.src","preCode":"  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n  {\n    final FirehoseFactory firehoseFactory = ingestionSchema.getIOConfig().getFirehoseFactory();\n\n    final File firehoseTempDir = toolbox.getFirehoseTemporaryDir();\n    \r\n    FileUtils.forceMkdir(firehoseTempDir);\n\n    final ParallelIndexTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n\n    final List<DataSegment> segments = generateSegments(toolbox, firehoseFactory, firehoseTempDir);\n    final List<PartitionStat> partitionStats = segments\n        .stream()\n        .map(segment -> new PartitionStat(\n            toolbox.getTaskExecutorNode().getHost(),\n            toolbox.getTaskExecutorNode().getPortToUse(),\n            toolbox.getTaskExecutorNode().isEnableTlsPort(),\n            segment.getInterval(),\n            segment.getShardSpec().getPartitionNum(),\n            null, \r\n            null  \r\n        ))\n        .collect(Collectors.toList());\n    taskClient.report(supervisorTaskId, new GeneratedPartitionsReport(getId(), partitionStats));\n\n    return TaskStatus.success(getId());\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialSegmentGenerateTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":207,"status":"M"},{"authorDate":"2019-08-27 10:27:41","commitOrder":2,"curCode":"  public TaskStatus runTask(final TaskToolbox toolbox) throws Exception\n  {\n    if (missingIntervalsInOverwriteMode) {\n      LOG.warn(\n          \"Intervals are missing in granularitySpec while this task is potentially overwriting existing segments. \"\n          + \"Forced to use timeChunk lock.\"\n      );\n    }\n    final FirehoseFactory firehoseFactory = ingestionSchema.getIOConfig().getFirehoseFactory();\n\n    final File firehoseTempDir = toolbox.getFirehoseTemporaryDir();\n    \r\n    FileUtils.forceMkdir(firehoseTempDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n    final Set<DataSegment> pushedSegments = generateAndPushSegments(\n        toolbox,\n        taskClient,\n        firehoseFactory,\n        firehoseTempDir\n    );\n\n    \r\n    final Set<DataSegment> allSegments = new HashSet<>(getSegmentLockHelper().getLockedExistingSegments());\n    allSegments.addAll(pushedSegments);\n    final VersionedIntervalTimeline<String, DataSegment> timeline = VersionedIntervalTimeline.forSegments(allSegments);\n    final Set<DataSegment> oldSegments = timeline.findFullyOvershadowed()\n                                                 .stream()\n                                                 .flatMap(holder -> holder.getObject().stream())\n                                                 .map(PartitionChunk::getObject)\n                                                 .collect(Collectors.toSet());\n    taskClient.report(supervisorTaskId, new PushedSegmentsReport(getId(), oldSegments, pushedSegments));\n\n    return TaskStatus.success(getId());\n  }\n","date":"2019-08-27 10:27:41","endLine":243,"groupId":"13980","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"runTask","params":"(finalTaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/9b/2d43d041ca7d3e6444736d717f91e451f7992a.src","preCode":"  public TaskStatus runTask(final TaskToolbox toolbox) throws Exception\n  {\n    if (missingIntervalsInOverwriteMode) {\n      LOG.warn(\n          \"Intervals are missing in granularitySpec while this task is potentially overwriting existing segments. \"\n          + \"Forced to use timeChunk lock.\"\n      );\n    }\n    final FirehoseFactory firehoseFactory = ingestionSchema.getIOConfig().getFirehoseFactory();\n\n    final File firehoseTempDir = toolbox.getFirehoseTemporaryDir();\n    \r\n    FileUtils.forceMkdir(firehoseTempDir);\n\n    final ParallelIndexTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n    final Set<DataSegment> pushedSegments = generateAndPushSegments(\n        toolbox,\n        taskClient,\n        firehoseFactory,\n        firehoseTempDir\n    );\n\n    \r\n    final Set<DataSegment> allSegments = new HashSet<>(getSegmentLockHelper().getLockedExistingSegments());\n    allSegments.addAll(pushedSegments);\n    final VersionedIntervalTimeline<String, DataSegment> timeline = VersionedIntervalTimeline.forSegments(allSegments);\n    final Set<DataSegment> oldSegments = timeline.findFullyOvershadowed()\n                                                 .stream()\n                                                 .flatMap(holder -> holder.getObject().stream())\n                                                 .map(PartitionChunk::getObject)\n                                                 .collect(Collectors.toSet());\n    taskClient.report(supervisorTaskId, new PushedSegmentsReport(getId(), oldSegments, pushedSegments));\n\n    return TaskStatus.success(getId());\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/SinglePhaseSubTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":203,"status":"M"}],"commitId":"e5ef5ddafa632256fe8a387a117557c415bee7df","commitMessage":"@@@Fix the shuffle with TLS enabled for parallel indexing; add an integration test; improve unit tests (#8350)\n\n* Fix shuffle with tls enabled; add an integration test; improve unit tests\n\n* remove debug log\n\n* fix tests\n\n* unused import\n\n* add javadoc\n\n* rename to getContent\n","date":"2019-08-27 10:27:41","modifiedFileCount":"37","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-11-16 01:22:09","codes":[{"authorDate":"2019-11-16 01:22:09","commitOrder":3,"curCode":"  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n  {\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final File tmpDir = toolbox.getIndexingTmpDir();\n    \r\n    FileUtils.forceMkdir(tmpDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n\n    final List<DataSegment> segments = generateSegments(toolbox, inputSource, tmpDir);\n    final List<PartitionStat> partitionStats = segments\n        .stream()\n        .map(segment -> new PartitionStat(\n            toolbox.getTaskExecutorNode().getHost(),\n            toolbox.getTaskExecutorNode().getPortToUse(),\n            toolbox.getTaskExecutorNode().isEnableTlsPort(),\n            segment.getInterval(),\n            segment.getShardSpec().getPartitionNum(),\n            null, \r\n            null  \r\n        ))\n        .collect(Collectors.toList());\n    taskClient.report(supervisorTaskId, new GeneratedPartitionsReport(getId(), partitionStats));\n\n    return TaskStatus.success(getId());\n  }\n","date":"2019-11-16 01:22:09","endLine":241,"groupId":"917","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"runTask","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/af/1772fc3dc8fa2cfb5da9616b026475bd17c07a.src","preCode":"  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n  {\n    final FirehoseFactory firehoseFactory = ingestionSchema.getIOConfig().getFirehoseFactory();\n\n    final File firehoseTempDir = toolbox.getFirehoseTemporaryDir();\n    \r\n    FileUtils.forceMkdir(firehoseTempDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n\n    final List<DataSegment> segments = generateSegments(toolbox, firehoseFactory, firehoseTempDir);\n    final List<PartitionStat> partitionStats = segments\n        .stream()\n        .map(segment -> new PartitionStat(\n            toolbox.getTaskExecutorNode().getHost(),\n            toolbox.getTaskExecutorNode().getPortToUse(),\n            toolbox.getTaskExecutorNode().isEnableTlsPort(),\n            segment.getInterval(),\n            segment.getShardSpec().getPartitionNum(),\n            null, \r\n            null  \r\n        ))\n        .collect(Collectors.toList());\n    taskClient.report(supervisorTaskId, new GeneratedPartitionsReport(getId(), partitionStats));\n\n    return TaskStatus.success(getId());\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialSegmentGenerateTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":207,"status":"M"},{"authorDate":"2019-11-16 01:22:09","commitOrder":3,"curCode":"  public TaskStatus runTask(final TaskToolbox toolbox) throws Exception\n  {\n    if (missingIntervalsInOverwriteMode) {\n      LOG.warn(\n          \"Intervals are missing in granularitySpec while this task is potentially overwriting existing segments. \"\n          + \"Forced to use timeChunk lock.\"\n      );\n    }\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final File tmpDir = toolbox.getIndexingTmpDir();\n    \r\n    FileUtils.forceMkdir(tmpDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n    final Set<DataSegment> pushedSegments = generateAndPushSegments(\n        toolbox,\n        taskClient,\n        inputSource,\n        tmpDir\n    );\n\n    \r\n    final Set<DataSegment> allSegments = new HashSet<>(getSegmentLockHelper().getLockedExistingSegments());\n    allSegments.addAll(pushedSegments);\n    final VersionedIntervalTimeline<String, DataSegment> timeline = VersionedIntervalTimeline.forSegments(allSegments);\n    final Set<DataSegment> oldSegments = timeline.findFullyOvershadowed()\n                                                 .stream()\n                                                 .flatMap(holder -> holder.getObject().stream())\n                                                 .map(PartitionChunk::getObject)\n                                                 .collect(Collectors.toSet());\n    taskClient.report(supervisorTaskId, new PushedSegmentsReport(getId(), oldSegments, pushedSegments));\n\n    return TaskStatus.success(getId());\n  }\n","date":"2019-11-16 01:22:09","endLine":249,"groupId":"15604","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"runTask","params":"(finalTaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/64/08299d084f9517b7ff15511aaf174333d6bc89.src","preCode":"  public TaskStatus runTask(final TaskToolbox toolbox) throws Exception\n  {\n    if (missingIntervalsInOverwriteMode) {\n      LOG.warn(\n          \"Intervals are missing in granularitySpec while this task is potentially overwriting existing segments. \"\n          + \"Forced to use timeChunk lock.\"\n      );\n    }\n    final FirehoseFactory firehoseFactory = ingestionSchema.getIOConfig().getFirehoseFactory();\n\n    final File firehoseTempDir = toolbox.getFirehoseTemporaryDir();\n    \r\n    FileUtils.forceMkdir(firehoseTempDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n    final Set<DataSegment> pushedSegments = generateAndPushSegments(\n        toolbox,\n        taskClient,\n        firehoseFactory,\n        firehoseTempDir\n    );\n\n    \r\n    final Set<DataSegment> allSegments = new HashSet<>(getSegmentLockHelper().getLockedExistingSegments());\n    allSegments.addAll(pushedSegments);\n    final VersionedIntervalTimeline<String, DataSegment> timeline = VersionedIntervalTimeline.forSegments(allSegments);\n    final Set<DataSegment> oldSegments = timeline.findFullyOvershadowed()\n                                                 .stream()\n                                                 .flatMap(holder -> holder.getObject().stream())\n                                                 .map(PartitionChunk::getObject)\n                                                 .collect(Collectors.toSet());\n    taskClient.report(supervisorTaskId, new PushedSegmentsReport(getId(), oldSegments, pushedSegments));\n\n    return TaskStatus.success(getId());\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/SinglePhaseSubTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":207,"status":"M"}],"commitId":"1611792855ad9def8b6f5b1375862d05d1acca0a","commitMessage":"@@@Add InputSource and InputFormat interfaces (#8823)\n\n* Add InputSource and InputFormat interfaces\n\n* revert orc dependency\n\n* fix dimension exclusions and failing unit tests\n\n* fix tests\n\n* fix test\n\n* fix test\n\n* fix firehose and inputSource for parallel indexing task\n\n* fix tc\n\n* fix tc: remove unused method\n\n* Formattable\n\n* add needsFormat(); renamed to ObjectSource; pass metricsName for reader\n\n* address comments\n\n* fix closing resource\n\n* fix checkstyle\n\n* fix tests\n\n* remove verify from csv\n\n* Revert \"remove verify from csv\"\n\nThis reverts commit 1ea7758489cc8c9d708bd691fd48e62085fd9455.\n\n* address comments\n\n* fix import order and javadoc\n\n* flatMap\n\n* sampleLine\n\n* Add IntermediateRowParsingReader\n\n* Address comments\n\n* move csv reader test\n\n* remove test for verify\n\n* adjust comments\n\n* Fix InputEntityIteratingReader\n\n* rename source -> entity\n\n* address comments\n","date":"2019-11-16 01:22:09","modifiedFileCount":"72","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-11-16 01:22:09","codes":[{"authorDate":"2019-11-21 09:24:12","commitOrder":4,"curCode":"  public final TaskStatus runTask(TaskToolbox toolbox) throws Exception\n  {\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final File tmpDir = toolbox.getIndexingTmpDir();\n    \r\n    FileUtils.forceMkdir(tmpDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n\n    final List<DataSegment> segments = generateSegments(toolbox, inputSource, tmpDir);\n    taskClient.report(supervisorTaskId, createGeneratedPartitionsReport(toolbox, segments));\n\n    return TaskStatus.success(getId());\n  }\n","date":"2019-11-21 09:24:12","endLine":124,"groupId":"917","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"runTask","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/4d/abf7ea02c53820515ed47fa74a20932199fb54.src","preCode":"  public TaskStatus runTask(TaskToolbox toolbox) throws Exception\n  {\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final File tmpDir = toolbox.getIndexingTmpDir();\n    \r\n    FileUtils.forceMkdir(tmpDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n\n    final List<DataSegment> segments = generateSegments(toolbox, inputSource, tmpDir);\n    final List<PartitionStat> partitionStats = segments\n        .stream()\n        .map(segment -> new PartitionStat(\n            toolbox.getTaskExecutorNode().getHost(),\n            toolbox.getTaskExecutorNode().getPortToUse(),\n            toolbox.getTaskExecutorNode().isEnableTlsPort(),\n            segment.getInterval(),\n            segment.getShardSpec().getPartitionNum(),\n            null, \r\n            null  \r\n        ))\n        .collect(Collectors.toList());\n    taskClient.report(supervisorTaskId, new GeneratedPartitionsReport(getId(), partitionStats));\n\n    return TaskStatus.success(getId());\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialSegmentGenerateTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":102,"status":"M"},{"authorDate":"2019-11-16 01:22:09","commitOrder":4,"curCode":"  public TaskStatus runTask(final TaskToolbox toolbox) throws Exception\n  {\n    if (missingIntervalsInOverwriteMode) {\n      LOG.warn(\n          \"Intervals are missing in granularitySpec while this task is potentially overwriting existing segments. \"\n          + \"Forced to use timeChunk lock.\"\n      );\n    }\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final File tmpDir = toolbox.getIndexingTmpDir();\n    \r\n    FileUtils.forceMkdir(tmpDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n    final Set<DataSegment> pushedSegments = generateAndPushSegments(\n        toolbox,\n        taskClient,\n        inputSource,\n        tmpDir\n    );\n\n    \r\n    final Set<DataSegment> allSegments = new HashSet<>(getSegmentLockHelper().getLockedExistingSegments());\n    allSegments.addAll(pushedSegments);\n    final VersionedIntervalTimeline<String, DataSegment> timeline = VersionedIntervalTimeline.forSegments(allSegments);\n    final Set<DataSegment> oldSegments = timeline.findFullyOvershadowed()\n                                                 .stream()\n                                                 .flatMap(holder -> holder.getObject().stream())\n                                                 .map(PartitionChunk::getObject)\n                                                 .collect(Collectors.toSet());\n    taskClient.report(supervisorTaskId, new PushedSegmentsReport(getId(), oldSegments, pushedSegments));\n\n    return TaskStatus.success(getId());\n  }\n","date":"2019-11-16 01:22:09","endLine":249,"groupId":"15604","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"runTask","params":"(finalTaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/64/08299d084f9517b7ff15511aaf174333d6bc89.src","preCode":"  public TaskStatus runTask(final TaskToolbox toolbox) throws Exception\n  {\n    if (missingIntervalsInOverwriteMode) {\n      LOG.warn(\n          \"Intervals are missing in granularitySpec while this task is potentially overwriting existing segments. \"\n          + \"Forced to use timeChunk lock.\"\n      );\n    }\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final File tmpDir = toolbox.getIndexingTmpDir();\n    \r\n    FileUtils.forceMkdir(tmpDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n    final Set<DataSegment> pushedSegments = generateAndPushSegments(\n        toolbox,\n        taskClient,\n        inputSource,\n        tmpDir\n    );\n\n    \r\n    final Set<DataSegment> allSegments = new HashSet<>(getSegmentLockHelper().getLockedExistingSegments());\n    allSegments.addAll(pushedSegments);\n    final VersionedIntervalTimeline<String, DataSegment> timeline = VersionedIntervalTimeline.forSegments(allSegments);\n    final Set<DataSegment> oldSegments = timeline.findFullyOvershadowed()\n                                                 .stream()\n                                                 .flatMap(holder -> holder.getObject().stream())\n                                                 .map(PartitionChunk::getObject)\n                                                 .collect(Collectors.toSet());\n    taskClient.report(supervisorTaskId, new PushedSegmentsReport(getId(), oldSegments, pushedSegments));\n\n    return TaskStatus.success(getId());\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/SinglePhaseSubTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":207,"status":"N"}],"commitId":"ff6217365bb5803dae364dddbe7bfc870bd406bc","commitMessage":"@@@Refactor parallel indexing perfect rollup partitioning (#8852)\n\n* Refactor parallel indexing perfect rollup partitioning\n\nRefactoring to make it easier to later add range partitioning for\nperfect rollup parallel indexing. This is accomplished by adding several\nnew base classes (e.g..  PerfectRollupWorkerTask) and new classes for\nencapsulating logic that needs to be changed for different partitioning\nstrategies (e.g..  IndexTaskInputRowIteratorBuilder).\n\nThe code is functionally equivalent to before except for the following\nsmall behavior changes:\n\n1) PartialSegmentMergeTask: Previously.  this task had a priority of\n   DEFAULT_TASK_PRIORITY. It now has a priority of\n   DEFAULT_BATCH_INDEX_TASK_PRIORITY (via the new PerfectRollupWorkerTask\n   base class).  since it is a batch index task.\n\n2) ParallelIndexPhaseRunner: A decorator was added to\n   subTaskSpecIterator to ensure the subtasks are generated with unique\n   ids. Previously.  only tests (i.e..  MultiPhaseParallelIndexingTest)\n   would have this decorator.  but this behavior is desired for non-test\n   code as well.\n\n* Fix forbidden apis and pmd warnings\n\n* Fix analyze dependencies warnings\n\n* Fix IndexTask json and add IT diags\n\n* Fix parallel index supervisor<->worker serde\n\n* Fix TeamCity inspection errors/warnings\n\n* Fix TeamCity inspection errors/warnings again\n\n* Integrate changes with those from #8823\n\n* Address review comments\n\n* Address more review comments\n\n* Fix forbidden apis\n\n* Address more review comments\n","date":"2019-11-21 09:24:12","modifiedFileCount":"34","status":"M","submitter":"Chi Cao Minh"},{"authorTime":"2020-02-08 08:23:07","codes":[{"authorDate":"2020-02-08 08:23:07","commitOrder":5,"curCode":"  public final TaskStatus runTask(TaskToolbox toolbox) throws Exception\n  {\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final File tmpDir = toolbox.getIndexingTmpDir();\n    \r\n    FileUtils.forceMkdir(tmpDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n\n    final List<DataSegment> segments = generateSegments(toolbox, taskClient, inputSource, tmpDir);\n    taskClient.report(supervisorTaskId, createGeneratedPartitionsReport(toolbox, segments));\n\n    return TaskStatus.success(getId());\n  }\n","date":"2020-02-08 08:23:07","endLine":127,"groupId":"917","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"runTask","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/65/e570a9d84d58f3b7b0b3cbaec3fb37cdd21aea.src","preCode":"  public final TaskStatus runTask(TaskToolbox toolbox) throws Exception\n  {\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final File tmpDir = toolbox.getIndexingTmpDir();\n    \r\n    FileUtils.forceMkdir(tmpDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n\n    final List<DataSegment> segments = generateSegments(toolbox, inputSource, tmpDir);\n    taskClient.report(supervisorTaskId, createGeneratedPartitionsReport(toolbox, segments));\n\n    return TaskStatus.success(getId());\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialSegmentGenerateTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":105,"status":"M"},{"authorDate":"2020-02-08 08:23:07","commitOrder":5,"curCode":"  public TaskStatus runTask(final TaskToolbox toolbox) throws Exception\n  {\n    if (missingIntervalsInOverwriteMode) {\n      LOG.warn(\n          \"Intervals are missing in granularitySpec while this task is potentially overwriting existing segments. \"\n          + \"Forced to use timeChunk lock.\"\n      );\n    }\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final File tmpDir = toolbox.getIndexingTmpDir();\n    \r\n    FileUtils.forceMkdir(tmpDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n    final Set<DataSegment> pushedSegments = generateAndPushSegments(\n        toolbox,\n        taskClient,\n        inputSource,\n        tmpDir\n    );\n\n    \r\n    final Set<DataSegment> allSegments = new HashSet<>(getTaskLockHelper().getLockedExistingSegments());\n    allSegments.addAll(pushedSegments);\n    final VersionedIntervalTimeline<String, DataSegment> timeline = VersionedIntervalTimeline.forSegments(allSegments);\n    final Set<DataSegment> oldSegments = timeline.findFullyOvershadowed()\n                                                 .stream()\n                                                 .flatMap(holder -> holder.getObject().stream())\n                                                 .map(PartitionChunk::getObject)\n                                                 .collect(Collectors.toSet());\n    taskClient.report(supervisorTaskId, new PushedSegmentsReport(getId(), oldSegments, pushedSegments));\n\n    return TaskStatus.success(getId());\n  }\n","date":"2020-02-08 08:23:07","endLine":234,"groupId":"15604","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"runTask","params":"(finalTaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5b/9d1f5c34b44723e7f2209deb4f7915c397355e.src","preCode":"  public TaskStatus runTask(final TaskToolbox toolbox) throws Exception\n  {\n    if (missingIntervalsInOverwriteMode) {\n      LOG.warn(\n          \"Intervals are missing in granularitySpec while this task is potentially overwriting existing segments. \"\n          + \"Forced to use timeChunk lock.\"\n      );\n    }\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final File tmpDir = toolbox.getIndexingTmpDir();\n    \r\n    FileUtils.forceMkdir(tmpDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n    final Set<DataSegment> pushedSegments = generateAndPushSegments(\n        toolbox,\n        taskClient,\n        inputSource,\n        tmpDir\n    );\n\n    \r\n    final Set<DataSegment> allSegments = new HashSet<>(getSegmentLockHelper().getLockedExistingSegments());\n    allSegments.addAll(pushedSegments);\n    final VersionedIntervalTimeline<String, DataSegment> timeline = VersionedIntervalTimeline.forSegments(allSegments);\n    final Set<DataSegment> oldSegments = timeline.findFullyOvershadowed()\n                                                 .stream()\n                                                 .flatMap(holder -> holder.getObject().stream())\n                                                 .map(PartitionChunk::getObject)\n                                                 .collect(Collectors.toSet());\n    taskClient.report(supervisorTaskId, new PushedSegmentsReport(getId(), oldSegments, pushedSegments));\n\n    return TaskStatus.success(getId());\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/SinglePhaseSubTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":192,"status":"M"}],"commitId":"e81230f9abd092b2ac742258abf534f6e17f35e8","commitMessage":"@@@Refactoring some codes around ingestion (#9274)\n\n* Refactoring codes around ingestion:\n\n- Parallel index task and simple task now use the same segment allocator implementation. This is reusable for the future implementation as well.\n- Added PartitionAnalysis to store the analysis of the partitioning\n- Move some util methods to SegmentLockHelper and rename it to TaskLockHelper\n\n* fix build\n\n* fix SingleDimensionShardSpecFactory\n\n* optimize SingledimensionShardSpecFactory\n\n* fix test\n\n* shard spec builder\n\n* import order\n\n* shardSpecBuilder -> partialShardSpec\n\n* build -> complete\n\n* fix comment; add unit tests for partitionBoundaries\n\n* add more tests and fix javadoc\n\n* fix toString(); add serde tests for HashBasedNumberedPartialShardSpec and SegmentAllocateAction\n\n* fix test\n\n* add equality test for hash and range partial shard specs\n","date":"2020-02-08 08:23:07","modifiedFileCount":"49","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-03-11 04:22:19","codes":[{"authorDate":"2020-02-08 08:23:07","commitOrder":6,"curCode":"  public final TaskStatus runTask(TaskToolbox toolbox) throws Exception\n  {\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final File tmpDir = toolbox.getIndexingTmpDir();\n    \r\n    FileUtils.forceMkdir(tmpDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n\n    final List<DataSegment> segments = generateSegments(toolbox, taskClient, inputSource, tmpDir);\n    taskClient.report(supervisorTaskId, createGeneratedPartitionsReport(toolbox, segments));\n\n    return TaskStatus.success(getId());\n  }\n","date":"2020-02-08 08:23:07","endLine":127,"groupId":"917","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"runTask","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/65/e570a9d84d58f3b7b0b3cbaec3fb37cdd21aea.src","preCode":"  public final TaskStatus runTask(TaskToolbox toolbox) throws Exception\n  {\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final File tmpDir = toolbox.getIndexingTmpDir();\n    \r\n    FileUtils.forceMkdir(tmpDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n\n    final List<DataSegment> segments = generateSegments(toolbox, taskClient, inputSource, tmpDir);\n    taskClient.report(supervisorTaskId, createGeneratedPartitionsReport(toolbox, segments));\n\n    return TaskStatus.success(getId());\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialSegmentGenerateTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":105,"status":"N"},{"authorDate":"2020-03-11 04:22:19","commitOrder":6,"curCode":"  public TaskStatus runTask(final TaskToolbox toolbox) throws Exception\n  {\n    if (missingIntervalsInOverwriteMode) {\n      LOG.warn(\n          \"Intervals are missing in granularitySpec while this task is potentially overwriting existing segments. \"\n          + \"Forced to use timeChunk lock.\"\n      );\n    }\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final File tmpDir = toolbox.getIndexingTmpDir();\n    \r\n    FileUtils.forceMkdir(tmpDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n    final Set<DataSegment> pushedSegments = generateAndPushSegments(\n        toolbox,\n        taskClient,\n        inputSource,\n        tmpDir\n    );\n\n    \r\n    final Set<DataSegment> allSegments = new HashSet<>(getTaskLockHelper().getLockedExistingSegments());\n    allSegments.addAll(pushedSegments);\n    final VersionedIntervalTimeline<String, DataSegment> timeline = VersionedIntervalTimeline.forSegments(allSegments);\n    final Set<DataSegment> oldSegments = FluentIterable.from(timeline.findFullyOvershadowed())\n                                                       .transformAndConcat(TimelineObjectHolder::getObject)\n                                                       .transform(PartitionChunk::getObject)\n                                                       .toSet();\n    taskClient.report(supervisorTaskId, new PushedSegmentsReport(getId(), oldSegments, pushedSegments));\n\n    return TaskStatus.success(getId());\n  }\n","date":"2020-03-11 04:22:19","endLine":235,"groupId":"15604","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"runTask","params":"(finalTaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/9f/1dc52c1ff4f31470dc5b820ec0d548e22ef467.src","preCode":"  public TaskStatus runTask(final TaskToolbox toolbox) throws Exception\n  {\n    if (missingIntervalsInOverwriteMode) {\n      LOG.warn(\n          \"Intervals are missing in granularitySpec while this task is potentially overwriting existing segments. \"\n          + \"Forced to use timeChunk lock.\"\n      );\n    }\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final File tmpDir = toolbox.getIndexingTmpDir();\n    \r\n    FileUtils.forceMkdir(tmpDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n    final Set<DataSegment> pushedSegments = generateAndPushSegments(\n        toolbox,\n        taskClient,\n        inputSource,\n        tmpDir\n    );\n\n    \r\n    final Set<DataSegment> allSegments = new HashSet<>(getTaskLockHelper().getLockedExistingSegments());\n    allSegments.addAll(pushedSegments);\n    final VersionedIntervalTimeline<String, DataSegment> timeline = VersionedIntervalTimeline.forSegments(allSegments);\n    final Set<DataSegment> oldSegments = timeline.findFullyOvershadowed()\n                                                 .stream()\n                                                 .flatMap(holder -> holder.getObject().stream())\n                                                 .map(PartitionChunk::getObject)\n                                                 .collect(Collectors.toSet());\n    taskClient.report(supervisorTaskId, new PushedSegmentsReport(getId(), oldSegments, pushedSegments));\n\n    return TaskStatus.success(getId());\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/SinglePhaseSubTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":194,"status":"M"}],"commitId":"7401bb3f9315bf0622b6e0fe7c7fd855d1c65052","commitMessage":"@@@Improve OvershadowableManager performance (#9441)\n\n* Use the iterator instead of higherKey(); use the iterator API instead of stream\n\n* Fix tests; fix a concurrency bug in timeline\n\n* fix test\n\n* add tests for findNonOvershadowedObjectsInInterval\n\n* fix test\n\n* add missing tests; fix a bug in QueueEntry\n\n* equals tests\n\n* fix test","date":"2020-03-11 04:22:19","modifiedFileCount":"18","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-06-26 05:41:22","codes":[{"authorDate":"2020-06-26 05:41:22","commitOrder":7,"curCode":"  public final TaskStatus runTask(TaskToolbox toolbox) throws Exception\n  {\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n\n    final List<DataSegment> segments = generateSegments(\n        toolbox,\n        taskClient,\n        inputSource,\n        toolbox.getIndexingTmpDir()\n    );\n    taskClient.report(supervisorTaskId, createGeneratedPartitionsReport(toolbox, segments));\n\n    return TaskStatus.success(getId());\n  }\n","date":"2020-06-26 05:41:22","endLine":126,"groupId":"22245","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"runTask","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/44/b788e44dfd45f3becd7ab9406c07ce84944db0.src","preCode":"  public final TaskStatus runTask(TaskToolbox toolbox) throws Exception\n  {\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final File tmpDir = toolbox.getIndexingTmpDir();\n    \r\n    FileUtils.forceMkdir(tmpDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n\n    final List<DataSegment> segments = generateSegments(toolbox, taskClient, inputSource, tmpDir);\n    taskClient.report(supervisorTaskId, createGeneratedPartitionsReport(toolbox, segments));\n\n    return TaskStatus.success(getId());\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialSegmentGenerateTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":103,"status":"M"},{"authorDate":"2020-06-26 05:41:22","commitOrder":7,"curCode":"  public TaskStatus runTask(final TaskToolbox toolbox) throws Exception\n  {\n    if (missingIntervalsInOverwriteMode) {\n      LOG.warn(\n          \"Intervals are missing in granularitySpec while this task is potentially overwriting existing segments. \"\n          + \"Forced to use timeChunk lock.\"\n      );\n    }\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n    final Set<DataSegment> pushedSegments = generateAndPushSegments(\n        toolbox,\n        taskClient,\n        inputSource,\n        toolbox.getIndexingTmpDir()\n    );\n\n    \r\n    final Set<DataSegment> allSegments = new HashSet<>(getTaskLockHelper().getLockedExistingSegments());\n    allSegments.addAll(pushedSegments);\n    final VersionedIntervalTimeline<String, DataSegment> timeline = VersionedIntervalTimeline.forSegments(allSegments);\n    final Set<DataSegment> oldSegments = FluentIterable.from(timeline.findFullyOvershadowed())\n                                                       .transformAndConcat(TimelineObjectHolder::getObject)\n                                                       .transform(PartitionChunk::getObject)\n                                                       .toSet();\n    taskClient.report(supervisorTaskId, new PushedSegmentsReport(getId(), oldSegments, pushedSegments));\n\n    return TaskStatus.success(getId());\n  }\n","date":"2020-06-26 05:41:22","endLine":230,"groupId":"13980","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"runTask","params":"(finalTaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b4/a7c8b95f22b9d01ef2c1c78a1c68f8d0d4e749.src","preCode":"  public TaskStatus runTask(final TaskToolbox toolbox) throws Exception\n  {\n    if (missingIntervalsInOverwriteMode) {\n      LOG.warn(\n          \"Intervals are missing in granularitySpec while this task is potentially overwriting existing segments. \"\n          + \"Forced to use timeChunk lock.\"\n      );\n    }\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final File tmpDir = toolbox.getIndexingTmpDir();\n    \r\n    FileUtils.forceMkdir(tmpDir);\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n    final Set<DataSegment> pushedSegments = generateAndPushSegments(\n        toolbox,\n        taskClient,\n        inputSource,\n        tmpDir\n    );\n\n    \r\n    final Set<DataSegment> allSegments = new HashSet<>(getTaskLockHelper().getLockedExistingSegments());\n    allSegments.addAll(pushedSegments);\n    final VersionedIntervalTimeline<String, DataSegment> timeline = VersionedIntervalTimeline.forSegments(allSegments);\n    final Set<DataSegment> oldSegments = FluentIterable.from(timeline.findFullyOvershadowed())\n                                                       .transformAndConcat(TimelineObjectHolder::getObject)\n                                                       .transform(PartitionChunk::getObject)\n                                                       .toSet();\n    taskClient.report(supervisorTaskId, new PushedSegmentsReport(getId(), oldSegments, pushedSegments));\n\n    return TaskStatus.success(getId());\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/SinglePhaseSubTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":193,"status":"M"}],"commitId":"f6594fff608d4b2e071c7bdd6d86d7f87398ce4f","commitMessage":"@@@Fix missing temp dir for native single_dim (#10046)\n\n* Fix missing temp dir for native single_dim\n\nNative single dim indexing throws a file not found exception from\nInputEntityIteratingReader.java:81.  This MR creates the required\ntemporary directory when setting up the\nPartialDimensionDistributionTask.  The change was tested on a Druid\ncluster.  After installing the change native single_dim indexing\ncompletes successfully.\n\n* Fix indentation\n\n* Use SinglePhaseSubTask as example for creating the temp dir\n\n* Move temporary indexing dir creation in to TaskToolbox\n\n* Remove unused dependency\n\nCo-authored-by: Morri Feldman <morri@appsflyer.com>","date":"2020-06-26 05:41:22","modifiedFileCount":"6","status":"M","submitter":"morrifeldman"},{"authorTime":"2020-08-27 08:08:12","codes":[{"authorDate":"2020-08-27 08:08:12","commitOrder":8,"curCode":"  public final TaskStatus runTask(TaskToolbox toolbox) throws Exception\n  {\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final ParallelIndexSupervisorTaskClient taskClient = toolbox.getSupervisorTaskClientFactory().build(\n        new ClientBasedTaskInfoProvider(toolbox.getIndexingServiceClient()),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n\n    final List<DataSegment> segments = generateSegments(\n        toolbox,\n        taskClient,\n        inputSource,\n        toolbox.getIndexingTmpDir()\n    );\n    taskClient.report(supervisorTaskId, createGeneratedPartitionsReport(toolbox, segments));\n\n    return TaskStatus.success(getId());\n  }\n","date":"2020-08-27 08:08:12","endLine":112,"groupId":"16414","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"runTask","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b6/5d4f2eeec1adcf27d5ff7d9b44b4162cd4ba1d.src","preCode":"  public final TaskStatus runTask(TaskToolbox toolbox) throws Exception\n  {\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n\n    final List<DataSegment> segments = generateSegments(\n        toolbox,\n        taskClient,\n        inputSource,\n        toolbox.getIndexingTmpDir()\n    );\n    taskClient.report(supervisorTaskId, createGeneratedPartitionsReport(toolbox, segments));\n\n    return TaskStatus.success(getId());\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialSegmentGenerateTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":89,"status":"M"},{"authorDate":"2020-08-27 08:08:12","commitOrder":8,"curCode":"  public TaskStatus runTask(final TaskToolbox toolbox) throws Exception\n  {\n    if (missingIntervalsInOverwriteMode) {\n      LOG.warn(\n          \"Intervals are missing in granularitySpec while this task is potentially overwriting existing segments. \"\n          + \"Forced to use timeChunk lock.\"\n      );\n    }\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final ParallelIndexSupervisorTaskClient taskClient = toolbox.getSupervisorTaskClientFactory().build(\n        new ClientBasedTaskInfoProvider(toolbox.getIndexingServiceClient()),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n    final Set<DataSegment> pushedSegments = generateAndPushSegments(\n        toolbox,\n        taskClient,\n        inputSource,\n        toolbox.getIndexingTmpDir()\n    );\n\n    \r\n    final Set<DataSegment> allSegments = new HashSet<>(getTaskLockHelper().getLockedExistingSegments());\n    allSegments.addAll(pushedSegments);\n    final VersionedIntervalTimeline<String, DataSegment> timeline = VersionedIntervalTimeline.forSegments(allSegments);\n    final Set<DataSegment> oldSegments = FluentIterable.from(timeline.findFullyOvershadowed())\n                                                       .transformAndConcat(TimelineObjectHolder::getObject)\n                                                       .transform(PartitionChunk::getObject)\n                                                       .toSet();\n    taskClient.report(supervisorTaskId, new PushedSegmentsReport(getId(), oldSegments, pushedSegments));\n\n    return TaskStatus.success(getId());\n  }\n","date":"2020-08-27 08:08:12","endLine":217,"groupId":"13980","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"runTask","params":"(finalTaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ba/1316f0ed8d413db531e59300c1a43b469a8cbc.src","preCode":"  public TaskStatus runTask(final TaskToolbox toolbox) throws Exception\n  {\n    if (missingIntervalsInOverwriteMode) {\n      LOG.warn(\n          \"Intervals are missing in granularitySpec while this task is potentially overwriting existing segments. \"\n          + \"Forced to use timeChunk lock.\"\n      );\n    }\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final ParallelIndexSupervisorTaskClient taskClient = taskClientFactory.build(\n        new ClientBasedTaskInfoProvider(indexingServiceClient),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n    final Set<DataSegment> pushedSegments = generateAndPushSegments(\n        toolbox,\n        taskClient,\n        inputSource,\n        toolbox.getIndexingTmpDir()\n    );\n\n    \r\n    final Set<DataSegment> allSegments = new HashSet<>(getTaskLockHelper().getLockedExistingSegments());\n    allSegments.addAll(pushedSegments);\n    final VersionedIntervalTimeline<String, DataSegment> timeline = VersionedIntervalTimeline.forSegments(allSegments);\n    final Set<DataSegment> oldSegments = FluentIterable.from(timeline.findFullyOvershadowed())\n                                                       .transformAndConcat(TimelineObjectHolder::getObject)\n                                                       .transform(PartitionChunk::getObject)\n                                                       .toSet();\n    taskClient.report(supervisorTaskId, new PushedSegmentsReport(getId(), oldSegments, pushedSegments));\n\n    return TaskStatus.success(getId());\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/SinglePhaseSubTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":180,"status":"M"}],"commitId":"f82fd22fa7de175200b7127c34c2eb2900bf7317","commitMessage":"@@@Move tools for indexing to TaskToolbox instead of injecting them in constructor (#10308)\n\n* Move tools for indexing to TaskToolbox instead of injecting them in constructor\n\n* oops.  other changes\n\n* fix test\n\n* unnecessary new file\n\n* fix test\n\n* fix build","date":"2020-08-27 08:08:12","modifiedFileCount":"67","status":"M","submitter":"Jihoon Son"},{"authorTime":"2021-09-17 02:58:11","codes":[{"authorDate":"2020-08-27 08:08:12","commitOrder":9,"curCode":"  public final TaskStatus runTask(TaskToolbox toolbox) throws Exception\n  {\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final ParallelIndexSupervisorTaskClient taskClient = toolbox.getSupervisorTaskClientFactory().build(\n        new ClientBasedTaskInfoProvider(toolbox.getIndexingServiceClient()),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n\n    final List<DataSegment> segments = generateSegments(\n        toolbox,\n        taskClient,\n        inputSource,\n        toolbox.getIndexingTmpDir()\n    );\n    taskClient.report(supervisorTaskId, createGeneratedPartitionsReport(toolbox, segments));\n\n    return TaskStatus.success(getId());\n  }\n","date":"2020-08-27 08:08:12","endLine":112,"groupId":"104756","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"runTask","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b6/5d4f2eeec1adcf27d5ff7d9b44b4162cd4ba1d.src","preCode":"  public final TaskStatus runTask(TaskToolbox toolbox) throws Exception\n  {\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final ParallelIndexSupervisorTaskClient taskClient = toolbox.getSupervisorTaskClientFactory().build(\n        new ClientBasedTaskInfoProvider(toolbox.getIndexingServiceClient()),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n\n    final List<DataSegment> segments = generateSegments(\n        toolbox,\n        taskClient,\n        inputSource,\n        toolbox.getIndexingTmpDir()\n    );\n    taskClient.report(supervisorTaskId, createGeneratedPartitionsReport(toolbox, segments));\n\n    return TaskStatus.success(getId());\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/PartialSegmentGenerateTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":89,"status":"N"},{"authorDate":"2021-09-17 02:58:11","commitOrder":9,"curCode":"  public TaskStatus runTask(final TaskToolbox toolbox)\n  {\n    try {\n      if (missingIntervalsInOverwriteMode) {\n        LOG.warn(\n            \"Intervals are missing in granularitySpec while this task is potentially overwriting existing segments. \"\n            + \"Forced to use timeChunk lock.\"\n        );\n      }\n      this.authorizerMapper = toolbox.getAuthorizerMapper();\n\n      toolbox.getChatHandlerProvider().register(getId(), this, false);\n\n      rowIngestionMeters = toolbox.getRowIngestionMetersFactory().createRowIngestionMeters();\n      parseExceptionHandler = new ParseExceptionHandler(\n          rowIngestionMeters,\n          ingestionSchema.getTuningConfig().isLogParseExceptions(),\n          ingestionSchema.getTuningConfig().getMaxParseExceptions(),\n          ingestionSchema.getTuningConfig().getMaxSavedParseExceptions()\n      );\n\n      final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n          ingestionSchema.getDataSchema().getParser()\n      );\n\n      final ParallelIndexSupervisorTaskClient taskClient = toolbox.getSupervisorTaskClientFactory().build(\n          new ClientBasedTaskInfoProvider(toolbox.getIndexingServiceClient()),\n          getId(),\n          1, \r\n          ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n          ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n      );\n      ingestionState = IngestionState.BUILD_SEGMENTS;\n      final Set<DataSegment> pushedSegments = generateAndPushSegments(\n          toolbox,\n          taskClient,\n          inputSource,\n          toolbox.getIndexingTmpDir()\n      );\n      \n      \r\n      final Set<DataSegment> allSegments = new HashSet<>(getTaskLockHelper().getLockedExistingSegments());\n      allSegments.addAll(pushedSegments);\n      final VersionedIntervalTimeline<String, DataSegment> timeline = VersionedIntervalTimeline.forSegments(allSegments);\n      final Set<DataSegment> oldSegments = FluentIterable.from(timeline.findFullyOvershadowed())\n                                                         .transformAndConcat(TimelineObjectHolder::getObject)\n                                                         .transform(PartitionChunk::getObject)\n                                                         .toSet();\n\n      Map<String, TaskReport> taskReport = getTaskCompletionReports();\n      taskClient.report(supervisorTaskId, new PushedSegmentsReport(getId(), oldSegments, pushedSegments, taskReport));\n\n      toolbox.getTaskReportFileWriter().write(getId(), taskReport);\n\n      return TaskStatus.success(getId());\n    }\n    catch (Exception e) {\n      LOG.error(e, \"Encountered exception in parallel sub task.\");\n      errorMsg = Throwables.getStackTraceAsString(e);\n      toolbox.getTaskReportFileWriter().write(getId(), getTaskCompletionReports());\n      return TaskStatus.failure(\n          getId(),\n          errorMsg\n      );\n    }\n    finally {\n      toolbox.getChatHandlerProvider().unregister(getId());\n    }\n  }\n","date":"2021-09-17 02:58:11","endLine":294,"groupId":"104756","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"runTask","params":"(finalTaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/08/cba47f6f8985100e813f95111a95c891f91bb1.src","preCode":"  public TaskStatus runTask(final TaskToolbox toolbox) throws Exception\n  {\n    if (missingIntervalsInOverwriteMode) {\n      LOG.warn(\n          \"Intervals are missing in granularitySpec while this task is potentially overwriting existing segments. \"\n          + \"Forced to use timeChunk lock.\"\n      );\n    }\n    final InputSource inputSource = ingestionSchema.getIOConfig().getNonNullInputSource(\n        ingestionSchema.getDataSchema().getParser()\n    );\n\n    final ParallelIndexSupervisorTaskClient taskClient = toolbox.getSupervisorTaskClientFactory().build(\n        new ClientBasedTaskInfoProvider(toolbox.getIndexingServiceClient()),\n        getId(),\n        1, \r\n        ingestionSchema.getTuningConfig().getChatHandlerTimeout(),\n        ingestionSchema.getTuningConfig().getChatHandlerNumRetries()\n    );\n    final Set<DataSegment> pushedSegments = generateAndPushSegments(\n        toolbox,\n        taskClient,\n        inputSource,\n        toolbox.getIndexingTmpDir()\n    );\n\n    \r\n    final Set<DataSegment> allSegments = new HashSet<>(getTaskLockHelper().getLockedExistingSegments());\n    allSegments.addAll(pushedSegments);\n    final VersionedIntervalTimeline<String, DataSegment> timeline = VersionedIntervalTimeline.forSegments(allSegments);\n    final Set<DataSegment> oldSegments = FluentIterable.from(timeline.findFullyOvershadowed())\n                                                       .transformAndConcat(TimelineObjectHolder::getObject)\n                                                       .transform(PartitionChunk::getObject)\n                                                       .toSet();\n    taskClient.report(supervisorTaskId, new PushedSegmentsReport(getId(), oldSegments, pushedSegments));\n\n    return TaskStatus.success(getId());\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/SinglePhaseSubTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":226,"status":"M"}],"commitId":"22b41ddbbfe2b07b085e295ba171bcdc07e04900","commitMessage":"@@@Task reports for parallel task: single phase and sequential mode (#11688)\n\n* Task reports for parallel task: single phase and sequential mode\n\n* Address comments\n\n* Add null check for currentSubTaskHolder","date":"2021-09-17 02:58:11","modifiedFileCount":"13","status":"M","submitter":"Jonathan Wei"}]
