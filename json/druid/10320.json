[{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  private DataSegment createDataSegment(String dataSource)\n  {\n    return new DataSegment(\n        dataSource,\n        Intervals.of(\"0/3000\"),\n        DateTimes.nowUtc().toString(),\n        Maps.newHashMap(),\n        Lists.newArrayList(),\n        Lists.newArrayList(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n  }\n","date":"2018-08-31 00:56:26","endLine":701,"groupId":"12685","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"createDataSegment","params":"(StringdataSource)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/86/c1b73d5a2c15a978cc4c5533441822c80848a6.src","preCode":"  private DataSegment createDataSegment(String dataSource)\n  {\n    return new DataSegment(\n        dataSource,\n        Intervals.of(\"0/3000\"),\n        DateTimes.nowUtc().toString(),\n        Maps.newHashMap(),\n        Lists.newArrayList(),\n        Lists.newArrayList(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n  }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/rules/LoadRuleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":688,"status":"B"},{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public void setUp()\n  {\n    smallSegment = new DataSegment(\n        \"small_source\",\n        Intervals.of(\"0/1000\"),\n        DateTimes.nowUtc().toString(),\n        Maps.newHashMap(),\n        Lists.newArrayList(),\n        Lists.newArrayList(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n\n    for (int i = 0; i < 3; i++) {\n      largeSegments.add(\n          new DataSegment(\n              \"large_source\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              Maps.newHashMap(),\n              Lists.newArrayList(),\n              Lists.newArrayList(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    for (int i = 0; i < 2; i++) {\n      largeSegments2.add(\n          new DataSegment(\n              \"large_source2\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              Maps.newHashMap(),\n              Lists.newArrayList(),\n              Lists.newArrayList(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    holderOfSmallSegment = new ServerHolder(\n        new DruidServer(\n            \"serverHot2\",\n            \"hostHot2\",\n            null,\n            1000,\n            ServerType.HISTORICAL,\n            \"hot\",\n            0\n        ).addDataSegment(smallSegment)\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester()\n    );\n\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot1\",\n                \"hostHot1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm1\",\n                \"hostNorm1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm2\",\n                \"hostNorm2\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(2))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot3\",\n                \"hostHot3\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments2.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm3\",\n                \"hostNorm3\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments2.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    druidCluster = new DruidCluster(\n        null,\n        ImmutableMap.of(\n            \"hot\",\n            Stream.of(\n                holdersOfLargeSegments.get(0),\n                holderOfSmallSegment,\n                holdersOfLargeSegments2.get(0)\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder()))),\n            DruidServer.DEFAULT_TIER,\n            Stream.of(\n                holdersOfLargeSegments.get(1),\n                holdersOfLargeSegments.get(2),\n                holdersOfLargeSegments2.get(1)\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder())))\n        )\n    );\n  }\n","date":"2018-08-31 00:56:26","endLine":216,"groupId":"12685","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/77/eaf1318e6705ab80602a36db7dc20245969c86.src","preCode":"  public void setUp()\n  {\n    smallSegment = new DataSegment(\n        \"small_source\",\n        Intervals.of(\"0/1000\"),\n        DateTimes.nowUtc().toString(),\n        Maps.newHashMap(),\n        Lists.newArrayList(),\n        Lists.newArrayList(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n\n    for (int i = 0; i < 3; i++) {\n      largeSegments.add(\n          new DataSegment(\n              \"large_source\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              Maps.newHashMap(),\n              Lists.newArrayList(),\n              Lists.newArrayList(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    for (int i = 0; i < 2; i++) {\n      largeSegments2.add(\n          new DataSegment(\n              \"large_source2\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              Maps.newHashMap(),\n              Lists.newArrayList(),\n              Lists.newArrayList(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    holderOfSmallSegment = new ServerHolder(\n        new DruidServer(\n            \"serverHot2\",\n            \"hostHot2\",\n            null,\n            1000,\n            ServerType.HISTORICAL,\n            \"hot\",\n            0\n        ).addDataSegment(smallSegment)\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester()\n    );\n\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot1\",\n                \"hostHot1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm1\",\n                \"hostNorm1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm2\",\n                \"hostNorm2\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(2))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot3\",\n                \"hostHot3\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments2.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm3\",\n                \"hostNorm3\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments2.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    druidCluster = new DruidCluster(\n        null,\n        ImmutableMap.of(\n            \"hot\",\n            Stream.of(\n                holdersOfLargeSegments.get(0),\n                holderOfSmallSegment,\n                holdersOfLargeSegments2.get(0)\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder()))),\n            DruidServer.DEFAULT_TIER,\n            Stream.of(\n                holdersOfLargeSegments.get(1),\n                holdersOfLargeSegments.get(2),\n                holdersOfLargeSegments2.get(1)\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder())))\n        )\n    );\n  }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/rules/BroadcastDistributionRuleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":62,"status":"B"}],"commitId":"431d3d8497f9079857c3baa7ae7ab4cb44a22355","commitMessage":"@@@Rename io.druid to org.apache.druid. (#6266)\n\n* Rename io.druid to org.apache.druid.\n\n* Fix META-INF files and remove some benchmark results.\n\n* MonitorsConfig update for metrics package migration.\n\n* Reorder some dimensions in inner queries for some reason.\n\n* Fix protobuf tests.\n","date":"2018-08-31 00:56:26","modifiedFileCount":"5","status":"B","submitter":"Gian Merlino"},{"authorTime":"2018-10-29 20:02:43","codes":[{"authorDate":"2018-10-29 20:02:43","commitOrder":2,"curCode":"  private DataSegment createDataSegment(String dataSource)\n  {\n    return new DataSegment(\n        dataSource,\n        Intervals.of(\"0/3000\"),\n        DateTimes.nowUtc().toString(),\n        new HashMap<>(),\n        new ArrayList<>(),\n        new ArrayList<>(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n  }\n","date":"2018-10-29 20:02:43","endLine":701,"groupId":"12685","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"createDataSegment","params":"(StringdataSource)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/19/80b9799b69107013e05e724588df1e6a5aed95.src","preCode":"  private DataSegment createDataSegment(String dataSource)\n  {\n    return new DataSegment(\n        dataSource,\n        Intervals.of(\"0/3000\"),\n        DateTimes.nowUtc().toString(),\n        Maps.newHashMap(),\n        Lists.newArrayList(),\n        Lists.newArrayList(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n  }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/rules/LoadRuleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":688,"status":"M"},{"authorDate":"2018-10-29 20:02:43","commitOrder":2,"curCode":"  public void setUp()\n  {\n    smallSegment = new DataSegment(\n        \"small_source\",\n        Intervals.of(\"0/1000\"),\n        DateTimes.nowUtc().toString(),\n        new HashMap<>(),\n        new ArrayList<>(),\n        new ArrayList<>(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n\n    for (int i = 0; i < 3; i++) {\n      largeSegments.add(\n          new DataSegment(\n              \"large_source\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              new HashMap<>(),\n              new ArrayList<>(),\n              new ArrayList<>(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    for (int i = 0; i < 2; i++) {\n      largeSegments2.add(\n          new DataSegment(\n              \"large_source2\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              new HashMap<>(),\n              new ArrayList<>(),\n              new ArrayList<>(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    holderOfSmallSegment = new ServerHolder(\n        new DruidServer(\n            \"serverHot2\",\n            \"hostHot2\",\n            null,\n            1000,\n            ServerType.HISTORICAL,\n            \"hot\",\n            0\n        ).addDataSegment(smallSegment)\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester()\n    );\n\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot1\",\n                \"hostHot1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm1\",\n                \"hostNorm1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm2\",\n                \"hostNorm2\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(2))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot3\",\n                \"hostHot3\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments2.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm3\",\n                \"hostNorm3\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments2.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    druidCluster = new DruidCluster(\n        null,\n        ImmutableMap.of(\n            \"hot\",\n            Stream.of(\n                holdersOfLargeSegments.get(0),\n                holderOfSmallSegment,\n                holdersOfLargeSegments2.get(0)\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder()))),\n            DruidServer.DEFAULT_TIER,\n            Stream.of(\n                holdersOfLargeSegments.get(1),\n                holdersOfLargeSegments.get(2),\n                holdersOfLargeSegments2.get(1)\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder())))\n        )\n    );\n  }\n","date":"2018-10-29 20:02:43","endLine":217,"groupId":"12685","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/04/5b160ebe8cddb6d2d82f1fbe55c67dc733c3d2.src","preCode":"  public void setUp()\n  {\n    smallSegment = new DataSegment(\n        \"small_source\",\n        Intervals.of(\"0/1000\"),\n        DateTimes.nowUtc().toString(),\n        Maps.newHashMap(),\n        Lists.newArrayList(),\n        Lists.newArrayList(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n\n    for (int i = 0; i < 3; i++) {\n      largeSegments.add(\n          new DataSegment(\n              \"large_source\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              Maps.newHashMap(),\n              Lists.newArrayList(),\n              Lists.newArrayList(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    for (int i = 0; i < 2; i++) {\n      largeSegments2.add(\n          new DataSegment(\n              \"large_source2\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              Maps.newHashMap(),\n              Lists.newArrayList(),\n              Lists.newArrayList(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    holderOfSmallSegment = new ServerHolder(\n        new DruidServer(\n            \"serverHot2\",\n            \"hostHot2\",\n            null,\n            1000,\n            ServerType.HISTORICAL,\n            \"hot\",\n            0\n        ).addDataSegment(smallSegment)\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester()\n    );\n\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot1\",\n                \"hostHot1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm1\",\n                \"hostNorm1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm2\",\n                \"hostNorm2\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(2))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot3\",\n                \"hostHot3\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments2.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm3\",\n                \"hostNorm3\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments2.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    druidCluster = new DruidCluster(\n        null,\n        ImmutableMap.of(\n            \"hot\",\n            Stream.of(\n                holdersOfLargeSegments.get(0),\n                holderOfSmallSegment,\n                holdersOfLargeSegments2.get(0)\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder()))),\n            DruidServer.DEFAULT_TIER,\n            Stream.of(\n                holdersOfLargeSegments.get(1),\n                holdersOfLargeSegments.get(2),\n                holdersOfLargeSegments2.get(1)\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder())))\n        )\n    );\n  }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/rules/BroadcastDistributionRuleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":63,"status":"M"}],"commitId":"676f5e6d7f184101b8763e4249b18b237bbe0ec7","commitMessage":"@@@Prohibit some guava collection APIs and use JDK collection APIs directly (#6511)\n\n* Prohibit some guava collection APIs and use JDK APIs directly\n\n* reset files that changed by accident\n\n* sort codestyle/druid-forbidden-apis.txt alphabetically\n","date":"2018-10-29 20:02:43","modifiedFileCount":"427","status":"M","submitter":"QiuMM"},{"authorTime":"2019-02-05 10:11:00","codes":[{"authorDate":"2018-10-29 20:02:43","commitOrder":3,"curCode":"  private DataSegment createDataSegment(String dataSource)\n  {\n    return new DataSegment(\n        dataSource,\n        Intervals.of(\"0/3000\"),\n        DateTimes.nowUtc().toString(),\n        new HashMap<>(),\n        new ArrayList<>(),\n        new ArrayList<>(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n  }\n","date":"2018-10-29 20:02:43","endLine":701,"groupId":"12685","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"createDataSegment","params":"(StringdataSource)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/19/80b9799b69107013e05e724588df1e6a5aed95.src","preCode":"  private DataSegment createDataSegment(String dataSource)\n  {\n    return new DataSegment(\n        dataSource,\n        Intervals.of(\"0/3000\"),\n        DateTimes.nowUtc().toString(),\n        new HashMap<>(),\n        new ArrayList<>(),\n        new ArrayList<>(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n  }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/rules/LoadRuleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":688,"status":"N"},{"authorDate":"2019-02-05 10:11:00","commitOrder":3,"curCode":"  public void setUp()\n  {\n    smallSegment = new DataSegment(\n        \"small_source\",\n        Intervals.of(\"0/1000\"),\n        DateTimes.nowUtc().toString(),\n        new HashMap<>(),\n        new ArrayList<>(),\n        new ArrayList<>(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n\n    for (int i = 0; i < 3; i++) {\n      largeSegments.add(\n          new DataSegment(\n              \"large_source\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              new HashMap<>(),\n              new ArrayList<>(),\n              new ArrayList<>(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    for (int i = 0; i < 2; i++) {\n      largeSegments2.add(\n          new DataSegment(\n              \"large_source2\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              new HashMap<>(),\n              new ArrayList<>(),\n              new ArrayList<>(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    holderOfSmallSegment = new ServerHolder(\n        new DruidServer(\n            \"serverHot2\",\n            \"hostHot2\",\n            null,\n            1000,\n            ServerType.HISTORICAL,\n            \"hot\",\n            0\n        ).addDataSegment(smallSegment)\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester()\n    );\n\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot1\",\n                \"hostHot1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm1\",\n                \"hostNorm1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm2\",\n                \"hostNorm2\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(2))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot3\",\n                \"hostHot3\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments2.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm3\",\n                \"hostNorm3\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments2.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    generalServer = new ServerHolder(\n        new DruidServer(\n            \"general\",\n            \"host1\",\n            null,\n            100,\n            ServerType.HISTORICAL,\n            \"tier1\",\n            0\n        ).addDataSegment(largeSegments.get(0))\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester()\n    );\n\n    maintenanceServer1 = new ServerHolder(\n        new DruidServer(\n            \"maintenance1\",\n            \"host2\",\n            null,\n            100,\n            ServerType.HISTORICAL,\n            \"tier1\",\n            0\n        ).addDataSegment(smallSegment)\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester(),\n        true\n    );\n\n    maintenanceServer2 = new ServerHolder(\n        new DruidServer(\n            \"maintenance2\",\n            \"host3\",\n            null,\n            100,\n            ServerType.HISTORICAL,\n            \"tier1\",\n            0\n        ).addDataSegment(largeSegments.get(1))\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester(),\n        true\n    );\n\n    druidCluster = new DruidCluster(\n        null,\n        ImmutableMap.of(\n            \"hot\",\n            Stream.of(\n                holdersOfLargeSegments.get(0),\n                holderOfSmallSegment,\n                holdersOfLargeSegments2.get(0)\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder()))),\n            DruidServer.DEFAULT_TIER,\n            Stream.of(\n                holdersOfLargeSegments.get(1),\n                holdersOfLargeSegments.get(2),\n                holdersOfLargeSegments2.get(1)\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder())))\n        )\n    );\n\n    secondCluster = new DruidCluster(\n        null,\n        ImmutableMap.of(\n            \"tier1\",\n            Stream.of(\n                generalServer,\n                maintenanceServer1,\n                maintenanceServer2\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder())))\n        )\n    );\n  }\n","date":"2019-02-05 10:11:00","endLine":276,"groupId":"0","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/35/9fb6895ccec3cb8a0d82fa24e96f66c0f0e6a3.src","preCode":"  public void setUp()\n  {\n    smallSegment = new DataSegment(\n        \"small_source\",\n        Intervals.of(\"0/1000\"),\n        DateTimes.nowUtc().toString(),\n        new HashMap<>(),\n        new ArrayList<>(),\n        new ArrayList<>(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n\n    for (int i = 0; i < 3; i++) {\n      largeSegments.add(\n          new DataSegment(\n              \"large_source\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              new HashMap<>(),\n              new ArrayList<>(),\n              new ArrayList<>(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    for (int i = 0; i < 2; i++) {\n      largeSegments2.add(\n          new DataSegment(\n              \"large_source2\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              new HashMap<>(),\n              new ArrayList<>(),\n              new ArrayList<>(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    holderOfSmallSegment = new ServerHolder(\n        new DruidServer(\n            \"serverHot2\",\n            \"hostHot2\",\n            null,\n            1000,\n            ServerType.HISTORICAL,\n            \"hot\",\n            0\n        ).addDataSegment(smallSegment)\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester()\n    );\n\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot1\",\n                \"hostHot1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm1\",\n                \"hostNorm1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm2\",\n                \"hostNorm2\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(2))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot3\",\n                \"hostHot3\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments2.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm3\",\n                \"hostNorm3\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments2.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    druidCluster = new DruidCluster(\n        null,\n        ImmutableMap.of(\n            \"hot\",\n            Stream.of(\n                holdersOfLargeSegments.get(0),\n                holderOfSmallSegment,\n                holdersOfLargeSegments2.get(0)\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder()))),\n            DruidServer.DEFAULT_TIER,\n            Stream.of(\n                holdersOfLargeSegments.get(1),\n                holdersOfLargeSegments.get(2),\n                holdersOfLargeSegments2.get(1)\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder())))\n        )\n    );\n  }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/rules/BroadcastDistributionRuleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":66,"status":"M"}],"commitId":"97b6407983f597fc039bb90339b41086bbaaea56","commitMessage":"@@@maintenance mode for Historical (#6349)\n\n* maintenance mode for Historical\n\nforbidden api fix.  config deserialization fix\n\nlogging fix.  unit tests\n\n* addressed comments\n\n* addressed comments\n\n* a style fix\n\n* addressed comments\n\n* a unit-test fix due to recent code-refactoring\n\n* docs & refactoring\n\n* addressed comments\n\n* addressed a LoadRule drop flaw\n\n* post merge cleaning up\n","date":"2019-02-05 10:11:00","modifiedFileCount":"18","status":"M","submitter":"Egor Riashin"},{"authorTime":"2019-03-09 08:33:51","codes":[{"authorDate":"2018-10-29 20:02:43","commitOrder":4,"curCode":"  private DataSegment createDataSegment(String dataSource)\n  {\n    return new DataSegment(\n        dataSource,\n        Intervals.of(\"0/3000\"),\n        DateTimes.nowUtc().toString(),\n        new HashMap<>(),\n        new ArrayList<>(),\n        new ArrayList<>(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n  }\n","date":"2018-10-29 20:02:43","endLine":701,"groupId":"12685","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"createDataSegment","params":"(StringdataSource)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/19/80b9799b69107013e05e724588df1e6a5aed95.src","preCode":"  private DataSegment createDataSegment(String dataSource)\n  {\n    return new DataSegment(\n        dataSource,\n        Intervals.of(\"0/3000\"),\n        DateTimes.nowUtc().toString(),\n        new HashMap<>(),\n        new ArrayList<>(),\n        new ArrayList<>(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n  }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/rules/LoadRuleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":688,"status":"N"},{"authorDate":"2019-03-09 08:33:51","commitOrder":4,"curCode":"  public void setUp()\n  {\n    smallSegment = new DataSegment(\n        \"small_source\",\n        Intervals.of(\"0/1000\"),\n        DateTimes.nowUtc().toString(),\n        new HashMap<>(),\n        new ArrayList<>(),\n        new ArrayList<>(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n\n    for (int i = 0; i < 3; i++) {\n      largeSegments.add(\n          new DataSegment(\n              \"large_source\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              new HashMap<>(),\n              new ArrayList<>(),\n              new ArrayList<>(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    for (int i = 0; i < 2; i++) {\n      largeSegments2.add(\n          new DataSegment(\n              \"large_source2\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              new HashMap<>(),\n              new ArrayList<>(),\n              new ArrayList<>(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    holderOfSmallSegment = new ServerHolder(\n        new DruidServer(\n            \"serverHot2\",\n            \"hostHot2\",\n            null,\n            1000,\n            ServerType.HISTORICAL,\n            \"hot\",\n            0\n        ).addDataSegment(smallSegment)\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester()\n    );\n\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot1\",\n                \"hostHot1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm1\",\n                \"hostNorm1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm2\",\n                \"hostNorm2\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(2))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot3\",\n                \"hostHot3\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments2.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm3\",\n                \"hostNorm3\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments2.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    activeServer = new ServerHolder(\n        new DruidServer(\n            \"active\",\n            \"host1\",\n            null,\n            100,\n            ServerType.HISTORICAL,\n            \"tier1\",\n            0\n        ).addDataSegment(largeSegments.get(0))\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester()\n    );\n\n    decommissioningServer1 = new ServerHolder(\n        new DruidServer(\n            \"decommissioning1\",\n            \"host2\",\n            null,\n            100,\n            ServerType.HISTORICAL,\n            \"tier1\",\n            0\n        ).addDataSegment(smallSegment)\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester(),\n        true\n    );\n\n    decommissioningServer2 = new ServerHolder(\n        new DruidServer(\n            \"decommissioning2\",\n            \"host3\",\n            null,\n            100,\n            ServerType.HISTORICAL,\n            \"tier1\",\n            0\n        ).addDataSegment(largeSegments.get(1))\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester(),\n        true\n    );\n\n    druidCluster = new DruidCluster(\n        null,\n        ImmutableMap.of(\n            \"hot\",\n            Stream.of(\n                holdersOfLargeSegments.get(0),\n                holderOfSmallSegment,\n                holdersOfLargeSegments2.get(0)\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder()))),\n            DruidServer.DEFAULT_TIER,\n            Stream.of(\n                holdersOfLargeSegments.get(1),\n                holdersOfLargeSegments.get(2),\n                holdersOfLargeSegments2.get(1)\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder())))\n        )\n    );\n\n    secondCluster = new DruidCluster(\n        null,\n        ImmutableMap.of(\n            \"tier1\",\n            Stream.of(\n                activeServer,\n                decommissioningServer1,\n                decommissioningServer2\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder())))\n        )\n    );\n  }\n","date":"2019-03-09 08:33:51","endLine":276,"groupId":"12685","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/83/398d7f6dc4cd6380b3bca5a4d01d989152f708.src","preCode":"  public void setUp()\n  {\n    smallSegment = new DataSegment(\n        \"small_source\",\n        Intervals.of(\"0/1000\"),\n        DateTimes.nowUtc().toString(),\n        new HashMap<>(),\n        new ArrayList<>(),\n        new ArrayList<>(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n\n    for (int i = 0; i < 3; i++) {\n      largeSegments.add(\n          new DataSegment(\n              \"large_source\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              new HashMap<>(),\n              new ArrayList<>(),\n              new ArrayList<>(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    for (int i = 0; i < 2; i++) {\n      largeSegments2.add(\n          new DataSegment(\n              \"large_source2\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              new HashMap<>(),\n              new ArrayList<>(),\n              new ArrayList<>(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    holderOfSmallSegment = new ServerHolder(\n        new DruidServer(\n            \"serverHot2\",\n            \"hostHot2\",\n            null,\n            1000,\n            ServerType.HISTORICAL,\n            \"hot\",\n            0\n        ).addDataSegment(smallSegment)\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester()\n    );\n\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot1\",\n                \"hostHot1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm1\",\n                \"hostNorm1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm2\",\n                \"hostNorm2\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(2))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot3\",\n                \"hostHot3\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments2.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm3\",\n                \"hostNorm3\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments2.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    generalServer = new ServerHolder(\n        new DruidServer(\n            \"general\",\n            \"host1\",\n            null,\n            100,\n            ServerType.HISTORICAL,\n            \"tier1\",\n            0\n        ).addDataSegment(largeSegments.get(0))\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester()\n    );\n\n    maintenanceServer1 = new ServerHolder(\n        new DruidServer(\n            \"maintenance1\",\n            \"host2\",\n            null,\n            100,\n            ServerType.HISTORICAL,\n            \"tier1\",\n            0\n        ).addDataSegment(smallSegment)\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester(),\n        true\n    );\n\n    maintenanceServer2 = new ServerHolder(\n        new DruidServer(\n            \"maintenance2\",\n            \"host3\",\n            null,\n            100,\n            ServerType.HISTORICAL,\n            \"tier1\",\n            0\n        ).addDataSegment(largeSegments.get(1))\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester(),\n        true\n    );\n\n    druidCluster = new DruidCluster(\n        null,\n        ImmutableMap.of(\n            \"hot\",\n            Stream.of(\n                holdersOfLargeSegments.get(0),\n                holderOfSmallSegment,\n                holdersOfLargeSegments2.get(0)\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder()))),\n            DruidServer.DEFAULT_TIER,\n            Stream.of(\n                holdersOfLargeSegments.get(1),\n                holdersOfLargeSegments.get(2),\n                holdersOfLargeSegments2.get(1)\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder())))\n        )\n    );\n\n    secondCluster = new DruidCluster(\n        null,\n        ImmutableMap.of(\n            \"tier1\",\n            Stream.of(\n                generalServer,\n                maintenanceServer1,\n                maintenanceServer2\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder())))\n        )\n    );\n  }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/rules/BroadcastDistributionRuleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":66,"status":"M"}],"commitId":"a44df6522c6d0ff66e4a21fec0fef5cb8c5f8246","commitMessage":"@@@rename maintenance mode to decommission (#7154)\n\n* rename maintenance mode to decommission\n\n* review changes\n\n* missed one\n\n* fix straggler.  add doc about decommissioning stalling if no active servers\n\n* fix missed typo.  docs\n\n* refine docs\n\n* doc changes.  replace generals\n\n* add explicit comment to mention suppressed stats for balanceTier\n\n* rename decommissioningVelocity to decommissioningMaxSegmentsToMovePercent and update docs\n\n* fix precondition check\n\n* decommissioningMaxPercentOfMaxSegmentsToMove\n\n* fix test\n\n* fix test\n\n* fixes\n","date":"2019-03-09 08:33:51","modifiedFileCount":"10","status":"M","submitter":"Clint Wylie"},{"authorTime":"2019-07-17 22:18:48","codes":[{"authorDate":"2018-10-29 20:02:43","commitOrder":5,"curCode":"  private DataSegment createDataSegment(String dataSource)\n  {\n    return new DataSegment(\n        dataSource,\n        Intervals.of(\"0/3000\"),\n        DateTimes.nowUtc().toString(),\n        new HashMap<>(),\n        new ArrayList<>(),\n        new ArrayList<>(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n  }\n","date":"2018-10-29 20:02:43","endLine":701,"groupId":"10320","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"createDataSegment","params":"(StringdataSource)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/19/80b9799b69107013e05e724588df1e6a5aed95.src","preCode":"  private DataSegment createDataSegment(String dataSource)\n  {\n    return new DataSegment(\n        dataSource,\n        Intervals.of(\"0/3000\"),\n        DateTimes.nowUtc().toString(),\n        new HashMap<>(),\n        new ArrayList<>(),\n        new ArrayList<>(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n  }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/rules/LoadRuleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":688,"status":"N"},{"authorDate":"2019-07-17 22:18:48","commitOrder":5,"curCode":"  public void setUp()\n  {\n    smallSegment = new DataSegment(\n        \"small_source\",\n        Intervals.of(\"0/1000\"),\n        DateTimes.nowUtc().toString(),\n        new HashMap<>(),\n        new ArrayList<>(),\n        new ArrayList<>(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n\n    for (int i = 0; i < 3; i++) {\n      largeSegments.add(\n          new DataSegment(\n              \"large_source\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              new HashMap<>(),\n              new ArrayList<>(),\n              new ArrayList<>(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    for (int i = 0; i < 2; i++) {\n      largeSegments2.add(\n          new DataSegment(\n              \"large_source2\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              new HashMap<>(),\n              new ArrayList<>(),\n              new ArrayList<>(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    holderOfSmallSegment = new ServerHolder(\n        new DruidServer(\n            \"serverHot2\",\n            \"hostHot2\",\n            null,\n            1000,\n            ServerType.HISTORICAL,\n            \"hot\",\n            0\n        ).addDataSegment(smallSegment)\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester()\n    );\n\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot1\",\n                \"hostHot1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm1\",\n                \"hostNorm1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm2\",\n                \"hostNorm2\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(2))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot3\",\n                \"hostHot3\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments2.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm3\",\n                \"hostNorm3\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments2.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    activeServer = new ServerHolder(\n        new DruidServer(\n            \"active\",\n            \"host1\",\n            null,\n            100,\n            ServerType.HISTORICAL,\n            \"tier1\",\n            0\n        ).addDataSegment(largeSegments.get(0))\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester()\n    );\n\n    decommissioningServer1 = new ServerHolder(\n        new DruidServer(\n            \"decommissioning1\",\n            \"host2\",\n            null,\n            100,\n            ServerType.HISTORICAL,\n            \"tier1\",\n            0\n        ).addDataSegment(smallSegment)\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester(),\n        true\n    );\n\n    decommissioningServer2 = new ServerHolder(\n        new DruidServer(\n            \"decommissioning2\",\n            \"host3\",\n            null,\n            100,\n            ServerType.HISTORICAL,\n            \"tier1\",\n            0\n        ).addDataSegment(largeSegments.get(1))\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester(),\n        true\n    );\n\n    druidCluster = DruidClusterBuilder\n        .newBuilder()\n        .addTier(\n            \"hot\",\n            holdersOfLargeSegments.get(0),\n            holderOfSmallSegment,\n            holdersOfLargeSegments2.get(0)\n        )\n        .addTier(\n            DruidServer.DEFAULT_TIER,\n            holdersOfLargeSegments.get(1),\n            holdersOfLargeSegments.get(2),\n            holdersOfLargeSegments2.get(1)\n        )\n        .build();\n\n    secondCluster = DruidClusterBuilder\n        .newBuilder()\n        .addTier(\n            \"tier1\",\n            activeServer,\n            decommissioningServer1,\n            decommissioningServer2\n        )\n        .build();\n  }\n","date":"2019-07-17 22:18:48","endLine":266,"groupId":"10320","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"setUp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/cd/b5e416da8224fcf3970c55bc9515c0d098daf7.src","preCode":"  public void setUp()\n  {\n    smallSegment = new DataSegment(\n        \"small_source\",\n        Intervals.of(\"0/1000\"),\n        DateTimes.nowUtc().toString(),\n        new HashMap<>(),\n        new ArrayList<>(),\n        new ArrayList<>(),\n        NoneShardSpec.instance(),\n        0,\n        0\n    );\n\n    for (int i = 0; i < 3; i++) {\n      largeSegments.add(\n          new DataSegment(\n              \"large_source\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              new HashMap<>(),\n              new ArrayList<>(),\n              new ArrayList<>(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    for (int i = 0; i < 2; i++) {\n      largeSegments2.add(\n          new DataSegment(\n              \"large_source2\",\n              Intervals.of((i * 1000) + \"/\" + ((i + 1) * 1000)),\n              DateTimes.nowUtc().toString(),\n              new HashMap<>(),\n              new ArrayList<>(),\n              new ArrayList<>(),\n              NoneShardSpec.instance(),\n              0,\n              100\n          )\n      );\n    }\n\n    holderOfSmallSegment = new ServerHolder(\n        new DruidServer(\n            \"serverHot2\",\n            \"hostHot2\",\n            null,\n            1000,\n            ServerType.HISTORICAL,\n            \"hot\",\n            0\n        ).addDataSegment(smallSegment)\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester()\n    );\n\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot1\",\n                \"hostHot1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm1\",\n                \"hostNorm1\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm2\",\n                \"hostNorm2\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments.get(2))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverHot3\",\n                \"hostHot3\",\n                null,\n                1000,\n                ServerType.HISTORICAL,\n                \"hot\",\n                0\n            ).addDataSegment(largeSegments2.get(0))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n    holdersOfLargeSegments2.add(\n        new ServerHolder(\n            new DruidServer(\n                \"serverNorm3\",\n                \"hostNorm3\",\n                null,\n                100,\n                ServerType.HISTORICAL,\n                DruidServer.DEFAULT_TIER,\n                0\n            ).addDataSegment(largeSegments2.get(1))\n             .toImmutableDruidServer(),\n            new LoadQueuePeonTester()\n        )\n    );\n\n    activeServer = new ServerHolder(\n        new DruidServer(\n            \"active\",\n            \"host1\",\n            null,\n            100,\n            ServerType.HISTORICAL,\n            \"tier1\",\n            0\n        ).addDataSegment(largeSegments.get(0))\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester()\n    );\n\n    decommissioningServer1 = new ServerHolder(\n        new DruidServer(\n            \"decommissioning1\",\n            \"host2\",\n            null,\n            100,\n            ServerType.HISTORICAL,\n            \"tier1\",\n            0\n        ).addDataSegment(smallSegment)\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester(),\n        true\n    );\n\n    decommissioningServer2 = new ServerHolder(\n        new DruidServer(\n            \"decommissioning2\",\n            \"host3\",\n            null,\n            100,\n            ServerType.HISTORICAL,\n            \"tier1\",\n            0\n        ).addDataSegment(largeSegments.get(1))\n         .toImmutableDruidServer(),\n        new LoadQueuePeonTester(),\n        true\n    );\n\n    druidCluster = new DruidCluster(\n        null,\n        ImmutableMap.of(\n            \"hot\",\n            Stream.of(\n                holdersOfLargeSegments.get(0),\n                holderOfSmallSegment,\n                holdersOfLargeSegments2.get(0)\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder()))),\n            DruidServer.DEFAULT_TIER,\n            Stream.of(\n                holdersOfLargeSegments.get(1),\n                holdersOfLargeSegments.get(2),\n                holdersOfLargeSegments2.get(1)\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder())))\n        )\n    );\n\n    secondCluster = new DruidCluster(\n        null,\n        ImmutableMap.of(\n            \"tier1\",\n            Stream.of(\n                activeServer,\n                decommissioningServer1,\n                decommissioningServer2\n            ).collect(Collectors.toCollection(() -> new TreeSet<>(Collections.reverseOrder())))\n        )\n    );\n  }\n","realPath":"server/src/test/java/org/apache/druid/server/coordinator/rules/BroadcastDistributionRuleTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":60,"status":"M"}],"commitId":"ceb969903f06a72d669bb55ab83223bddb5355ee","commitMessage":"@@@Refactor SQLMetadataSegmentManager; Change contract of REST met? (#7653)\n\n* Refactor SQLMetadataSegmentManager; Change contract of REST methods in DataSourcesResource\n\n* Style fixes\n\n* Unused imports\n\n* Fix tests\n\n* Fix style\n\n* Comments\n\n* Comment fix\n\n* Remove unresolvable Javadoc references; address comments\n\n* Add comments to ImmutableDruidDataSource\n\n* Merge with master\n\n* Fix bad web-console merge\n\n* Fixes in api-reference.md\n\n* Rename in DruidCoordinatorRuntimeParams\n\n* Fix compilation\n\n* Residual changes\n","date":"2019-07-17 22:18:48","modifiedFileCount":"77","status":"M","submitter":"Roman Leventov"}]
