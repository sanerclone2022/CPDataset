[{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public void testSingleThreadedIndexingAndQuery() throws Exception\n  {\n    final int dimensionCount = 5;\n    final ArrayList<AggregatorFactory> ingestAggregatorFactories = new ArrayList<>();\n    ingestAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      ingestAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n      ingestAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n    }\n\n    final IncrementalIndex index = closerRule.closeLater(\n        indexCreator.createIndex(\n            ingestAggregatorFactories.toArray(\n                new AggregatorFactory[0]\n            )\n        )\n    );\n\n    final long timestamp = System.currentTimeMillis();\n\n    final int rows = 50;\n\n    \r\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    \r\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>();\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(Intervals.of(\"2000/2030\")))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(index, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.NoopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n\n\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), new HashMap<String, Object>()).toList();\n    Result<TimeseriesResultValue> result = Iterables.getOnlyElement(results);\n    boolean isRollup = index.isRollup();\n    Assert.assertEquals(rows * (isRollup ? 1 : 2), result.getValue().getLongMetric(\"rows\").intValue());\n    for (int i = 0; i < dimensionCount; ++i) {\n      Assert.assertEquals(\n          \"Failed long sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getLongMetric(\"sumResult\" + i).intValue()\n      );\n      Assert.assertEquals(\n          \"Failed double sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getDoubleMetric(\"doubleSumResult\" + i).intValue()\n      );\n    }\n  }\n","date":"2018-08-31 00:56:26","endLine":462,"groupId":"12409","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testSingleThreadedIndexingAndQuery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0a/affb972e2e714979d5b86cbf8870a134c30f74.src","preCode":"  public void testSingleThreadedIndexingAndQuery() throws Exception\n  {\n    final int dimensionCount = 5;\n    final ArrayList<AggregatorFactory> ingestAggregatorFactories = new ArrayList<>();\n    ingestAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      ingestAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n      ingestAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n    }\n\n    final IncrementalIndex index = closerRule.closeLater(\n        indexCreator.createIndex(\n            ingestAggregatorFactories.toArray(\n                new AggregatorFactory[0]\n            )\n        )\n    );\n\n    final long timestamp = System.currentTimeMillis();\n\n    final int rows = 50;\n\n    \r\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    \r\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>();\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(Intervals.of(\"2000/2030\")))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(index, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.NoopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n\n\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), new HashMap<String, Object>()).toList();\n    Result<TimeseriesResultValue> result = Iterables.getOnlyElement(results);\n    boolean isRollup = index.isRollup();\n    Assert.assertEquals(rows * (isRollup ? 1 : 2), result.getValue().getLongMetric(\"rows\").intValue());\n    for (int i = 0; i < dimensionCount; ++i) {\n      Assert.assertEquals(\n          \"Failed long sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getLongMetric(\"sumResult\" + i).intValue()\n      );\n      Assert.assertEquals(\n          \"Failed double sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getDoubleMetric(\"doubleSumResult\" + i).intValue()\n      );\n    }\n  }\n","realPath":"processing/src/test/java/org/apache/druid/segment/data/IncrementalIndexTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":368,"status":"B"},{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public void testConcurrentAddRead()\n      throws InterruptedException, ExecutionException, NoSuchMethodException, IllegalAccessException,\n             InvocationTargetException, InstantiationException\n  {\n\n    final int taskCount = 30;\n    final int concurrentThreads = 3;\n    final int elementsPerThread = 1 << 15;\n\n    final IncrementalIndex incrementalIndex = this.incrementalIndex.getConstructor(\n        IncrementalIndexSchema.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        int.class\n    ).newInstance(\n        new IncrementalIndexSchema.Builder().withMetrics(factories).build(),\n        true,\n        true,\n        false,\n        true,\n        elementsPerThread * taskCount\n    );\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>(dimensionCount + 1);\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    final ListeningExecutorService indexExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"index-executor-%d\")\n                .setPriority(Thread.MIN_PRIORITY)\n                .build()\n        )\n    );\n    final ListeningExecutorService queryExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"query-executor-%d\")\n                .build()\n        )\n    );\n    final long timestamp = System.currentTimeMillis();\n    final Interval queryInterval = Intervals.of(\"1900-01-01T00:00:00Z/2900-01-01T00:00:00Z\");\n    final List<ListenableFuture<?>> indexFutures = new LinkedList<>();\n    final List<ListenableFuture<?>> queryFutures = new LinkedList<>();\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(incrementalIndex, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.NoopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final AtomicInteger currentlyRunning = new AtomicInteger(0);\n    final AtomicBoolean concurrentlyRan = new AtomicBoolean(false);\n    final AtomicBoolean someoneRan = new AtomicBoolean(false);\n    for (int j = 0; j < taskCount; j++) {\n      indexFutures.add(\n          indexExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  currentlyRunning.incrementAndGet();\n                  try {\n                    for (int i = 0; i < elementsPerThread; i++) {\n                      incrementalIndex.add(getLongRow(timestamp + i, 1, dimensionCount));\n                    }\n                  }\n                  catch (IndexSizeExceededException e) {\n                    throw Throwables.propagate(e);\n                  }\n                  currentlyRunning.decrementAndGet();\n                  someoneRan.set(true);\n                }\n              }\n          )\n      );\n\n      queryFutures.add(\n          queryExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n                      factory.createRunner(incrementalIndexSegment),\n                      factory.getToolchest()\n                  );\n                  TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                                .dataSource(\"xxx\")\n                                                .granularity(Granularities.ALL)\n                                                .intervals(ImmutableList.of(queryInterval))\n                                                .aggregators(queryAggregatorFactories)\n                                                .build();\n                  Map<String, Object> context = new HashMap<String, Object>();\n                  List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), context).toList();\n                  for (Result<TimeseriesResultValue> result : results) {\n                    if (someoneRan.get()) {\n                      Assert.assertTrue(result.getValue().getDoubleMetric(\"doubleSumResult0\") > 0);\n                    }\n                  }\n                  if (currentlyRunning.get() > 0) {\n                    concurrentlyRan.set(true);\n                  }\n                }\n              }\n          )\n      );\n\n    }\n    List<ListenableFuture<?>> allFutures = new ArrayList<>(queryFutures.size() + indexFutures.size());\n    allFutures.addAll(queryFutures);\n    allFutures.addAll(indexFutures);\n    Futures.allAsList(allFutures).get();\n    \r\n    queryExecutor.shutdown();\n    indexExecutor.shutdown();\n    QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(queryInterval))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n    Map<String, Object> context = new HashMap<String, Object>();\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), context).toList();\n    final int expectedVal = elementsPerThread * taskCount;\n    for (Result<TimeseriesResultValue> result : results) {\n      Assert.assertEquals(elementsPerThread, result.getValue().getLongMetric(\"rows\").intValue());\n      for (int i = 0; i < dimensionCount; ++i) {\n        Assert.assertEquals(\n            StringUtils.format(\"Failed long sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getLongMetric(StringUtils.format(\"sumResult%s\", i)).intValue()\n        );\n        Assert.assertEquals(\n            StringUtils.format(\"Failed double sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getDoubleMetric(StringUtils.format(\"doubleSumResult%s\", i)).intValue()\n        );\n      }\n    }\n  }\n","date":"2018-08-31 00:56:26","endLine":453,"groupId":"8400","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testConcurrentAddRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/fc/c2fc06b3ccb3eece17c81171c23f890bb65426.src","preCode":"  public void testConcurrentAddRead()\n      throws InterruptedException, ExecutionException, NoSuchMethodException, IllegalAccessException,\n             InvocationTargetException, InstantiationException\n  {\n\n    final int taskCount = 30;\n    final int concurrentThreads = 3;\n    final int elementsPerThread = 1 << 15;\n\n    final IncrementalIndex incrementalIndex = this.incrementalIndex.getConstructor(\n        IncrementalIndexSchema.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        int.class\n    ).newInstance(\n        new IncrementalIndexSchema.Builder().withMetrics(factories).build(),\n        true,\n        true,\n        false,\n        true,\n        elementsPerThread * taskCount\n    );\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>(dimensionCount + 1);\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    final ListeningExecutorService indexExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"index-executor-%d\")\n                .setPriority(Thread.MIN_PRIORITY)\n                .build()\n        )\n    );\n    final ListeningExecutorService queryExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"query-executor-%d\")\n                .build()\n        )\n    );\n    final long timestamp = System.currentTimeMillis();\n    final Interval queryInterval = Intervals.of(\"1900-01-01T00:00:00Z/2900-01-01T00:00:00Z\");\n    final List<ListenableFuture<?>> indexFutures = new LinkedList<>();\n    final List<ListenableFuture<?>> queryFutures = new LinkedList<>();\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(incrementalIndex, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.NoopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final AtomicInteger currentlyRunning = new AtomicInteger(0);\n    final AtomicBoolean concurrentlyRan = new AtomicBoolean(false);\n    final AtomicBoolean someoneRan = new AtomicBoolean(false);\n    for (int j = 0; j < taskCount; j++) {\n      indexFutures.add(\n          indexExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  currentlyRunning.incrementAndGet();\n                  try {\n                    for (int i = 0; i < elementsPerThread; i++) {\n                      incrementalIndex.add(getLongRow(timestamp + i, 1, dimensionCount));\n                    }\n                  }\n                  catch (IndexSizeExceededException e) {\n                    throw Throwables.propagate(e);\n                  }\n                  currentlyRunning.decrementAndGet();\n                  someoneRan.set(true);\n                }\n              }\n          )\n      );\n\n      queryFutures.add(\n          queryExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n                      factory.createRunner(incrementalIndexSegment),\n                      factory.getToolchest()\n                  );\n                  TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                                .dataSource(\"xxx\")\n                                                .granularity(Granularities.ALL)\n                                                .intervals(ImmutableList.of(queryInterval))\n                                                .aggregators(queryAggregatorFactories)\n                                                .build();\n                  Map<String, Object> context = new HashMap<String, Object>();\n                  List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), context).toList();\n                  for (Result<TimeseriesResultValue> result : results) {\n                    if (someoneRan.get()) {\n                      Assert.assertTrue(result.getValue().getDoubleMetric(\"doubleSumResult0\") > 0);\n                    }\n                  }\n                  if (currentlyRunning.get() > 0) {\n                    concurrentlyRan.set(true);\n                  }\n                }\n              }\n          )\n      );\n\n    }\n    List<ListenableFuture<?>> allFutures = new ArrayList<>(queryFutures.size() + indexFutures.size());\n    allFutures.addAll(queryFutures);\n    allFutures.addAll(indexFutures);\n    Futures.allAsList(allFutures).get();\n    \r\n    queryExecutor.shutdown();\n    indexExecutor.shutdown();\n    QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(queryInterval))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n    Map<String, Object> context = new HashMap<String, Object>();\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), context).toList();\n    final int expectedVal = elementsPerThread * taskCount;\n    for (Result<TimeseriesResultValue> result : results) {\n      Assert.assertEquals(elementsPerThread, result.getValue().getLongMetric(\"rows\").intValue());\n      for (int i = 0; i < dimensionCount; ++i) {\n        Assert.assertEquals(\n            StringUtils.format(\"Failed long sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getLongMetric(StringUtils.format(\"sumResult%s\", i)).intValue()\n        );\n        Assert.assertEquals(\n            StringUtils.format(\"Failed double sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getDoubleMetric(StringUtils.format(\"doubleSumResult%s\", i)).intValue()\n        );\n      }\n    }\n  }\n","realPath":"processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexBenchmark.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":288,"status":"B"}],"commitId":"431d3d8497f9079857c3baa7ae7ab4cb44a22355","commitMessage":"@@@Rename io.druid to org.apache.druid. (#6266)\n\n* Rename io.druid to org.apache.druid.\n\n* Fix META-INF files and remove some benchmark results.\n\n* MonitorsConfig update for metrics package migration.\n\n* Reorder some dimensions in inner queries for some reason.\n\n* Fix protobuf tests.\n","date":"2018-08-31 00:56:26","modifiedFileCount":"5","status":"B","submitter":"Gian Merlino"},{"authorTime":"2018-09-14 09:07:06","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":2,"curCode":"  public void testSingleThreadedIndexingAndQuery() throws Exception\n  {\n    final int dimensionCount = 5;\n    final ArrayList<AggregatorFactory> ingestAggregatorFactories = new ArrayList<>();\n    ingestAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      ingestAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n      ingestAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n    }\n\n    final IncrementalIndex index = closerRule.closeLater(\n        indexCreator.createIndex(\n            ingestAggregatorFactories.toArray(\n                new AggregatorFactory[0]\n            )\n        )\n    );\n\n    final long timestamp = System.currentTimeMillis();\n\n    final int rows = 50;\n\n    \r\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    \r\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>();\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(Intervals.of(\"2000/2030\")))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(index, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.NoopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n\n\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), new HashMap<String, Object>()).toList();\n    Result<TimeseriesResultValue> result = Iterables.getOnlyElement(results);\n    boolean isRollup = index.isRollup();\n    Assert.assertEquals(rows * (isRollup ? 1 : 2), result.getValue().getLongMetric(\"rows\").intValue());\n    for (int i = 0; i < dimensionCount; ++i) {\n      Assert.assertEquals(\n          \"Failed long sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getLongMetric(\"sumResult\" + i).intValue()\n      );\n      Assert.assertEquals(\n          \"Failed double sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getDoubleMetric(\"doubleSumResult\" + i).intValue()\n      );\n    }\n  }\n","date":"2018-08-31 00:56:26","endLine":462,"groupId":"12409","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testSingleThreadedIndexingAndQuery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0a/affb972e2e714979d5b86cbf8870a134c30f74.src","preCode":"  public void testSingleThreadedIndexingAndQuery() throws Exception\n  {\n    final int dimensionCount = 5;\n    final ArrayList<AggregatorFactory> ingestAggregatorFactories = new ArrayList<>();\n    ingestAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      ingestAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n      ingestAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n    }\n\n    final IncrementalIndex index = closerRule.closeLater(\n        indexCreator.createIndex(\n            ingestAggregatorFactories.toArray(\n                new AggregatorFactory[0]\n            )\n        )\n    );\n\n    final long timestamp = System.currentTimeMillis();\n\n    final int rows = 50;\n\n    \r\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    \r\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>();\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(Intervals.of(\"2000/2030\")))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(index, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.NoopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n\n\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), new HashMap<String, Object>()).toList();\n    Result<TimeseriesResultValue> result = Iterables.getOnlyElement(results);\n    boolean isRollup = index.isRollup();\n    Assert.assertEquals(rows * (isRollup ? 1 : 2), result.getValue().getLongMetric(\"rows\").intValue());\n    for (int i = 0; i < dimensionCount; ++i) {\n      Assert.assertEquals(\n          \"Failed long sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getLongMetric(\"sumResult\" + i).intValue()\n      );\n      Assert.assertEquals(\n          \"Failed double sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getDoubleMetric(\"doubleSumResult\" + i).intValue()\n      );\n    }\n  }\n","realPath":"processing/src/test/java/org/apache/druid/segment/data/IncrementalIndexTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":368,"status":"N"},{"authorDate":"2018-09-14 09:07:06","commitOrder":2,"curCode":"  public void testConcurrentAddRead()\n      throws InterruptedException, ExecutionException, NoSuchMethodException, IllegalAccessException,\n             InvocationTargetException, InstantiationException\n  {\n\n    final int taskCount = 30;\n    final int concurrentThreads = 3;\n    final int elementsPerThread = 1 << 15;\n\n    final IncrementalIndex incrementalIndex = this.incrementalIndex.getConstructor(\n        IncrementalIndexSchema.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        int.class\n    ).newInstance(\n        new IncrementalIndexSchema.Builder().withMetrics(factories).build(),\n        true,\n        true,\n        false,\n        true,\n        elementsPerThread * taskCount\n    );\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>(dimensionCount + 1);\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    final ListeningExecutorService indexExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"index-executor-%d\")\n                .setPriority(Thread.MIN_PRIORITY)\n                .build()\n        )\n    );\n    final ListeningExecutorService queryExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"query-executor-%d\")\n                .build()\n        )\n    );\n    final long timestamp = System.currentTimeMillis();\n    final Interval queryInterval = Intervals.of(\"1900-01-01T00:00:00Z/2900-01-01T00:00:00Z\");\n    final List<ListenableFuture<?>> indexFutures = new ArrayList<>();\n    final List<ListenableFuture<?>> queryFutures = new ArrayList<>();\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(incrementalIndex, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.NoopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final AtomicInteger currentlyRunning = new AtomicInteger(0);\n    final AtomicBoolean concurrentlyRan = new AtomicBoolean(false);\n    final AtomicBoolean someoneRan = new AtomicBoolean(false);\n    for (int j = 0; j < taskCount; j++) {\n      indexFutures.add(\n          indexExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  currentlyRunning.incrementAndGet();\n                  try {\n                    for (int i = 0; i < elementsPerThread; i++) {\n                      incrementalIndex.add(getLongRow(timestamp + i, 1, dimensionCount));\n                    }\n                  }\n                  catch (IndexSizeExceededException e) {\n                    throw Throwables.propagate(e);\n                  }\n                  currentlyRunning.decrementAndGet();\n                  someoneRan.set(true);\n                }\n              }\n          )\n      );\n\n      queryFutures.add(\n          queryExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n                      factory.createRunner(incrementalIndexSegment),\n                      factory.getToolchest()\n                  );\n                  TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                                .dataSource(\"xxx\")\n                                                .granularity(Granularities.ALL)\n                                                .intervals(ImmutableList.of(queryInterval))\n                                                .aggregators(queryAggregatorFactories)\n                                                .build();\n                  Map<String, Object> context = new HashMap<String, Object>();\n                  List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), context).toList();\n                  for (Result<TimeseriesResultValue> result : results) {\n                    if (someoneRan.get()) {\n                      Assert.assertTrue(result.getValue().getDoubleMetric(\"doubleSumResult0\") > 0);\n                    }\n                  }\n                  if (currentlyRunning.get() > 0) {\n                    concurrentlyRan.set(true);\n                  }\n                }\n              }\n          )\n      );\n\n    }\n    List<ListenableFuture<?>> allFutures = new ArrayList<>(queryFutures.size() + indexFutures.size());\n    allFutures.addAll(queryFutures);\n    allFutures.addAll(indexFutures);\n    Futures.allAsList(allFutures).get();\n    \r\n    queryExecutor.shutdown();\n    indexExecutor.shutdown();\n    QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(queryInterval))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n    Map<String, Object> context = new HashMap<String, Object>();\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), context).toList();\n    final int expectedVal = elementsPerThread * taskCount;\n    for (Result<TimeseriesResultValue> result : results) {\n      Assert.assertEquals(elementsPerThread, result.getValue().getLongMetric(\"rows\").intValue());\n      for (int i = 0; i < dimensionCount; ++i) {\n        Assert.assertEquals(\n            StringUtils.format(\"Failed long sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getLongMetric(StringUtils.format(\"sumResult%s\", i)).intValue()\n        );\n        Assert.assertEquals(\n            StringUtils.format(\"Failed double sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getDoubleMetric(StringUtils.format(\"doubleSumResult%s\", i)).intValue()\n        );\n      }\n    }\n  }\n","date":"2018-09-14 09:07:06","endLine":452,"groupId":"8400","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testConcurrentAddRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/eb/4609cb6881034c00fe5ddb29c1fe6b9ed0536f.src","preCode":"  public void testConcurrentAddRead()\n      throws InterruptedException, ExecutionException, NoSuchMethodException, IllegalAccessException,\n             InvocationTargetException, InstantiationException\n  {\n\n    final int taskCount = 30;\n    final int concurrentThreads = 3;\n    final int elementsPerThread = 1 << 15;\n\n    final IncrementalIndex incrementalIndex = this.incrementalIndex.getConstructor(\n        IncrementalIndexSchema.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        int.class\n    ).newInstance(\n        new IncrementalIndexSchema.Builder().withMetrics(factories).build(),\n        true,\n        true,\n        false,\n        true,\n        elementsPerThread * taskCount\n    );\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>(dimensionCount + 1);\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    final ListeningExecutorService indexExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"index-executor-%d\")\n                .setPriority(Thread.MIN_PRIORITY)\n                .build()\n        )\n    );\n    final ListeningExecutorService queryExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"query-executor-%d\")\n                .build()\n        )\n    );\n    final long timestamp = System.currentTimeMillis();\n    final Interval queryInterval = Intervals.of(\"1900-01-01T00:00:00Z/2900-01-01T00:00:00Z\");\n    final List<ListenableFuture<?>> indexFutures = new LinkedList<>();\n    final List<ListenableFuture<?>> queryFutures = new LinkedList<>();\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(incrementalIndex, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.NoopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final AtomicInteger currentlyRunning = new AtomicInteger(0);\n    final AtomicBoolean concurrentlyRan = new AtomicBoolean(false);\n    final AtomicBoolean someoneRan = new AtomicBoolean(false);\n    for (int j = 0; j < taskCount; j++) {\n      indexFutures.add(\n          indexExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  currentlyRunning.incrementAndGet();\n                  try {\n                    for (int i = 0; i < elementsPerThread; i++) {\n                      incrementalIndex.add(getLongRow(timestamp + i, 1, dimensionCount));\n                    }\n                  }\n                  catch (IndexSizeExceededException e) {\n                    throw Throwables.propagate(e);\n                  }\n                  currentlyRunning.decrementAndGet();\n                  someoneRan.set(true);\n                }\n              }\n          )\n      );\n\n      queryFutures.add(\n          queryExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n                      factory.createRunner(incrementalIndexSegment),\n                      factory.getToolchest()\n                  );\n                  TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                                .dataSource(\"xxx\")\n                                                .granularity(Granularities.ALL)\n                                                .intervals(ImmutableList.of(queryInterval))\n                                                .aggregators(queryAggregatorFactories)\n                                                .build();\n                  Map<String, Object> context = new HashMap<String, Object>();\n                  List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), context).toList();\n                  for (Result<TimeseriesResultValue> result : results) {\n                    if (someoneRan.get()) {\n                      Assert.assertTrue(result.getValue().getDoubleMetric(\"doubleSumResult0\") > 0);\n                    }\n                  }\n                  if (currentlyRunning.get() > 0) {\n                    concurrentlyRan.set(true);\n                  }\n                }\n              }\n          )\n      );\n\n    }\n    List<ListenableFuture<?>> allFutures = new ArrayList<>(queryFutures.size() + indexFutures.size());\n    allFutures.addAll(queryFutures);\n    allFutures.addAll(indexFutures);\n    Futures.allAsList(allFutures).get();\n    \r\n    queryExecutor.shutdown();\n    indexExecutor.shutdown();\n    QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(queryInterval))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n    Map<String, Object> context = new HashMap<String, Object>();\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), context).toList();\n    final int expectedVal = elementsPerThread * taskCount;\n    for (Result<TimeseriesResultValue> result : results) {\n      Assert.assertEquals(elementsPerThread, result.getValue().getLongMetric(\"rows\").intValue());\n      for (int i = 0; i < dimensionCount; ++i) {\n        Assert.assertEquals(\n            StringUtils.format(\"Failed long sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getLongMetric(StringUtils.format(\"sumResult%s\", i)).intValue()\n        );\n        Assert.assertEquals(\n            StringUtils.format(\"Failed double sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getDoubleMetric(StringUtils.format(\"doubleSumResult%s\", i)).intValue()\n        );\n      }\n    }\n  }\n","realPath":"processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexBenchmark.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":287,"status":"M"}],"commitId":"d50b69e6d4c29913de4d670b05523a719dbfd97f","commitMessage":"@@@Prohibit LinkedList (#6112)\n\n* Prohibit LinkedList\n\n* Fix tests\n\n* Fix\n\n* Remove unused import\n","date":"2018-09-14 09:07:06","modifiedFileCount":"24","status":"M","submitter":"Roman Leventov"},{"authorTime":"2019-02-05 01:18:12","codes":[{"authorDate":"2019-02-05 01:18:12","commitOrder":3,"curCode":"  public void testSingleThreadedIndexingAndQuery() throws Exception\n  {\n    final int dimensionCount = 5;\n    final ArrayList<AggregatorFactory> ingestAggregatorFactories = new ArrayList<>();\n    ingestAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      ingestAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n      ingestAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n    }\n\n    final IncrementalIndex index = closerRule.closeLater(\n        indexCreator.createIndex(\n            ingestAggregatorFactories.toArray(\n                new AggregatorFactory[0]\n            )\n        )\n    );\n\n    final long timestamp = System.currentTimeMillis();\n\n    final int rows = 50;\n\n    \r\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    \r\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>();\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(Intervals.of(\"2000/2030\")))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(index, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.noopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n\n\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), new HashMap<String, Object>()).toList();\n    Result<TimeseriesResultValue> result = Iterables.getOnlyElement(results);\n    boolean isRollup = index.isRollup();\n    Assert.assertEquals(rows * (isRollup ? 1 : 2), result.getValue().getLongMetric(\"rows\").intValue());\n    for (int i = 0; i < dimensionCount; ++i) {\n      Assert.assertEquals(\n          \"Failed long sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getLongMetric(\"sumResult\" + i).intValue()\n      );\n      Assert.assertEquals(\n          \"Failed double sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getDoubleMetric(\"doubleSumResult\" + i).intValue()\n      );\n    }\n  }\n","date":"2019-02-05 01:18:12","endLine":462,"groupId":"12409","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testSingleThreadedIndexingAndQuery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/a3/c3478ebedd035d626f762ec395556d298a77c4.src","preCode":"  public void testSingleThreadedIndexingAndQuery() throws Exception\n  {\n    final int dimensionCount = 5;\n    final ArrayList<AggregatorFactory> ingestAggregatorFactories = new ArrayList<>();\n    ingestAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      ingestAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n      ingestAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n    }\n\n    final IncrementalIndex index = closerRule.closeLater(\n        indexCreator.createIndex(\n            ingestAggregatorFactories.toArray(\n                new AggregatorFactory[0]\n            )\n        )\n    );\n\n    final long timestamp = System.currentTimeMillis();\n\n    final int rows = 50;\n\n    \r\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    \r\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>();\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(Intervals.of(\"2000/2030\")))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(index, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.NoopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n\n\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), new HashMap<String, Object>()).toList();\n    Result<TimeseriesResultValue> result = Iterables.getOnlyElement(results);\n    boolean isRollup = index.isRollup();\n    Assert.assertEquals(rows * (isRollup ? 1 : 2), result.getValue().getLongMetric(\"rows\").intValue());\n    for (int i = 0; i < dimensionCount; ++i) {\n      Assert.assertEquals(\n          \"Failed long sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getLongMetric(\"sumResult\" + i).intValue()\n      );\n      Assert.assertEquals(\n          \"Failed double sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getDoubleMetric(\"doubleSumResult\" + i).intValue()\n      );\n    }\n  }\n","realPath":"processing/src/test/java/org/apache/druid/segment/data/IncrementalIndexTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":368,"status":"M"},{"authorDate":"2019-02-05 01:18:12","commitOrder":3,"curCode":"  public void testConcurrentAddRead()\n      throws InterruptedException, ExecutionException, NoSuchMethodException, IllegalAccessException,\n             InvocationTargetException, InstantiationException\n  {\n\n    final int taskCount = 30;\n    final int concurrentThreads = 3;\n    final int elementsPerThread = 1 << 15;\n\n    final IncrementalIndex incrementalIndex = this.incrementalIndex.getConstructor(\n        IncrementalIndexSchema.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        int.class\n    ).newInstance(\n        new IncrementalIndexSchema.Builder().withMetrics(factories).build(),\n        true,\n        true,\n        false,\n        true,\n        elementsPerThread * taskCount\n    );\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>(dimensionCount + 1);\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    final ListeningExecutorService indexExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"index-executor-%d\")\n                .setPriority(Thread.MIN_PRIORITY)\n                .build()\n        )\n    );\n    final ListeningExecutorService queryExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"query-executor-%d\")\n                .build()\n        )\n    );\n    final long timestamp = System.currentTimeMillis();\n    final Interval queryInterval = Intervals.of(\"1900-01-01T00:00:00Z/2900-01-01T00:00:00Z\");\n    final List<ListenableFuture<?>> indexFutures = new ArrayList<>();\n    final List<ListenableFuture<?>> queryFutures = new ArrayList<>();\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(incrementalIndex, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.noopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final AtomicInteger currentlyRunning = new AtomicInteger(0);\n    final AtomicBoolean concurrentlyRan = new AtomicBoolean(false);\n    final AtomicBoolean someoneRan = new AtomicBoolean(false);\n    for (int j = 0; j < taskCount; j++) {\n      indexFutures.add(\n          indexExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  currentlyRunning.incrementAndGet();\n                  try {\n                    for (int i = 0; i < elementsPerThread; i++) {\n                      incrementalIndex.add(getLongRow(timestamp + i, 1, dimensionCount));\n                    }\n                  }\n                  catch (IndexSizeExceededException e) {\n                    throw Throwables.propagate(e);\n                  }\n                  currentlyRunning.decrementAndGet();\n                  someoneRan.set(true);\n                }\n              }\n          )\n      );\n\n      queryFutures.add(\n          queryExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n                      factory.createRunner(incrementalIndexSegment),\n                      factory.getToolchest()\n                  );\n                  TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                                .dataSource(\"xxx\")\n                                                .granularity(Granularities.ALL)\n                                                .intervals(ImmutableList.of(queryInterval))\n                                                .aggregators(queryAggregatorFactories)\n                                                .build();\n                  Map<String, Object> context = new HashMap<String, Object>();\n                  List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), context).toList();\n                  for (Result<TimeseriesResultValue> result : results) {\n                    if (someoneRan.get()) {\n                      Assert.assertTrue(result.getValue().getDoubleMetric(\"doubleSumResult0\") > 0);\n                    }\n                  }\n                  if (currentlyRunning.get() > 0) {\n                    concurrentlyRan.set(true);\n                  }\n                }\n              }\n          )\n      );\n\n    }\n    List<ListenableFuture<?>> allFutures = new ArrayList<>(queryFutures.size() + indexFutures.size());\n    allFutures.addAll(queryFutures);\n    allFutures.addAll(indexFutures);\n    Futures.allAsList(allFutures).get();\n    \r\n    queryExecutor.shutdown();\n    indexExecutor.shutdown();\n    QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(queryInterval))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n    Map<String, Object> context = new HashMap<String, Object>();\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), context).toList();\n    final int expectedVal = elementsPerThread * taskCount;\n    for (Result<TimeseriesResultValue> result : results) {\n      Assert.assertEquals(elementsPerThread, result.getValue().getLongMetric(\"rows\").intValue());\n      for (int i = 0; i < dimensionCount; ++i) {\n        Assert.assertEquals(\n            StringUtils.format(\"Failed long sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getLongMetric(StringUtils.format(\"sumResult%s\", i)).intValue()\n        );\n        Assert.assertEquals(\n            StringUtils.format(\"Failed double sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getDoubleMetric(StringUtils.format(\"doubleSumResult%s\", i)).intValue()\n        );\n      }\n    }\n  }\n","date":"2019-02-05 01:18:12","endLine":452,"groupId":"8400","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testConcurrentAddRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/09/6e4d3de4325c2bcde0f73442869fc813201e22.src","preCode":"  public void testConcurrentAddRead()\n      throws InterruptedException, ExecutionException, NoSuchMethodException, IllegalAccessException,\n             InvocationTargetException, InstantiationException\n  {\n\n    final int taskCount = 30;\n    final int concurrentThreads = 3;\n    final int elementsPerThread = 1 << 15;\n\n    final IncrementalIndex incrementalIndex = this.incrementalIndex.getConstructor(\n        IncrementalIndexSchema.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        int.class\n    ).newInstance(\n        new IncrementalIndexSchema.Builder().withMetrics(factories).build(),\n        true,\n        true,\n        false,\n        true,\n        elementsPerThread * taskCount\n    );\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>(dimensionCount + 1);\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    final ListeningExecutorService indexExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"index-executor-%d\")\n                .setPriority(Thread.MIN_PRIORITY)\n                .build()\n        )\n    );\n    final ListeningExecutorService queryExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"query-executor-%d\")\n                .build()\n        )\n    );\n    final long timestamp = System.currentTimeMillis();\n    final Interval queryInterval = Intervals.of(\"1900-01-01T00:00:00Z/2900-01-01T00:00:00Z\");\n    final List<ListenableFuture<?>> indexFutures = new ArrayList<>();\n    final List<ListenableFuture<?>> queryFutures = new ArrayList<>();\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(incrementalIndex, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.NoopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final AtomicInteger currentlyRunning = new AtomicInteger(0);\n    final AtomicBoolean concurrentlyRan = new AtomicBoolean(false);\n    final AtomicBoolean someoneRan = new AtomicBoolean(false);\n    for (int j = 0; j < taskCount; j++) {\n      indexFutures.add(\n          indexExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  currentlyRunning.incrementAndGet();\n                  try {\n                    for (int i = 0; i < elementsPerThread; i++) {\n                      incrementalIndex.add(getLongRow(timestamp + i, 1, dimensionCount));\n                    }\n                  }\n                  catch (IndexSizeExceededException e) {\n                    throw Throwables.propagate(e);\n                  }\n                  currentlyRunning.decrementAndGet();\n                  someoneRan.set(true);\n                }\n              }\n          )\n      );\n\n      queryFutures.add(\n          queryExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n                      factory.createRunner(incrementalIndexSegment),\n                      factory.getToolchest()\n                  );\n                  TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                                .dataSource(\"xxx\")\n                                                .granularity(Granularities.ALL)\n                                                .intervals(ImmutableList.of(queryInterval))\n                                                .aggregators(queryAggregatorFactories)\n                                                .build();\n                  Map<String, Object> context = new HashMap<String, Object>();\n                  List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), context).toList();\n                  for (Result<TimeseriesResultValue> result : results) {\n                    if (someoneRan.get()) {\n                      Assert.assertTrue(result.getValue().getDoubleMetric(\"doubleSumResult0\") > 0);\n                    }\n                  }\n                  if (currentlyRunning.get() > 0) {\n                    concurrentlyRan.set(true);\n                  }\n                }\n              }\n          )\n      );\n\n    }\n    List<ListenableFuture<?>> allFutures = new ArrayList<>(queryFutures.size() + indexFutures.size());\n    allFutures.addAll(queryFutures);\n    allFutures.addAll(indexFutures);\n    Futures.allAsList(allFutures).get();\n    \r\n    queryExecutor.shutdown();\n    indexExecutor.shutdown();\n    QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(queryInterval))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n    Map<String, Object> context = new HashMap<String, Object>();\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), context).toList();\n    final int expectedVal = elementsPerThread * taskCount;\n    for (Result<TimeseriesResultValue> result : results) {\n      Assert.assertEquals(elementsPerThread, result.getValue().getLongMetric(\"rows\").intValue());\n      for (int i = 0; i < dimensionCount; ++i) {\n        Assert.assertEquals(\n            StringUtils.format(\"Failed long sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getLongMetric(StringUtils.format(\"sumResult%s\", i)).intValue()\n        );\n        Assert.assertEquals(\n            StringUtils.format(\"Failed double sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getDoubleMetric(StringUtils.format(\"doubleSumResult%s\", i)).intValue()\n        );\n      }\n    }\n  }\n","realPath":"processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexBenchmark.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":287,"status":"M"}],"commitId":"0e926e865276f892ddc5b62685d331b822104022","commitMessage":"@@@Prohibit assigning concurrent maps into Map-typed variables and fields and fix a race condition in CoordinatorRuleManager (#6898)\n\n* Prohibit assigning concurrent maps into Map-types variables and fields; Fix a race condition in CoordinatorRuleManager; improve logic in DirectDruidClient and ResourcePool\n\n* Enforce that if compute().  computeIfAbsent().  computeIfPresent() or merge() is called on a ConcurrentHashMap.  it's stored in a ConcurrentHashMap-typed variable.  not ConcurrentMap; add comments explaining get()-before-computeIfAbsent() optimization; refactor Counters; fix a race condition in Intialization.java\n\n* Remove unnecessary comment\n\n* Checkstyle\n\n* Fix getFromExtensions()\n\n* Add a reference to the comment about guarded computeIfAbsent() optimization; IdentityHashMap optimization\n\n* Fix UriCacheGeneratorTest\n\n* Workaround issue with MaterializedViewQueryQueryToolChest\n\n* Strengthen Appenderator's contract regarding concurrency\n","date":"2019-02-05 01:18:12","modifiedFileCount":"101","status":"M","submitter":"Roman Leventov"},{"authorTime":"2019-03-15 05:28:33","codes":[{"authorDate":"2019-02-05 01:18:12","commitOrder":4,"curCode":"  public void testSingleThreadedIndexingAndQuery() throws Exception\n  {\n    final int dimensionCount = 5;\n    final ArrayList<AggregatorFactory> ingestAggregatorFactories = new ArrayList<>();\n    ingestAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      ingestAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n      ingestAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n    }\n\n    final IncrementalIndex index = closerRule.closeLater(\n        indexCreator.createIndex(\n            ingestAggregatorFactories.toArray(\n                new AggregatorFactory[0]\n            )\n        )\n    );\n\n    final long timestamp = System.currentTimeMillis();\n\n    final int rows = 50;\n\n    \r\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    \r\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>();\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(Intervals.of(\"2000/2030\")))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(index, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.noopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n\n\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), new HashMap<String, Object>()).toList();\n    Result<TimeseriesResultValue> result = Iterables.getOnlyElement(results);\n    boolean isRollup = index.isRollup();\n    Assert.assertEquals(rows * (isRollup ? 1 : 2), result.getValue().getLongMetric(\"rows\").intValue());\n    for (int i = 0; i < dimensionCount; ++i) {\n      Assert.assertEquals(\n          \"Failed long sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getLongMetric(\"sumResult\" + i).intValue()\n      );\n      Assert.assertEquals(\n          \"Failed double sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getDoubleMetric(\"doubleSumResult\" + i).intValue()\n      );\n    }\n  }\n","date":"2019-02-05 01:18:12","endLine":462,"groupId":"12409","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testSingleThreadedIndexingAndQuery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/a3/c3478ebedd035d626f762ec395556d298a77c4.src","preCode":"  public void testSingleThreadedIndexingAndQuery() throws Exception\n  {\n    final int dimensionCount = 5;\n    final ArrayList<AggregatorFactory> ingestAggregatorFactories = new ArrayList<>();\n    ingestAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      ingestAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n      ingestAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n    }\n\n    final IncrementalIndex index = closerRule.closeLater(\n        indexCreator.createIndex(\n            ingestAggregatorFactories.toArray(\n                new AggregatorFactory[0]\n            )\n        )\n    );\n\n    final long timestamp = System.currentTimeMillis();\n\n    final int rows = 50;\n\n    \r\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    \r\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>();\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(Intervals.of(\"2000/2030\")))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(index, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.noopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n\n\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), new HashMap<String, Object>()).toList();\n    Result<TimeseriesResultValue> result = Iterables.getOnlyElement(results);\n    boolean isRollup = index.isRollup();\n    Assert.assertEquals(rows * (isRollup ? 1 : 2), result.getValue().getLongMetric(\"rows\").intValue());\n    for (int i = 0; i < dimensionCount; ++i) {\n      Assert.assertEquals(\n          \"Failed long sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getLongMetric(\"sumResult\" + i).intValue()\n      );\n      Assert.assertEquals(\n          \"Failed double sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getDoubleMetric(\"doubleSumResult\" + i).intValue()\n      );\n    }\n  }\n","realPath":"processing/src/test/java/org/apache/druid/segment/data/IncrementalIndexTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":368,"status":"N"},{"authorDate":"2019-03-15 05:28:33","commitOrder":4,"curCode":"  public void testConcurrentAddRead()\n      throws InterruptedException, ExecutionException, NoSuchMethodException, IllegalAccessException,\n             InvocationTargetException, InstantiationException\n  {\n\n    final int taskCount = 30;\n    final int concurrentThreads = 3;\n    final int elementsPerThread = 1 << 15;\n\n    final IncrementalIndex incrementalIndex = this.incrementalIndex.getConstructor(\n        IncrementalIndexSchema.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        int.class\n    ).newInstance(\n        new IncrementalIndexSchema.Builder().withMetrics(factories).build(),\n        true,\n        true,\n        false,\n        true,\n        elementsPerThread * taskCount\n    );\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>(dimensionCount + 1);\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    final ListeningExecutorService indexExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"index-executor-%d\")\n                .setPriority(Thread.MIN_PRIORITY)\n                .build()\n        )\n    );\n    final ListeningExecutorService queryExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"query-executor-%d\")\n                .build()\n        )\n    );\n    final long timestamp = System.currentTimeMillis();\n    final Interval queryInterval = Intervals.of(\"1900-01-01T00:00:00Z/2900-01-01T00:00:00Z\");\n    final List<ListenableFuture<?>> indexFutures = new ArrayList<>();\n    final List<ListenableFuture<?>> queryFutures = new ArrayList<>();\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(incrementalIndex, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.noopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final AtomicInteger currentlyRunning = new AtomicInteger(0);\n    final AtomicBoolean concurrentlyRan = new AtomicBoolean(false);\n    final AtomicBoolean someoneRan = new AtomicBoolean(false);\n    for (int j = 0; j < taskCount; j++) {\n      indexFutures.add(\n          indexExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  currentlyRunning.incrementAndGet();\n                  try {\n                    for (int i = 0; i < elementsPerThread; i++) {\n                      incrementalIndex.add(getLongRow(timestamp + i, 1, dimensionCount));\n                    }\n                  }\n                  catch (IndexSizeExceededException e) {\n                    throw new RuntimeException(e);\n                  }\n                  currentlyRunning.decrementAndGet();\n                  someoneRan.set(true);\n                }\n              }\n          )\n      );\n\n      queryFutures.add(\n          queryExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n                      factory.createRunner(incrementalIndexSegment),\n                      factory.getToolchest()\n                  );\n                  TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                                .dataSource(\"xxx\")\n                                                .granularity(Granularities.ALL)\n                                                .intervals(ImmutableList.of(queryInterval))\n                                                .aggregators(queryAggregatorFactories)\n                                                .build();\n                  Map<String, Object> context = new HashMap<String, Object>();\n                  List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), context).toList();\n                  for (Result<TimeseriesResultValue> result : results) {\n                    if (someoneRan.get()) {\n                      Assert.assertTrue(result.getValue().getDoubleMetric(\"doubleSumResult0\") > 0);\n                    }\n                  }\n                  if (currentlyRunning.get() > 0) {\n                    concurrentlyRan.set(true);\n                  }\n                }\n              }\n          )\n      );\n\n    }\n    List<ListenableFuture<?>> allFutures = new ArrayList<>(queryFutures.size() + indexFutures.size());\n    allFutures.addAll(queryFutures);\n    allFutures.addAll(indexFutures);\n    Futures.allAsList(allFutures).get();\n    \r\n    queryExecutor.shutdown();\n    indexExecutor.shutdown();\n    QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(queryInterval))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n    Map<String, Object> context = new HashMap<String, Object>();\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), context).toList();\n    final int expectedVal = elementsPerThread * taskCount;\n    for (Result<TimeseriesResultValue> result : results) {\n      Assert.assertEquals(elementsPerThread, result.getValue().getLongMetric(\"rows\").intValue());\n      for (int i = 0; i < dimensionCount; ++i) {\n        Assert.assertEquals(\n            StringUtils.format(\"Failed long sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getLongMetric(StringUtils.format(\"sumResult%s\", i)).intValue()\n        );\n        Assert.assertEquals(\n            StringUtils.format(\"Failed double sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getDoubleMetric(StringUtils.format(\"doubleSumResult%s\", i)).intValue()\n        );\n      }\n    }\n  }\n","date":"2019-03-15 05:28:33","endLine":448,"groupId":"0","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testConcurrentAddRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0f/1944af3dd4ef69b00342cddca9d69dc5182bbe.src","preCode":"  public void testConcurrentAddRead()\n      throws InterruptedException, ExecutionException, NoSuchMethodException, IllegalAccessException,\n             InvocationTargetException, InstantiationException\n  {\n\n    final int taskCount = 30;\n    final int concurrentThreads = 3;\n    final int elementsPerThread = 1 << 15;\n\n    final IncrementalIndex incrementalIndex = this.incrementalIndex.getConstructor(\n        IncrementalIndexSchema.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        int.class\n    ).newInstance(\n        new IncrementalIndexSchema.Builder().withMetrics(factories).build(),\n        true,\n        true,\n        false,\n        true,\n        elementsPerThread * taskCount\n    );\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>(dimensionCount + 1);\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    final ListeningExecutorService indexExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"index-executor-%d\")\n                .setPriority(Thread.MIN_PRIORITY)\n                .build()\n        )\n    );\n    final ListeningExecutorService queryExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"query-executor-%d\")\n                .build()\n        )\n    );\n    final long timestamp = System.currentTimeMillis();\n    final Interval queryInterval = Intervals.of(\"1900-01-01T00:00:00Z/2900-01-01T00:00:00Z\");\n    final List<ListenableFuture<?>> indexFutures = new ArrayList<>();\n    final List<ListenableFuture<?>> queryFutures = new ArrayList<>();\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(incrementalIndex, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.noopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final AtomicInteger currentlyRunning = new AtomicInteger(0);\n    final AtomicBoolean concurrentlyRan = new AtomicBoolean(false);\n    final AtomicBoolean someoneRan = new AtomicBoolean(false);\n    for (int j = 0; j < taskCount; j++) {\n      indexFutures.add(\n          indexExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  currentlyRunning.incrementAndGet();\n                  try {\n                    for (int i = 0; i < elementsPerThread; i++) {\n                      incrementalIndex.add(getLongRow(timestamp + i, 1, dimensionCount));\n                    }\n                  }\n                  catch (IndexSizeExceededException e) {\n                    throw Throwables.propagate(e);\n                  }\n                  currentlyRunning.decrementAndGet();\n                  someoneRan.set(true);\n                }\n              }\n          )\n      );\n\n      queryFutures.add(\n          queryExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n                      factory.createRunner(incrementalIndexSegment),\n                      factory.getToolchest()\n                  );\n                  TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                                .dataSource(\"xxx\")\n                                                .granularity(Granularities.ALL)\n                                                .intervals(ImmutableList.of(queryInterval))\n                                                .aggregators(queryAggregatorFactories)\n                                                .build();\n                  Map<String, Object> context = new HashMap<String, Object>();\n                  List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), context).toList();\n                  for (Result<TimeseriesResultValue> result : results) {\n                    if (someoneRan.get()) {\n                      Assert.assertTrue(result.getValue().getDoubleMetric(\"doubleSumResult0\") > 0);\n                    }\n                  }\n                  if (currentlyRunning.get() > 0) {\n                    concurrentlyRan.set(true);\n                  }\n                }\n              }\n          )\n      );\n\n    }\n    List<ListenableFuture<?>> allFutures = new ArrayList<>(queryFutures.size() + indexFutures.size());\n    allFutures.addAll(queryFutures);\n    allFutures.addAll(indexFutures);\n    Futures.allAsList(allFutures).get();\n    \r\n    queryExecutor.shutdown();\n    indexExecutor.shutdown();\n    QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(queryInterval))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n    Map<String, Object> context = new HashMap<String, Object>();\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), context).toList();\n    final int expectedVal = elementsPerThread * taskCount;\n    for (Result<TimeseriesResultValue> result : results) {\n      Assert.assertEquals(elementsPerThread, result.getValue().getLongMetric(\"rows\").intValue());\n      for (int i = 0; i < dimensionCount; ++i) {\n        Assert.assertEquals(\n            StringUtils.format(\"Failed long sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getLongMetric(StringUtils.format(\"sumResult%s\", i)).intValue()\n        );\n        Assert.assertEquals(\n            StringUtils.format(\"Failed double sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getDoubleMetric(StringUtils.format(\"doubleSumResult%s\", i)).intValue()\n        );\n      }\n    }\n  }\n","realPath":"processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexBenchmark.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":283,"status":"M"}],"commitId":"7ada1c49f9735a37808f3ed7656d93ae88b8b925","commitMessage":"@@@Prohibit Throwables.propagate() (#7121)\n\n* Throw caught exception.\n\n* Throw caught exceptions.\n\n* Related checkstyle rule is added to prevent further bugs.\n\n* RuntimeException() is used instead of Throwables.propagate().\n\n* Missing import is added.\n\n* Throwables are propogated if possible.\n\n* Throwables are propogated if possible.\n\n* Throwables are propogated if possible.\n\n* Throwables are propogated if possible.\n\n* * Checkstyle definition is improved.\n* Throwables.propagate() usages are removed.\n\n* Checkstyle pattern is changed for only scanning \"Throwables.propagate(\" instead of checking lookbehind.\n\n* Throwable is kept before firing a Runtime Exception.\n\n* Fix unused assignments.\n","date":"2019-03-15 05:28:33","modifiedFileCount":"228","status":"M","submitter":"Furkan KAMACI"},{"authorTime":"2019-07-24 23:29:03","codes":[{"authorDate":"2019-07-24 23:29:03","commitOrder":5,"curCode":"  public void testSingleThreadedIndexingAndQuery() throws Exception\n  {\n    final int dimensionCount = 5;\n    final ArrayList<AggregatorFactory> ingestAggregatorFactories = new ArrayList<>();\n    ingestAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      ingestAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n      ingestAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n    }\n\n    final IncrementalIndex index = closerRule.closeLater(\n        indexCreator.createIndex(\n            ingestAggregatorFactories.toArray(\n                new AggregatorFactory[0]\n            )\n        )\n    );\n\n    final long timestamp = System.currentTimeMillis();\n\n    final int rows = 50;\n\n    \r\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    \r\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>();\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(Intervals.of(\"2000/2030\")))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(index, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.noopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n\n\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n    Result<TimeseriesResultValue> result = Iterables.getOnlyElement(results);\n    boolean isRollup = index.isRollup();\n    Assert.assertEquals(rows * (isRollup ? 1 : 2), result.getValue().getLongMetric(\"rows\").intValue());\n    for (int i = 0; i < dimensionCount; ++i) {\n      Assert.assertEquals(\n          \"Failed long sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getLongMetric(\"sumResult\" + i).intValue()\n      );\n      Assert.assertEquals(\n          \"Failed double sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getDoubleMetric(\"doubleSumResult\" + i).intValue()\n      );\n    }\n  }\n","date":"2019-07-24 23:29:03","endLine":459,"groupId":"12409","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testSingleThreadedIndexingAndQuery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/31/9e66c81b984a8b847e0bec560e05144d43b0d4.src","preCode":"  public void testSingleThreadedIndexingAndQuery() throws Exception\n  {\n    final int dimensionCount = 5;\n    final ArrayList<AggregatorFactory> ingestAggregatorFactories = new ArrayList<>();\n    ingestAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      ingestAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n      ingestAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n    }\n\n    final IncrementalIndex index = closerRule.closeLater(\n        indexCreator.createIndex(\n            ingestAggregatorFactories.toArray(\n                new AggregatorFactory[0]\n            )\n        )\n    );\n\n    final long timestamp = System.currentTimeMillis();\n\n    final int rows = 50;\n\n    \r\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    \r\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>();\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(Intervals.of(\"2000/2030\")))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(index, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.noopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n\n\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), new HashMap<String, Object>()).toList();\n    Result<TimeseriesResultValue> result = Iterables.getOnlyElement(results);\n    boolean isRollup = index.isRollup();\n    Assert.assertEquals(rows * (isRollup ? 1 : 2), result.getValue().getLongMetric(\"rows\").intValue());\n    for (int i = 0; i < dimensionCount; ++i) {\n      Assert.assertEquals(\n          \"Failed long sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getLongMetric(\"sumResult\" + i).intValue()\n      );\n      Assert.assertEquals(\n          \"Failed double sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getDoubleMetric(\"doubleSumResult\" + i).intValue()\n      );\n    }\n  }\n","realPath":"processing/src/test/java/org/apache/druid/segment/data/IncrementalIndexTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":365,"status":"M"},{"authorDate":"2019-07-24 23:29:03","commitOrder":5,"curCode":"  public void testConcurrentAddRead()\n      throws InterruptedException, ExecutionException, NoSuchMethodException, IllegalAccessException,\n             InvocationTargetException, InstantiationException\n  {\n\n    final int taskCount = 30;\n    final int concurrentThreads = 3;\n    final int elementsPerThread = 1 << 15;\n\n    final IncrementalIndex incrementalIndex = this.incrementalIndex.getConstructor(\n        IncrementalIndexSchema.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        int.class\n    ).newInstance(\n        new IncrementalIndexSchema.Builder().withMetrics(factories).build(),\n        true,\n        true,\n        false,\n        true,\n        elementsPerThread * taskCount\n    );\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>(dimensionCount + 1);\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    final ListeningExecutorService indexExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"index-executor-%d\")\n                .setPriority(Thread.MIN_PRIORITY)\n                .build()\n        )\n    );\n    final ListeningExecutorService queryExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"query-executor-%d\")\n                .build()\n        )\n    );\n    final long timestamp = System.currentTimeMillis();\n    final Interval queryInterval = Intervals.of(\"1900-01-01T00:00:00Z/2900-01-01T00:00:00Z\");\n    final List<ListenableFuture<?>> indexFutures = new ArrayList<>();\n    final List<ListenableFuture<?>> queryFutures = new ArrayList<>();\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(incrementalIndex, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.noopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final AtomicInteger currentlyRunning = new AtomicInteger(0);\n    final AtomicBoolean concurrentlyRan = new AtomicBoolean(false);\n    final AtomicBoolean someoneRan = new AtomicBoolean(false);\n    for (int j = 0; j < taskCount; j++) {\n      indexFutures.add(\n          indexExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  currentlyRunning.incrementAndGet();\n                  try {\n                    for (int i = 0; i < elementsPerThread; i++) {\n                      incrementalIndex.add(getLongRow(timestamp + i, 1, dimensionCount));\n                    }\n                  }\n                  catch (IndexSizeExceededException e) {\n                    throw new RuntimeException(e);\n                  }\n                  currentlyRunning.decrementAndGet();\n                  someoneRan.set(true);\n                }\n              }\n          )\n      );\n\n      queryFutures.add(\n          queryExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n                      factory.createRunner(incrementalIndexSegment),\n                      factory.getToolchest()\n                  );\n                  TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                                .dataSource(\"xxx\")\n                                                .granularity(Granularities.ALL)\n                                                .intervals(ImmutableList.of(queryInterval))\n                                                .aggregators(queryAggregatorFactories)\n                                                .build();\n                  List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n                  for (Result<TimeseriesResultValue> result : results) {\n                    if (someoneRan.get()) {\n                      Assert.assertTrue(result.getValue().getDoubleMetric(\"doubleSumResult0\") > 0);\n                    }\n                  }\n                  if (currentlyRunning.get() > 0) {\n                    concurrentlyRan.set(true);\n                  }\n                }\n              }\n          )\n      );\n\n    }\n    List<ListenableFuture<?>> allFutures = new ArrayList<>(queryFutures.size() + indexFutures.size());\n    allFutures.addAll(queryFutures);\n    allFutures.addAll(indexFutures);\n    Futures.allAsList(allFutures).get();\n    \r\n    queryExecutor.shutdown();\n    indexExecutor.shutdown();\n    QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(queryInterval))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n    final int expectedVal = elementsPerThread * taskCount;\n    for (Result<TimeseriesResultValue> result : results) {\n      Assert.assertEquals(elementsPerThread, result.getValue().getLongMetric(\"rows\").intValue());\n      for (int i = 0; i < dimensionCount; ++i) {\n        Assert.assertEquals(\n            StringUtils.format(\"Failed long sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getLongMetric(StringUtils.format(\"sumResult%s\", i)).intValue()\n        );\n        Assert.assertEquals(\n            StringUtils.format(\"Failed double sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getDoubleMetric(StringUtils.format(\"doubleSumResult%s\", i)).intValue()\n        );\n      }\n    }\n  }\n","date":"2019-07-24 23:29:03","endLine":444,"groupId":"16665","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testConcurrentAddRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/92/4ab996c8f42c7328c4d871dff5deb36b28bf47.src","preCode":"  public void testConcurrentAddRead()\n      throws InterruptedException, ExecutionException, NoSuchMethodException, IllegalAccessException,\n             InvocationTargetException, InstantiationException\n  {\n\n    final int taskCount = 30;\n    final int concurrentThreads = 3;\n    final int elementsPerThread = 1 << 15;\n\n    final IncrementalIndex incrementalIndex = this.incrementalIndex.getConstructor(\n        IncrementalIndexSchema.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        int.class\n    ).newInstance(\n        new IncrementalIndexSchema.Builder().withMetrics(factories).build(),\n        true,\n        true,\n        false,\n        true,\n        elementsPerThread * taskCount\n    );\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>(dimensionCount + 1);\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    final ListeningExecutorService indexExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"index-executor-%d\")\n                .setPriority(Thread.MIN_PRIORITY)\n                .build()\n        )\n    );\n    final ListeningExecutorService queryExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"query-executor-%d\")\n                .build()\n        )\n    );\n    final long timestamp = System.currentTimeMillis();\n    final Interval queryInterval = Intervals.of(\"1900-01-01T00:00:00Z/2900-01-01T00:00:00Z\");\n    final List<ListenableFuture<?>> indexFutures = new ArrayList<>();\n    final List<ListenableFuture<?>> queryFutures = new ArrayList<>();\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(incrementalIndex, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.noopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final AtomicInteger currentlyRunning = new AtomicInteger(0);\n    final AtomicBoolean concurrentlyRan = new AtomicBoolean(false);\n    final AtomicBoolean someoneRan = new AtomicBoolean(false);\n    for (int j = 0; j < taskCount; j++) {\n      indexFutures.add(\n          indexExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  currentlyRunning.incrementAndGet();\n                  try {\n                    for (int i = 0; i < elementsPerThread; i++) {\n                      incrementalIndex.add(getLongRow(timestamp + i, 1, dimensionCount));\n                    }\n                  }\n                  catch (IndexSizeExceededException e) {\n                    throw new RuntimeException(e);\n                  }\n                  currentlyRunning.decrementAndGet();\n                  someoneRan.set(true);\n                }\n              }\n          )\n      );\n\n      queryFutures.add(\n          queryExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n                      factory.createRunner(incrementalIndexSegment),\n                      factory.getToolchest()\n                  );\n                  TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                                .dataSource(\"xxx\")\n                                                .granularity(Granularities.ALL)\n                                                .intervals(ImmutableList.of(queryInterval))\n                                                .aggregators(queryAggregatorFactories)\n                                                .build();\n                  Map<String, Object> context = new HashMap<String, Object>();\n                  List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), context).toList();\n                  for (Result<TimeseriesResultValue> result : results) {\n                    if (someoneRan.get()) {\n                      Assert.assertTrue(result.getValue().getDoubleMetric(\"doubleSumResult0\") > 0);\n                    }\n                  }\n                  if (currentlyRunning.get() > 0) {\n                    concurrentlyRan.set(true);\n                  }\n                }\n              }\n          )\n      );\n\n    }\n    List<ListenableFuture<?>> allFutures = new ArrayList<>(queryFutures.size() + indexFutures.size());\n    allFutures.addAll(queryFutures);\n    allFutures.addAll(indexFutures);\n    Futures.allAsList(allFutures).get();\n    \r\n    queryExecutor.shutdown();\n    indexExecutor.shutdown();\n    QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(queryInterval))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n    Map<String, Object> context = new HashMap<String, Object>();\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query), context).toList();\n    final int expectedVal = elementsPerThread * taskCount;\n    for (Result<TimeseriesResultValue> result : results) {\n      Assert.assertEquals(elementsPerThread, result.getValue().getLongMetric(\"rows\").intValue());\n      for (int i = 0; i < dimensionCount; ++i) {\n        Assert.assertEquals(\n            StringUtils.format(\"Failed long sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getLongMetric(StringUtils.format(\"sumResult%s\", i)).intValue()\n        );\n        Assert.assertEquals(\n            StringUtils.format(\"Failed double sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getDoubleMetric(StringUtils.format(\"doubleSumResult%s\", i)).intValue()\n        );\n      }\n    }\n  }\n","realPath":"processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexBenchmark.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":281,"status":"M"}],"commitId":"799d20249fe6333ea86b020f6d09c91fa4d3f998","commitMessage":"@@@Response context refactoring (#8110)\n\n* Response context refactoring\n\n* Serialization/Deserialization of ResponseContext\n\n* Added java doc comments\n\n* Renamed vars related to ResponseContext\n\n* Renamed empty() methods to createEmpty()\n\n* Fixed ResponseContext usage\n\n* Renamed multiple ResponseContext static fields\n\n* Added PublicApi annotations\n\n* Renamed QueryResponseContext class to ResourceIOReaderWriter\n\n* Moved the protected method below public static constants\n\n* Added createEmpty method to ResponseContext with DefaultResponseContext creation\n\n* Fixed inspection error\n\n* Added comments to the ResponseContext length limit and ResponseContext\nhttp header name\n\n* Added a comment of possible future refactoring\n\n* Removed .gitignore file of indexing-service\n\n* Removed a never-used method\n\n* VisibleForTesting method reducing boilerplate\n\nCo-Authored-By: Clint Wylie <cjwylie@gmail.com>\n\n* Reduced boilerplate\n\n* Renamed the method serialize to serializeWith\n\n* Removed unused import\n\n* Fixed incorrectly refactored test method\n\n* Added comments for ResponseContext keys\n\n* Fixed incorrectly refactored test method\n\n* Fixed IntervalChunkingQueryRunnerTest mocks\n","date":"2019-07-24 23:29:03","modifiedFileCount":"142","status":"M","submitter":"Eugene Sevastianov"},{"authorTime":"2019-08-23 18:13:54","codes":[{"authorDate":"2019-07-24 23:29:03","commitOrder":6,"curCode":"  public void testSingleThreadedIndexingAndQuery() throws Exception\n  {\n    final int dimensionCount = 5;\n    final ArrayList<AggregatorFactory> ingestAggregatorFactories = new ArrayList<>();\n    ingestAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      ingestAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n      ingestAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n    }\n\n    final IncrementalIndex index = closerRule.closeLater(\n        indexCreator.createIndex(\n            ingestAggregatorFactories.toArray(\n                new AggregatorFactory[0]\n            )\n        )\n    );\n\n    final long timestamp = System.currentTimeMillis();\n\n    final int rows = 50;\n\n    \r\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    \r\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>();\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(Intervals.of(\"2000/2030\")))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(index, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.noopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n\n\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n    Result<TimeseriesResultValue> result = Iterables.getOnlyElement(results);\n    boolean isRollup = index.isRollup();\n    Assert.assertEquals(rows * (isRollup ? 1 : 2), result.getValue().getLongMetric(\"rows\").intValue());\n    for (int i = 0; i < dimensionCount; ++i) {\n      Assert.assertEquals(\n          \"Failed long sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getLongMetric(\"sumResult\" + i).intValue()\n      );\n      Assert.assertEquals(\n          \"Failed double sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getDoubleMetric(\"doubleSumResult\" + i).intValue()\n      );\n    }\n  }\n","date":"2019-07-24 23:29:03","endLine":459,"groupId":"12409","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testSingleThreadedIndexingAndQuery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/31/9e66c81b984a8b847e0bec560e05144d43b0d4.src","preCode":"  public void testSingleThreadedIndexingAndQuery() throws Exception\n  {\n    final int dimensionCount = 5;\n    final ArrayList<AggregatorFactory> ingestAggregatorFactories = new ArrayList<>();\n    ingestAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      ingestAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n      ingestAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n    }\n\n    final IncrementalIndex index = closerRule.closeLater(\n        indexCreator.createIndex(\n            ingestAggregatorFactories.toArray(\n                new AggregatorFactory[0]\n            )\n        )\n    );\n\n    final long timestamp = System.currentTimeMillis();\n\n    final int rows = 50;\n\n    \r\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    \r\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>();\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(Intervals.of(\"2000/2030\")))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(index, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.noopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n\n\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n    Result<TimeseriesResultValue> result = Iterables.getOnlyElement(results);\n    boolean isRollup = index.isRollup();\n    Assert.assertEquals(rows * (isRollup ? 1 : 2), result.getValue().getLongMetric(\"rows\").intValue());\n    for (int i = 0; i < dimensionCount; ++i) {\n      Assert.assertEquals(\n          \"Failed long sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getLongMetric(\"sumResult\" + i).intValue()\n      );\n      Assert.assertEquals(\n          \"Failed double sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getDoubleMetric(\"doubleSumResult\" + i).intValue()\n      );\n    }\n  }\n","realPath":"processing/src/test/java/org/apache/druid/segment/data/IncrementalIndexTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":365,"status":"N"},{"authorDate":"2019-08-23 18:13:54","commitOrder":6,"curCode":"  public void testConcurrentAddRead()\n      throws InterruptedException, ExecutionException, NoSuchMethodException, IllegalAccessException,\n             InvocationTargetException, InstantiationException\n  {\n\n    final int taskCount = 30;\n    final int concurrentThreads = 3;\n    final int elementsPerThread = 1 << 15;\n\n    final IncrementalIndex incrementalIndex = this.incrementalIndex.getConstructor(\n        IncrementalIndexSchema.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        int.class\n    ).newInstance(\n        new IncrementalIndexSchema.Builder().withMetrics(factories).build(),\n        true,\n        true,\n        false,\n        true,\n        elementsPerThread * taskCount\n    );\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>(DIMENSION_COUNT + 1);\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < DIMENSION_COUNT; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    final ListeningExecutorService indexExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"index-executor-%d\")\n                .setPriority(Thread.MIN_PRIORITY)\n                .build()\n        )\n    );\n    final ListeningExecutorService queryExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"query-executor-%d\")\n                .build()\n        )\n    );\n    final long timestamp = System.currentTimeMillis();\n    final Interval queryInterval = Intervals.of(\"1900-01-01T00:00:00Z/2900-01-01T00:00:00Z\");\n    final List<ListenableFuture<?>> indexFutures = new ArrayList<>();\n    final List<ListenableFuture<?>> queryFutures = new ArrayList<>();\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(incrementalIndex, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.noopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final AtomicInteger currentlyRunning = new AtomicInteger(0);\n    final AtomicBoolean concurrentlyRan = new AtomicBoolean(false);\n    final AtomicBoolean someoneRan = new AtomicBoolean(false);\n    for (int j = 0; j < taskCount; j++) {\n      indexFutures.add(\n          indexExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  currentlyRunning.incrementAndGet();\n                  try {\n                    for (int i = 0; i < elementsPerThread; i++) {\n                      incrementalIndex.add(getLongRow(timestamp + i, 1, DIMENSION_COUNT));\n                    }\n                  }\n                  catch (IndexSizeExceededException e) {\n                    throw new RuntimeException(e);\n                  }\n                  currentlyRunning.decrementAndGet();\n                  someoneRan.set(true);\n                }\n              }\n          )\n      );\n\n      queryFutures.add(\n          queryExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n                      factory.createRunner(incrementalIndexSegment),\n                      factory.getToolchest()\n                  );\n                  TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                                .dataSource(\"xxx\")\n                                                .granularity(Granularities.ALL)\n                                                .intervals(ImmutableList.of(queryInterval))\n                                                .aggregators(queryAggregatorFactories)\n                                                .build();\n                  List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n                  for (Result<TimeseriesResultValue> result : results) {\n                    if (someoneRan.get()) {\n                      Assert.assertTrue(result.getValue().getDoubleMetric(\"doubleSumResult0\") > 0);\n                    }\n                  }\n                  if (currentlyRunning.get() > 0) {\n                    concurrentlyRan.set(true);\n                  }\n                }\n              }\n          )\n      );\n\n    }\n    List<ListenableFuture<?>> allFutures = new ArrayList<>(queryFutures.size() + indexFutures.size());\n    allFutures.addAll(queryFutures);\n    allFutures.addAll(indexFutures);\n    Futures.allAsList(allFutures).get();\n    \r\n    queryExecutor.shutdown();\n    indexExecutor.shutdown();\n    QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(queryInterval))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n    final int expectedVal = elementsPerThread * taskCount;\n    for (Result<TimeseriesResultValue> result : results) {\n      Assert.assertEquals(elementsPerThread, result.getValue().getLongMetric(\"rows\").intValue());\n      for (int i = 0; i < DIMENSION_COUNT; ++i) {\n        Assert.assertEquals(\n            StringUtils.format(\"Failed long sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getLongMetric(StringUtils.format(\"sumResult%s\", i)).intValue()\n        );\n        Assert.assertEquals(\n            StringUtils.format(\"Failed double sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getDoubleMetric(StringUtils.format(\"doubleSumResult%s\", i)).intValue()\n        );\n      }\n    }\n  }\n","date":"2019-08-23 18:13:54","endLine":444,"groupId":"16665","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testConcurrentAddRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/17/607728b735c4251a21b169c7787541ca3495ec.src","preCode":"  public void testConcurrentAddRead()\n      throws InterruptedException, ExecutionException, NoSuchMethodException, IllegalAccessException,\n             InvocationTargetException, InstantiationException\n  {\n\n    final int taskCount = 30;\n    final int concurrentThreads = 3;\n    final int elementsPerThread = 1 << 15;\n\n    final IncrementalIndex incrementalIndex = this.incrementalIndex.getConstructor(\n        IncrementalIndexSchema.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        int.class\n    ).newInstance(\n        new IncrementalIndexSchema.Builder().withMetrics(factories).build(),\n        true,\n        true,\n        false,\n        true,\n        elementsPerThread * taskCount\n    );\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>(dimensionCount + 1);\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    final ListeningExecutorService indexExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"index-executor-%d\")\n                .setPriority(Thread.MIN_PRIORITY)\n                .build()\n        )\n    );\n    final ListeningExecutorService queryExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"query-executor-%d\")\n                .build()\n        )\n    );\n    final long timestamp = System.currentTimeMillis();\n    final Interval queryInterval = Intervals.of(\"1900-01-01T00:00:00Z/2900-01-01T00:00:00Z\");\n    final List<ListenableFuture<?>> indexFutures = new ArrayList<>();\n    final List<ListenableFuture<?>> queryFutures = new ArrayList<>();\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(incrementalIndex, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.noopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final AtomicInteger currentlyRunning = new AtomicInteger(0);\n    final AtomicBoolean concurrentlyRan = new AtomicBoolean(false);\n    final AtomicBoolean someoneRan = new AtomicBoolean(false);\n    for (int j = 0; j < taskCount; j++) {\n      indexFutures.add(\n          indexExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  currentlyRunning.incrementAndGet();\n                  try {\n                    for (int i = 0; i < elementsPerThread; i++) {\n                      incrementalIndex.add(getLongRow(timestamp + i, 1, dimensionCount));\n                    }\n                  }\n                  catch (IndexSizeExceededException e) {\n                    throw new RuntimeException(e);\n                  }\n                  currentlyRunning.decrementAndGet();\n                  someoneRan.set(true);\n                }\n              }\n          )\n      );\n\n      queryFutures.add(\n          queryExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n                      factory.createRunner(incrementalIndexSegment),\n                      factory.getToolchest()\n                  );\n                  TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                                .dataSource(\"xxx\")\n                                                .granularity(Granularities.ALL)\n                                                .intervals(ImmutableList.of(queryInterval))\n                                                .aggregators(queryAggregatorFactories)\n                                                .build();\n                  List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n                  for (Result<TimeseriesResultValue> result : results) {\n                    if (someoneRan.get()) {\n                      Assert.assertTrue(result.getValue().getDoubleMetric(\"doubleSumResult0\") > 0);\n                    }\n                  }\n                  if (currentlyRunning.get() > 0) {\n                    concurrentlyRan.set(true);\n                  }\n                }\n              }\n          )\n      );\n\n    }\n    List<ListenableFuture<?>> allFutures = new ArrayList<>(queryFutures.size() + indexFutures.size());\n    allFutures.addAll(queryFutures);\n    allFutures.addAll(indexFutures);\n    Futures.allAsList(allFutures).get();\n    \r\n    queryExecutor.shutdown();\n    indexExecutor.shutdown();\n    QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(queryInterval))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n    final int expectedVal = elementsPerThread * taskCount;\n    for (Result<TimeseriesResultValue> result : results) {\n      Assert.assertEquals(elementsPerThread, result.getValue().getLongMetric(\"rows\").intValue());\n      for (int i = 0; i < dimensionCount; ++i) {\n        Assert.assertEquals(\n            StringUtils.format(\"Failed long sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getLongMetric(StringUtils.format(\"sumResult%s\", i)).intValue()\n        );\n        Assert.assertEquals(\n            StringUtils.format(\"Failed double sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getDoubleMetric(StringUtils.format(\"doubleSumResult%s\", i)).intValue()\n        );\n      }\n    }\n  }\n","realPath":"processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexBenchmark.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":281,"status":"M"}],"commitId":"33f0753a70361e7d345a488034f76a889f7c3682","commitMessage":"@@@Add Checkstyle for constant name static final (#8060)\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* check ctyle for constant field name\n\n* merging with upstream\n\n* review-1\n\n* unknow changes\n\n* unknow changes\n\n* review-2\n\n* merging with master\n\n* review-2 1 changes\n\n* review changes-2 2\n\n* bug fix\n","date":"2019-08-23 18:13:54","modifiedFileCount":"298","status":"M","submitter":"SandishKumarHN"},{"authorTime":"2020-01-20 09:14:23","codes":[{"authorDate":"2020-01-20 09:14:23","commitOrder":7,"curCode":"  public void testSingleThreadedIndexingAndQuery() throws Exception\n  {\n    final int dimensionCount = 5;\n    final ArrayList<AggregatorFactory> ingestAggregatorFactories = new ArrayList<>();\n    ingestAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      ingestAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n      ingestAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n    }\n\n    final IncrementalIndex index = closerRule.closeLater(\n        indexCreator.createIndex(\n            ingestAggregatorFactories.toArray(\n                new AggregatorFactory[0]\n            )\n        )\n    );\n\n    final long timestamp = System.currentTimeMillis();\n\n    final int rows = 50;\n\n    \r\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    \r\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>();\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(Intervals.of(\"2000/2030\")))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(index, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n\n\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n    Result<TimeseriesResultValue> result = Iterables.getOnlyElement(results);\n    boolean isRollup = index.isRollup();\n    Assert.assertEquals(rows * (isRollup ? 1 : 2), result.getValue().getLongMetric(\"rows\").intValue());\n    for (int i = 0; i < dimensionCount; ++i) {\n      Assert.assertEquals(\n          \"Failed long sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getLongMetric(\"sumResult\" + i).intValue()\n      );\n      Assert.assertEquals(\n          \"Failed double sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getDoubleMetric(\"doubleSumResult\" + i).intValue()\n      );\n    }\n  }\n","date":"2020-01-20 09:14:23","endLine":460,"groupId":"12409","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testSingleThreadedIndexingAndQuery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/df/c3e533b2895f24a6c4ec3629557314cf0efa57.src","preCode":"  public void testSingleThreadedIndexingAndQuery() throws Exception\n  {\n    final int dimensionCount = 5;\n    final ArrayList<AggregatorFactory> ingestAggregatorFactories = new ArrayList<>();\n    ingestAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      ingestAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n      ingestAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n    }\n\n    final IncrementalIndex index = closerRule.closeLater(\n        indexCreator.createIndex(\n            ingestAggregatorFactories.toArray(\n                new AggregatorFactory[0]\n            )\n        )\n    );\n\n    final long timestamp = System.currentTimeMillis();\n\n    final int rows = 50;\n\n    \r\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    \r\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>();\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(Intervals.of(\"2000/2030\")))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(index, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.noopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n\n\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n    Result<TimeseriesResultValue> result = Iterables.getOnlyElement(results);\n    boolean isRollup = index.isRollup();\n    Assert.assertEquals(rows * (isRollup ? 1 : 2), result.getValue().getLongMetric(\"rows\").intValue());\n    for (int i = 0; i < dimensionCount; ++i) {\n      Assert.assertEquals(\n          \"Failed long sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getLongMetric(\"sumResult\" + i).intValue()\n      );\n      Assert.assertEquals(\n          \"Failed double sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getDoubleMetric(\"doubleSumResult\" + i).intValue()\n      );\n    }\n  }\n","realPath":"processing/src/test/java/org/apache/druid/segment/data/IncrementalIndexTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":366,"status":"M"},{"authorDate":"2020-01-20 09:14:23","commitOrder":7,"curCode":"  public void testConcurrentAddRead()\n      throws InterruptedException, ExecutionException, NoSuchMethodException, IllegalAccessException,\n             InvocationTargetException, InstantiationException\n  {\n\n    final int taskCount = 30;\n    final int concurrentThreads = 3;\n    final int elementsPerThread = 1 << 15;\n\n    final IncrementalIndex incrementalIndex = this.incrementalIndex.getConstructor(\n        IncrementalIndexSchema.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        int.class\n    ).newInstance(\n        new IncrementalIndexSchema.Builder().withMetrics(factories).build(),\n        true,\n        true,\n        false,\n        true,\n        elementsPerThread * taskCount\n    );\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>(DIMENSION_COUNT + 1);\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < DIMENSION_COUNT; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    final ListeningExecutorService indexExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"index-executor-%d\")\n                .setPriority(Thread.MIN_PRIORITY)\n                .build()\n        )\n    );\n    final ListeningExecutorService queryExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"query-executor-%d\")\n                .build()\n        )\n    );\n    final long timestamp = System.currentTimeMillis();\n    final Interval queryInterval = Intervals.of(\"1900-01-01T00:00:00Z/2900-01-01T00:00:00Z\");\n    final List<ListenableFuture<?>> indexFutures = new ArrayList<>();\n    final List<ListenableFuture<?>> queryFutures = new ArrayList<>();\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(incrementalIndex, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final AtomicInteger currentlyRunning = new AtomicInteger(0);\n    final AtomicBoolean concurrentlyRan = new AtomicBoolean(false);\n    final AtomicBoolean someoneRan = new AtomicBoolean(false);\n    for (int j = 0; j < taskCount; j++) {\n      indexFutures.add(\n          indexExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  currentlyRunning.incrementAndGet();\n                  try {\n                    for (int i = 0; i < elementsPerThread; i++) {\n                      incrementalIndex.add(getLongRow(timestamp + i, 1, DIMENSION_COUNT));\n                    }\n                  }\n                  catch (IndexSizeExceededException e) {\n                    throw new RuntimeException(e);\n                  }\n                  currentlyRunning.decrementAndGet();\n                  someoneRan.set(true);\n                }\n              }\n          )\n      );\n\n      queryFutures.add(\n          queryExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n                      factory.createRunner(incrementalIndexSegment),\n                      factory.getToolchest()\n                  );\n                  TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                                .dataSource(\"xxx\")\n                                                .granularity(Granularities.ALL)\n                                                .intervals(ImmutableList.of(queryInterval))\n                                                .aggregators(queryAggregatorFactories)\n                                                .build();\n                  List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n                  for (Result<TimeseriesResultValue> result : results) {\n                    if (someoneRan.get()) {\n                      Assert.assertTrue(result.getValue().getDoubleMetric(\"doubleSumResult0\") > 0);\n                    }\n                  }\n                  if (currentlyRunning.get() > 0) {\n                    concurrentlyRan.set(true);\n                  }\n                }\n              }\n          )\n      );\n\n    }\n    List<ListenableFuture<?>> allFutures = new ArrayList<>(queryFutures.size() + indexFutures.size());\n    allFutures.addAll(queryFutures);\n    allFutures.addAll(indexFutures);\n    Futures.allAsList(allFutures).get();\n    \r\n    queryExecutor.shutdown();\n    indexExecutor.shutdown();\n    QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(queryInterval))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n    final int expectedVal = elementsPerThread * taskCount;\n    for (Result<TimeseriesResultValue> result : results) {\n      Assert.assertEquals(elementsPerThread, result.getValue().getLongMetric(\"rows\").intValue());\n      for (int i = 0; i < DIMENSION_COUNT; ++i) {\n        Assert.assertEquals(\n            StringUtils.format(\"Failed long sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getLongMetric(StringUtils.format(\"sumResult%s\", i)).intValue()\n        );\n        Assert.assertEquals(\n            StringUtils.format(\"Failed double sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getDoubleMetric(StringUtils.format(\"doubleSumResult%s\", i)).intValue()\n        );\n      }\n    }\n  }\n","date":"2020-01-20 09:14:23","endLine":444,"groupId":"19774","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testConcurrentAddRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/55/dba3f7ceeb48cf2f50937a28d3e21a393ea2a7.src","preCode":"  public void testConcurrentAddRead()\n      throws InterruptedException, ExecutionException, NoSuchMethodException, IllegalAccessException,\n             InvocationTargetException, InstantiationException\n  {\n\n    final int taskCount = 30;\n    final int concurrentThreads = 3;\n    final int elementsPerThread = 1 << 15;\n\n    final IncrementalIndex incrementalIndex = this.incrementalIndex.getConstructor(\n        IncrementalIndexSchema.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        int.class\n    ).newInstance(\n        new IncrementalIndexSchema.Builder().withMetrics(factories).build(),\n        true,\n        true,\n        false,\n        true,\n        elementsPerThread * taskCount\n    );\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>(DIMENSION_COUNT + 1);\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < DIMENSION_COUNT; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    final ListeningExecutorService indexExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"index-executor-%d\")\n                .setPriority(Thread.MIN_PRIORITY)\n                .build()\n        )\n    );\n    final ListeningExecutorService queryExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"query-executor-%d\")\n                .build()\n        )\n    );\n    final long timestamp = System.currentTimeMillis();\n    final Interval queryInterval = Intervals.of(\"1900-01-01T00:00:00Z/2900-01-01T00:00:00Z\");\n    final List<ListenableFuture<?>> indexFutures = new ArrayList<>();\n    final List<ListenableFuture<?>> queryFutures = new ArrayList<>();\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(incrementalIndex, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(QueryRunnerTestHelper.noopIntervalChunkingQueryRunnerDecorator()),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final AtomicInteger currentlyRunning = new AtomicInteger(0);\n    final AtomicBoolean concurrentlyRan = new AtomicBoolean(false);\n    final AtomicBoolean someoneRan = new AtomicBoolean(false);\n    for (int j = 0; j < taskCount; j++) {\n      indexFutures.add(\n          indexExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  currentlyRunning.incrementAndGet();\n                  try {\n                    for (int i = 0; i < elementsPerThread; i++) {\n                      incrementalIndex.add(getLongRow(timestamp + i, 1, DIMENSION_COUNT));\n                    }\n                  }\n                  catch (IndexSizeExceededException e) {\n                    throw new RuntimeException(e);\n                  }\n                  currentlyRunning.decrementAndGet();\n                  someoneRan.set(true);\n                }\n              }\n          )\n      );\n\n      queryFutures.add(\n          queryExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n                      factory.createRunner(incrementalIndexSegment),\n                      factory.getToolchest()\n                  );\n                  TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                                .dataSource(\"xxx\")\n                                                .granularity(Granularities.ALL)\n                                                .intervals(ImmutableList.of(queryInterval))\n                                                .aggregators(queryAggregatorFactories)\n                                                .build();\n                  List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n                  for (Result<TimeseriesResultValue> result : results) {\n                    if (someoneRan.get()) {\n                      Assert.assertTrue(result.getValue().getDoubleMetric(\"doubleSumResult0\") > 0);\n                    }\n                  }\n                  if (currentlyRunning.get() > 0) {\n                    concurrentlyRan.set(true);\n                  }\n                }\n              }\n          )\n      );\n\n    }\n    List<ListenableFuture<?>> allFutures = new ArrayList<>(queryFutures.size() + indexFutures.size());\n    allFutures.addAll(queryFutures);\n    allFutures.addAll(indexFutures);\n    Futures.allAsList(allFutures).get();\n    \r\n    queryExecutor.shutdown();\n    indexExecutor.shutdown();\n    QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(queryInterval))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n    final int expectedVal = elementsPerThread * taskCount;\n    for (Result<TimeseriesResultValue> result : results) {\n      Assert.assertEquals(elementsPerThread, result.getValue().getLongMetric(\"rows\").intValue());\n      for (int i = 0; i < DIMENSION_COUNT; ++i) {\n        Assert.assertEquals(\n            StringUtils.format(\"Failed long sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getLongMetric(StringUtils.format(\"sumResult%s\", i)).intValue()\n        );\n        Assert.assertEquals(\n            StringUtils.format(\"Failed double sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getDoubleMetric(StringUtils.format(\"doubleSumResult%s\", i)).intValue()\n        );\n      }\n    }\n  }\n","realPath":"processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexBenchmark.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":281,"status":"M"}],"commitId":"d21054f7c5f72f9db1ecfb72b46ee866876f1e4b","commitMessage":"@@@Remove the deprecated interval-chunking stuff. (#9216)\n\n* Remove the deprecated interval-chunking stuff.\n\nSee https://github.com/apache/druid/pull/6591.  https://github.com/apache/druid/pull/4004#issuecomment-284171911 for details.\n\n* Remove unused import.\n\n* Remove chunkInterval too.\n","date":"2020-01-20 09:14:23","modifiedFileCount":"65","status":"M","submitter":"Gian Merlino"},{"authorTime":"2020-01-20 09:14:23","codes":[{"authorDate":"2021-01-08 14:18:47","commitOrder":8,"curCode":"  public void testSingleThreadedIndexingAndQuery() throws Exception\n  {\n    final int dimensionCount = 5;\n    final ArrayList<AggregatorFactory> ingestAggregatorFactories = new ArrayList<>();\n    ingestAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      ingestAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n      ingestAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n    }\n\n    final IncrementalIndex<?> index = indexCreator.createIndex(\n        (Object) ingestAggregatorFactories.toArray(\n            new AggregatorFactory[0]\n        )\n    );\n\n    final long timestamp = System.currentTimeMillis();\n\n    final int rows = 50;\n\n    \r\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    \r\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>();\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(Intervals.of(\"2000/2030\")))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(index, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n\n\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n    Result<TimeseriesResultValue> result = Iterables.getOnlyElement(results);\n    boolean isRollup = index.isRollup();\n    Assert.assertEquals(rows * (isRollup ? 1 : 2), result.getValue().getLongMetric(\"rows\").intValue());\n    for (int i = 0; i < dimensionCount; ++i) {\n      Assert.assertEquals(\n          \"Failed long sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getLongMetric(\"sumResult\" + i).intValue()\n      );\n      Assert.assertEquals(\n          \"Failed double sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getDoubleMetric(\"doubleSumResult\" + i).intValue()\n      );\n    }\n  }\n","date":"2021-01-08 14:18:47","endLine":408,"groupId":"105623","id":15,"instanceNumber":1,"isCurCommit":1,"methodName":"testSingleThreadedIndexingAndQuery","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/9b/5d2f3b12025e2c29d3090eb493cdae1cdd518b.src","preCode":"  public void testSingleThreadedIndexingAndQuery() throws Exception\n  {\n    final int dimensionCount = 5;\n    final ArrayList<AggregatorFactory> ingestAggregatorFactories = new ArrayList<>();\n    ingestAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      ingestAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n      ingestAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"Dim_%s\", i)\n          )\n      );\n    }\n\n    final IncrementalIndex index = closerRule.closeLater(\n        indexCreator.createIndex(\n            ingestAggregatorFactories.toArray(\n                new AggregatorFactory[0]\n            )\n        )\n    );\n\n    final long timestamp = System.currentTimeMillis();\n\n    final int rows = 50;\n\n    \r\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    for (int i = 0; i < rows; i++) {\n      index.add(getLongRow(timestamp + i, dimensionCount));\n    }\n\n    \r\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>();\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < dimensionCount; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(Intervals.of(\"2000/2030\")))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(index, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n\n\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n    Result<TimeseriesResultValue> result = Iterables.getOnlyElement(results);\n    boolean isRollup = index.isRollup();\n    Assert.assertEquals(rows * (isRollup ? 1 : 2), result.getValue().getLongMetric(\"rows\").intValue());\n    for (int i = 0; i < dimensionCount; ++i) {\n      Assert.assertEquals(\n          \"Failed long sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getLongMetric(\"sumResult\" + i).intValue()\n      );\n      Assert.assertEquals(\n          \"Failed double sum on dimension \" + i,\n          2 * rows,\n          result.getValue().getDoubleMetric(\"doubleSumResult\" + i).intValue()\n      );\n    }\n  }\n","realPath":"processing/src/test/java/org/apache/druid/segment/data/IncrementalIndexTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":316,"status":"M"},{"authorDate":"2020-01-20 09:14:23","commitOrder":8,"curCode":"  public void testConcurrentAddRead()\n      throws InterruptedException, ExecutionException, NoSuchMethodException, IllegalAccessException,\n             InvocationTargetException, InstantiationException\n  {\n\n    final int taskCount = 30;\n    final int concurrentThreads = 3;\n    final int elementsPerThread = 1 << 15;\n\n    final IncrementalIndex incrementalIndex = this.incrementalIndex.getConstructor(\n        IncrementalIndexSchema.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        int.class\n    ).newInstance(\n        new IncrementalIndexSchema.Builder().withMetrics(factories).build(),\n        true,\n        true,\n        false,\n        true,\n        elementsPerThread * taskCount\n    );\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>(DIMENSION_COUNT + 1);\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < DIMENSION_COUNT; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    final ListeningExecutorService indexExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"index-executor-%d\")\n                .setPriority(Thread.MIN_PRIORITY)\n                .build()\n        )\n    );\n    final ListeningExecutorService queryExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"query-executor-%d\")\n                .build()\n        )\n    );\n    final long timestamp = System.currentTimeMillis();\n    final Interval queryInterval = Intervals.of(\"1900-01-01T00:00:00Z/2900-01-01T00:00:00Z\");\n    final List<ListenableFuture<?>> indexFutures = new ArrayList<>();\n    final List<ListenableFuture<?>> queryFutures = new ArrayList<>();\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(incrementalIndex, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final AtomicInteger currentlyRunning = new AtomicInteger(0);\n    final AtomicBoolean concurrentlyRan = new AtomicBoolean(false);\n    final AtomicBoolean someoneRan = new AtomicBoolean(false);\n    for (int j = 0; j < taskCount; j++) {\n      indexFutures.add(\n          indexExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  currentlyRunning.incrementAndGet();\n                  try {\n                    for (int i = 0; i < elementsPerThread; i++) {\n                      incrementalIndex.add(getLongRow(timestamp + i, 1, DIMENSION_COUNT));\n                    }\n                  }\n                  catch (IndexSizeExceededException e) {\n                    throw new RuntimeException(e);\n                  }\n                  currentlyRunning.decrementAndGet();\n                  someoneRan.set(true);\n                }\n              }\n          )\n      );\n\n      queryFutures.add(\n          queryExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n                      factory.createRunner(incrementalIndexSegment),\n                      factory.getToolchest()\n                  );\n                  TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                                .dataSource(\"xxx\")\n                                                .granularity(Granularities.ALL)\n                                                .intervals(ImmutableList.of(queryInterval))\n                                                .aggregators(queryAggregatorFactories)\n                                                .build();\n                  List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n                  for (Result<TimeseriesResultValue> result : results) {\n                    if (someoneRan.get()) {\n                      Assert.assertTrue(result.getValue().getDoubleMetric(\"doubleSumResult0\") > 0);\n                    }\n                  }\n                  if (currentlyRunning.get() > 0) {\n                    concurrentlyRan.set(true);\n                  }\n                }\n              }\n          )\n      );\n\n    }\n    List<ListenableFuture<?>> allFutures = new ArrayList<>(queryFutures.size() + indexFutures.size());\n    allFutures.addAll(queryFutures);\n    allFutures.addAll(indexFutures);\n    Futures.allAsList(allFutures).get();\n    \r\n    queryExecutor.shutdown();\n    indexExecutor.shutdown();\n    QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(queryInterval))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n    final int expectedVal = elementsPerThread * taskCount;\n    for (Result<TimeseriesResultValue> result : results) {\n      Assert.assertEquals(elementsPerThread, result.getValue().getLongMetric(\"rows\").intValue());\n      for (int i = 0; i < DIMENSION_COUNT; ++i) {\n        Assert.assertEquals(\n            StringUtils.format(\"Failed long sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getLongMetric(StringUtils.format(\"sumResult%s\", i)).intValue()\n        );\n        Assert.assertEquals(\n            StringUtils.format(\"Failed double sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getDoubleMetric(StringUtils.format(\"doubleSumResult%s\", i)).intValue()\n        );\n      }\n    }\n  }\n","date":"2020-01-20 09:14:23","endLine":444,"groupId":"105623","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testConcurrentAddRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/55/dba3f7ceeb48cf2f50937a28d3e21a393ea2a7.src","preCode":"  public void testConcurrentAddRead()\n      throws InterruptedException, ExecutionException, NoSuchMethodException, IllegalAccessException,\n             InvocationTargetException, InstantiationException\n  {\n\n    final int taskCount = 30;\n    final int concurrentThreads = 3;\n    final int elementsPerThread = 1 << 15;\n\n    final IncrementalIndex incrementalIndex = this.incrementalIndex.getConstructor(\n        IncrementalIndexSchema.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        boolean.class,\n        int.class\n    ).newInstance(\n        new IncrementalIndexSchema.Builder().withMetrics(factories).build(),\n        true,\n        true,\n        false,\n        true,\n        elementsPerThread * taskCount\n    );\n    final ArrayList<AggregatorFactory> queryAggregatorFactories = new ArrayList<>(DIMENSION_COUNT + 1);\n    queryAggregatorFactories.add(new CountAggregatorFactory(\"rows\"));\n    for (int i = 0; i < DIMENSION_COUNT; ++i) {\n      queryAggregatorFactories.add(\n          new LongSumAggregatorFactory(\n              StringUtils.format(\"sumResult%s\", i),\n              StringUtils.format(\"sumResult%s\", i)\n          )\n      );\n      queryAggregatorFactories.add(\n          new DoubleSumAggregatorFactory(\n              StringUtils.format(\"doubleSumResult%s\", i),\n              StringUtils.format(\"doubleSumResult%s\", i)\n          )\n      );\n    }\n\n    final ListeningExecutorService indexExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"index-executor-%d\")\n                .setPriority(Thread.MIN_PRIORITY)\n                .build()\n        )\n    );\n    final ListeningExecutorService queryExecutor = MoreExecutors.listeningDecorator(\n        Executors.newFixedThreadPool(\n            concurrentThreads,\n            new ThreadFactoryBuilder()\n                .setDaemon(false)\n                .setNameFormat(\"query-executor-%d\")\n                .build()\n        )\n    );\n    final long timestamp = System.currentTimeMillis();\n    final Interval queryInterval = Intervals.of(\"1900-01-01T00:00:00Z/2900-01-01T00:00:00Z\");\n    final List<ListenableFuture<?>> indexFutures = new ArrayList<>();\n    final List<ListenableFuture<?>> queryFutures = new ArrayList<>();\n    final Segment incrementalIndexSegment = new IncrementalIndexSegment(incrementalIndex, null);\n    final QueryRunnerFactory factory = new TimeseriesQueryRunnerFactory(\n        new TimeseriesQueryQueryToolChest(),\n        new TimeseriesQueryEngine(),\n        QueryRunnerTestHelper.NOOP_QUERYWATCHER\n    );\n    final AtomicInteger currentlyRunning = new AtomicInteger(0);\n    final AtomicBoolean concurrentlyRan = new AtomicBoolean(false);\n    final AtomicBoolean someoneRan = new AtomicBoolean(false);\n    for (int j = 0; j < taskCount; j++) {\n      indexFutures.add(\n          indexExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  currentlyRunning.incrementAndGet();\n                  try {\n                    for (int i = 0; i < elementsPerThread; i++) {\n                      incrementalIndex.add(getLongRow(timestamp + i, 1, DIMENSION_COUNT));\n                    }\n                  }\n                  catch (IndexSizeExceededException e) {\n                    throw new RuntimeException(e);\n                  }\n                  currentlyRunning.decrementAndGet();\n                  someoneRan.set(true);\n                }\n              }\n          )\n      );\n\n      queryFutures.add(\n          queryExecutor.submit(\n              new Runnable()\n              {\n                @Override\n                public void run()\n                {\n                  QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n                      factory.createRunner(incrementalIndexSegment),\n                      factory.getToolchest()\n                  );\n                  TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                                .dataSource(\"xxx\")\n                                                .granularity(Granularities.ALL)\n                                                .intervals(ImmutableList.of(queryInterval))\n                                                .aggregators(queryAggregatorFactories)\n                                                .build();\n                  List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n                  for (Result<TimeseriesResultValue> result : results) {\n                    if (someoneRan.get()) {\n                      Assert.assertTrue(result.getValue().getDoubleMetric(\"doubleSumResult0\") > 0);\n                    }\n                  }\n                  if (currentlyRunning.get() > 0) {\n                    concurrentlyRan.set(true);\n                  }\n                }\n              }\n          )\n      );\n\n    }\n    List<ListenableFuture<?>> allFutures = new ArrayList<>(queryFutures.size() + indexFutures.size());\n    allFutures.addAll(queryFutures);\n    allFutures.addAll(indexFutures);\n    Futures.allAsList(allFutures).get();\n    \r\n    queryExecutor.shutdown();\n    indexExecutor.shutdown();\n    QueryRunner<Result<TimeseriesResultValue>> runner = new FinalizeResultsQueryRunner<Result<TimeseriesResultValue>>(\n        factory.createRunner(incrementalIndexSegment),\n        factory.getToolchest()\n    );\n    TimeseriesQuery query = Druids.newTimeseriesQueryBuilder()\n                                  .dataSource(\"xxx\")\n                                  .granularity(Granularities.ALL)\n                                  .intervals(ImmutableList.of(queryInterval))\n                                  .aggregators(queryAggregatorFactories)\n                                  .build();\n    List<Result<TimeseriesResultValue>> results = runner.run(QueryPlus.wrap(query)).toList();\n    final int expectedVal = elementsPerThread * taskCount;\n    for (Result<TimeseriesResultValue> result : results) {\n      Assert.assertEquals(elementsPerThread, result.getValue().getLongMetric(\"rows\").intValue());\n      for (int i = 0; i < DIMENSION_COUNT; ++i) {\n        Assert.assertEquals(\n            StringUtils.format(\"Failed long sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getLongMetric(StringUtils.format(\"sumResult%s\", i)).intValue()\n        );\n        Assert.assertEquals(\n            StringUtils.format(\"Failed double sum on dimension %d\", i),\n            expectedVal,\n            result.getValue().getDoubleMetric(StringUtils.format(\"doubleSumResult%s\", i)).intValue()\n        );\n      }\n    }\n  }\n","realPath":"processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexBenchmark.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":281,"status":"N"}],"commitId":"08ab82f55ca856d60dfe1088c1c0393428b0bb6d","commitMessage":"@@@IncrementalIndex Tests and Benchmarks Parametrization (#10593)\n\n* Remove redundant IncrementalIndex.Builder\n\n* Parametrize incremental index tests and benchmarks\n\n- Reveal and fix a bug in OffheapIncrementalIndex\n\n* Fix forbiddenapis error: Forbidden method invocation: java.lang.String#format(java.lang.String. java.lang.Object[]) [Uses default locale]\n\n* Fix Intellij errors: declared exception is never thrown\n\n* Add documentation and validate before closing objects on tearDown.\n\n* Add documentation to OffheapIncrementalIndexTestSpec\n\n* Doc corrections and minor changes.\n\n* Add logging for generated rows.\n\n* Refactor new tests/benchmarks.\n\n* Improve IncrementalIndexCreator documentation\n\n* Add required tests for DataGenerator\n\n* Revert \"rollupOpportunity\" to be a string","date":"2021-01-08 14:18:47","modifiedFileCount":"62","status":"M","submitter":"Liran Funaro"}]
