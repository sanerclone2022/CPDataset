[{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public void testNoDataIngestionTasks() throws Exception\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null, false);\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\");\n    taskQueue.shutdown(\"id2\");\n    taskQueue.shutdown(\"id3\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2018-08-31 00:56:26","endLine":2026,"groupId":"18827","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testNoDataIngestionTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5d/773db661d0c8e87703ae229b128551316998e5.src","preCode":"  public void testNoDataIngestionTasks() throws Exception\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null, false);\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\");\n    taskQueue.shutdown(\"id2\");\n    taskQueue.shutdown(\"id3\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1942,"status":"B"},{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException, ExecutionException, TimeoutException, JsonProcessingException\n  {\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null, false);\n    \r\n    final Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    final Map<Integer, Long> fakeCheckpoints = Collections.emptyMap();\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        ((KafkaIndexTask) id1).getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, checkpoints.get(0))),\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, fakeCheckpoints))\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","date":"2018-08-31 00:56:26","endLine":2129,"groupId":"979","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testCheckpointForInactiveTaskGroup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5d/773db661d0c8e87703ae229b128551316998e5.src","preCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException, ExecutionException, TimeoutException, JsonProcessingException\n  {\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null, false);\n    \r\n    final Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    final Map<Integer, Long> fakeCheckpoints = Collections.emptyMap();\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        ((KafkaIndexTask) id1).getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, checkpoints.get(0))),\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, fakeCheckpoints))\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2029,"status":"B"}],"commitId":"431d3d8497f9079857c3baa7ae7ab4cb44a22355","commitMessage":"@@@Rename io.druid to org.apache.druid. (#6266)\n\n* Rename io.druid to org.apache.druid.\n\n* Fix META-INF files and remove some benchmark results.\n\n* MonitorsConfig update for metrics package migration.\n\n* Reorder some dimensions in inner queries for some reason.\n\n* Fix protobuf tests.\n","date":"2018-08-31 00:56:26","modifiedFileCount":"5","status":"B","submitter":"Gian Merlino"},{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-11-16 10:01:56","commitOrder":2,"curCode":"  public void testNoDataIngestionTasks() throws Exception\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null, false);\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2018-11-16 10:01:56","endLine":2056,"groupId":"18827","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testNoDataIngestionTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/9b/4a5a6cf4afd687a410813d77e5a040693c65ce.src","preCode":"  public void testNoDataIngestionTasks() throws Exception\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null, false);\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\");\n    taskQueue.shutdown(\"id2\");\n    taskQueue.shutdown(\"id3\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1972,"status":"M"},{"authorDate":"2018-08-31 00:56:26","commitOrder":2,"curCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException, ExecutionException, TimeoutException, JsonProcessingException\n  {\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null, false);\n    \r\n    final Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    final Map<Integer, Long> fakeCheckpoints = Collections.emptyMap();\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        ((KafkaIndexTask) id1).getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, checkpoints.get(0))),\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, fakeCheckpoints))\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","date":"2018-08-31 00:56:26","endLine":2129,"groupId":"979","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testCheckpointForInactiveTaskGroup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5d/773db661d0c8e87703ae229b128551316998e5.src","preCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException, ExecutionException, TimeoutException, JsonProcessingException\n  {\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null, false);\n    \r\n    final Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    final Map<Integer, Long> fakeCheckpoints = Collections.emptyMap();\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        ((KafkaIndexTask) id1).getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, checkpoints.get(0))),\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, fakeCheckpoints))\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2029,"status":"N"}],"commitId":"d738ce4d2a4430cf95919e27b0eede171cbdf66c","commitMessage":"@@@Enforce logging when killing a task (#6621)\n\n* Enforce logging when killing a task\n\n* fix test\n\n* address comment\n\n* address comment\n","date":"2018-11-16 10:01:56","modifiedFileCount":"14","status":"M","submitter":"Jihoon Son"},{"authorTime":"2018-12-22 03:49:24","codes":[{"authorDate":"2018-12-22 03:49:24","commitOrder":3,"curCode":"  public void testNoDataIngestionTasks() throws Exception\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null, false);\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2018-12-22 03:49:24","endLine":2161,"groupId":"3871","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testNoDataIngestionTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/42/7b9d1742dec9346159f08cf020ea57960091eb.src","preCode":"  public void testNoDataIngestionTasks() throws Exception\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null, false);\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2068,"status":"M"},{"authorDate":"2018-12-22 03:49:24","commitOrder":3,"curCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException, ExecutionException, TimeoutException, JsonProcessingException\n  {\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null, false);\n    \r\n    final Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    final Map<Integer, Long> fakeCheckpoints = Collections.emptyMap();\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        ((KafkaIndexTask) id1).getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, checkpoints.get(0))),\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, fakeCheckpoints))\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","date":"2018-12-22 03:49:24","endLine":2264,"groupId":"18526","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testCheckpointForInactiveTaskGroup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/42/7b9d1742dec9346159f08cf020ea57960091eb.src","preCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException, ExecutionException, TimeoutException, JsonProcessingException\n  {\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null, false);\n    \r\n    final Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE))\n        .andReturn(new KafkaDataSourceMetadata(null))\n        .anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    final Map<Integer, Long> fakeCheckpoints = Collections.emptyMap();\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        ((KafkaIndexTask) id1).getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, checkpoints.get(0))),\n        new KafkaDataSourceMetadata(new KafkaPartitions(topic, fakeCheckpoints))\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2164,"status":"M"}],"commitId":"7c7997e8a1183a7bffad731ca94e8b4c381e8665","commitMessage":"@@@Add Kinesis Indexing Service to core Druid (#6431)\n\n* created seekablestream classes\n\n* created seekablestreamsupervisor class\n\n* first attempt to integrate kafa indexing service to use SeekableStream\n\n* seekablestream bug fixes\n\n* kafkarecordsupplier\n\n* integrated kafka indexing service with seekablestream\n\n* implemented resume/suspend and refactored some package names\n\n* moved kinesis indexing service into core druid extensions\n\n* merged some changes from kafka supervisor race condition\n\n* integrated kinesis-indexing-service with seekablestream\n\n* unite tests for kinesis-indexing-service\n\n* various bug fixes for kinesis-indexing-service\n\n* refactored kinesisindexingtask\n\n* finished up more kinesis unit tests\n\n* more bug fixes for kinesis-indexing-service\n\n* finsihed refactoring kinesis unit tests\n\n* removed KinesisParititons and KafkaPartitions to use SeekableStreamPartitions\n\n* kinesis-indexing-service code cleanup and docs\n\n* merge #6291\n\nmerge #6337\n\nmerge #6383\n\n* added more docs and reordered methods\n\n* fixd kinesis tests after merging master and added docs in seekablestream\n\n* fix various things from pr comment\n\n* improve recordsupplier and add unit tests\n\n* migrated to aws-java-sdk-kinesis\n\n* merge changes from master\n\n* fix pom files and forbiddenapi checks\n\n* checkpoint JavaType bug fix\n\n* fix pom and stuff\n\n* disable checkpointing in kinesis\n\n* fix kinesis sequence number null in closed shard\n\n* merge changes from master\n\n* fixes for kinesis tasks\n\n* capitalized <partitionType.  sequenceType>\n\n* removed abstract class loggers\n\n* conform to guava api restrictions\n\n* add docker for travis other modules test\n\n* address comments\n\n* improve RecordSupplier to supply records in batch\n\n* fix strict compile issue\n\n* add test scope for localstack dependency\n\n* kinesis indexing task refactoring\n\n* comments\n\n* github comments\n\n* minor fix\n\n* removed unneeded readme\n\n* fix deserialization bug\n\n* fix various bugs\n\n* KinesisRecordSupplier unable to catch up to earliest position in stream bug fix\n\n* minor changes to kinesis\n\n* implement deaggregate for kinesis\n\n* Merge remote-tracking branch 'upstream/master' into seekablestream\n\n* fix kinesis offset discrepancy with kafka\n\n* kinesis record supplier disable getPosition\n\n* pr comments\n\n* mock for kinesis tests and remove docker dependency for unit tests\n\n* PR comments\n\n* avg lag in kafkasupervisor #6587\n\n* refacotred SequenceMetadata in taskRunners\n\n* small fix\n\n* more small fix\n\n* recordsupplier resource leak\n\n* revert .travis.yml formatting\n\n* fix style\n\n* kinesis docs\n\n* doc part2\n\n* more docs\n\n* comments\n\n* comments*2\n\n* revert string replace changes\n\n* comments\n\n* teamcity\n\n* comments part 1\n\n* comments part 2\n\n* comments part 3\n\n* merge #6754\n\n* fix injection binding\n\n* comments\n\n* KinesisRegion refactor\n\n* comments part idk lol\n\n* can't think of a commit msg anymore\n\n* remove possiblyResetDataSourceMetadata() for IncrementalPublishingTaskRunner\n\n* commmmmmmmmmments\n\n* extra error handling in KinesisRecordSupplier getRecords\n\n* comments\n\n* quickfix\n\n* typo\n\n* oof\n","date":"2018-12-22 03:49:24","modifiedFileCount":"22","status":"M","submitter":"Joshua Sun"},{"authorTime":"2019-02-19 03:50:08","codes":[{"authorDate":"2019-02-19 03:50:08","commitOrder":4,"curCode":"  public void testNoDataIngestionTasks() throws Exception\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null);\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2019-02-19 03:50:08","endLine":2159,"groupId":"3871","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testNoDataIngestionTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f2/db280c51eb85380271ce661d1c415120afa0e0.src","preCode":"  public void testNoDataIngestionTasks() throws Exception\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null, false);\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2066,"status":"M"},{"authorDate":"2019-02-19 03:50:08","commitOrder":4,"curCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException, ExecutionException, TimeoutException, JsonProcessingException\n  {\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null);\n    \r\n    final Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    final Map<Integer, Long> fakeCheckpoints = Collections.emptyMap();\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        ((KafkaIndexTask) id1).getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, checkpoints.get(0))),\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, fakeCheckpoints))\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","date":"2019-02-19 03:50:08","endLine":2262,"groupId":"18526","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testCheckpointForInactiveTaskGroup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f2/db280c51eb85380271ce661d1c415120afa0e0.src","preCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException, ExecutionException, TimeoutException, JsonProcessingException\n  {\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null, false);\n    \r\n    final Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    final Map<Integer, Long> fakeCheckpoints = Collections.emptyMap();\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        ((KafkaIndexTask) id1).getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, checkpoints.get(0))),\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, fakeCheckpoints))\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2162,"status":"M"}],"commitId":"80a2ef7be46c4fc807ac2a74507b8ba8b6a44049","commitMessage":"@@@Support kafka transactional topics (#5404) (#6496)\n\n* Support kafka transactional topics\n\n* update kafka to version 2.0.0\n* Remove the skipOffsetGaps option since it's not used anymore\n* Adjust kafka consumer to use transactional semantics\n* Update tests\n\n* Remove unused import from test\n\n* Fix compilation\n\n* Invoke transaction api to fix a unit test\n\n* temporary modification of travis.yml for debugging\n\n* another attempt to get travis tasklogs\n\n* update kafka to 2.0.1 at all places\n\n* Remove druid-kafka-eight dependency from integration-tests.  remove the kafka firehose test and deprecate kafka-eight classes\n\n* Add deprecated in docs for kafka-eight and kafka-simple extensions\n\n* Remove skipOffsetGaps and code changes for transaction support\n\n* Fix indentation\n\n* remove skipOffsetGaps from kinesis\n\n* Add transaction api to KafkaRecordSupplierTest\n\n* Fix indent\n\n* Fix test\n\n* update kafka version to 2.1.0\n","date":"2019-02-19 03:50:08","modifiedFileCount":"25","status":"M","submitter":"Surekha"},{"authorTime":"2019-03-22 04:12:22","codes":[{"authorDate":"2019-03-22 04:12:22","commitOrder":5,"curCode":"  public void testNoDataIngestionTasks() throws Exception\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null);\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2019-03-22 04:12:22","endLine":2231,"groupId":"3871","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testNoDataIngestionTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b3/0b75ee7886fa6f4c326a81770066d6778e2033.src","preCode":"  public void testNoDataIngestionTasks() throws Exception\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null);\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2138,"status":"M"},{"authorDate":"2019-03-22 04:12:22","commitOrder":5,"curCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException, ExecutionException, TimeoutException, JsonProcessingException\n  {\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null);\n    \r\n    final Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    final Map<Integer, Long> fakeCheckpoints = Collections.emptyMap();\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        ((KafkaIndexTask) id1).getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(new SeekableStreamStartSequenceNumbers<>(topic, checkpoints.get(0), ImmutableSet.of())),\n        new KafkaDataSourceMetadata(new SeekableStreamStartSequenceNumbers<>(topic, fakeCheckpoints, fakeCheckpoints.keySet()))\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","date":"2019-03-22 04:12:22","endLine":2343,"groupId":"18526","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testCheckpointForInactiveTaskGroup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b3/0b75ee7886fa6f4c326a81770066d6778e2033.src","preCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException, ExecutionException, TimeoutException, JsonProcessingException\n  {\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null);\n    \r\n    final Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(topic, ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    final Map<Integer, Long> fakeCheckpoints = Collections.emptyMap();\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        ((KafkaIndexTask) id1).getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, checkpoints.get(0))),\n        new KafkaDataSourceMetadata(new SeekableStreamPartitions<>(topic, fakeCheckpoints))\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2234,"status":"M"}],"commitId":"0c5dcf5586e33607849e397209f3eb0b10661f1e","commitMessage":"@@@Fix exclusivity for start offset in kinesis indexing service & check exclusivity properly in IndexerSQLMetadataStorageCoordinator (#7291)\n\n* Fix exclusivity for start offset in kinesis indexing service\n\n* some adjustment\n\n* Fix SeekableStreamDataSourceMetadata\n\n* Add missing javadocs\n\n* Add missing comments and unit test\n\n* fix SeekableStreamStartSequenceNumbers.plus and add comments\n\n* remove extra exclusivePartitions in KafkaIOConfig and fix downgrade issue\n\n* Add javadocs\n\n* fix compilation\n\n* fix test\n\n* remove unused variable\n","date":"2019-03-22 04:12:22","modifiedFileCount":"30","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-04-11 09:16:38","codes":[{"authorDate":"2019-04-11 09:16:38","commitOrder":6,"curCode":"  public void testNoDataIngestionTasks() throws Exception\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2019-04-11 09:16:38","endLine":2199,"groupId":"3871","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testNoDataIngestionTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b6/e7b3406771642225b6ce082b4b2bc68e735a67.src","preCode":"  public void testNoDataIngestionTasks() throws Exception\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null);\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2106,"status":"M"},{"authorDate":"2019-04-11 09:16:38","commitOrder":6,"curCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException, ExecutionException, TimeoutException, JsonProcessingException\n  {\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    \r\n    final Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    final Map<Integer, Long> fakeCheckpoints = Collections.emptyMap();\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        ((KafkaIndexTask) id1).getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(new SeekableStreamStartSequenceNumbers<>(\n            topic,\n            checkpoints.get(0),\n            ImmutableSet.of()\n        )),\n        new KafkaDataSourceMetadata(new SeekableStreamStartSequenceNumbers<>(\n            topic,\n            fakeCheckpoints,\n            fakeCheckpoints.keySet()\n        ))\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","date":"2019-04-11 09:16:38","endLine":2319,"groupId":"18526","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testCheckpointForInactiveTaskGroup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b6/e7b3406771642225b6ce082b4b2bc68e735a67.src","preCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException, ExecutionException, TimeoutException, JsonProcessingException\n  {\n    supervisor = getSupervisor(2, 1, true, \"PT1S\", null, null);\n    \r\n    final Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    final Map<Integer, Long> fakeCheckpoints = Collections.emptyMap();\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        ((KafkaIndexTask) id1).getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(new SeekableStreamStartSequenceNumbers<>(topic, checkpoints.get(0), ImmutableSet.of())),\n        new KafkaDataSourceMetadata(new SeekableStreamStartSequenceNumbers<>(topic, fakeCheckpoints, fakeCheckpoints.keySet()))\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2202,"status":"M"}],"commitId":"2771ed50b0f07b0ee519da72ed9f4877466f8be4","commitMessage":"@@@Support Kafka supervisor adopting running tasks between versions  (#7212)\n\n* Recompute hash in isTaskCurrent() and added tests\n\n* Fixed checkstyle stuff\n\n* Fixed failing tests\n\n* Make TestableKafkaSupervisorWithCustomIsTaskCurrent static\n\n* Add doc\n\n* baseSequenceName change\n\n* Added comment\n\n* WIP\n\n* Fixed imports\n\n* Undid lambda change for diff sake\n\n* Cleanup\n\n* Added comment\n\n* Reinsert Kafka tests\n\n* Readded kinesis test\n\n* Readd bad partition assignment in kinesis supervisor test\n\n* Nit\n\n* Misnamed var\n","date":"2019-04-11 09:16:38","modifiedFileCount":"6","status":"M","submitter":"Justin Borromeo"},{"authorTime":"2019-06-01 08:16:01","codes":[{"authorDate":"2019-06-01 08:16:01","commitOrder":7,"curCode":"  public void testNoDataIngestionTasks() throws Exception\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2019-06-01 08:16:01","endLine":2217,"groupId":"3871","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testNoDataIngestionTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/2e/ff41c7106c8b4e0ad8fd243280839cac2354e7.src","preCode":"  public void testNoDataIngestionTasks() throws Exception\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2122,"status":"M"},{"authorDate":"2019-06-01 08:16:01","commitOrder":7,"curCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException, ExecutionException, TimeoutException, JsonProcessingException\n  {\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    final Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    final Map<Integer, Long> fakeCheckpoints = Collections.emptyMap();\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        ((KafkaIndexTask) id1).getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(new SeekableStreamStartSequenceNumbers<>(\n            topic,\n            checkpoints.get(0),\n            ImmutableSet.of()\n        )),\n        new KafkaDataSourceMetadata(new SeekableStreamStartSequenceNumbers<>(\n            topic,\n            fakeCheckpoints,\n            fakeCheckpoints.keySet()\n        ))\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","date":"2019-06-01 08:16:01","endLine":2339,"groupId":"18526","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testCheckpointForInactiveTaskGroup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/2e/ff41c7106c8b4e0ad8fd243280839cac2354e7.src","preCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException, ExecutionException, TimeoutException, JsonProcessingException\n  {\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    \r\n    final Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    final Map<Integer, Long> fakeCheckpoints = Collections.emptyMap();\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        ((KafkaIndexTask) id1).getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(new SeekableStreamStartSequenceNumbers<>(\n            topic,\n            checkpoints.get(0),\n            ImmutableSet.of()\n        )),\n        new KafkaDataSourceMetadata(new SeekableStreamStartSequenceNumbers<>(\n            topic,\n            fakeCheckpoints,\n            fakeCheckpoints.keySet()\n        ))\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2220,"status":"M"}],"commitId":"8032c4add8f78d0c15044d0847201c618e27dc25","commitMessage":"@@@Add errors and state to stream supervisor status API endpoint (#7428)\n\n* Add state and error tracking for seekable stream supervisors\n\n* Fixed nits in docs\n\n* Made inner class static and updated spec test with jackson inject\n\n* Review changes\n\n* Remove redundant config param in supervisor\n\n* Style\n\n* Applied some of Jon's recommendations\n\n* Add transience field\n\n* write test\n\n* implement code review changes except for reconsidering logic of markRunFinishedAndEvaluateHealth()\n\n* remove transience reporting and fix SeekableStreamSupervisorStateManager impl\n\n* move call to stateManager.markRunFinished() from RunNotice to runInternal() for tests\n\n* remove stateHistory because it wasn't adding much value.  some fixes.  and add more tests\n\n* fix tests\n\n* code review changes and add HTTP health check status\n\n* fix test failure\n\n* refactor to split into a generic SupervisorStateManager and a specific SeekableStreamSupervisorStateManager\n\n* fixup after merge\n\n* code review changes - add additional docs\n\n* cleanup KafkaIndexTaskTest\n\n* add additional documentation for Kinesis indexing\n\n* remove unused throws class\n","date":"2019-06-01 08:16:01","modifiedFileCount":"32","status":"M","submitter":"Justin Borromeo"},{"authorTime":"2019-07-07 00:33:12","codes":[{"authorDate":"2019-07-07 00:33:12","commitOrder":8,"curCode":"  public void testNoDataIngestionTasks()\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    EasyMock.expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    EasyMock.replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2019-07-07 00:33:12","endLine":2205,"groupId":"13963","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testNoDataIngestionTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/af/f5639cd0e0e7c6a18b14f694e3cf2c6cb73dec.src","preCode":"  public void testNoDataIngestionTasks() throws Exception\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2110,"status":"M"},{"authorDate":"2019-07-07 00:33:12","commitOrder":8,"curCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException\n  {\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    final KafkaIndexTask id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    final Map<Integer, Long> fakeCheckpoints = Collections.emptyMap();\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        id1.getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(new SeekableStreamStartSequenceNumbers<>(\n            topic,\n            checkpoints.get(0),\n            ImmutableSet.of()\n        )),\n        new KafkaDataSourceMetadata(new SeekableStreamStartSequenceNumbers<>(\n            topic,\n            fakeCheckpoints,\n            fakeCheckpoints.keySet()\n        ))\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","date":"2019-07-07 00:33:12","endLine":2327,"groupId":"10582","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testCheckpointForInactiveTaskGroup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/af/f5639cd0e0e7c6a18b14f694e3cf2c6cb73dec.src","preCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException, ExecutionException, TimeoutException, JsonProcessingException\n  {\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    final Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    final Map<Integer, Long> fakeCheckpoints = Collections.emptyMap();\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        ((KafkaIndexTask) id1).getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(new SeekableStreamStartSequenceNumbers<>(\n            topic,\n            checkpoints.get(0),\n            ImmutableSet.of()\n        )),\n        new KafkaDataSourceMetadata(new SeekableStreamStartSequenceNumbers<>(\n            topic,\n            fakeCheckpoints,\n            fakeCheckpoints.keySet()\n        ))\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2208,"status":"M"}],"commitId":"1166bbcb75d432817715fdd429737f86730b5591","commitMessage":"@@@Remove static imports from tests (#8036)\n\nMake static imports forbidden in tests and remove all occurrences to be\nconsistent with the non-test code.\n\nAlso.  various changes to files affected by above:\n- Reformat to adhere to druid style guide\n- Fix various IntelliJ warnings\n- Fix various SonarLint warnings (e.g..  the expected/actual args to\n  Assert.assertEquals() were flipped)","date":"2019-07-07 00:33:12","modifiedFileCount":"98","status":"M","submitter":"Chi Cao Minh"},{"authorTime":"2019-08-22 01:58:22","codes":[{"authorDate":"2019-07-07 00:33:12","commitOrder":9,"curCode":"  public void testNoDataIngestionTasks()\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    EasyMock.expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    EasyMock.replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2019-07-07 00:33:12","endLine":2205,"groupId":"13963","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"testNoDataIngestionTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/af/f5639cd0e0e7c6a18b14f694e3cf2c6cb73dec.src","preCode":"  public void testNoDataIngestionTasks()\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    EasyMock.expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    EasyMock.replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2110,"status":"N"},{"authorDate":"2019-08-22 01:58:22","commitOrder":9,"curCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException\n  {\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    final KafkaIndexTask id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        id1.getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(\n            new SeekableStreamStartSequenceNumbers<>(topic, checkpoints.get(0), ImmutableSet.of())\n        )\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","date":"2019-08-22 01:58:22","endLine":2361,"groupId":"10582","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testCheckpointForInactiveTaskGroup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/91/b11b3c8556f6990e7479fd1b53fc7500596411.src","preCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException\n  {\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    final KafkaIndexTask id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    final Map<Integer, Long> fakeCheckpoints = Collections.emptyMap();\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        id1.getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(new SeekableStreamStartSequenceNumbers<>(\n            topic,\n            checkpoints.get(0),\n            ImmutableSet.of()\n        )),\n        new KafkaDataSourceMetadata(new SeekableStreamStartSequenceNumbers<>(\n            topic,\n            fakeCheckpoints,\n            fakeCheckpoints.keySet()\n        ))\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2250,"status":"M"}],"commitId":"22d6384d364a851fd2b5cecafea9d72b004cb03b","commitMessage":"@@@Fix unrealistic test variables in KafkaSupervisorTest and tidy up unused variable in checkpointing process (#7319)\n\n* Fix unrealistic test arguments in KafkaSupervisorTest\n\n* remove currentCheckpoint from checkpoint action\n\n* rename variable\n","date":"2019-08-22 01:58:22","modifiedFileCount":"12","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-08-23 05:51:25","codes":[{"authorDate":"2019-08-23 05:51:25","commitOrder":10,"curCode":"  public void testNoDataIngestionTasks()\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    EasyMock.expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    EasyMock.replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2019-08-23 05:51:25","endLine":2286,"groupId":"13963","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"testNoDataIngestionTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0d/255b66d24d58735694d9fe14274069e9ba5ee9.src","preCode":"  public void testNoDataIngestionTasks()\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    EasyMock.expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    EasyMock.replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2187,"status":"M"},{"authorDate":"2019-08-23 05:51:25","commitOrder":10,"curCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException\n  {\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    final KafkaIndexTask id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        id1.getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(\n            new SeekableStreamStartSequenceNumbers<>(topic, checkpoints.get(0), ImmutableSet.of())\n        )\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","date":"2019-08-23 05:51:25","endLine":2404,"groupId":"10582","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"testCheckpointForInactiveTaskGroup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0d/255b66d24d58735694d9fe14274069e9ba5ee9.src","preCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException\n  {\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    final KafkaIndexTask id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        id1.getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(\n            new SeekableStreamStartSequenceNumbers<>(topic, checkpoints.get(0), ImmutableSet.of())\n        )\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2289,"status":"M"}],"commitId":"fba92ae469b512cca6cdf86ffc1c1a2090808453","commitMessage":"@@@Fix to always use end sequenceNumber for reset (#8305)\n\n* Fix to always use end sequenceNumber for reset\n\n* fix checkstyle\n\n* fix style and add log\n","date":"2019-08-23 05:51:25","modifiedFileCount":"8","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-09-27 07:15:24","codes":[{"authorDate":"2019-09-27 07:15:24","commitOrder":11,"curCode":"  public void testNoDataIngestionTasks()\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE)).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    EasyMock.expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    EasyMock.replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2019-09-27 07:15:24","endLine":2274,"groupId":"13963","id":21,"instanceNumber":1,"isCurCommit":0,"methodName":"testNoDataIngestionTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/75/9182feac2928302ae8f3579d872a1883a232c5.src","preCode":"  public void testNoDataIngestionTasks()\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    EasyMock.expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    EasyMock.replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2175,"status":"M"},{"authorDate":"2019-09-27 07:15:24","commitOrder":11,"curCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException\n  {\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    final KafkaIndexTask id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE)).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        id1.getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(\n            new SeekableStreamStartSequenceNumbers<>(topic, checkpoints.get(0), ImmutableSet.of())\n        )\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","date":"2019-09-27 07:15:24","endLine":2392,"groupId":"10582","id":22,"instanceNumber":2,"isCurCommit":0,"methodName":"testCheckpointForInactiveTaskGroup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/75/9182feac2928302ae8f3579d872a1883a232c5.src","preCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException\n  {\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    final KafkaIndexTask id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        id1.getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(\n            new SeekableStreamStartSequenceNumbers<>(topic, checkpoints.get(0), ImmutableSet.of())\n        )\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2277,"status":"M"}],"commitId":"7f2b6577ef19f18523e8353336ad496e8dc4a270","commitMessage":"@@@get active task by datasource when supervisor discover tasks (#8450)\n\n* get active task by datasource when supervisor discover tasks\n\n* fix ut\n\n* fix ut\n\n* fix ut\n\n* remove unnecessary condition check\n\n* fix ut\n\n* remove stream in hot loop\n","date":"2019-09-27 07:15:24","modifiedFileCount":"7","status":"M","submitter":"elloooooo"},{"authorTime":"2019-10-24 07:51:16","codes":[{"authorDate":"2019-09-27 07:15:24","commitOrder":12,"curCode":"  public void testNoDataIngestionTasks()\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE)).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    EasyMock.expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    EasyMock.replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2019-09-27 07:15:24","endLine":2274,"groupId":"13963","id":23,"instanceNumber":1,"isCurCommit":0,"methodName":"testNoDataIngestionTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/75/9182feac2928302ae8f3579d872a1883a232c5.src","preCode":"  public void testNoDataIngestionTasks()\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE)).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    EasyMock.expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    EasyMock.replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2175,"status":"N"},{"authorDate":"2019-10-24 07:51:16","commitOrder":12,"curCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException\n  {\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    final KafkaIndexTask id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE)).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        new KafkaDataSourceMetadata(\n            new SeekableStreamStartSequenceNumbers<>(topic, checkpoints.get(0), ImmutableSet.of())\n        )\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","date":"2019-10-24 07:51:16","endLine":2395,"groupId":"10582","id":24,"instanceNumber":2,"isCurCommit":0,"methodName":"testCheckpointForInactiveTaskGroup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/92/b4e5e5ecd3796db90765e9e18cacba6ae91f59.src","preCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException\n  {\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    final KafkaIndexTask id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE)).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        id1.getIOConfig().getBaseSequenceName(),\n        new KafkaDataSourceMetadata(\n            new SeekableStreamStartSequenceNumbers<>(topic, checkpoints.get(0), ImmutableSet.of())\n        )\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2281,"status":"M"}],"commitId":"2518478b20dfb22f7133be375f354fbe7bf1ff1e","commitMessage":"@@@Remove deprecated parameter for Checkpoint request (#8707)\n\n* Remove deprecated parameter for Checkpoint request\n\n* fix wrong doc\n","date":"2019-10-24 07:51:16","modifiedFileCount":"11","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-01-28 03:24:29","codes":[{"authorDate":"2020-01-28 03:24:29","commitOrder":13,"curCode":"  public void testNoDataIngestionTasks()\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE))\n            .andReturn(ImmutableList.of(id1, id2, id3))\n            .anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.retrieveDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    EasyMock.expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    EasyMock.replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2020-01-28 03:24:29","endLine":2400,"groupId":"102492","id":25,"instanceNumber":1,"isCurCommit":0,"methodName":"testNoDataIngestionTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0b/29b65a628bb6f0503d980831a84ce2d009150d.src","preCode":"  public void testNoDataIngestionTasks()\n  {\n    final DateTime startTime = DateTimes.nowUtc();\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE))\n            .andReturn(ImmutableList.of(id1, id2, id3))\n            .anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    EasyMock.expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id1\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    EasyMock.replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2299,"status":"M"},{"authorDate":"2020-01-28 03:24:29","commitOrder":13,"curCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException\n  {\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    final KafkaIndexTask id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE))\n            .andReturn(ImmutableList.of(id1, id2, id3))\n            .anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(\n        indexerMetadataStorageCoordinator.retrieveDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        new KafkaDataSourceMetadata(\n            new SeekableStreamStartSequenceNumbers<>(topic, checkpoints.get(0), ImmutableSet.of())\n        )\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","date":"2020-01-28 03:24:29","endLine":2519,"groupId":"102492","id":26,"instanceNumber":2,"isCurCommit":0,"methodName":"testCheckpointForInactiveTaskGroup","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0b/29b65a628bb6f0503d980831a84ce2d009150d.src","preCode":"  public void testCheckpointForInactiveTaskGroup()\n      throws InterruptedException\n  {\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1S\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    supervisor.getStateManager().markRunFinished();\n\n    \r\n    final KafkaIndexTask id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(topic, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            topic,\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE))\n            .andReturn(ImmutableList.of(id1, id2, id3))\n            .anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(\n        indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(new KafkaDataSourceMetadata(null)\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n\n    final DateTime startTime = DateTimes.nowUtc();\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id1\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n\n    final TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id1\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n\n    supervisor.moveTaskGroupToPendingCompletion(0);\n    supervisor.checkpoint(\n        0,\n        new KafkaDataSourceMetadata(\n            new SeekableStreamStartSequenceNumbers<>(topic, checkpoints.get(0), ImmutableSet.of())\n        )\n    );\n\n    while (supervisor.getNoticesQueueSize() > 0) {\n      Thread.sleep(100);\n    }\n\n    verifyAll();\n\n    Assert.assertNull(serviceEmitter.getStackTrace(), serviceEmitter.getStackTrace());\n    Assert.assertNull(serviceEmitter.getExceptionMessage(), serviceEmitter.getExceptionMessage());\n    Assert.assertNull(serviceEmitter.getExceptionClass());\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2403,"status":"M"}],"commitId":"b9186f8f9ff2ff52aceda42bc5f24ffd47a7d17e","commitMessage":"@@@Reconcile terminology and method naming to 'used/unused segments'; Rename MetadataSegmentManager to MetadataSegmentsManager (#7306)\n\n* Reconcile terminology and method naming to 'used/unused segments'; Don't use terms 'enable/disable data source'; Rename MetadataSegmentManager to MetadataSegments; Make REST API methods which mark segments as used/unused to return server error instead of an empty response in case of error\n\n* Fix brace\n\n* Import order\n\n* Rename withKillDataSourceWhitelist to withSpecificDataSourcesToKill\n\n* Fix tests\n\n* Fix tests by adding proper methods without interval parameters to IndexerMetadataStorageCoordinator instead of hacking with Intervals.ETERNITY\n\n* More aligned names of DruidCoordinatorHelpers.  rename several CoordinatorDynamicConfig parameters\n\n* Rename ClientCompactTaskQuery to ClientCompactionTaskQuery for consistency with CompactionTask; ClientCompactQueryTuningConfig to ClientCompactionTaskQueryTuningConfig\n\n* More variable and method renames\n\n* Rename MetadataSegments to SegmentsMetadata\n\n* Javadoc update\n\n* Simplify SegmentsMetadata.getUnusedSegmentIntervals().  more javadocs\n\n* Update Javadoc of VersionedIntervalTimeline.iterateAllObjects()\n\n* Reorder imports\n\n* Rename SegmentsMetadata.tryMark... methods to mark... and make them to return boolean and the numbers of segments changed and relay exceptions to callers\n\n* Complete merge\n\n* Add CollectionUtils.newTreeSet(); Refactor DruidCoordinatorRuntimeParams creation in tests\n\n* Remove MetadataSegmentManager\n\n* Rename millisLagSinceCoordinatorBecomesLeaderBeforeCanMarkAsUnusedOvershadowedSegments to leadingTimeMillisBeforeCanMarkAsUnusedOvershadowedSegments\n\n* Fix tests.  refactor DruidCluster creation in tests into DruidClusterBuilder\n\n* Fix inspections\n\n* Fix SQLMetadataSegmentManagerEmptyTest and rename it to SqlSegmentsMetadataEmptyTest\n\n* Rename SegmentsAndMetadata to SegmentsAndCommitMetadata to reduce the similarity with SegmentsMetadata; Rename some methods\n\n* Rename DruidCoordinatorHelper to CoordinatorDuty.  refactor DruidCoordinator\n\n* Unused import\n\n* Optimize imports\n\n* Rename IndexerSQLMetadataStorageCoordinator.getDataSourceMetadata() to retrieveDataSourceMetadata()\n\n* Unused import\n\n* Update terminology in datasource-view.tsx\n\n* Fix label in datasource-view.spec.tsx.snap\n\n* Fix lint errors in datasource-view.tsx\n\n* Doc improvements\n\n* Another attempt to please TSLint\n\n* Another attempt to please TSLint\n\n* Style fixes\n\n* Fix IndexerSQLMetadataStorageCoordinator.createUsedSegmentsSqlQueryForIntervals() (wrong merge)\n\n* Try to fix docs build issue\n\n* Javadoc and spelling fixes\n\n* Rename SegmentsMetadata to SegmentsMetadataManager.  address other comments\n\n* Address more comments\n","date":"2020-01-28 03:24:29","modifiedFileCount":"127","status":"M","submitter":"Roman Leventov"}]
