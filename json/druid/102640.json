[{"authorTime":"2018-11-06 13:33:42","codes":[{"authorDate":"2018-11-06 13:33:42","commitOrder":1,"curCode":"  public void testReadParquetDecimalFixedLen() throws IOException, InterruptedException\n  {\n    \r\n    if (parserType.equals(ParquetExtensionsModule.PARQUET_AVRO_INPUT_PARSER_TYPE)) {\n      return;\n    }\n    HadoopDruidIndexerConfig config = transformHadoopDruidIndexerConfig(\n        \"example/decimals/dec_in_fix_len.json\",\n        parserType,\n        true\n    );\n    List<InputRow> rows = getAllRows(parserType, config);\n    assertEquals(\"2018-09-01T00:00:00.000Z\", rows.get(0).getTimestamp().toString());\n    assertEquals(\"1.0\", rows.get(0).getDimension(\"fixed_len_dec\").get(0));\n    assertEquals(new BigDecimal(\"1.0\"), rows.get(0).getMetric(\"metric1\"));\n  }\n","date":"2018-11-06 13:33:42","endLine":74,"groupId":"3058","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testReadParquetDecimalFixedLen","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/26/60d33bc07cb68c420c83ebf256bfe23e0bf45c.src","preCode":"  public void testReadParquetDecimalFixedLen() throws IOException, InterruptedException\n  {\n    \r\n    if (parserType.equals(ParquetExtensionsModule.PARQUET_AVRO_INPUT_PARSER_TYPE)) {\n      return;\n    }\n    HadoopDruidIndexerConfig config = transformHadoopDruidIndexerConfig(\n        \"example/decimals/dec_in_fix_len.json\",\n        parserType,\n        true\n    );\n    List<InputRow> rows = getAllRows(parserType, config);\n    assertEquals(\"2018-09-01T00:00:00.000Z\", rows.get(0).getTimestamp().toString());\n    assertEquals(\"1.0\", rows.get(0).getDimension(\"fixed_len_dec\").get(0));\n    assertEquals(new BigDecimal(\"1.0\"), rows.get(0).getMetric(\"metric1\"));\n  }\n","realPath":"extensions-core/parquet-extensions/src/test/java/org/apache/druid/data/input/parquet/DecimalParquetInputTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":59,"status":"B"},{"authorDate":"2018-11-06 13:33:42","commitOrder":1,"curCode":"  public void testReadParquetDecimali32() throws IOException, InterruptedException\n  {\n    \r\n    if (parserType.equals(ParquetExtensionsModule.PARQUET_AVRO_INPUT_PARSER_TYPE)) {\n      return;\n    }\n    HadoopDruidIndexerConfig config = transformHadoopDruidIndexerConfig(\n        \"example/decimals/dec_in_i32.json\",\n        parserType,\n        true\n    );\n    List<InputRow> rows = getAllRows(parserType, config);\n    assertEquals(\"2018-09-01T00:00:00.000Z\", rows.get(0).getTimestamp().toString());\n    assertEquals(\"100\", rows.get(0).getDimension(\"i32_dec\").get(0));\n    assertEquals(new BigDecimal(100), rows.get(0).getMetric(\"metric1\"));\n  }\n","date":"2018-11-06 13:33:42","endLine":92,"groupId":"3058","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testReadParquetDecimali32","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/26/60d33bc07cb68c420c83ebf256bfe23e0bf45c.src","preCode":"  public void testReadParquetDecimali32() throws IOException, InterruptedException\n  {\n    \r\n    if (parserType.equals(ParquetExtensionsModule.PARQUET_AVRO_INPUT_PARSER_TYPE)) {\n      return;\n    }\n    HadoopDruidIndexerConfig config = transformHadoopDruidIndexerConfig(\n        \"example/decimals/dec_in_i32.json\",\n        parserType,\n        true\n    );\n    List<InputRow> rows = getAllRows(parserType, config);\n    assertEquals(\"2018-09-01T00:00:00.000Z\", rows.get(0).getTimestamp().toString());\n    assertEquals(\"100\", rows.get(0).getDimension(\"i32_dec\").get(0));\n    assertEquals(new BigDecimal(100), rows.get(0).getMetric(\"metric1\"));\n  }\n","realPath":"extensions-core/parquet-extensions/src/test/java/org/apache/druid/data/input/parquet/DecimalParquetInputTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":77,"status":"B"}],"commitId":"1224d8b74611bd6530dbddf3d196e7d7ecf2a99e","commitMessage":"@@@overhaul 'druid-parquet-extensions' module.  promoting from 'contrib' to 'core' (#6360)\n\n* move parquet-extensions from contrib to core.  adds new hadoop parquet parser that does not convert to avro first and supports flattenSpec and int96 columns.  add support for flattenSpec for parquet-avro conversion parser.  much test with a bunch of files lifted from spark-sql\n\n* fix avro flattener to support nullable primitives for auto discovery and now only supports primitive arrays instead of all arrays\n\n* remove leftover print\n\n* convert micro timestamp to millis\n\n* checkstyle\n\n* add ignore for .parquet and .parq to rat exclude\n\n* fix legit test failure from avro flattern behavior change\n\n* fix rebase\n\n* add exclusions to pom to cut down on redundant jars\n\n* refactor tests.  add support for unwrapping lists for parquet-avro.  review comments\n\n* more comment\n\n* fix oops\n\n* tweak parquet-avro list handling\n\n* more docs\n\n* fix style\n\n* grr styles\n","date":"2018-11-06 13:33:42","modifiedFileCount":"4","status":"B","submitter":"Clint Wylie"},{"authorTime":"2019-07-07 00:33:12","codes":[{"authorDate":"2019-07-07 00:33:12","commitOrder":2,"curCode":"  public void testReadParquetDecimalFixedLen() throws IOException, InterruptedException\n  {\n    \r\n    if (parserType.equals(ParquetExtensionsModule.PARQUET_AVRO_INPUT_PARSER_TYPE)) {\n      return;\n    }\n    HadoopDruidIndexerConfig config = transformHadoopDruidIndexerConfig(\n        \"example/decimals/dec_in_fix_len.json\",\n        parserType,\n        true\n    );\n    List<InputRow> rows = getAllRows(parserType, config);\n    Assert.assertEquals(\"2018-09-01T00:00:00.000Z\", rows.get(0).getTimestamp().toString());\n    Assert.assertEquals(\"1.0\", rows.get(0).getDimension(\"fixed_len_dec\").get(0));\n    Assert.assertEquals(new BigDecimal(\"1.0\"), rows.get(0).getMetric(\"metric1\"));\n  }\n","date":"2019-07-07 00:33:12","endLine":69,"groupId":"102640","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testReadParquetDecimalFixedLen","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/de/2f3f2cfcae9f6914a5916fa65629480396bed1.src","preCode":"  public void testReadParquetDecimalFixedLen() throws IOException, InterruptedException\n  {\n    \r\n    if (parserType.equals(ParquetExtensionsModule.PARQUET_AVRO_INPUT_PARSER_TYPE)) {\n      return;\n    }\n    HadoopDruidIndexerConfig config = transformHadoopDruidIndexerConfig(\n        \"example/decimals/dec_in_fix_len.json\",\n        parserType,\n        true\n    );\n    List<InputRow> rows = getAllRows(parserType, config);\n    assertEquals(\"2018-09-01T00:00:00.000Z\", rows.get(0).getTimestamp().toString());\n    assertEquals(\"1.0\", rows.get(0).getDimension(\"fixed_len_dec\").get(0));\n    assertEquals(new BigDecimal(\"1.0\"), rows.get(0).getMetric(\"metric1\"));\n  }\n","realPath":"extensions-core/parquet-extensions/src/test/java/org/apache/druid/data/input/parquet/DecimalParquetInputTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":54,"status":"M"},{"authorDate":"2019-07-07 00:33:12","commitOrder":2,"curCode":"  public void testReadParquetDecimali32() throws IOException, InterruptedException\n  {\n    \r\n    if (parserType.equals(ParquetExtensionsModule.PARQUET_AVRO_INPUT_PARSER_TYPE)) {\n      return;\n    }\n    HadoopDruidIndexerConfig config = transformHadoopDruidIndexerConfig(\n        \"example/decimals/dec_in_i32.json\",\n        parserType,\n        true\n    );\n    List<InputRow> rows = getAllRows(parserType, config);\n    Assert.assertEquals(\"2018-09-01T00:00:00.000Z\", rows.get(0).getTimestamp().toString());\n    Assert.assertEquals(\"100\", rows.get(0).getDimension(\"i32_dec\").get(0));\n    Assert.assertEquals(new BigDecimal(100), rows.get(0).getMetric(\"metric1\"));\n  }\n","date":"2019-07-07 00:33:12","endLine":87,"groupId":"102640","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testReadParquetDecimali32","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/de/2f3f2cfcae9f6914a5916fa65629480396bed1.src","preCode":"  public void testReadParquetDecimali32() throws IOException, InterruptedException\n  {\n    \r\n    if (parserType.equals(ParquetExtensionsModule.PARQUET_AVRO_INPUT_PARSER_TYPE)) {\n      return;\n    }\n    HadoopDruidIndexerConfig config = transformHadoopDruidIndexerConfig(\n        \"example/decimals/dec_in_i32.json\",\n        parserType,\n        true\n    );\n    List<InputRow> rows = getAllRows(parserType, config);\n    assertEquals(\"2018-09-01T00:00:00.000Z\", rows.get(0).getTimestamp().toString());\n    assertEquals(\"100\", rows.get(0).getDimension(\"i32_dec\").get(0));\n    assertEquals(new BigDecimal(100), rows.get(0).getMetric(\"metric1\"));\n  }\n","realPath":"extensions-core/parquet-extensions/src/test/java/org/apache/druid/data/input/parquet/DecimalParquetInputTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":72,"status":"M"}],"commitId":"1166bbcb75d432817715fdd429737f86730b5591","commitMessage":"@@@Remove static imports from tests (#8036)\n\nMake static imports forbidden in tests and remove all occurrences to be\nconsistent with the non-test code.\n\nAlso.  various changes to files affected by above:\n- Reformat to adhere to druid style guide\n- Fix various IntelliJ warnings\n- Fix various SonarLint warnings (e.g..  the expected/actual args to\n  Assert.assertEquals() were flipped)","date":"2019-07-07 00:33:12","modifiedFileCount":"98","status":"M","submitter":"Chi Cao Minh"}]
