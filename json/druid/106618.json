[{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public void testPartialLimitPushDownMerge()\n  {\n    \r\n\n    QueryToolChest<Row, GroupByQuery> toolChest = groupByFactory.getToolchest();\n    QueryRunner<Row> theRunner = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            groupByFactory.mergeRunners(executorService, getRunner1())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner2 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            tooSmallGroupByFactory.mergeRunners(executorService, getRunner2())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner3 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            new QueryRunner<Row>()\n            {\n              @Override\n              public Sequence<Row> run(QueryPlus<Row> queryPlus, Map<String, Object> responseContext)\n              {\n                return Sequences\n                    .simple(\n                        ImmutableList.of(\n                            theRunner.run(queryPlus, responseContext),\n                            theRunner2.run(queryPlus, responseContext)\n                        )\n                    )\n                    .flatMerge(Function.identity(), queryPlus.getQuery().getResultOrdering());\n              }\n            }\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QuerySegmentSpec intervalSpec = new MultipleIntervalSegmentSpec(\n        Collections.singletonList(Intervals.utc(0, 1000000))\n    );\n\n    GroupByQuery query = GroupByQuery\n        .builder()\n        .setDataSource(\"blah\")\n        .setQuerySegmentSpec(intervalSpec)\n        .setDimensions(new DefaultDimensionSpec(\"dimA\", null))\n        .setAggregatorSpecs(new LongSumAggregatorFactory(\"metA\", \"metA\"))\n        .setLimitSpec(\n            new DefaultLimitSpec(\n                Collections.singletonList(new OrderByColumnSpec(\"dimA\", OrderByColumnSpec.Direction.DESCENDING)),\n                3\n            )\n        )\n        .setGranularity(Granularities.ALL)\n        .build();\n\n    Sequence<Row> queryResult = theRunner3.run(QueryPlus.wrap(query), Maps.newHashMap());\n    List<Row> results = queryResult.toList();\n\n    Row expectedRow0 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zortaxx\",\n        \"metA\", 999L\n    );\n    Row expectedRow1 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zebra\",\n        \"metA\", 180L\n    );\n    Row expectedRow2 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"world\",\n        \"metA\", 150L\n    );\n\n    Assert.assertEquals(3, results.size());\n    Assert.assertEquals(expectedRow0, results.get(0));\n    Assert.assertEquals(expectedRow1, results.get(1));\n    Assert.assertEquals(expectedRow2, results.get(2));\n  }\n","date":"2018-08-31 00:56:26","endLine":512,"groupId":"20439","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testPartialLimitPushDownMerge","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/99/a8b646a6615a7f64b98f7ac65a3d4bcd25e353.src","preCode":"  public void testPartialLimitPushDownMerge()\n  {\n    \r\n\n    QueryToolChest<Row, GroupByQuery> toolChest = groupByFactory.getToolchest();\n    QueryRunner<Row> theRunner = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            groupByFactory.mergeRunners(executorService, getRunner1())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner2 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            tooSmallGroupByFactory.mergeRunners(executorService, getRunner2())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner3 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            new QueryRunner<Row>()\n            {\n              @Override\n              public Sequence<Row> run(QueryPlus<Row> queryPlus, Map<String, Object> responseContext)\n              {\n                return Sequences\n                    .simple(\n                        ImmutableList.of(\n                            theRunner.run(queryPlus, responseContext),\n                            theRunner2.run(queryPlus, responseContext)\n                        )\n                    )\n                    .flatMerge(Function.identity(), queryPlus.getQuery().getResultOrdering());\n              }\n            }\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QuerySegmentSpec intervalSpec = new MultipleIntervalSegmentSpec(\n        Collections.singletonList(Intervals.utc(0, 1000000))\n    );\n\n    GroupByQuery query = GroupByQuery\n        .builder()\n        .setDataSource(\"blah\")\n        .setQuerySegmentSpec(intervalSpec)\n        .setDimensions(new DefaultDimensionSpec(\"dimA\", null))\n        .setAggregatorSpecs(new LongSumAggregatorFactory(\"metA\", \"metA\"))\n        .setLimitSpec(\n            new DefaultLimitSpec(\n                Collections.singletonList(new OrderByColumnSpec(\"dimA\", OrderByColumnSpec.Direction.DESCENDING)),\n                3\n            )\n        )\n        .setGranularity(Granularities.ALL)\n        .build();\n\n    Sequence<Row> queryResult = theRunner3.run(QueryPlus.wrap(query), Maps.newHashMap());\n    List<Row> results = queryResult.toList();\n\n    Row expectedRow0 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zortaxx\",\n        \"metA\", 999L\n    );\n    Row expectedRow1 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zebra\",\n        \"metA\", 180L\n    );\n    Row expectedRow2 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"world\",\n        \"metA\", 150L\n    );\n\n    Assert.assertEquals(3, results.size());\n    Assert.assertEquals(expectedRow0, results.get(0));\n    Assert.assertEquals(expectedRow1, results.get(1));\n    Assert.assertEquals(expectedRow2, results.get(2));\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByLimitPushDownInsufficientBufferTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":430,"status":"B"},{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public void testPartialLimitPushDownMergeForceAggs()\n  {\n    \r\n\n    QueryToolChest<Row, GroupByQuery> toolChest = groupByFactory.getToolchest();\n    QueryRunner<Row> theRunner = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            groupByFactory.mergeRunners(executorService, getRunner1())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n\n    QueryRunner<Row> theRunner2 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            tooSmallGroupByFactory.mergeRunners(executorService, getRunner2())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner3 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            new QueryRunner<Row>()\n            {\n              @Override\n              public Sequence<Row> run(QueryPlus<Row> queryPlus, Map<String, Object> responseContext)\n              {\n                return Sequences\n                    .simple(\n                        ImmutableList.of(\n                            theRunner.run(queryPlus, responseContext),\n                            theRunner2.run(queryPlus, responseContext)\n                        )\n                    )\n                    .flatMerge(Function.identity(), queryPlus.getQuery().getResultOrdering());\n              }\n            }\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QuerySegmentSpec intervalSpec = new MultipleIntervalSegmentSpec(\n        Collections.singletonList(Intervals.utc(0, 1000000))\n    );\n\n    GroupByQuery query = GroupByQuery\n        .builder()\n        .setDataSource(\"blah\")\n        .setQuerySegmentSpec(intervalSpec)\n        .setDimensions(new DefaultDimensionSpec(\"dimA\", null))\n        .setAggregatorSpecs(new LongSumAggregatorFactory(\"metA\", \"metA\"))\n        .setLimitSpec(\n            new DefaultLimitSpec(\n                Collections.singletonList(\n                    new OrderByColumnSpec(\"metA\", OrderByColumnSpec.Direction.DESCENDING, StringComparators.NUMERIC)\n                ),\n                3\n            )\n        )\n        .setGranularity(Granularities.ALL)\n        .setContext(\n            ImmutableMap.of(\n              GroupByQueryConfig.CTX_KEY_FORCE_LIMIT_PUSH_DOWN,\n              true\n            )\n        )\n        .build();\n\n    Sequence<Row> queryResult = theRunner3.run(QueryPlus.wrap(query), Maps.newHashMap());\n    List<Row> results = queryResult.toList();\n\n    Row expectedRow0 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zortaxx\",\n        \"metA\", 999L\n    );\n    Row expectedRow1 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"foo\",\n        \"metA\", 200L\n    );\n    Row expectedRow2 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"mango\",\n        \"metA\", 190L\n    );\n\n    Assert.assertEquals(3, results.size());\n    Assert.assertEquals(expectedRow0, results.get(0));\n    Assert.assertEquals(expectedRow1, results.get(1));\n    Assert.assertEquals(expectedRow2, results.get(2));\n  }\n","date":"2018-08-31 00:56:26","endLine":606,"groupId":"1175","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testPartialLimitPushDownMergeForceAggs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/99/a8b646a6615a7f64b98f7ac65a3d4bcd25e353.src","preCode":"  public void testPartialLimitPushDownMergeForceAggs()\n  {\n    \r\n\n    QueryToolChest<Row, GroupByQuery> toolChest = groupByFactory.getToolchest();\n    QueryRunner<Row> theRunner = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            groupByFactory.mergeRunners(executorService, getRunner1())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n\n    QueryRunner<Row> theRunner2 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            tooSmallGroupByFactory.mergeRunners(executorService, getRunner2())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner3 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            new QueryRunner<Row>()\n            {\n              @Override\n              public Sequence<Row> run(QueryPlus<Row> queryPlus, Map<String, Object> responseContext)\n              {\n                return Sequences\n                    .simple(\n                        ImmutableList.of(\n                            theRunner.run(queryPlus, responseContext),\n                            theRunner2.run(queryPlus, responseContext)\n                        )\n                    )\n                    .flatMerge(Function.identity(), queryPlus.getQuery().getResultOrdering());\n              }\n            }\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QuerySegmentSpec intervalSpec = new MultipleIntervalSegmentSpec(\n        Collections.singletonList(Intervals.utc(0, 1000000))\n    );\n\n    GroupByQuery query = GroupByQuery\n        .builder()\n        .setDataSource(\"blah\")\n        .setQuerySegmentSpec(intervalSpec)\n        .setDimensions(new DefaultDimensionSpec(\"dimA\", null))\n        .setAggregatorSpecs(new LongSumAggregatorFactory(\"metA\", \"metA\"))\n        .setLimitSpec(\n            new DefaultLimitSpec(\n                Collections.singletonList(\n                    new OrderByColumnSpec(\"metA\", OrderByColumnSpec.Direction.DESCENDING, StringComparators.NUMERIC)\n                ),\n                3\n            )\n        )\n        .setGranularity(Granularities.ALL)\n        .setContext(\n            ImmutableMap.of(\n              GroupByQueryConfig.CTX_KEY_FORCE_LIMIT_PUSH_DOWN,\n              true\n            )\n        )\n        .build();\n\n    Sequence<Row> queryResult = theRunner3.run(QueryPlus.wrap(query), Maps.newHashMap());\n    List<Row> results = queryResult.toList();\n\n    Row expectedRow0 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zortaxx\",\n        \"metA\", 999L\n    );\n    Row expectedRow1 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"foo\",\n        \"metA\", 200L\n    );\n    Row expectedRow2 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"mango\",\n        \"metA\", 190L\n    );\n\n    Assert.assertEquals(3, results.size());\n    Assert.assertEquals(expectedRow0, results.get(0));\n    Assert.assertEquals(expectedRow1, results.get(1));\n    Assert.assertEquals(expectedRow2, results.get(2));\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByLimitPushDownInsufficientBufferTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":515,"status":"B"}],"commitId":"431d3d8497f9079857c3baa7ae7ab4cb44a22355","commitMessage":"@@@Rename io.druid to org.apache.druid. (#6266)\n\n* Rename io.druid to org.apache.druid.\n\n* Fix META-INF files and remove some benchmark results.\n\n* MonitorsConfig update for metrics package migration.\n\n* Reorder some dimensions in inner queries for some reason.\n\n* Fix protobuf tests.\n","date":"2018-08-31 00:56:26","modifiedFileCount":"5","status":"B","submitter":"Gian Merlino"},{"authorTime":"2018-10-29 20:02:43","codes":[{"authorDate":"2018-10-29 20:02:43","commitOrder":2,"curCode":"  public void testPartialLimitPushDownMerge()\n  {\n    \r\n\n    QueryToolChest<Row, GroupByQuery> toolChest = groupByFactory.getToolchest();\n    QueryRunner<Row> theRunner = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            groupByFactory.mergeRunners(executorService, getRunner1())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner2 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            tooSmallGroupByFactory.mergeRunners(executorService, getRunner2())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner3 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            new QueryRunner<Row>()\n            {\n              @Override\n              public Sequence<Row> run(QueryPlus<Row> queryPlus, Map<String, Object> responseContext)\n              {\n                return Sequences\n                    .simple(\n                        ImmutableList.of(\n                            theRunner.run(queryPlus, responseContext),\n                            theRunner2.run(queryPlus, responseContext)\n                        )\n                    )\n                    .flatMerge(Function.identity(), queryPlus.getQuery().getResultOrdering());\n              }\n            }\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QuerySegmentSpec intervalSpec = new MultipleIntervalSegmentSpec(\n        Collections.singletonList(Intervals.utc(0, 1000000))\n    );\n\n    GroupByQuery query = GroupByQuery\n        .builder()\n        .setDataSource(\"blah\")\n        .setQuerySegmentSpec(intervalSpec)\n        .setDimensions(new DefaultDimensionSpec(\"dimA\", null))\n        .setAggregatorSpecs(new LongSumAggregatorFactory(\"metA\", \"metA\"))\n        .setLimitSpec(\n            new DefaultLimitSpec(\n                Collections.singletonList(new OrderByColumnSpec(\"dimA\", OrderByColumnSpec.Direction.DESCENDING)),\n                3\n            )\n        )\n        .setGranularity(Granularities.ALL)\n        .build();\n\n    Sequence<Row> queryResult = theRunner3.run(QueryPlus.wrap(query), new HashMap<>());\n    List<Row> results = queryResult.toList();\n\n    Row expectedRow0 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zortaxx\",\n        \"metA\", 999L\n    );\n    Row expectedRow1 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zebra\",\n        \"metA\", 180L\n    );\n    Row expectedRow2 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"world\",\n        \"metA\", 150L\n    );\n\n    Assert.assertEquals(3, results.size());\n    Assert.assertEquals(expectedRow0, results.get(0));\n    Assert.assertEquals(expectedRow1, results.get(1));\n    Assert.assertEquals(expectedRow2, results.get(2));\n  }\n","date":"2018-10-29 20:02:43","endLine":510,"groupId":"20439","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testPartialLimitPushDownMerge","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/8d/2885ed75d30e07335e095949efc44fa709b1b5.src","preCode":"  public void testPartialLimitPushDownMerge()\n  {\n    \r\n\n    QueryToolChest<Row, GroupByQuery> toolChest = groupByFactory.getToolchest();\n    QueryRunner<Row> theRunner = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            groupByFactory.mergeRunners(executorService, getRunner1())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner2 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            tooSmallGroupByFactory.mergeRunners(executorService, getRunner2())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner3 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            new QueryRunner<Row>()\n            {\n              @Override\n              public Sequence<Row> run(QueryPlus<Row> queryPlus, Map<String, Object> responseContext)\n              {\n                return Sequences\n                    .simple(\n                        ImmutableList.of(\n                            theRunner.run(queryPlus, responseContext),\n                            theRunner2.run(queryPlus, responseContext)\n                        )\n                    )\n                    .flatMerge(Function.identity(), queryPlus.getQuery().getResultOrdering());\n              }\n            }\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QuerySegmentSpec intervalSpec = new MultipleIntervalSegmentSpec(\n        Collections.singletonList(Intervals.utc(0, 1000000))\n    );\n\n    GroupByQuery query = GroupByQuery\n        .builder()\n        .setDataSource(\"blah\")\n        .setQuerySegmentSpec(intervalSpec)\n        .setDimensions(new DefaultDimensionSpec(\"dimA\", null))\n        .setAggregatorSpecs(new LongSumAggregatorFactory(\"metA\", \"metA\"))\n        .setLimitSpec(\n            new DefaultLimitSpec(\n                Collections.singletonList(new OrderByColumnSpec(\"dimA\", OrderByColumnSpec.Direction.DESCENDING)),\n                3\n            )\n        )\n        .setGranularity(Granularities.ALL)\n        .build();\n\n    Sequence<Row> queryResult = theRunner3.run(QueryPlus.wrap(query), Maps.newHashMap());\n    List<Row> results = queryResult.toList();\n\n    Row expectedRow0 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zortaxx\",\n        \"metA\", 999L\n    );\n    Row expectedRow1 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zebra\",\n        \"metA\", 180L\n    );\n    Row expectedRow2 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"world\",\n        \"metA\", 150L\n    );\n\n    Assert.assertEquals(3, results.size());\n    Assert.assertEquals(expectedRow0, results.get(0));\n    Assert.assertEquals(expectedRow1, results.get(1));\n    Assert.assertEquals(expectedRow2, results.get(2));\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByLimitPushDownInsufficientBufferTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":428,"status":"M"},{"authorDate":"2018-10-29 20:02:43","commitOrder":2,"curCode":"  public void testPartialLimitPushDownMergeForceAggs()\n  {\n    \r\n\n    QueryToolChest<Row, GroupByQuery> toolChest = groupByFactory.getToolchest();\n    QueryRunner<Row> theRunner = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            groupByFactory.mergeRunners(executorService, getRunner1())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n\n    QueryRunner<Row> theRunner2 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            tooSmallGroupByFactory.mergeRunners(executorService, getRunner2())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner3 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            new QueryRunner<Row>()\n            {\n              @Override\n              public Sequence<Row> run(QueryPlus<Row> queryPlus, Map<String, Object> responseContext)\n              {\n                return Sequences\n                    .simple(\n                        ImmutableList.of(\n                            theRunner.run(queryPlus, responseContext),\n                            theRunner2.run(queryPlus, responseContext)\n                        )\n                    )\n                    .flatMerge(Function.identity(), queryPlus.getQuery().getResultOrdering());\n              }\n            }\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QuerySegmentSpec intervalSpec = new MultipleIntervalSegmentSpec(\n        Collections.singletonList(Intervals.utc(0, 1000000))\n    );\n\n    GroupByQuery query = GroupByQuery\n        .builder()\n        .setDataSource(\"blah\")\n        .setQuerySegmentSpec(intervalSpec)\n        .setDimensions(new DefaultDimensionSpec(\"dimA\", null))\n        .setAggregatorSpecs(new LongSumAggregatorFactory(\"metA\", \"metA\"))\n        .setLimitSpec(\n            new DefaultLimitSpec(\n                Collections.singletonList(\n                    new OrderByColumnSpec(\"metA\", OrderByColumnSpec.Direction.DESCENDING, StringComparators.NUMERIC)\n                ),\n                3\n            )\n        )\n        .setGranularity(Granularities.ALL)\n        .setContext(\n            ImmutableMap.of(\n              GroupByQueryConfig.CTX_KEY_FORCE_LIMIT_PUSH_DOWN,\n              true\n            )\n        )\n        .build();\n\n    Sequence<Row> queryResult = theRunner3.run(QueryPlus.wrap(query), new HashMap<>());\n    List<Row> results = queryResult.toList();\n\n    Row expectedRow0 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zortaxx\",\n        \"metA\", 999L\n    );\n    Row expectedRow1 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"foo\",\n        \"metA\", 200L\n    );\n    Row expectedRow2 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"mango\",\n        \"metA\", 190L\n    );\n\n    Assert.assertEquals(3, results.size());\n    Assert.assertEquals(expectedRow0, results.get(0));\n    Assert.assertEquals(expectedRow1, results.get(1));\n    Assert.assertEquals(expectedRow2, results.get(2));\n  }\n","date":"2018-10-29 20:02:43","endLine":604,"groupId":"1175","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testPartialLimitPushDownMergeForceAggs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/8d/2885ed75d30e07335e095949efc44fa709b1b5.src","preCode":"  public void testPartialLimitPushDownMergeForceAggs()\n  {\n    \r\n\n    QueryToolChest<Row, GroupByQuery> toolChest = groupByFactory.getToolchest();\n    QueryRunner<Row> theRunner = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            groupByFactory.mergeRunners(executorService, getRunner1())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n\n    QueryRunner<Row> theRunner2 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            tooSmallGroupByFactory.mergeRunners(executorService, getRunner2())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner3 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            new QueryRunner<Row>()\n            {\n              @Override\n              public Sequence<Row> run(QueryPlus<Row> queryPlus, Map<String, Object> responseContext)\n              {\n                return Sequences\n                    .simple(\n                        ImmutableList.of(\n                            theRunner.run(queryPlus, responseContext),\n                            theRunner2.run(queryPlus, responseContext)\n                        )\n                    )\n                    .flatMerge(Function.identity(), queryPlus.getQuery().getResultOrdering());\n              }\n            }\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QuerySegmentSpec intervalSpec = new MultipleIntervalSegmentSpec(\n        Collections.singletonList(Intervals.utc(0, 1000000))\n    );\n\n    GroupByQuery query = GroupByQuery\n        .builder()\n        .setDataSource(\"blah\")\n        .setQuerySegmentSpec(intervalSpec)\n        .setDimensions(new DefaultDimensionSpec(\"dimA\", null))\n        .setAggregatorSpecs(new LongSumAggregatorFactory(\"metA\", \"metA\"))\n        .setLimitSpec(\n            new DefaultLimitSpec(\n                Collections.singletonList(\n                    new OrderByColumnSpec(\"metA\", OrderByColumnSpec.Direction.DESCENDING, StringComparators.NUMERIC)\n                ),\n                3\n            )\n        )\n        .setGranularity(Granularities.ALL)\n        .setContext(\n            ImmutableMap.of(\n              GroupByQueryConfig.CTX_KEY_FORCE_LIMIT_PUSH_DOWN,\n              true\n            )\n        )\n        .build();\n\n    Sequence<Row> queryResult = theRunner3.run(QueryPlus.wrap(query), Maps.newHashMap());\n    List<Row> results = queryResult.toList();\n\n    Row expectedRow0 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zortaxx\",\n        \"metA\", 999L\n    );\n    Row expectedRow1 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"foo\",\n        \"metA\", 200L\n    );\n    Row expectedRow2 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"mango\",\n        \"metA\", 190L\n    );\n\n    Assert.assertEquals(3, results.size());\n    Assert.assertEquals(expectedRow0, results.get(0));\n    Assert.assertEquals(expectedRow1, results.get(1));\n    Assert.assertEquals(expectedRow2, results.get(2));\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByLimitPushDownInsufficientBufferTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":513,"status":"M"}],"commitId":"676f5e6d7f184101b8763e4249b18b237bbe0ec7","commitMessage":"@@@Prohibit some guava collection APIs and use JDK collection APIs directly (#6511)\n\n* Prohibit some guava collection APIs and use JDK APIs directly\n\n* reset files that changed by accident\n\n* sort codestyle/druid-forbidden-apis.txt alphabetically\n","date":"2018-10-29 20:02:43","modifiedFileCount":"427","status":"M","submitter":"QiuMM"},{"authorTime":"2019-07-24 23:29:03","codes":[{"authorDate":"2019-07-24 23:29:03","commitOrder":3,"curCode":"  public void testPartialLimitPushDownMerge()\n  {\n    \r\n\n    QueryToolChest<Row, GroupByQuery> toolChest = groupByFactory.getToolchest();\n    QueryRunner<Row> theRunner = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            groupByFactory.mergeRunners(executorService, getRunner1())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner2 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            tooSmallGroupByFactory.mergeRunners(executorService, getRunner2())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner3 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            new QueryRunner<Row>()\n            {\n              @Override\n              public Sequence<Row> run(QueryPlus<Row> queryPlus, ResponseContext responseContext)\n              {\n                return Sequences\n                    .simple(\n                        ImmutableList.of(\n                            theRunner.run(queryPlus, responseContext),\n                            theRunner2.run(queryPlus, responseContext)\n                        )\n                    )\n                    .flatMerge(Function.identity(), queryPlus.getQuery().getResultOrdering());\n              }\n            }\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QuerySegmentSpec intervalSpec = new MultipleIntervalSegmentSpec(\n        Collections.singletonList(Intervals.utc(0, 1000000))\n    );\n\n    GroupByQuery query = GroupByQuery\n        .builder()\n        .setDataSource(\"blah\")\n        .setQuerySegmentSpec(intervalSpec)\n        .setDimensions(new DefaultDimensionSpec(\"dimA\", null))\n        .setAggregatorSpecs(new LongSumAggregatorFactory(\"metA\", \"metA\"))\n        .setLimitSpec(\n            new DefaultLimitSpec(\n                Collections.singletonList(new OrderByColumnSpec(\"dimA\", OrderByColumnSpec.Direction.DESCENDING)),\n                3\n            )\n        )\n        .setGranularity(Granularities.ALL)\n        .build();\n\n    Sequence<Row> queryResult = theRunner3.run(QueryPlus.wrap(query));\n    List<Row> results = queryResult.toList();\n\n    Row expectedRow0 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zortaxx\",\n        \"metA\", 999L\n    );\n    Row expectedRow1 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zebra\",\n        \"metA\", 180L\n    );\n    Row expectedRow2 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"world\",\n        \"metA\", 150L\n    );\n\n    Assert.assertEquals(3, results.size());\n    Assert.assertEquals(expectedRow0, results.get(0));\n    Assert.assertEquals(expectedRow1, results.get(1));\n    Assert.assertEquals(expectedRow2, results.get(2));\n  }\n","date":"2019-07-24 23:29:03","endLine":512,"groupId":"20439","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testPartialLimitPushDownMerge","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c7/76cecc7104ba1136d8b3e3ee709124332154c1.src","preCode":"  public void testPartialLimitPushDownMerge()\n  {\n    \r\n\n    QueryToolChest<Row, GroupByQuery> toolChest = groupByFactory.getToolchest();\n    QueryRunner<Row> theRunner = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            groupByFactory.mergeRunners(executorService, getRunner1())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner2 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            tooSmallGroupByFactory.mergeRunners(executorService, getRunner2())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner3 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            new QueryRunner<Row>()\n            {\n              @Override\n              public Sequence<Row> run(QueryPlus<Row> queryPlus, Map<String, Object> responseContext)\n              {\n                return Sequences\n                    .simple(\n                        ImmutableList.of(\n                            theRunner.run(queryPlus, responseContext),\n                            theRunner2.run(queryPlus, responseContext)\n                        )\n                    )\n                    .flatMerge(Function.identity(), queryPlus.getQuery().getResultOrdering());\n              }\n            }\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QuerySegmentSpec intervalSpec = new MultipleIntervalSegmentSpec(\n        Collections.singletonList(Intervals.utc(0, 1000000))\n    );\n\n    GroupByQuery query = GroupByQuery\n        .builder()\n        .setDataSource(\"blah\")\n        .setQuerySegmentSpec(intervalSpec)\n        .setDimensions(new DefaultDimensionSpec(\"dimA\", null))\n        .setAggregatorSpecs(new LongSumAggregatorFactory(\"metA\", \"metA\"))\n        .setLimitSpec(\n            new DefaultLimitSpec(\n                Collections.singletonList(new OrderByColumnSpec(\"dimA\", OrderByColumnSpec.Direction.DESCENDING)),\n                3\n            )\n        )\n        .setGranularity(Granularities.ALL)\n        .build();\n\n    Sequence<Row> queryResult = theRunner3.run(QueryPlus.wrap(query), new HashMap<>());\n    List<Row> results = queryResult.toList();\n\n    Row expectedRow0 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zortaxx\",\n        \"metA\", 999L\n    );\n    Row expectedRow1 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zebra\",\n        \"metA\", 180L\n    );\n    Row expectedRow2 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"world\",\n        \"metA\", 150L\n    );\n\n    Assert.assertEquals(3, results.size());\n    Assert.assertEquals(expectedRow0, results.get(0));\n    Assert.assertEquals(expectedRow1, results.get(1));\n    Assert.assertEquals(expectedRow2, results.get(2));\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByLimitPushDownInsufficientBufferTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":430,"status":"M"},{"authorDate":"2019-07-24 23:29:03","commitOrder":3,"curCode":"  public void testPartialLimitPushDownMergeForceAggs()\n  {\n    \r\n\n    QueryToolChest<Row, GroupByQuery> toolChest = groupByFactory.getToolchest();\n    QueryRunner<Row> theRunner = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            groupByFactory.mergeRunners(executorService, getRunner1())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n\n    QueryRunner<Row> theRunner2 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            tooSmallGroupByFactory.mergeRunners(executorService, getRunner2())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner3 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            new QueryRunner<Row>()\n            {\n              @Override\n              public Sequence<Row> run(QueryPlus<Row> queryPlus, ResponseContext responseContext)\n              {\n                return Sequences\n                    .simple(\n                        ImmutableList.of(\n                            theRunner.run(queryPlus, responseContext),\n                            theRunner2.run(queryPlus, responseContext)\n                        )\n                    )\n                    .flatMerge(Function.identity(), queryPlus.getQuery().getResultOrdering());\n              }\n            }\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QuerySegmentSpec intervalSpec = new MultipleIntervalSegmentSpec(\n        Collections.singletonList(Intervals.utc(0, 1000000))\n    );\n\n    GroupByQuery query = GroupByQuery\n        .builder()\n        .setDataSource(\"blah\")\n        .setQuerySegmentSpec(intervalSpec)\n        .setDimensions(new DefaultDimensionSpec(\"dimA\", null))\n        .setAggregatorSpecs(new LongSumAggregatorFactory(\"metA\", \"metA\"))\n        .setLimitSpec(\n            new DefaultLimitSpec(\n                Collections.singletonList(\n                    new OrderByColumnSpec(\"metA\", OrderByColumnSpec.Direction.DESCENDING, StringComparators.NUMERIC)\n                ),\n                3\n            )\n        )\n        .setGranularity(Granularities.ALL)\n        .setContext(\n            ImmutableMap.of(\n              GroupByQueryConfig.CTX_KEY_FORCE_LIMIT_PUSH_DOWN,\n              true\n            )\n        )\n        .build();\n\n    Sequence<Row> queryResult = theRunner3.run(QueryPlus.wrap(query));\n    List<Row> results = queryResult.toList();\n\n    Row expectedRow0 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zortaxx\",\n        \"metA\", 999L\n    );\n    Row expectedRow1 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"foo\",\n        \"metA\", 200L\n    );\n    Row expectedRow2 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"mango\",\n        \"metA\", 190L\n    );\n\n    Assert.assertEquals(3, results.size());\n    Assert.assertEquals(expectedRow0, results.get(0));\n    Assert.assertEquals(expectedRow1, results.get(1));\n    Assert.assertEquals(expectedRow2, results.get(2));\n  }\n","date":"2019-07-24 23:29:03","endLine":606,"groupId":"1175","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testPartialLimitPushDownMergeForceAggs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c7/76cecc7104ba1136d8b3e3ee709124332154c1.src","preCode":"  public void testPartialLimitPushDownMergeForceAggs()\n  {\n    \r\n\n    QueryToolChest<Row, GroupByQuery> toolChest = groupByFactory.getToolchest();\n    QueryRunner<Row> theRunner = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            groupByFactory.mergeRunners(executorService, getRunner1())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n\n    QueryRunner<Row> theRunner2 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            tooSmallGroupByFactory.mergeRunners(executorService, getRunner2())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner3 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            new QueryRunner<Row>()\n            {\n              @Override\n              public Sequence<Row> run(QueryPlus<Row> queryPlus, Map<String, Object> responseContext)\n              {\n                return Sequences\n                    .simple(\n                        ImmutableList.of(\n                            theRunner.run(queryPlus, responseContext),\n                            theRunner2.run(queryPlus, responseContext)\n                        )\n                    )\n                    .flatMerge(Function.identity(), queryPlus.getQuery().getResultOrdering());\n              }\n            }\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QuerySegmentSpec intervalSpec = new MultipleIntervalSegmentSpec(\n        Collections.singletonList(Intervals.utc(0, 1000000))\n    );\n\n    GroupByQuery query = GroupByQuery\n        .builder()\n        .setDataSource(\"blah\")\n        .setQuerySegmentSpec(intervalSpec)\n        .setDimensions(new DefaultDimensionSpec(\"dimA\", null))\n        .setAggregatorSpecs(new LongSumAggregatorFactory(\"metA\", \"metA\"))\n        .setLimitSpec(\n            new DefaultLimitSpec(\n                Collections.singletonList(\n                    new OrderByColumnSpec(\"metA\", OrderByColumnSpec.Direction.DESCENDING, StringComparators.NUMERIC)\n                ),\n                3\n            )\n        )\n        .setGranularity(Granularities.ALL)\n        .setContext(\n            ImmutableMap.of(\n              GroupByQueryConfig.CTX_KEY_FORCE_LIMIT_PUSH_DOWN,\n              true\n            )\n        )\n        .build();\n\n    Sequence<Row> queryResult = theRunner3.run(QueryPlus.wrap(query), new HashMap<>());\n    List<Row> results = queryResult.toList();\n\n    Row expectedRow0 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zortaxx\",\n        \"metA\", 999L\n    );\n    Row expectedRow1 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"foo\",\n        \"metA\", 200L\n    );\n    Row expectedRow2 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"mango\",\n        \"metA\", 190L\n    );\n\n    Assert.assertEquals(3, results.size());\n    Assert.assertEquals(expectedRow0, results.get(0));\n    Assert.assertEquals(expectedRow1, results.get(1));\n    Assert.assertEquals(expectedRow2, results.get(2));\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByLimitPushDownInsufficientBufferTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":515,"status":"M"}],"commitId":"799d20249fe6333ea86b020f6d09c91fa4d3f998","commitMessage":"@@@Response context refactoring (#8110)\n\n* Response context refactoring\n\n* Serialization/Deserialization of ResponseContext\n\n* Added java doc comments\n\n* Renamed vars related to ResponseContext\n\n* Renamed empty() methods to createEmpty()\n\n* Fixed ResponseContext usage\n\n* Renamed multiple ResponseContext static fields\n\n* Added PublicApi annotations\n\n* Renamed QueryResponseContext class to ResourceIOReaderWriter\n\n* Moved the protected method below public static constants\n\n* Added createEmpty method to ResponseContext with DefaultResponseContext creation\n\n* Fixed inspection error\n\n* Added comments to the ResponseContext length limit and ResponseContext\nhttp header name\n\n* Added a comment of possible future refactoring\n\n* Removed .gitignore file of indexing-service\n\n* Removed a never-used method\n\n* VisibleForTesting method reducing boilerplate\n\nCo-Authored-By: Clint Wylie <cjwylie@gmail.com>\n\n* Reduced boilerplate\n\n* Renamed the method serialize to serializeWith\n\n* Removed unused import\n\n* Fixed incorrectly refactored test method\n\n* Added comments for ResponseContext keys\n\n* Fixed incorrectly refactored test method\n\n* Fixed IntervalChunkingQueryRunnerTest mocks\n","date":"2019-07-24 23:29:03","modifiedFileCount":"142","status":"M","submitter":"Eugene Sevastianov"},{"authorTime":"2019-08-01 07:15:12","codes":[{"authorDate":"2019-08-01 07:15:12","commitOrder":4,"curCode":"  public void testPartialLimitPushDownMerge()\n  {\n    \r\n\n    QueryToolChest<ResultRow, GroupByQuery> toolChest = groupByFactory.getToolchest();\n    QueryRunner<ResultRow> theRunner = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            groupByFactory.mergeRunners(executorService, getRunner1())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<ResultRow> theRunner2 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            tooSmallGroupByFactory.mergeRunners(executorService, getRunner2())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<ResultRow> theRunner3 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            new QueryRunner<ResultRow>()\n            {\n              @Override\n              public Sequence<ResultRow> run(QueryPlus<ResultRow> queryPlus, ResponseContext responseContext)\n              {\n                return Sequences\n                    .simple(\n                        ImmutableList.of(\n                            theRunner.run(queryPlus, responseContext),\n                            theRunner2.run(queryPlus, responseContext)\n                        )\n                    )\n                    .flatMerge(Function.identity(), queryPlus.getQuery().getResultOrdering());\n              }\n            }\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QuerySegmentSpec intervalSpec = new MultipleIntervalSegmentSpec(\n        Collections.singletonList(Intervals.utc(0, 1000000))\n    );\n\n    GroupByQuery query = GroupByQuery\n        .builder()\n        .setDataSource(\"blah\")\n        .setQuerySegmentSpec(intervalSpec)\n        .setDimensions(new DefaultDimensionSpec(\"dimA\", null))\n        .setAggregatorSpecs(new LongSumAggregatorFactory(\"metA\", \"metA\"))\n        .setLimitSpec(\n            new DefaultLimitSpec(\n                Collections.singletonList(new OrderByColumnSpec(\"dimA\", OrderByColumnSpec.Direction.DESCENDING)),\n                3\n            )\n        )\n        .setGranularity(Granularities.ALL)\n        .build();\n\n    Sequence<ResultRow> queryResult = theRunner3.run(QueryPlus.wrap(query), ResponseContext.createEmpty());\n    List<ResultRow> results = queryResult.toList();\n\n    ResultRow expectedRow0 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        query,\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zortaxx\",\n        \"metA\", 999L\n    );\n    ResultRow expectedRow1 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        query,\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zebra\",\n        \"metA\", 180L\n    );\n    ResultRow expectedRow2 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        query,\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"world\",\n        \"metA\", 150L\n    );\n\n    Assert.assertEquals(3, results.size());\n    Assert.assertEquals(expectedRow0, results.get(0));\n    Assert.assertEquals(expectedRow1, results.get(1));\n    Assert.assertEquals(expectedRow2, results.get(2));\n  }\n","date":"2019-08-01 07:15:12","endLine":514,"groupId":"106618","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testPartialLimitPushDownMerge","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/35/a59459bff32b70aa33701a82ee1885d3064d96.src","preCode":"  public void testPartialLimitPushDownMerge()\n  {\n    \r\n\n    QueryToolChest<Row, GroupByQuery> toolChest = groupByFactory.getToolchest();\n    QueryRunner<Row> theRunner = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            groupByFactory.mergeRunners(executorService, getRunner1())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner2 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            tooSmallGroupByFactory.mergeRunners(executorService, getRunner2())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner3 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            new QueryRunner<Row>()\n            {\n              @Override\n              public Sequence<Row> run(QueryPlus<Row> queryPlus, ResponseContext responseContext)\n              {\n                return Sequences\n                    .simple(\n                        ImmutableList.of(\n                            theRunner.run(queryPlus, responseContext),\n                            theRunner2.run(queryPlus, responseContext)\n                        )\n                    )\n                    .flatMerge(Function.identity(), queryPlus.getQuery().getResultOrdering());\n              }\n            }\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QuerySegmentSpec intervalSpec = new MultipleIntervalSegmentSpec(\n        Collections.singletonList(Intervals.utc(0, 1000000))\n    );\n\n    GroupByQuery query = GroupByQuery\n        .builder()\n        .setDataSource(\"blah\")\n        .setQuerySegmentSpec(intervalSpec)\n        .setDimensions(new DefaultDimensionSpec(\"dimA\", null))\n        .setAggregatorSpecs(new LongSumAggregatorFactory(\"metA\", \"metA\"))\n        .setLimitSpec(\n            new DefaultLimitSpec(\n                Collections.singletonList(new OrderByColumnSpec(\"dimA\", OrderByColumnSpec.Direction.DESCENDING)),\n                3\n            )\n        )\n        .setGranularity(Granularities.ALL)\n        .build();\n\n    Sequence<Row> queryResult = theRunner3.run(QueryPlus.wrap(query));\n    List<Row> results = queryResult.toList();\n\n    Row expectedRow0 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zortaxx\",\n        \"metA\", 999L\n    );\n    Row expectedRow1 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zebra\",\n        \"metA\", 180L\n    );\n    Row expectedRow2 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"world\",\n        \"metA\", 150L\n    );\n\n    Assert.assertEquals(3, results.size());\n    Assert.assertEquals(expectedRow0, results.get(0));\n    Assert.assertEquals(expectedRow1, results.get(1));\n    Assert.assertEquals(expectedRow2, results.get(2));\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByLimitPushDownInsufficientBufferTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":429,"status":"M"},{"authorDate":"2019-08-01 07:15:12","commitOrder":4,"curCode":"  public void testPartialLimitPushDownMergeForceAggs()\n  {\n    \r\n\n    QueryToolChest<ResultRow, GroupByQuery> toolChest = groupByFactory.getToolchest();\n    QueryRunner<ResultRow> theRunner = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            groupByFactory.mergeRunners(executorService, getRunner1())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n\n    QueryRunner<ResultRow> theRunner2 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            tooSmallGroupByFactory.mergeRunners(executorService, getRunner2())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<ResultRow> theRunner3 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            new QueryRunner<ResultRow>()\n            {\n              @Override\n              public Sequence<ResultRow> run(QueryPlus<ResultRow> queryPlus, ResponseContext responseContext)\n              {\n                return Sequences\n                    .simple(\n                        ImmutableList.of(\n                            theRunner.run(queryPlus, responseContext),\n                            theRunner2.run(queryPlus, responseContext)\n                        )\n                    )\n                    .flatMerge(Function.identity(), queryPlus.getQuery().getResultOrdering());\n              }\n            }\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QuerySegmentSpec intervalSpec = new MultipleIntervalSegmentSpec(\n        Collections.singletonList(Intervals.utc(0, 1000000))\n    );\n\n    GroupByQuery query = GroupByQuery\n        .builder()\n        .setDataSource(\"blah\")\n        .setQuerySegmentSpec(intervalSpec)\n        .setDimensions(new DefaultDimensionSpec(\"dimA\", null))\n        .setAggregatorSpecs(new LongSumAggregatorFactory(\"metA\", \"metA\"))\n        .setLimitSpec(\n            new DefaultLimitSpec(\n                Collections.singletonList(\n                    new OrderByColumnSpec(\"metA\", OrderByColumnSpec.Direction.DESCENDING, StringComparators.NUMERIC)\n                ),\n                3\n            )\n        )\n        .setGranularity(Granularities.ALL)\n        .setContext(\n            ImmutableMap.of(\n                GroupByQueryConfig.CTX_KEY_FORCE_LIMIT_PUSH_DOWN,\n                true\n            )\n        )\n        .build();\n\n    Sequence<ResultRow> queryResult = theRunner3.run(QueryPlus.wrap(query), ResponseContext.createEmpty());\n    List<ResultRow> results = queryResult.toList();\n\n    ResultRow expectedRow0 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        query,\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zortaxx\",\n        \"metA\", 999L\n    );\n    ResultRow expectedRow1 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        query,\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"foo\",\n        \"metA\", 200L\n    );\n    ResultRow expectedRow2 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        query,\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"mango\",\n        \"metA\", 190L\n    );\n\n    Assert.assertEquals(3, results.size());\n    Assert.assertEquals(expectedRow0, results.get(0));\n    Assert.assertEquals(expectedRow1, results.get(1));\n    Assert.assertEquals(expectedRow2, results.get(2));\n  }\n","date":"2019-08-01 07:15:12","endLine":611,"groupId":"106618","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testPartialLimitPushDownMergeForceAggs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/35/a59459bff32b70aa33701a82ee1885d3064d96.src","preCode":"  public void testPartialLimitPushDownMergeForceAggs()\n  {\n    \r\n\n    QueryToolChest<Row, GroupByQuery> toolChest = groupByFactory.getToolchest();\n    QueryRunner<Row> theRunner = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            groupByFactory.mergeRunners(executorService, getRunner1())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n\n    QueryRunner<Row> theRunner2 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            tooSmallGroupByFactory.mergeRunners(executorService, getRunner2())\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QueryRunner<Row> theRunner3 = new FinalizeResultsQueryRunner<>(\n        toolChest.mergeResults(\n            new QueryRunner<Row>()\n            {\n              @Override\n              public Sequence<Row> run(QueryPlus<Row> queryPlus, ResponseContext responseContext)\n              {\n                return Sequences\n                    .simple(\n                        ImmutableList.of(\n                            theRunner.run(queryPlus, responseContext),\n                            theRunner2.run(queryPlus, responseContext)\n                        )\n                    )\n                    .flatMerge(Function.identity(), queryPlus.getQuery().getResultOrdering());\n              }\n            }\n        ),\n        (QueryToolChest) toolChest\n    );\n\n    QuerySegmentSpec intervalSpec = new MultipleIntervalSegmentSpec(\n        Collections.singletonList(Intervals.utc(0, 1000000))\n    );\n\n    GroupByQuery query = GroupByQuery\n        .builder()\n        .setDataSource(\"blah\")\n        .setQuerySegmentSpec(intervalSpec)\n        .setDimensions(new DefaultDimensionSpec(\"dimA\", null))\n        .setAggregatorSpecs(new LongSumAggregatorFactory(\"metA\", \"metA\"))\n        .setLimitSpec(\n            new DefaultLimitSpec(\n                Collections.singletonList(\n                    new OrderByColumnSpec(\"metA\", OrderByColumnSpec.Direction.DESCENDING, StringComparators.NUMERIC)\n                ),\n                3\n            )\n        )\n        .setGranularity(Granularities.ALL)\n        .setContext(\n            ImmutableMap.of(\n              GroupByQueryConfig.CTX_KEY_FORCE_LIMIT_PUSH_DOWN,\n              true\n            )\n        )\n        .build();\n\n    Sequence<Row> queryResult = theRunner3.run(QueryPlus.wrap(query));\n    List<Row> results = queryResult.toList();\n\n    Row expectedRow0 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"zortaxx\",\n        \"metA\", 999L\n    );\n    Row expectedRow1 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"foo\",\n        \"metA\", 200L\n    );\n    Row expectedRow2 = GroupByQueryRunnerTestHelper.createExpectedRow(\n        \"1970-01-01T00:00:00.000Z\",\n        \"dimA\", \"mango\",\n        \"metA\", 190L\n    );\n\n    Assert.assertEquals(3, results.size());\n    Assert.assertEquals(expectedRow0, results.get(0));\n    Assert.assertEquals(expectedRow1, results.get(1));\n    Assert.assertEquals(expectedRow2, results.get(2));\n  }\n","realPath":"processing/src/test/java/org/apache/druid/query/groupby/GroupByLimitPushDownInsufficientBufferTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":517,"status":"M"}],"commitId":"77297f4e6f2e9d617c96cd46852bb5a772961e85","commitMessage":"@@@GroupBy array-based result rows. (#8196)\n\n* GroupBy array-based result rows.\n\nFixes #8118; see that proposal for details.\n\nOther than the GroupBy changes.  the main other \"interesting\" classes are:\n\n- ResultRow: The array-based result type.\n- BaseQuery: T is no longer required to be Comparable.\n- QueryToolChest: Adds \"decorateObjectMapper\" to enable query-aware serialization\n  and deserialization of result rows (necessary due to their positional nature).\n- QueryResource: Uses the new decoration functionality.\n- DirectDruidClient: Also uses the new decoration functionality.\n- QueryMaker (in Druid SQL): Modifications to read ResultRows.\n\nThese classes weren't changed.  but got some new javadocs:\n\n- BySegmentQueryRunner\n- FinalizeResultsQueryRunner\n- Query\n\n* Adjustments for TC stuff.\n","date":"2019-08-01 07:15:12","modifiedFileCount":"111","status":"M","submitter":"Gian Merlino"}]
