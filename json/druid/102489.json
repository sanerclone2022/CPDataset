[{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\");\n    expectLastCall().times(2);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2018-08-31 00:56:26","endLine":1739,"groupId":"5276","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5d/773db661d0c8e87703ae229b128551316998e5.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\");\n    expectLastCall().times(2);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1644,"status":"B"},{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\");\n    taskQueue.shutdown(\"id3\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2018-08-31 00:56:26","endLine":1939,"groupId":"5276","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testResetRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5d/773db661d0c8e87703ae229b128551316998e5.src","preCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\");\n    taskQueue.shutdown(\"id3\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1850,"status":"B"}],"commitId":"431d3d8497f9079857c3baa7ae7ab4cb44a22355","commitMessage":"@@@Rename io.druid to org.apache.druid. (#6266)\n\n* Rename io.druid to org.apache.druid.\n\n* Fix META-INF files and remove some benchmark results.\n\n* MonitorsConfig update for metrics package migration.\n\n* Reorder some dimensions in inner queries for some reason.\n\n* Fix protobuf tests.\n","date":"2018-08-31 00:56:26","modifiedFileCount":"5","status":"B","submitter":"Gian Merlino"},{"authorTime":"2018-11-16 10:01:56","codes":[{"authorDate":"2018-11-16 10:01:56","commitOrder":2,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2018-11-16 10:01:56","endLine":1769,"groupId":"5276","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/9b/4a5a6cf4afd687a410813d77e5a040693c65ce.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\");\n    expectLastCall().times(2);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1672,"status":"M"},{"authorDate":"2018-11-16 10:01:56","commitOrder":2,"curCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2018-11-16 10:01:56","endLine":1969,"groupId":"5276","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testResetRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/9b/4a5a6cf4afd687a410813d77e5a040693c65ce.src","preCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\");\n    taskQueue.shutdown(\"id3\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1880,"status":"M"}],"commitId":"d738ce4d2a4430cf95919e27b0eede171cbdf66c","commitMessage":"@@@Enforce logging when killing a task (#6621)\n\n* Enforce logging when killing a task\n\n* fix test\n\n* address comment\n\n* address comment\n","date":"2018-11-16 10:01:56","modifiedFileCount":"14","status":"M","submitter":"Jihoon Son"},{"authorTime":"2018-12-22 03:49:24","codes":[{"authorDate":"2018-12-22 03:49:24","commitOrder":3,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2018-12-22 03:49:24","endLine":1855,"groupId":"16026","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/42/7b9d1742dec9346159f08cf020ea57960091eb.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1749,"status":"M"},{"authorDate":"2018-12-22 03:49:24","commitOrder":3,"curCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2018-12-22 03:49:24","endLine":2065,"groupId":"16026","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testResetRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/42/7b9d1742dec9346159f08cf020ea57960091eb.src","preCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new KafkaPartitions(\"topic\", ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(KafkaIndexTask.Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1967,"status":"M"}],"commitId":"7c7997e8a1183a7bffad731ca94e8b4c381e8665","commitMessage":"@@@Add Kinesis Indexing Service to core Druid (#6431)\n\n* created seekablestream classes\n\n* created seekablestreamsupervisor class\n\n* first attempt to integrate kafa indexing service to use SeekableStream\n\n* seekablestream bug fixes\n\n* kafkarecordsupplier\n\n* integrated kafka indexing service with seekablestream\n\n* implemented resume/suspend and refactored some package names\n\n* moved kinesis indexing service into core druid extensions\n\n* merged some changes from kafka supervisor race condition\n\n* integrated kinesis-indexing-service with seekablestream\n\n* unite tests for kinesis-indexing-service\n\n* various bug fixes for kinesis-indexing-service\n\n* refactored kinesisindexingtask\n\n* finished up more kinesis unit tests\n\n* more bug fixes for kinesis-indexing-service\n\n* finsihed refactoring kinesis unit tests\n\n* removed KinesisParititons and KafkaPartitions to use SeekableStreamPartitions\n\n* kinesis-indexing-service code cleanup and docs\n\n* merge #6291\n\nmerge #6337\n\nmerge #6383\n\n* added more docs and reordered methods\n\n* fixd kinesis tests after merging master and added docs in seekablestream\n\n* fix various things from pr comment\n\n* improve recordsupplier and add unit tests\n\n* migrated to aws-java-sdk-kinesis\n\n* merge changes from master\n\n* fix pom files and forbiddenapi checks\n\n* checkpoint JavaType bug fix\n\n* fix pom and stuff\n\n* disable checkpointing in kinesis\n\n* fix kinesis sequence number null in closed shard\n\n* merge changes from master\n\n* fixes for kinesis tasks\n\n* capitalized <partitionType.  sequenceType>\n\n* removed abstract class loggers\n\n* conform to guava api restrictions\n\n* add docker for travis other modules test\n\n* address comments\n\n* improve RecordSupplier to supply records in batch\n\n* fix strict compile issue\n\n* add test scope for localstack dependency\n\n* kinesis indexing task refactoring\n\n* comments\n\n* github comments\n\n* minor fix\n\n* removed unneeded readme\n\n* fix deserialization bug\n\n* fix various bugs\n\n* KinesisRecordSupplier unable to catch up to earliest position in stream bug fix\n\n* minor changes to kinesis\n\n* implement deaggregate for kinesis\n\n* Merge remote-tracking branch 'upstream/master' into seekablestream\n\n* fix kinesis offset discrepancy with kafka\n\n* kinesis record supplier disable getPosition\n\n* pr comments\n\n* mock for kinesis tests and remove docker dependency for unit tests\n\n* PR comments\n\n* avg lag in kafkasupervisor #6587\n\n* refacotred SequenceMetadata in taskRunners\n\n* small fix\n\n* more small fix\n\n* recordsupplier resource leak\n\n* revert .travis.yml formatting\n\n* fix style\n\n* kinesis docs\n\n* doc part2\n\n* more docs\n\n* comments\n\n* comments*2\n\n* revert string replace changes\n\n* comments\n\n* teamcity\n\n* comments part 1\n\n* comments part 2\n\n* comments part 3\n\n* merge #6754\n\n* fix injection binding\n\n* comments\n\n* KinesisRegion refactor\n\n* comments part idk lol\n\n* can't think of a commit msg anymore\n\n* remove possiblyResetDataSourceMetadata() for IncrementalPublishingTaskRunner\n\n* commmmmmmmmmments\n\n* extra error handling in KinesisRecordSupplier getRecords\n\n* comments\n\n* quickfix\n\n* typo\n\n* oof\n","date":"2018-12-22 03:49:24","modifiedFileCount":"22","status":"M","submitter":"Joshua Sun"},{"authorTime":"2019-02-19 03:50:08","codes":[{"authorDate":"2019-02-19 03:50:08","commitOrder":4,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2019-02-19 03:50:08","endLine":1853,"groupId":"16026","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f2/db280c51eb85380271ce661d1c415120afa0e0.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1747,"status":"M"},{"authorDate":"2019-02-19 03:50:08","commitOrder":4,"curCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2019-02-19 03:50:08","endLine":2063,"groupId":"16026","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testResetRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f2/db280c51eb85380271ce661d1c415120afa0e0.src","preCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null, false);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1965,"status":"M"}],"commitId":"80a2ef7be46c4fc807ac2a74507b8ba8b6a44049","commitMessage":"@@@Support kafka transactional topics (#5404) (#6496)\n\n* Support kafka transactional topics\n\n* update kafka to version 2.0.0\n* Remove the skipOffsetGaps option since it's not used anymore\n* Adjust kafka consumer to use transactional semantics\n* Update tests\n\n* Remove unused import from test\n\n* Fix compilation\n\n* Invoke transaction api to fix a unit test\n\n* temporary modification of travis.yml for debugging\n\n* another attempt to get travis tasklogs\n\n* update kafka to 2.0.1 at all places\n\n* Remove druid-kafka-eight dependency from integration-tests.  remove the kafka firehose test and deprecate kafka-eight classes\n\n* Add deprecated in docs for kafka-eight and kafka-simple extensions\n\n* Remove skipOffsetGaps and code changes for transaction support\n\n* Fix indentation\n\n* remove skipOffsetGaps from kinesis\n\n* Add transaction api to KafkaRecordSupplierTest\n\n* Fix indent\n\n* Fix test\n\n* update kafka version to 2.1.0\n","date":"2019-02-19 03:50:08","modifiedFileCount":"25","status":"M","submitter":"Surekha"},{"authorTime":"2019-03-22 04:12:22","codes":[{"authorDate":"2019-03-22 04:12:22","commitOrder":5,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2019-03-22 04:12:22","endLine":1926,"groupId":"16026","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b3/0b75ee7886fa6f4c326a81770066d6778e2033.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1820,"status":"M"},{"authorDate":"2019-03-22 04:12:22","commitOrder":5,"curCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2019-03-22 04:12:22","endLine":2135,"groupId":"16026","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testResetRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b3/0b75ee7886fa6f4c326a81770066d6778e2033.src","preCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamPartitions<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L)),\n        new SeekableStreamPartitions<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2037,"status":"M"}],"commitId":"0c5dcf5586e33607849e397209f3eb0b10661f1e","commitMessage":"@@@Fix exclusivity for start offset in kinesis indexing service & check exclusivity properly in IndexerSQLMetadataStorageCoordinator (#7291)\n\n* Fix exclusivity for start offset in kinesis indexing service\n\n* some adjustment\n\n* Fix SeekableStreamDataSourceMetadata\n\n* Add missing javadocs\n\n* Add missing comments and unit test\n\n* fix SeekableStreamStartSequenceNumbers.plus and add comments\n\n* remove extra exclusivePartitions in KafkaIOConfig and fix downgrade issue\n\n* Add javadocs\n\n* fix compilation\n\n* fix test\n\n* remove unused variable\n","date":"2019-03-22 04:12:22","modifiedFileCount":"30","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-04-11 09:16:38","codes":[{"authorDate":"2019-04-11 09:16:38","commitOrder":6,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2019-04-11 09:16:38","endLine":1890,"groupId":"16026","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b6/e7b3406771642225b6ce082b4b2bc68e735a67.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1784,"status":"M"},{"authorDate":"2019-04-11 09:16:38","commitOrder":6,"curCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2019-04-11 09:16:38","endLine":2103,"groupId":"16026","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testResetRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b6/e7b3406771642225b6ce082b4b2bc68e735a67.src","preCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2005,"status":"M"}],"commitId":"2771ed50b0f07b0ee519da72ed9f4877466f8be4","commitMessage":"@@@Support Kafka supervisor adopting running tasks between versions  (#7212)\n\n* Recompute hash in isTaskCurrent() and added tests\n\n* Fixed checkstyle stuff\n\n* Fixed failing tests\n\n* Make TestableKafkaSupervisorWithCustomIsTaskCurrent static\n\n* Add doc\n\n* baseSequenceName change\n\n* Added comment\n\n* WIP\n\n* Fixed imports\n\n* Undid lambda change for diff sake\n\n* Cleanup\n\n* Added comment\n\n* Reinsert Kafka tests\n\n* Readded kinesis test\n\n* Readd bad partition assignment in kinesis supervisor test\n\n* Nit\n\n* Misnamed var\n","date":"2019-04-11 09:16:38","modifiedFileCount":"6","status":"M","submitter":"Justin Borromeo"},{"authorTime":"2019-07-07 00:33:12","codes":[{"authorDate":"2019-07-07 00:33:12","commitOrder":7,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    EasyMock.replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2019-07-07 00:33:12","endLine":1894,"groupId":"10584","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/af/f5639cd0e0e7c6a18b14f694e3cf2c6cb73dec.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskClient.pauseAsync(\"id2\"))\n        .andReturn(Futures.immediateFuture((Map<Integer, Long>) ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n        .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    expectLastCall().times(1);\n\n    replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1788,"status":"M"},{"authorDate":"2019-07-07 00:33:12","commitOrder":7,"curCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    EasyMock.expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    EasyMock.replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2019-07-07 00:33:12","endLine":2107,"groupId":"10584","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testResetRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/af/f5639cd0e0e7c6a18b14f694e3cf2c6cb73dec.src","preCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n    expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n        .andReturn(Futures.immediateFuture(checkpoints))\n        .times(1);\n\n    taskRunner.registerListener(anyObject(TaskRunnerListener.class), anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2009,"status":"M"}],"commitId":"1166bbcb75d432817715fdd429737f86730b5591","commitMessage":"@@@Remove static imports from tests (#8036)\n\nMake static imports forbidden in tests and remove all occurrences to be\nconsistent with the non-test code.\n\nAlso.  various changes to files affected by above:\n- Reformat to adhere to druid style guide\n- Fix various IntelliJ warnings\n- Fix various SonarLint warnings (e.g..  the expected/actual args to\n  Assert.assertEquals() were flipped)","date":"2019-07-07 00:33:12","modifiedFileCount":"98","status":"M","submitter":"Chi Cao Minh"},{"authorTime":"2019-08-23 05:51:25","codes":[{"authorDate":"2019-08-23 05:51:25","commitOrder":8,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    EasyMock.replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2019-08-23 05:51:25","endLine":1929,"groupId":"10584","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0d/255b66d24d58735694d9fe14274069e9ba5ee9.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    EasyMock.replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1819,"status":"M"},{"authorDate":"2019-08-23 05:51:25","commitOrder":8,"curCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    EasyMock.expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    EasyMock.replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2019-08-23 05:51:25","endLine":2184,"groupId":"10584","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testResetRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0d/255b66d24d58735694d9fe14274069e9ba5ee9.src","preCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    EasyMock.expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    EasyMock.replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2082,"status":"M"}],"commitId":"fba92ae469b512cca6cdf86ffc1c1a2090808453","commitMessage":"@@@Fix to always use end sequenceNumber for reset (#8305)\n\n* Fix to always use end sequenceNumber for reset\n\n* fix checkstyle\n\n* fix style and add log\n","date":"2019-08-23 05:51:25","modifiedFileCount":"8","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-09-27 07:15:24","codes":[{"authorDate":"2019-09-27 07:15:24","commitOrder":9,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE)).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    EasyMock.replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2019-09-27 07:15:24","endLine":1917,"groupId":"10584","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/75/9182feac2928302ae8f3579d872a1883a232c5.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    EasyMock.replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1807,"status":"M"},{"authorDate":"2019-09-27 07:15:24","commitOrder":9,"curCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE)).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    EasyMock.expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    EasyMock.replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2019-09-27 07:15:24","endLine":2172,"groupId":"10584","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testResetRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/75/9182feac2928302ae8f3579d872a1883a232c5.src","preCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasks()).andReturn(ImmutableList.of(id1, id2, id3)).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    EasyMock.expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    EasyMock.replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2070,"status":"M"}],"commitId":"7f2b6577ef19f18523e8353336ad496e8dc4a270","commitMessage":"@@@get active task by datasource when supervisor discover tasks (#8450)\n\n* get active task by datasource when supervisor discover tasks\n\n* fix ut\n\n* fix ut\n\n* fix ut\n\n* remove unnecessary condition check\n\n* fix ut\n\n* remove stream in hot loop\n","date":"2019-09-27 07:15:24","modifiedFileCount":"7","status":"M","submitter":"elloooooo"},{"authorTime":"2020-01-28 03:24:29","codes":[{"authorDate":"2020-01-28 03:24:29","commitOrder":10,"curCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE))\n            .andReturn(ImmutableList.of(id1, id2, id3))\n            .anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.retrieveDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    EasyMock.replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","date":"2020-01-28 03:24:29","endLine":2037,"groupId":"102489","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"testStopGracefully","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0b/29b65a628bb6f0503d980831a84ce2d009150d.src","preCode":"  public void testStopGracefully() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE))\n            .andReturn(ImmutableList.of(id1, id2, id3))\n            .anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    \r\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskRunner, taskClient, taskQueue);\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskClient.pauseAsync(\"id2\"))\n            .andReturn(Futures.immediateFuture(ImmutableMap.of(0, 15L, 1, 25L, 2, 30L)));\n    EasyMock.expect(taskClient.setEndOffsetsAsync(\"id2\", ImmutableMap.of(0, 15L, 1, 25L, 2, 30L), true))\n            .andReturn(Futures.immediateFuture(true));\n    taskQueue.shutdown(\"id3\", \"Killing task for graceful shutdown\");\n    EasyMock.expectLastCall().times(1);\n    taskQueue.shutdown(\"id3\", \"Killing task [%s] which hasn't been assigned to a worker\", \"id3\");\n    EasyMock.expectLastCall().times(1);\n\n    EasyMock.replay(taskRunner, taskClient, taskQueue);\n\n    supervisor.gracefulShutdownInternal();\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1925,"status":"M"},{"authorDate":"2020-01-28 03:24:29","commitOrder":10,"curCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE))\n            .andReturn(ImmutableList.of(id1, id2, id3))\n            .anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.retrieveDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    EasyMock.expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    EasyMock.replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","date":"2020-01-28 03:24:29","endLine":2296,"groupId":"102489","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"testResetRunningTasks","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0b/29b65a628bb6f0503d980831a84ce2d009150d.src","preCode":"  public void testResetRunningTasks() throws Exception\n  {\n    final TaskLocation location1 = new TaskLocation(\"testHost\", 1234, -1);\n    final TaskLocation location2 = new TaskLocation(\"testHost2\", 145, -1);\n    final DateTime startTime = DateTimes.nowUtc();\n\n    supervisor = getTestableSupervisor(2, 1, true, \"PT1H\", null, null);\n    final KafkaSupervisorTuningConfig tuningConfig = supervisor.getTuningConfig();\n    addSomeEvents(1);\n\n    Task id1 = createKafkaIndexTask(\n        \"id1\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 0L, 1, 0L, 2, 0L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id2 = createKafkaIndexTask(\n        \"id2\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Task id3 = createKafkaIndexTask(\n        \"id3\",\n        DATASOURCE,\n        0,\n        new SeekableStreamStartSequenceNumbers<>(\"topic\", ImmutableMap.of(0, 10L, 1, 20L, 2, 30L), ImmutableSet.of()),\n        new SeekableStreamEndSequenceNumbers<>(\n            \"topic\",\n            ImmutableMap.of(0, Long.MAX_VALUE, 1, Long.MAX_VALUE, 2, Long.MAX_VALUE)\n        ),\n        null,\n        null,\n        tuningConfig\n    );\n\n    Collection workItems = new ArrayList<>();\n    workItems.add(new TestTaskRunnerWorkItem(id1, null, location1));\n    workItems.add(new TestTaskRunnerWorkItem(id2, null, location2));\n\n    EasyMock.expect(taskMaster.getTaskQueue()).andReturn(Optional.of(taskQueue)).anyTimes();\n    EasyMock.expect(taskMaster.getTaskRunner()).andReturn(Optional.of(taskRunner)).anyTimes();\n    EasyMock.expect(taskRunner.getRunningTasks()).andReturn(workItems).anyTimes();\n    EasyMock.expect(taskStorage.getActiveTasksByDatasource(DATASOURCE))\n            .andReturn(ImmutableList.of(id1, id2, id3))\n            .anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id1\")).andReturn(Optional.of(TaskStatus.running(\"id1\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id2\")).andReturn(Optional.of(TaskStatus.running(\"id2\"))).anyTimes();\n    EasyMock.expect(taskStorage.getStatus(\"id3\")).andReturn(Optional.of(TaskStatus.running(\"id3\"))).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id1\")).andReturn(Optional.of(id1)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id2\")).andReturn(Optional.of(id2)).anyTimes();\n    EasyMock.expect(taskStorage.getTask(\"id3\")).andReturn(Optional.of(id3)).anyTimes();\n    EasyMock.expect(indexerMetadataStorageCoordinator.getDataSourceMetadata(DATASOURCE)).andReturn(\n        new KafkaDataSourceMetadata(\n            null\n        )\n    ).anyTimes();\n    EasyMock.expect(taskClient.getStatusAsync(\"id1\")).andReturn(Futures.immediateFuture(Status.PUBLISHING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id2\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStatusAsync(\"id3\")).andReturn(Futures.immediateFuture(Status.READING));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id2\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getStartTimeAsync(\"id3\")).andReturn(Futures.immediateFuture(startTime));\n    EasyMock.expect(taskClient.getEndOffsets(\"id1\")).andReturn(ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n\n    TreeMap<Integer, Map<Integer, Long>> checkpoints = new TreeMap<>();\n    checkpoints.put(0, ImmutableMap.of(0, 10L, 1, 20L, 2, 30L));\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id2\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n    EasyMock.expect(taskClient.getCheckpointsAsync(EasyMock.contains(\"id3\"), EasyMock.anyBoolean()))\n            .andReturn(Futures.immediateFuture(checkpoints))\n            .times(1);\n\n    taskRunner.registerListener(EasyMock.anyObject(TaskRunnerListener.class), EasyMock.anyObject(Executor.class));\n    replayAll();\n\n    supervisor.start();\n    supervisor.runInternal();\n    verifyAll();\n\n    EasyMock.reset(taskQueue, indexerMetadataStorageCoordinator);\n    EasyMock.expect(indexerMetadataStorageCoordinator.deleteDataSourceMetadata(DATASOURCE)).andReturn(true);\n    taskQueue.shutdown(\"id2\", \"DataSourceMetadata is not found while reset\");\n    taskQueue.shutdown(\"id3\", \"DataSourceMetadata is not found while reset\");\n    EasyMock.replay(taskQueue, indexerMetadataStorageCoordinator);\n\n    supervisor.resetInternal(null);\n    verifyAll();\n  }\n","realPath":"extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/supervisor/KafkaSupervisorTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":2192,"status":"M"}],"commitId":"b9186f8f9ff2ff52aceda42bc5f24ffd47a7d17e","commitMessage":"@@@Reconcile terminology and method naming to 'used/unused segments'; Rename MetadataSegmentManager to MetadataSegmentsManager (#7306)\n\n* Reconcile terminology and method naming to 'used/unused segments'; Don't use terms 'enable/disable data source'; Rename MetadataSegmentManager to MetadataSegments; Make REST API methods which mark segments as used/unused to return server error instead of an empty response in case of error\n\n* Fix brace\n\n* Import order\n\n* Rename withKillDataSourceWhitelist to withSpecificDataSourcesToKill\n\n* Fix tests\n\n* Fix tests by adding proper methods without interval parameters to IndexerMetadataStorageCoordinator instead of hacking with Intervals.ETERNITY\n\n* More aligned names of DruidCoordinatorHelpers.  rename several CoordinatorDynamicConfig parameters\n\n* Rename ClientCompactTaskQuery to ClientCompactionTaskQuery for consistency with CompactionTask; ClientCompactQueryTuningConfig to ClientCompactionTaskQueryTuningConfig\n\n* More variable and method renames\n\n* Rename MetadataSegments to SegmentsMetadata\n\n* Javadoc update\n\n* Simplify SegmentsMetadata.getUnusedSegmentIntervals().  more javadocs\n\n* Update Javadoc of VersionedIntervalTimeline.iterateAllObjects()\n\n* Reorder imports\n\n* Rename SegmentsMetadata.tryMark... methods to mark... and make them to return boolean and the numbers of segments changed and relay exceptions to callers\n\n* Complete merge\n\n* Add CollectionUtils.newTreeSet(); Refactor DruidCoordinatorRuntimeParams creation in tests\n\n* Remove MetadataSegmentManager\n\n* Rename millisLagSinceCoordinatorBecomesLeaderBeforeCanMarkAsUnusedOvershadowedSegments to leadingTimeMillisBeforeCanMarkAsUnusedOvershadowedSegments\n\n* Fix tests.  refactor DruidCluster creation in tests into DruidClusterBuilder\n\n* Fix inspections\n\n* Fix SQLMetadataSegmentManagerEmptyTest and rename it to SqlSegmentsMetadataEmptyTest\n\n* Rename SegmentsAndMetadata to SegmentsAndCommitMetadata to reduce the similarity with SegmentsMetadata; Rename some methods\n\n* Rename DruidCoordinatorHelper to CoordinatorDuty.  refactor DruidCoordinator\n\n* Unused import\n\n* Optimize imports\n\n* Rename IndexerSQLMetadataStorageCoordinator.getDataSourceMetadata() to retrieveDataSourceMetadata()\n\n* Unused import\n\n* Update terminology in datasource-view.tsx\n\n* Fix label in datasource-view.spec.tsx.snap\n\n* Fix lint errors in datasource-view.tsx\n\n* Doc improvements\n\n* Another attempt to please TSLint\n\n* Another attempt to please TSLint\n\n* Style fixes\n\n* Fix IndexerSQLMetadataStorageCoordinator.createUsedSegmentsSqlQueryForIntervals() (wrong merge)\n\n* Try to fix docs build issue\n\n* Javadoc and spelling fixes\n\n* Rename SegmentsMetadata to SegmentsMetadataManager.  address other comments\n\n* Address more comments\n","date":"2020-01-28 03:24:29","modifiedFileCount":"127","status":"M","submitter":"Roman Leventov"}]
