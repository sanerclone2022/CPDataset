[{"authorTime":"2018-09-29 02:16:35","codes":[{"authorDate":"2018-09-29 02:16:35","commitOrder":1,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        keepSegmentGranularity,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(5, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, Collections.singletonList(COMPACTION_INTERVAL));\n    }\n  }\n","date":"2018-09-29 02:16:35","endLine":416,"groupId":"14702","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5c/d5f1fbdbda0c802521cfd0ada3c308b677af81.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        keepSegmentGranularity,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(5, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, Collections.singletonList(COMPACTION_INTERVAL));\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":389,"status":"B"},{"authorDate":"2018-09-29 02:16:35","commitOrder":1,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        500000,\n        1000000L,\n        5L,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        5000,\n        true,\n        false,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        keepSegmentGranularity,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(5, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS, tuningConfig);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          tuningConfig\n      );\n    }\n  }\n","date":"2018-09-29 02:16:35","endLine":536,"groupId":"14702","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5c/d5f1fbdbda0c802521cfd0ada3c308b677af81.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        500000,\n        1000000L,\n        5L,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        5000,\n        true,\n        false,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        keepSegmentGranularity,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(5, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS, tuningConfig);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          tuningConfig\n      );\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":479,"status":"B"}],"commitId":"122caec7b187820a0a7fc89e85eb8216dd57df21","commitMessage":"@@@Add support targetCompactionSizeBytes for compactionTask (#6203)\n\n* Add support targetCompactionSizeBytes for compactionTask\n\n* fix test\n\n* fix a bug in keepSegmentGranularity\n\n* fix wrong noinspection comment\n\n* address comments\n","date":"2018-09-29 02:16:35","modifiedFileCount":"7","status":"B","submitter":"Jihoon Son"},{"authorTime":"2018-10-07 07:45:07","codes":[{"authorDate":"2018-09-29 02:16:35","commitOrder":2,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        keepSegmentGranularity,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(5, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, Collections.singletonList(COMPACTION_INTERVAL));\n    }\n  }\n","date":"2018-09-29 02:16:35","endLine":416,"groupId":"14702","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5c/d5f1fbdbda0c802521cfd0ada3c308b677af81.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        keepSegmentGranularity,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(5, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, Collections.singletonList(COMPACTION_INTERVAL));\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":389,"status":"N"},{"authorDate":"2018-10-07 07:45:07","commitOrder":2,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        500000,\n        1000000L,\n        5L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        5000,\n        true,\n        false,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        keepSegmentGranularity,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(5, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS, tuningConfig);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          tuningConfig\n      );\n    }\n  }\n","date":"2018-10-07 07:45:07","endLine":537,"groupId":"14702","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/83/7b33a50a0b77b089d5386604d5820f1fa0dc4d.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        500000,\n        1000000L,\n        5L,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        5000,\n        true,\n        false,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        keepSegmentGranularity,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(5, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS, tuningConfig);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          tuningConfig\n      );\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":479,"status":"M"}],"commitId":"45aa51a00c642a501834e2dfe54d68cbab8e0464","commitMessage":"@@@Add support hash partitioning by a subset of dimensions to indexTask (#6326)\n\n* Add support hash partitioning by a subset of dimensions to indexTask\n\n* add doc\n\n* fix style\n\n* fix test\n\n* fix doc\n\n* fix build\n","date":"2018-10-07 07:45:07","modifiedFileCount":"9","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-01-04 09:50:45","codes":[{"authorDate":"2019-01-04 09:50:45","commitOrder":3,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS, Granularities.MONTH);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","date":"2019-01-04 09:50:45","endLine":426,"groupId":"4596","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/86/7cdfe03c74e942da7029a636d3ee9e1e7c92b5.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        keepSegmentGranularity,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(5, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, Collections.singletonList(COMPACTION_INTERVAL));\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":393,"status":"M"},{"authorDate":"2019-01-04 09:50:45","commitOrder":3,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        500000,\n        1000000L,\n        6L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        5000,\n        true,\n        false,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          SEGMENT_INTERVALS,\n          tuningConfig,\n          Granularities.MONTH\n      );\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          tuningConfig,\n          Granularities.ALL\n      );\n    }\n  }\n","date":"2019-01-04 09:50:45","endLine":564,"groupId":"4596","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/86/7cdfe03c74e942da7029a636d3ee9e1e7c92b5.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        500000,\n        1000000L,\n        5L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        5000,\n        true,\n        false,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        keepSegmentGranularity,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(5, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS, tuningConfig);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          tuningConfig\n      );\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":498,"status":"M"}],"commitId":"9ad6a733a58e81ef2e0dee067b1df8477af1dab4","commitMessage":"@@@Add support segmentGranularity for CompactionTask (#6758)\n\n* Add support segmentGranularity\n\n* add doc and fix combination of options\n\n* improve doc\n","date":"2019-01-04 09:50:45","modifiedFileCount":"5","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-01-11 01:50:14","codes":[{"authorDate":"2019-01-11 01:50:14","commitOrder":4,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, AGGREGATORS, SEGMENT_INTERVALS, Granularities.MONTH);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","date":"2019-01-11 01:50:14","endLine":454,"groupId":"14702","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f7/59cc7c1c622766f0623fa48f3320ec1f3375f6.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, SEGMENT_INTERVALS, Granularities.MONTH);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":419,"status":"M"},{"authorDate":"2019-01-11 01:50:14","commitOrder":4,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        6L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        5000,\n        true,\n        false,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          SEGMENT_INTERVALS,\n          tuningConfig,\n          Granularities.MONTH\n      );\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          tuningConfig,\n          Granularities.ALL\n      );\n    }\n  }\n","date":"2019-01-11 01:50:14","endLine":600,"groupId":"14702","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f7/59cc7c1c622766f0623fa48f3320ec1f3375f6.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        500000,\n        1000000L,\n        6L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        5000,\n        true,\n        false,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          SEGMENT_INTERVALS,\n          tuningConfig,\n          Granularities.MONTH\n      );\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          tuningConfig,\n          Granularities.ALL\n      );\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":530,"status":"M"}],"commitId":"c35a39d70bf705aa49c3a3c97bab87959bb80a4e","commitMessage":"@@@Add support maxRowsPerSegment for auto compaction (#6780)\n\n* Add support maxRowsPerSegment for auto compaction\n\n* fix build\n\n* fix build\n\n* fix teamcity\n\n* add test\n\n* fix test\n\n* address comment\n","date":"2019-01-11 01:50:14","modifiedFileCount":"33","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-02-24 09:02:56","codes":[{"authorDate":"2019-02-24 09:02:56","commitOrder":5,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          SEGMENT_INTERVALS,\n          Granularities.MONTH\n      );\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","date":"2019-02-24 09:02:56","endLine":482,"groupId":"14702","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/51/17c1a31c88aed24b6a09f380d4c84fb2a9340b.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(ingestionSpecs, expectedDimensionsSpec, AGGREGATORS, SEGMENT_INTERVALS, Granularities.MONTH);\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":438,"status":"M"},{"authorDate":"2019-02-24 09:02:56","commitOrder":5,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        6L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        5000,\n        true,\n        false,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          SEGMENT_INTERVALS,\n          tuningConfig,\n          Granularities.MONTH\n      );\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          tuningConfig,\n          Granularities.ALL\n      );\n    }\n  }\n","date":"2019-02-24 09:02:56","endLine":634,"groupId":"14702","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/51/17c1a31c88aed24b6a09f380d4c84fb2a9340b.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        6L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        5000,\n        true,\n        false,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          SEGMENT_INTERVALS,\n          tuningConfig,\n          Granularities.MONTH\n      );\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          tuningConfig,\n          Granularities.ALL\n      );\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":561,"status":"M"}],"commitId":"1c2753ab9033ff0c785d6e80f5f7c07dc34c3889","commitMessage":"@@@ParallelIndexSubTask: support ingestSegment in delegating factories (#7089)\n\nIndexTask had special-cased code to properly send a TaskToolbox to a\nIngestSegmentFirehoseFactory that's nested inside a CombiningFirehoseFactory. \nbut ParallelIndexSubTask didn't.\n\nThis change refactors IngestSegmentFirehoseFactory so that it doesn't need a\nTaskToolbox; it instead gets a CoordinatorClient and a SegmentLoaderFactory\ndirectly injected into it.\n\nThis also refactors SegmentLoaderFactory so it doesn't depend on\nan injectable SegmentLoaderConfig.  since its only method always\nreplaces the preconfigured SegmentLoaderConfig anyway.\nThis makes it possible to use SegmentLoaderFactory without setting\ndruid.segmentCaches.locations to some dummy value.\n\nAnother goal of this PR is to make it possible for IngestSegmentFirehoseFactory\nto list data segments outside of connect() --- specifically.  to make it a\nFiniteFirehoseFactory which can query the coordinator in order to calculate its\nsplits. See #7048.\n\nThis also adds missing datasource name URL-encoding to an API used by\nCoordinatorBasedSegmentHandoffNotifier.","date":"2019-02-24 09:02:56","modifiedFileCount":"21","status":"M","submitter":"David Glasser"},{"authorTime":"2019-03-16 14:29:25","codes":[{"authorDate":"2019-02-24 09:02:56","commitOrder":6,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          SEGMENT_INTERVALS,\n          Granularities.MONTH\n      );\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","date":"2019-02-24 09:02:56","endLine":482,"groupId":"14702","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/51/17c1a31c88aed24b6a09f380d4c84fb2a9340b.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          SEGMENT_INTERVALS,\n          Granularities.MONTH\n      );\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":438,"status":"N"},{"authorDate":"2019-03-16 14:29:25","commitOrder":6,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        6L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        5000,\n        true,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          SEGMENT_INTERVALS,\n          tuningConfig,\n          Granularities.MONTH\n      );\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          tuningConfig,\n          Granularities.ALL\n      );\n    }\n  }\n","date":"2019-03-16 14:29:25","endLine":632,"groupId":"14702","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/cb/d52f58d57d58987778b453dacc8ecadae67401.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        6L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        5000,\n        true,\n        false,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          SEGMENT_INTERVALS,\n          tuningConfig,\n          Granularities.MONTH\n      );\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          tuningConfig,\n          Granularities.ALL\n      );\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":560,"status":"M"}],"commitId":"892d1d35d6cc00487d583f05f1cc138782180ef9","commitMessage":"@@@Deprecate NoneShardSpec and drop support for automatic segment merge (#6883)\n\n* Deprecate noneShardSpec\n\n* clean up noneShardSpec constructor\n\n* revert unnecessary change\n\n* Deprecate mergeTask\n\n* add more doc\n\n* remove convert from indexMerger\n\n* Remove mergeTask\n\n* remove HadoopDruidConverterConfig\n\n* fix build\n\n* fix build\n\n* fix teamcity\n\n* fix teamcity\n\n* fix ServerModule\n\n* fix compilation\n\n* fix compilation\n","date":"2019-03-16 14:29:25","modifiedFileCount":"40","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-06-04 03:59:15","codes":[{"authorDate":"2019-06-04 03:59:15","commitOrder":7,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-06-04 03:59:15","endLine":445,"groupId":"14702","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/69/f10f6be40030eba55eb7286babe6c6779c7cf6.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          SEGMENT_INTERVALS,\n          Granularities.MONTH\n      );\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          Granularities.ALL\n      );\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":415,"status":"M"},{"authorDate":"2019-06-04 03:59:15","commitOrder":7,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        6L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        5000,\n        true,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        null,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-06-04 03:59:15","endLine":565,"groupId":"14702","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/69/f10f6be40030eba55eb7286babe6c6779c7cf6.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        6L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        5000,\n        true,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        null,\n        keepSegmentGranularity,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration(\n        keepSegmentGranularity\n    );\n\n    if (keepSegmentGranularity) {\n      ingestionSpecs.sort(\n          (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n              s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n              s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n          )\n      );\n      Assert.assertEquals(6, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          SEGMENT_INTERVALS,\n          tuningConfig,\n          Granularities.MONTH\n      );\n    } else {\n      Assert.assertEquals(1, ingestionSpecs.size());\n      assertIngestionSchema(\n          ingestionSpecs,\n          expectedDimensionsSpec,\n          AGGREGATORS,\n          Collections.singletonList(COMPACTION_INTERVAL),\n          tuningConfig,\n          Granularities.ALL\n      );\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":508,"status":"M"}],"commitId":"61ec521135fd8ae40110524e8857610d0830e279","commitMessage":"@@@Remove keepSegmentGranularity option for compaction  (#7747)\n\n* Remove keepSegmentGranularity option from compaction\n\n* fix it test\n\n* clean up\n\n* remove from web console\n\n* fix test\n","date":"2019-06-04 03:59:15","modifiedFileCount":"15","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-07-11 03:22:24","codes":[{"authorDate":"2019-06-04 03:59:15","commitOrder":8,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-06-04 03:59:15","endLine":445,"groupId":"14702","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/69/f10f6be40030eba55eb7286babe6c6779c7cf6.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":415,"status":"N"},{"authorDate":"2019-07-11 03:22:24","commitOrder":8,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        6L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        5000,\n        true,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        null,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-07-11 03:22:24","endLine":568,"groupId":"14702","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/bb/6993848c940fa316b0e7b239df4e6fc8c66597.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        6L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        5000,\n        true,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        null,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":510,"status":"M"}],"commitId":"14aec7fceca90dfaf9b2ce4dae68186d04ffcc47","commitMessage":"@@@add config to optionally disable all compression  in intermediate segment persists while ingestion (#7919)\n\n* disable all compression in intermediate segment persists while ingestion\n\n* more changes and build fix\n\n* by default retain existing indexingSpec for intermediate persisted segments\n\n* document indexSpecForIntermediatePersists index tuning config\n\n* fix build issues\n\n* update serde tests\n","date":"2019-07-11 03:22:24","modifiedFileCount":"56","status":"M","submitter":"Himanshu"},{"authorTime":"2019-07-31 08:24:39","codes":[{"authorDate":"2019-06-04 03:59:15","commitOrder":9,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-06-04 03:59:15","endLine":445,"groupId":"14702","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/69/f10f6be40030eba55eb7286babe6c6779c7cf6.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":415,"status":"N"},{"authorDate":"2019-07-31 08:24:39","commitOrder":9,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        null,\n        null,\n        null,\n        new HashedPartitionsSpec(null, 6, null),\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        5000,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        null,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-07-31 08:24:39","endLine":575,"groupId":"14702","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b7/6c9984050098130532b8e2d70e1cf101bc0b30.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        6L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        5000,\n        true,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        null,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":517,"status":"M"}],"commitId":"385f492a555add279a8c6dd368954fde18c41dcb","commitMessage":"@@@Use PartitionsSpec for all task types (#8141)\n\n* Use partitionsSpec for all task types\n\n* fix doc\n\n* fix typos and revert to use isPushRequired\n\n* address comments\n\n* move partitionsSpec to core\n\n* remove hadoopPartitionsSpec\n","date":"2019-07-31 08:24:39","modifiedFileCount":"29","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-10-10 02:12:00","codes":[{"authorDate":"2019-10-10 02:12:00","commitOrder":10,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-10-10 02:12:00","endLine":457,"groupId":"14702","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/fd/201d933c67e13e31fa6cb784a9bb71bb1f5f06.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":427,"status":"M"},{"authorDate":"2019-10-10 02:12:00","commitOrder":10,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        null,\n        null,\n        null,\n        new HashedPartitionsSpec(null, 6, null),\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        5000,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-10-10 02:12:00","endLine":579,"groupId":"14702","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/fd/201d933c67e13e31fa6cb784a9bb71bb1f5f06.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        null,\n        null,\n        null,\n        new HashedPartitionsSpec(null, 6, null),\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        5000,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, COMPACTION_INTERVAL),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        null,\n        null,\n        objectMapper,\n        coordinatorClient,\n        segmentLoaderFactory,\n        retryPolicyFactory\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":521,"status":"M"}],"commitId":"96d8523ecbc0108e622084671172fded81065aa0","commitMessage":"@@@Use hash of Segment IDs instead of a list of explicit segments in auto compaction (#8571)\n\n* IOConfig for compaction task\n\n* add javadoc.  doc.  unit test\n\n* fix webconsole test\n\n* add spelling\n\n* address comments\n\n* fix build and test\n\n* address comments\n","date":"2019-10-10 02:12:00","modifiedFileCount":"16","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-10-16 13:57:42","codes":[{"authorDate":"2019-10-16 13:57:42","commitOrder":11,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-10-16 13:57:42","endLine":455,"groupId":"14702","id":21,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f8/fe11a53a9f9e2f6e577bc2211f5299510368f7.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(null, TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":425,"status":"M"},{"authorDate":"2019-10-16 13:57:42","commitOrder":11,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        null,\n        null,\n        null,\n        new HashedPartitionsSpec(null, 6, null),\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        5000,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-10-16 13:57:42","endLine":577,"groupId":"14702","id":22,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f8/fe11a53a9f9e2f6e577bc2211f5299510368f7.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        null,\n        null,\n        null,\n        new HashedPartitionsSpec(null, 6, null),\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        5000,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(null, tuningConfig),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":519,"status":"M"}],"commitId":"4046c86d62192c812cea87188dd17e745fb83b04","commitMessage":"@@@Stateful auto compaction (#8573)\n\n* Stateful auto compaction\n\n* javaodc\n\n* add removed test back\n\n* fix test\n\n* adding indexSpec to compactionState\n\n* fix build\n\n* add lastCompactionState\n\n* address comments\n\n* extract CompactionState\n\n* fix doc\n\n* fix build and test\n\n* Add a task context to store compaction state; add javadoc\n\n* fix it test\n","date":"2019-10-16 13:57:42","modifiedFileCount":"69","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-10-19 04:24:14","codes":[{"authorDate":"2019-10-19 04:24:14","commitOrder":12,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-10-19 04:24:14","endLine":468,"groupId":"14702","id":23,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/32/53fdc4f6721a3c50d3358c8490177d835b99c8.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":438,"status":"M"},{"authorDate":"2019-10-19 04:24:14","commitOrder":12,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final ParallelIndexTuningConfig tuningConfig = new ParallelIndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","date":"2019-10-19 04:24:14","endLine":602,"groupId":"14702","id":24,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/32/53fdc4f6721a3c50d3358c8490177d835b99c8.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final IndexTuningConfig tuningConfig = new IndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        null,\n        null,\n        null,\n        new HashedPartitionsSpec(null, 6, null),\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        5000,\n        true,\n        false,\n        null,\n        100L,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<IndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":538,"status":"M"}],"commitId":"30c15900bec7f4df7a72088ec076223839da258f","commitMessage":"@@@Auto compaction based on parallel indexing (#8570)\n\n* Auto compaction based on parallel indexing\n\n* javadoc and doc\n\n* typo\n\n* update spell\n\n* addressing comments\n\n* address comments\n\n* fix log\n\n* fix build\n\n* fix test\n\n* increase default max input segment bytes per task\n\n* fix test\n","date":"2019-10-19 04:24:14","modifiedFileCount":"28","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-06-26 04:37:31","codes":[{"authorDate":"2020-06-26 04:37:31","commitOrder":13,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2020-06-26 04:37:31","endLine":610,"groupId":"14702","id":25,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/db/cfd4ad251ad89e7b104537357321fc716ef554.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":579,"status":"M"},{"authorDate":"2020-06-26 04:37:31","commitOrder":13,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final ParallelIndexTuningConfig tuningConfig = new ParallelIndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","date":"2020-06-26 04:37:31","endLine":746,"groupId":"14702","id":26,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/db/cfd4ad251ad89e7b104537357321fc716ef554.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final ParallelIndexTuningConfig tuningConfig = new ParallelIndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":681,"status":"M"}],"commitId":"aaee72c781dea0f06bd5781d085b356d5d4241d0","commitMessage":"@@@Allow append to existing datasources when dynamic partitioning is used (#10033)\n\n* Fill in the core partition set size properly for batch ingestion with\ndynamic partitioning\n\n* incomplete javadoc\n\n* Address comments\n\n* fix tests\n\n* fix json serde.  add tests\n\n* checkstyle\n\n* Set core partition set size for hash-partitioned segments properly in\nbatch ingestion\n\n* test for both parallel and single-threaded task\n\n* unused variables\n\n* fix test\n\n* unused imports\n\n* add hash/range buckets\n\n* some test adjustment and missing json serde\n\n* centralized partition id allocation in parallel and simple tasks\n\n* remove string partition chunk\n\n* revive string partition chunk\n\n* fill numCorePartitions for hadoop\n\n* clean up hash stuffs\n\n* resolved todos\n\n* javadocs\n\n* Fix tests\n\n* add more tests\n\n* doc\n\n* unused imports\n\n* Allow append to existing datasources when dynamic partitioing is used\n\n* fix test\n\n* checkstyle\n\n* checkstyle\n\n* fix test\n\n* fix test\n\n* fix other tests..\n\n* checkstyle\n\n* hansle unknown core partitions size in overlord segment allocation\n\n* fail to append when numCorePartitions is unknown\n\n* log\n\n* fix comment; rename to be more intuitive\n\n* double append test\n\n* cleanup complete(); add tests\n\n* fix build\n\n* add tests\n\n* address comments\n\n* checkstyle","date":"2020-06-26 04:37:31","modifiedFileCount":"53","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-08-27 08:08:12","codes":[{"authorDate":"2020-08-27 08:08:12","commitOrder":14,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2020-08-27 08:08:12","endLine":578,"groupId":"14702","id":27,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e6/35381327075830831d9c03e9520b7941ab304c.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":548,"status":"M"},{"authorDate":"2020-08-27 08:08:12","commitOrder":14,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final ParallelIndexTuningConfig tuningConfig = new ParallelIndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","date":"2020-08-27 08:08:12","endLine":712,"groupId":"14702","id":28,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e6/35381327075830831d9c03e9520b7941ab304c.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final ParallelIndexTuningConfig tuningConfig = new ParallelIndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        OBJECT_MAPPER,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":648,"status":"M"}],"commitId":"f82fd22fa7de175200b7127c34c2eb2900bf7317","commitMessage":"@@@Move tools for indexing to TaskToolbox instead of injecting them in constructor (#10308)\n\n* Move tools for indexing to TaskToolbox instead of injecting them in constructor\n\n* oops.  other changes\n\n* fix test\n\n* unnecessary new file\n\n* fix test\n\n* fix build","date":"2020-08-27 08:08:12","modifiedFileCount":"67","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-10-24 09:34:26","codes":[{"authorDate":"2020-08-27 08:08:12","commitOrder":15,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2020-08-27 08:08:12","endLine":578,"groupId":"14702","id":29,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e6/35381327075830831d9c03e9520b7941ab304c.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":548,"status":"N"},{"authorDate":"2020-10-24 09:34:26","commitOrder":15,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final ParallelIndexTuningConfig tuningConfig = new ParallelIndexTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","date":"2020-10-24 09:34:26","endLine":716,"groupId":"14702","id":30,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/14/c0840be4bdc499e34ac2d0167f36d471fde02b.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final ParallelIndexTuningConfig tuningConfig = new ParallelIndexTuningConfig(\n        null,\n        null,\n        500000,\n        1000000L,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":651,"status":"M"}],"commitId":"f3a2903218573f5d336b082b1c9b8a60a19e8c54","commitMessage":"@@@Configurable Index Type (#10335)\n\n* Introduce a Configurable Index Type\n\n* Change to @UnstableApi\n\n* Add AppendableIndexSpecTest\n\n* Update doc\n\n* Add spelling exception\n\n* Add tests coverage\n\n* Revert some of the changes to reduce diff\n\n* Minor fixes\n\n* Update getMaxBytesInMemoryOrDefault() comment\n\n* Fix typo.  remove redundant interface\n\n* Remove off-heap spec (postponed to a later PR)\n\n* Add javadocs to AppendableIndexSpec\n\n* Describe testCreateTask()\n\n* Add tests for AppendableIndexSpec within TuningConfig\n\n* Modify hashCode() to conform with equals()\n\n* Add comment where building incremental-index\n\n* Add \"EqualsVerifier\" tests\n\n* Revert some of the API back to AppenderatorConfig\n\n* Don't use multi-line comments\n\n* Remove knob documentation (deferred)","date":"2020-10-24 09:34:26","modifiedFileCount":"72","status":"M","submitter":"Liran Funaro"},{"authorTime":"2021-01-06 14:19:09","codes":[{"authorDate":"2020-08-27 08:08:12","commitOrder":16,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2020-08-27 08:08:12","endLine":578,"groupId":"14702","id":31,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e6/35381327075830831d9c03e9520b7941ab304c.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":548,"status":"N"},{"authorDate":"2021-01-06 14:19:09","commitOrder":16,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final ParallelIndexTuningConfig tuningConfig = new ParallelIndexTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","date":"2021-01-06 14:19:09","endLine":735,"groupId":"14702","id":32,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/6e/ddb65d0550b6c0b81e562d4d17fbf35879c383.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final ParallelIndexTuningConfig tuningConfig = new ParallelIndexTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":669,"status":"M"}],"commitId":"68bb038b314c26bcc57aa96e1078c22d2f24fd35","commitMessage":"@@@Multiphase segment merge for IndexMergerV9 (#10689)\n\n* Multiphase merge for IndexMergerV9\n\n* JSON fix\n\n* Cleanup temp files\n\n* Docs\n\n* Address logging and add IT\n\n* Fix spelling and test unloader datasource name","date":"2021-01-06 14:19:09","modifiedFileCount":"40","status":"M","submitter":"Jonathan Wei"},{"authorTime":"2021-01-27 16:34:56","codes":[{"authorDate":"2020-08-27 08:08:12","commitOrder":17,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","date":"2020-08-27 08:08:12","endLine":578,"groupId":"14702","id":33,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e6/35381327075830831d9c03e9520b7941ab304c.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":548,"status":"N"},{"authorDate":"2021-01-27 16:34:56","commitOrder":17,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final ParallelIndexTuningConfig tuningConfig = new ParallelIndexTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","date":"2021-01-27 16:34:56","endLine":739,"groupId":"14702","id":34,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/21/b7f7397b3a2da91155b1b7134cc6c233bad026.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final ParallelIndexTuningConfig tuningConfig = new ParallelIndexTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":672,"status":"M"}],"commitId":"a46d561bd7e2b045a08a2e475847d4a7505a1c93","commitMessage":"@@@Fix byte calculation for maxBytesInMemory to take into account of Sink/Hydrant Object overhead (#10740)\n\n* Fix byte calculation for maxBytesInMemory to take into account of Sink/Hydrant Object overhead\n\n* Fix byte calculation for maxBytesInMemory to take into account of Sink/Hydrant Object overhead\n\n* Fix byte calculation for maxBytesInMemory to take into account of Sink/Hydrant Object overhead\n\n* Fix byte calculation for maxBytesInMemory to take into account of Sink/Hydrant Object overhead\n\n* fix checkstyle\n\n* Fix byte calculation for maxBytesInMemory to take into account of Sink/Hydrant Object overhead\n\n* Fix byte calculation for maxBytesInMemory to take into account of Sink/Hydrant Object overhead\n\n* fix test\n\n* fix test\n\n* add log\n\n* Fix byte calculation for maxBytesInMemory to take into account of Sink/Hydrant Object overhead\n\n* address comments\n\n* fix checkstyle\n\n* fix checkstyle\n\n* add config to skip overhead memory calculation\n\n* add test for the skipBytesInMemoryOverheadCheck config\n\n* add docs\n\n* fix checkstyle\n\n* fix checkstyle\n\n* fix spelling\n\n* address comments\n\n* fix travis\n\n* address comments","date":"2021-01-27 16:34:56","modifiedFileCount":"50","status":"M","submitter":"Maytas Monsereenusorn"},{"authorTime":"2021-03-03 03:23:52","codes":[{"authorDate":"2021-03-03 03:23:52","commitOrder":18,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","date":"2021-03-03 03:23:52","endLine":643,"groupId":"14702","id":35,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b0/deee546550836611f13834cf7230e1115b793c.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":612,"status":"M"},{"authorDate":"2021-03-03 03:23:52","commitOrder":18,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final ParallelIndexTuningConfig tuningConfig = new ParallelIndexTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","date":"2021-03-03 03:23:52","endLine":785,"groupId":"14702","id":36,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b0/deee546550836611f13834cf7230e1115b793c.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final ParallelIndexTuningConfig tuningConfig = new ParallelIndexTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":717,"status":"M"}],"commitId":"b7b0ee83627dd7887392e8f9d6fb5cb29465c28c","commitMessage":"@@@Add query granularity to compaction task (#10900)\n\n* add query granularity to compaction task\n\n* fix checkstyle\n\n* fix checkstyle\n\n* fix test\n\n* fix test\n\n* add tests\n\n* fix test\n\n* fix test\n\n* cleanup\n\n* rename class\n\n* fix test\n\n* fix test\n\n* add test\n\n* fix test","date":"2021-03-03 03:23:52","modifiedFileCount":"15","status":"M","submitter":"Maytas Monsereenusorn"},{"authorTime":"2021-04-09 12:03:00","codes":[{"authorDate":"2021-03-03 03:23:52","commitOrder":19,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","date":"2021-03-03 03:23:52","endLine":643,"groupId":"14702","id":37,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b0/deee546550836611f13834cf7230e1115b793c.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":612,"status":"N"},{"authorDate":"2021-04-09 12:03:00","commitOrder":19,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final CompactionTask.CompactionTuningConfig tuningConfig = new CompactionTask.CompactionTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","date":"2021-04-09 12:03:00","endLine":1014,"groupId":"14702","id":38,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f9/f3a66cab3a866f1891788d2026f7aedad17f30.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final ParallelIndexTuningConfig tuningConfig = new ParallelIndexTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":945,"status":"M"}],"commitId":"8264203cee688607091232897749e959e7706010","commitMessage":"@@@Allow client to configure batch ingestion task to wait to complete until segments are confirmed to be available by other (#10676)\n\n* Add ability to wait for segment availability for batch jobs\n\n* IT updates\n\n* fix queries in legacy hadoop IT\n\n* Fix broken indexing integration tests\n\n* address an lgtm flag\n\n* spell checker still flagging for hadoop doc. adding under that file header too\n\n* fix compaction IT\n\n* Updates to wait for availability method\n\n* improve unit testing for patch\n\n* fix bad indentation\n\n* refactor waitForSegmentAvailability\n\n* Fixes based off of review comments\n\n* cleanup to get compile after merging with master\n\n* fix failing test after previous logic update\n\n* add back code that must have gotten deleted during conflict resolution\n\n* update some logging code\n\n* fixes to get compilation working after merge with master\n\n* reset interrupt flag in catch block after code review pointed it out\n\n* small changes following self-review\n\n* fixup some issues brought on by merge with master\n\n* small changes after review\n\n* cleanup a little bit after merge with master\n\n* Fix potential resource leak in AbstractBatchIndexTask\n\n* syntax fix\n\n* Add a Compcation TuningConfig type\n\n* add docs stipulating the lack of support by Compaction tasks for the new config\n\n* Fixup compilation errors after merge with master\n\n* Remove erreneous newline","date":"2021-04-09 12:03:00","modifiedFileCount":"106","status":"M","submitter":"Lucas Capistrant"},{"authorTime":"2021-04-09 15:12:28","codes":[{"authorDate":"2021-04-09 15:12:28","commitOrder":20,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n  }\n","date":"2021-04-09 15:12:28","endLine":873,"groupId":"14702","id":39,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c4/faa5b2754ed36e4f3b71a6caa2cd64ceba3565.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":840,"status":"M"},{"authorDate":"2021-04-09 15:12:28","commitOrder":20,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final CompactionTask.CompactionTuningConfig tuningConfig = new CompactionTask.CompactionTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH,\n        Granularities.NONE,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n  }\n","date":"2021-04-09 15:12:28","endLine":1021,"groupId":"14702","id":40,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c4/faa5b2754ed36e4f3b71a6caa2cd64ceba3565.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final CompactionTask.CompactionTuningConfig tuningConfig = new CompactionTask.CompactionTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH,\n        Granularities.NONE\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":950,"status":"M"}],"commitId":"4576152e4a0213d17048a330e7089aa9d89f3972","commitMessage":"@@@Make dropExisting flag for Compaction configurable and add warning documentations (#11070)\n\n* Make dropExisting flag for Compaction configurable\n\n* fix checkstyle\n\n* fix checkstyle\n\n* fix test\n\n* add tests\n\n* fix spelling\n\n* fix docs\n\n* add IT\n\n* fix test\n\n* fix doc\n\n* fix doc","date":"2021-04-09 15:12:28","modifiedFileCount":"20","status":"M","submitter":"Maytas Monsereenusorn"},{"authorTime":"2021-07-21 02:44:19","codes":[{"authorDate":"2021-07-21 02:44:19","commitOrder":21,"curCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentCacheManagerFactory,\n        RETRY_POLICY_FACTORY,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n  }\n","date":"2021-07-21 02:44:19","endLine":873,"groupId":"104501","id":41,"instanceNumber":1,"isCurCommit":0,"methodName":"testCreateIngestionSchema","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ac/9e3d011aef9e7655bf6426661366cead8b80e9.src","preCode":"  public void testCreateIngestionSchema() throws IOException, SegmentLoadingException\n  {\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(TUNING_CONFIG),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        Granularities.MONTH,\n        Granularities.NONE,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":840,"status":"M"},{"authorDate":"2021-07-21 02:44:19","commitOrder":21,"curCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final CompactionTask.CompactionTuningConfig tuningConfig = new CompactionTask.CompactionTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentCacheManagerFactory,\n        RETRY_POLICY_FACTORY,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH,\n        Granularities.NONE,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n  }\n","date":"2021-07-21 02:44:19","endLine":1021,"groupId":"104501","id":42,"instanceNumber":2,"isCurCommit":0,"methodName":"testCreateIngestionSchemaWithMaxTotalRows","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ac/9e3d011aef9e7655bf6426661366cead8b80e9.src","preCode":"  public void testCreateIngestionSchemaWithMaxTotalRows() throws IOException, SegmentLoadingException\n  {\n    final CompactionTask.CompactionTuningConfig tuningConfig = new CompactionTask.CompactionTuningConfig(\n        null,\n        null,\n        null,\n        500000,\n        1000000L,\n        null,\n        1000000L,\n        null,\n        null,\n        null,\n        new IndexSpec(\n            new RoaringBitmapSerdeFactory(true),\n            CompressionStrategy.LZ4,\n            CompressionStrategy.LZF,\n            LongEncodingStrategy.LONGS\n        ),\n        null,\n        null,\n        false,\n        false,\n        5000L,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null,\n        null\n    );\n    final List<ParallelIndexIngestionSpec> ingestionSpecs = CompactionTask.createIngestionSchema(\n        toolbox,\n        LockGranularity.TIME_CHUNK,\n        new SegmentProvider(DATA_SOURCE, new CompactionIntervalSpec(COMPACTION_INTERVAL, null)),\n        new PartitionConfigurationManager(tuningConfig),\n        null,\n        null,\n        null,\n        COORDINATOR_CLIENT,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n    final List<DimensionsSpec> expectedDimensionsSpec = getExpectedDimensionsSpecForAutoGeneration();\n\n    ingestionSpecs.sort(\n        (s1, s2) -> Comparators.intervalsByStartThenEnd().compare(\n            s1.getDataSchema().getGranularitySpec().inputIntervals().get(0),\n            s2.getDataSchema().getGranularitySpec().inputIntervals().get(0)\n        )\n    );\n    Assert.assertEquals(6, ingestionSpecs.size());\n    assertIngestionSchema(\n        ingestionSpecs,\n        expectedDimensionsSpec,\n        AGGREGATORS,\n        SEGMENT_INTERVALS,\n        tuningConfig,\n        Granularities.MONTH,\n        Granularities.NONE,\n        IOConfig.DEFAULT_DROP_EXISTING\n    );\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":950,"status":"M"}],"commitId":"94c1671eaf7b050972602fdedcb1971cdbde692d","commitMessage":"@@@Split SegmentLoader into SegmentLoader and SegmentCacheManager (#11466)\n\nThis PR splits current SegmentLoader into SegmentLoader and SegmentCacheManager.\n\nSegmentLoader - this class is responsible for building the segment object but does not expose any methods for downloading.  cache space management.  etc. Default implementation delegates the download operations to SegmentCacheManager and only contains the logic for building segments once downloaded. . This class will be used in SegmentManager to construct Segment objects.\n\nSegmentCacheManager - this class manages the segment cache on the local disk. It fetches the segment files to the local disk.  can clean up the cache.  and in the future.  support reserve and release on cache space. [See https://github.com/Make SegmentLoader extensible and customizable #11398]. This class will be used in ingestion tasks such as compaction.  re-indexing where segment files need to be downloaded locally.","date":"2021-07-21 02:44:19","modifiedFileCount":"41","status":"M","submitter":"Abhishek Agarwal"}]
