[{"authorTime":"2019-03-15 05:28:33","codes":[{"authorDate":"2019-11-23 02:49:16","commitOrder":4,"curCode":"  public void configure(Binder binder)\n  {\n    \r\n    \r\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).toInstance(conf);\n  }\n","date":"2019-11-23 02:49:16","endLine":102,"groupId":"15630","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"configure","params":"(Binderbinder)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/dd/2cf4c34f824f8078c29203af1c02999a1d3719.src","preCode":"  public void configure(Binder binder)\n  {\n    \r\n    \r\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).toInstance(conf);\n  }\n","realPath":"extensions-core/parquet-extensions/src/main/java/org/apache/druid/data/input/parquet/ParquetExtensionsModule.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":69,"status":"B"},{"authorDate":"2019-03-15 05:28:33","commitOrder":4,"curCode":"  public void configure(Binder binder)\n  {\n    MapBinder.newMapBinder(binder, String.class, SearchableVersionedDataFinder.class)\n             .addBinding(SCHEME)\n             .to(HdfsFileTimestampVersionFinder.class)\n             .in(LazySingleton.class);\n\n    Binders.dataSegmentPusherBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentPusher.class).in(LazySingleton.class);\n    Binders.dataSegmentKillerBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentKiller.class).in(LazySingleton.class);\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).toInstance(conf);\n    JsonConfigProvider.bind(binder, \"druid.storage\", HdfsDataSegmentPusherConfig.class);\n\n    Binders.taskLogsBinder(binder).addBinding(\"hdfs\").to(HdfsTaskLogs.class);\n    JsonConfigProvider.bind(binder, \"druid.indexer.logs\", HdfsTaskLogsConfig.class);\n    binder.bind(HdfsTaskLogs.class).in(LazySingleton.class);\n    JsonConfigProvider.bind(binder, \"druid.hadoop.security.kerberos\", HdfsKerberosConfig.class);\n    binder.bind(HdfsStorageAuthentication.class).in(ManageLifecycle.class);\n    LifecycleModule.register(binder, HdfsStorageAuthentication.class);\n\n  }\n","date":"2019-03-15 05:28:33","endLine":132,"groupId":"15630","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"configure","params":"(Binderbinder)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/95/cd12f7a819e80c3a8c3307d0f631377816e6d8.src","preCode":"  public void configure(Binder binder)\n  {\n    MapBinder.newMapBinder(binder, String.class, SearchableVersionedDataFinder.class)\n             .addBinding(SCHEME)\n             .to(HdfsFileTimestampVersionFinder.class)\n             .in(LazySingleton.class);\n\n    Binders.dataSegmentPusherBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentPusher.class).in(LazySingleton.class);\n    Binders.dataSegmentKillerBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentKiller.class).in(LazySingleton.class);\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).toInstance(conf);\n    JsonConfigProvider.bind(binder, \"druid.storage\", HdfsDataSegmentPusherConfig.class);\n\n    Binders.taskLogsBinder(binder).addBinding(\"hdfs\").to(HdfsTaskLogs.class);\n    JsonConfigProvider.bind(binder, \"druid.indexer.logs\", HdfsTaskLogsConfig.class);\n    binder.bind(HdfsTaskLogs.class).in(LazySingleton.class);\n    JsonConfigProvider.bind(binder, \"druid.hadoop.security.kerberos\", HdfsKerberosConfig.class);\n    binder.bind(HdfsStorageAuthentication.class).in(ManageLifecycle.class);\n    LifecycleModule.register(binder, HdfsStorageAuthentication.class);\n\n  }\n","realPath":"extensions-core/hdfs-storage/src/main/java/org/apache/druid/storage/hdfs/HdfsStorageDruidModule.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":85,"status":"NB"}],"commitId":"72500103886693609d1e05927491907f584b4229","commitMessage":"@@@add parquet support to native batch (#8883)\n\n* add parquet support to native batch\n\n* cleanup\n\n* implement toJson for sampler support\n\n* better binaryAsString test\n\n* docs\n\n* i hate spellcheck\n\n* refactor toMap conversion so can be shared through flattenerMaker.  default impls should be good enough for orc+avro.  fixup for merge with latest\n\n* add comment.  fix some stuff\n\n* adjustments\n\n* fix accident\n\n* tweaks\n","date":"2019-11-23 02:49:16","modifiedFileCount":"15","status":"M","submitter":"Clint Wylie"},{"authorTime":"2019-12-12 09:30:44","codes":[{"authorDate":"2019-12-12 09:30:44","commitOrder":5,"curCode":"  public void configure(Binder binder)\n  {\n    \r\n    \r\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).annotatedWith(Parquet.class).toInstance(conf);\n  }\n","date":"2019-12-12 09:30:44","endLine":103,"groupId":"15630","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"configure","params":"(Binderbinder)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/17/fe50d42870c7b9b240f290b2287bcaf38f6df0.src","preCode":"  public void configure(Binder binder)\n  {\n    \r\n    \r\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).toInstance(conf);\n  }\n","realPath":"extensions-core/parquet-extensions/src/main/java/org/apache/druid/data/input/parquet/ParquetExtensionsModule.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":70,"status":"M"},{"authorDate":"2019-12-12 09:30:44","commitOrder":5,"curCode":"  public void configure(Binder binder)\n  {\n    MapBinder.newMapBinder(binder, String.class, SearchableVersionedDataFinder.class)\n             .addBinding(SCHEME)\n             .to(HdfsFileTimestampVersionFinder.class)\n             .in(LazySingleton.class);\n\n    Binders.dataSegmentPusherBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentPusher.class).in(LazySingleton.class);\n    Binders.dataSegmentKillerBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentKiller.class).in(LazySingleton.class);\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).annotatedWith(Hdfs.class).toInstance(conf);\n    JsonConfigProvider.bind(binder, \"druid.storage\", HdfsDataSegmentPusherConfig.class);\n\n    Binders.taskLogsBinder(binder).addBinding(\"hdfs\").to(HdfsTaskLogs.class);\n    JsonConfigProvider.bind(binder, \"druid.indexer.logs\", HdfsTaskLogsConfig.class);\n    binder.bind(HdfsTaskLogs.class).in(LazySingleton.class);\n    JsonConfigProvider.bind(binder, \"druid.hadoop.security.kerberos\", HdfsKerberosConfig.class);\n    binder.bind(HdfsStorageAuthentication.class).in(ManageLifecycle.class);\n    LifecycleModule.register(binder, HdfsStorageAuthentication.class);\n  }\n","date":"2019-12-12 09:30:44","endLine":121,"groupId":"15630","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"configure","params":"(Binderbinder)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d4/242bcddcfc2345ec36f617231bea6134327d8b.src","preCode":"  public void configure(Binder binder)\n  {\n    MapBinder.newMapBinder(binder, String.class, SearchableVersionedDataFinder.class)\n             .addBinding(SCHEME)\n             .to(HdfsFileTimestampVersionFinder.class)\n             .in(LazySingleton.class);\n\n    Binders.dataSegmentPusherBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentPusher.class).in(LazySingleton.class);\n    Binders.dataSegmentKillerBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentKiller.class).in(LazySingleton.class);\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).toInstance(conf);\n    JsonConfigProvider.bind(binder, \"druid.storage\", HdfsDataSegmentPusherConfig.class);\n\n    Binders.taskLogsBinder(binder).addBinding(\"hdfs\").to(HdfsTaskLogs.class);\n    JsonConfigProvider.bind(binder, \"druid.indexer.logs\", HdfsTaskLogsConfig.class);\n    binder.bind(HdfsTaskLogs.class).in(LazySingleton.class);\n    JsonConfigProvider.bind(binder, \"druid.hadoop.security.kerberos\", HdfsKerberosConfig.class);\n    binder.bind(HdfsStorageAuthentication.class).in(ManageLifecycle.class);\n    LifecycleModule.register(binder, HdfsStorageAuthentication.class);\n\n  }\n","realPath":"extensions-core/hdfs-storage/src/main/java/org/apache/druid/storage/hdfs/HdfsStorageDruidModule.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":75,"status":"M"}],"commitId":"66056b282620e82fa464d79b0f76b1687b92530e","commitMessage":"@@@Using annotation to distinguish Hadoop Configuration in each module (#9013)\n\n* Multibinding for NodeRole\n\n* Fix endpoints\n\n* fix doc\n\n* fix test\n\n* Using annotation to distinguish Hadoop Configuration in each module\n","date":"2019-12-12 09:30:44","modifiedFileCount":"15","status":"M","submitter":"Jihoon Son"},{"authorTime":"2021-03-07 03:43:00","codes":[{"authorDate":"2019-12-12 09:30:44","commitOrder":6,"curCode":"  public void configure(Binder binder)\n  {\n    \r\n    \r\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).annotatedWith(Parquet.class).toInstance(conf);\n  }\n","date":"2019-12-12 09:30:44","endLine":103,"groupId":"103410","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"configure","params":"(Binderbinder)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/17/fe50d42870c7b9b240f290b2287bcaf38f6df0.src","preCode":"  public void configure(Binder binder)\n  {\n    \r\n    \r\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).annotatedWith(Parquet.class).toInstance(conf);\n  }\n","realPath":"extensions-core/parquet-extensions/src/main/java/org/apache/druid/data/input/parquet/ParquetExtensionsModule.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":70,"status":"N"},{"authorDate":"2021-03-07 03:43:00","commitOrder":6,"curCode":"  public void configure(Binder binder)\n  {\n    MapBinder.newMapBinder(binder, String.class, SearchableVersionedDataFinder.class)\n             .addBinding(SCHEME)\n             .to(HdfsFileTimestampVersionFinder.class)\n             .in(LazySingleton.class);\n\n    Binders.dataSegmentPusherBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentPusher.class).in(LazySingleton.class);\n    Binders.dataSegmentKillerBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentKiller.class).in(LazySingleton.class);\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).annotatedWith(Hdfs.class).toInstance(conf);\n    JsonConfigProvider.bind(binder, \"druid.storage\", HdfsDataSegmentPusherConfig.class);\n\n    Binders.taskLogsBinder(binder).addBinding(\"hdfs\").to(HdfsTaskLogs.class);\n    JsonConfigProvider.bind(binder, \"druid.indexer.logs\", HdfsTaskLogsConfig.class);\n    binder.bind(HdfsTaskLogs.class).in(LazySingleton.class);\n    JsonConfigProvider.bind(binder, \"druid.hadoop.security.kerberos\", HdfsKerberosConfig.class);\n    binder.bind(HdfsStorageAuthentication.class).in(ManageLifecycle.class);\n    LifecycleModule.register(binder, HdfsStorageAuthentication.class);\n\n    JsonConfigProvider.bind(binder, \"druid.ingestion.hdfs\", HdfsInputSourceConfig.class);\n  }\n","date":"2021-03-07 03:43:00","endLine":124,"groupId":"103410","id":6,"instanceNumber":2,"isCurCommit":1,"methodName":"configure","params":"(Binderbinder)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3c/a8e23535e1de5041a408c4bdfe489d103962af.src","preCode":"  public void configure(Binder binder)\n  {\n    MapBinder.newMapBinder(binder, String.class, SearchableVersionedDataFinder.class)\n             .addBinding(SCHEME)\n             .to(HdfsFileTimestampVersionFinder.class)\n             .in(LazySingleton.class);\n\n    Binders.dataSegmentPusherBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentPusher.class).in(LazySingleton.class);\n    Binders.dataSegmentKillerBinder(binder).addBinding(SCHEME).to(HdfsDataSegmentKiller.class).in(LazySingleton.class);\n\n    final Configuration conf = new Configuration();\n\n    \r\n    conf.setClassLoader(getClass().getClassLoader());\n\n    \r\n    \r\n    ClassLoader currCtxCl = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(getClass().getClassLoader());\n      FileSystem.get(conf);\n    }\n    catch (IOException ex) {\n      throw new RuntimeException(ex);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(currCtxCl);\n    }\n\n    if (props != null) {\n      for (String propName : props.stringPropertyNames()) {\n        if (propName.startsWith(\"hadoop.\")) {\n          conf.set(propName.substring(\"hadoop.\".length()), props.getProperty(propName));\n        }\n      }\n    }\n\n    binder.bind(Configuration.class).annotatedWith(Hdfs.class).toInstance(conf);\n    JsonConfigProvider.bind(binder, \"druid.storage\", HdfsDataSegmentPusherConfig.class);\n\n    Binders.taskLogsBinder(binder).addBinding(\"hdfs\").to(HdfsTaskLogs.class);\n    JsonConfigProvider.bind(binder, \"druid.indexer.logs\", HdfsTaskLogsConfig.class);\n    binder.bind(HdfsTaskLogs.class).in(LazySingleton.class);\n    JsonConfigProvider.bind(binder, \"druid.hadoop.security.kerberos\", HdfsKerberosConfig.class);\n    binder.bind(HdfsStorageAuthentication.class).in(ManageLifecycle.class);\n    LifecycleModule.register(binder, HdfsStorageAuthentication.class);\n  }\n","realPath":"extensions-core/hdfs-storage/src/main/java/org/apache/druid/storage/hdfs/HdfsStorageDruidModule.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":76,"status":"M"}],"commitId":"9946306d4b2c16a7fc8bac97c9f4815ed4b46570","commitMessage":"@@@Add configurations for allowed protocols for HTTP and HDFS inputSources/firehoses (#10830)\n\n* Allow only HTTP and HTTPS protocols for the HTTP inputSource\n\n* rename\n\n* Update core/src/main/java/org/apache/druid/data/input/impl/HttpInputSource.java\n\nCo-authored-by: Abhishek Agarwal <1477457+abhishekagarwal87@users.noreply.github.com>\n\n* fix http firehose and update doc\n\n* HDFS inputSource\n\n* add configs for allowed protocols\n\n* fix checkstyle and doc\n\n* more checkstyle\n\n* remove stale doc\n\n* remove more doc\n\n* Apply doc suggestions from code review\n\nCo-authored-by: Charles Smith <38529548+techdocsmith@users.noreply.github.com>\n\n* update hdfs address in docs\n\n* fix test\n\nCo-authored-by: Abhishek Agarwal <1477457+abhishekagarwal87@users.noreply.github.com>\nCo-authored-by: Charles Smith <38529548+techdocsmith@users.noreply.github.com>","date":"2021-03-07 03:43:00","modifiedFileCount":"11","status":"M","submitter":"Jihoon Son"}]
