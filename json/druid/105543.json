[{"authorTime":"2018-10-15 11:37:37","codes":[{"authorDate":"2019-03-15 15:22:43","commitOrder":2,"curCode":"  public boolean equals(Object o)\n  {\n    if (this == o) {\n      return true;\n    }\n    if (o == null || getClass() != o.getClass()) {\n      return false;\n    }\n    OrderedSequenceNumber<?> that = (OrderedSequenceNumber<?>) o;\n    return isExclusive == that.isExclusive &&\n           Objects.equals(sequenceNumber, that.sequenceNumber);\n  }\n","date":"2019-03-15 15:22:42","endLine":68,"groupId":"1232","id":1,"instanceNumber":1,"isCurCommit":1,"methodName":"equals","params":"(Objecto)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/74/fd08d445ec65de946ac63239f543ca792b7f5d.src","preCode":"  public boolean equals(Object o)\n  {\n    if (this == o) {\n      return true;\n    }\n    if (o == null || getClass() != o.getClass()) {\n      return false;\n    }\n    OrderedSequenceNumber<?> that = (OrderedSequenceNumber<?>) o;\n    return isExclusive == that.isExclusive &&\n           Objects.equals(sequenceNumber, that.sequenceNumber);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/seekablestream/common/OrderedSequenceNumber.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":57,"status":"B"},{"authorDate":"2018-10-15 11:37:37","commitOrder":2,"curCode":"  public boolean equals(Object o)\n  {\n    if (this == o) {\n      return true;\n    }\n    if (o == null || getClass() != o.getClass()) {\n      return false;\n    }\n\n    PartitionHolder that = (PartitionHolder) o;\n\n    if (!holderSet.equals(that.holderSet)) {\n      return false;\n    }\n\n    return true;\n  }\n","date":"2018-10-15 11:37:37","endLine":162,"groupId":"20693","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"equals","params":"(Objecto)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5e/5f676e42f3fc368e5a9db734957e5851379437.src","preCode":"  public boolean equals(Object o)\n  {\n    if (this == o) {\n      return true;\n    }\n    if (o == null || getClass() != o.getClass()) {\n      return false;\n    }\n\n    PartitionHolder that = (PartitionHolder) o;\n\n    if (!holderSet.equals(that.holderSet)) {\n      return false;\n    }\n\n    return true;\n  }\n","realPath":"core/src/main/java/org/apache/druid/timeline/partition/PartitionHolder.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":146,"status":"NB"}],"commitId":"a8c7132482358e9d8113632b38e284a84b66c6db","commitMessage":"@@@Logic adjustments to SeekableStreamIndexTaskRunner. (#7267)\n\n* Logic adjustments to SeekableStreamIndexTaskRunner.\n\nA mix of simplifications and bug fixes. They are intermingled because\nsome of the bugs were made difficult to fix.  and also more likely to\nhappen in the first place.  by how the code was structured. I tried to\nkeep restructuring to a minimum. The changes are:\n\n- Remove \"initialOffsetsSnapshot\".  which was used to determine when to\n  skip start offsets. Replace it with \"lastReadOffsets\".  which I hope\n  is more intuitive. (There is a connection: start offsets must be\n  skipped if and only if they have already been read.  either by a\n  previous task or by a previous sequence in the same task.  post-restoring.)\n- Remove \"isStartingSequenceOffsetsExclusive\".  because it should always\n  be the opposite of isEndOffsetExclusive. The reason is that starts are\n  exclusive exactly when the prior ends are inclusive: they must match\n  up in that way for adjacent reads to link up properly.\n- Don't call \"seekToStartingSequence\" after the initial seek. There is\n  no reason to.  since we expect to read continuous message streams\n  throughout the task. And calling it makes offset-tracking logic\n  trickier.  so better to avoid the need for trickiness. I believe the\n  call being here was causing a bug in Kinesis ingestion where a\n  message might get double-read.\n- Remove the \"continue\" calls in the main read loop. They are bad\n  because they prevent keeping currOffsets and lastReadOffsets up to\n  date.  and prevent us from detecting that we have finished reading.\n- Rework \"verifyInitialRecordAndSkipExclusivePartition\" into\n  \"verifyRecordInRange\". It no longer has side effects. It does a sanity\n  check on the message offset and also makes sure that it is not past\n  the endOffsets.\n- Rework \"assignPartitions\" to replace inline comparisons with\n  \"isRecordAlreadyRead\" and \"isMoreToReadBeforeReadingRecord\" calls. I\n  believe this fixes an off-by-one error with Kinesis where the last\n  record would not get read. It also makes the logic easier to read.\n- When doing the final publish.  only adjust end offsets of the final\n  sequence.  rather than potentially adjusting any unpublished sequence.\n  Adjusting sequences other than the last one is a mistake since it\n  will extend their endOffsets beyond what they actually read. (I'm not\n  sure if this was an issue in practice.  since I'm not sure if real\n  world situations would have more than one unpublished sequence.)\n- Rename \"isEndSequenceOffsetsExclusive\" to \"isEndOffsetExclusive\". It's\n  shorter and more clear.  I think.\n- Add equals/hashCode/toString methods to OrderedSequenceNumber.\n\nKafka test changes:\n\n- Added a Kafka \"testRestoreAtEndOffset\" test to verify that restores at\n  the very end of the task lifecycle still work properly.\n\nKinesis test changes:\n\n- Renamed \"testRunOnNothing\" to \"testRunOnSingletonRange\". I think that\n  given Kinesis semantics.  the right behavior when start offset equals\n  end offset (and there aren't exclusive partitions set) is to read that\n  single offset. This is because they are both meant to be treated as\n  inclusive.\n- Adjusted \"testRestoreAfterPersistingSequences\" to expect one more\n  message read. I believe the old test was wrong; it expected the task\n  not to read message number 5.\n- Adjusted \"testRunContextSequenceAheadOfStartingOffsets\" to use a\n  checkpoint starting from 1 rather than 2. I believe the old test was\n  wrong here too; it was expecting the task to start reading from the\n  checkpointed offset.  but it actually should have started reading from\n  one past the checkpointed offset.\n- Adjusted \"testIncrementalHandOffReadsThroughEndOffsets\" to expect\n  11 messages read instead of 12. It's starting at message 0 and reading\n  up to 10.  which should be 11 messages.\n\n* Changes from code review.\n","date":"2019-03-15 15:22:42","modifiedFileCount":"8","status":"M","submitter":"Gian Merlino"},{"authorTime":"2019-07-17 22:18:48","codes":[{"authorDate":"2019-03-15 15:22:43","commitOrder":3,"curCode":"  public boolean equals(Object o)\n  {\n    if (this == o) {\n      return true;\n    }\n    if (o == null || getClass() != o.getClass()) {\n      return false;\n    }\n    OrderedSequenceNumber<?> that = (OrderedSequenceNumber<?>) o;\n    return isExclusive == that.isExclusive &&\n           Objects.equals(sequenceNumber, that.sequenceNumber);\n  }\n","date":"2019-03-15 15:22:42","endLine":68,"groupId":"1232","id":3,"instanceNumber":1,"isCurCommit":1,"methodName":"equals","params":"(Objecto)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/74/fd08d445ec65de946ac63239f543ca792b7f5d.src","preCode":"  public boolean equals(Object o)\n  {\n    if (this == o) {\n      return true;\n    }\n    if (o == null || getClass() != o.getClass()) {\n      return false;\n    }\n    OrderedSequenceNumber<?> that = (OrderedSequenceNumber<?>) o;\n    return isExclusive == that.isExclusive &&\n           Objects.equals(sequenceNumber, that.sequenceNumber);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/seekablestream/common/OrderedSequenceNumber.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":57,"status":"N"},{"authorDate":"2019-07-17 22:18:48","commitOrder":3,"curCode":"  public boolean equals(Object o)\n  {\n    if (this == o) {\n      return true;\n    }\n    if (o == null || getClass() != o.getClass()) {\n      return false;\n    }\n\n    PartitionHolder that = (PartitionHolder) o;\n\n    if (!holderMap.equals(that.holderMap)) {\n      return false;\n    }\n\n    return true;\n  }\n","date":"2019-07-17 22:18:48","endLine":151,"groupId":"9190","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"equals","params":"(Objecto)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/dc/f29aedc488fdc4dbeec5214440c261a436f54e.src","preCode":"  public boolean equals(Object o)\n  {\n    if (this == o) {\n      return true;\n    }\n    if (o == null || getClass() != o.getClass()) {\n      return false;\n    }\n\n    PartitionHolder that = (PartitionHolder) o;\n\n    if (!holderSet.equals(that.holderSet)) {\n      return false;\n    }\n\n    return true;\n  }\n","realPath":"core/src/main/java/org/apache/druid/timeline/partition/PartitionHolder.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":135,"status":"M"}],"commitId":"ceb969903f06a72d669bb55ab83223bddb5355ee","commitMessage":"@@@Refactor SQLMetadataSegmentManager; Change contract of REST met? (#7653)\n\n* Refactor SQLMetadataSegmentManager; Change contract of REST methods in DataSourcesResource\n\n* Style fixes\n\n* Unused imports\n\n* Fix tests\n\n* Fix style\n\n* Comments\n\n* Comment fix\n\n* Remove unresolvable Javadoc references; address comments\n\n* Add comments to ImmutableDruidDataSource\n\n* Merge with master\n\n* Fix bad web-console merge\n\n* Fixes in api-reference.md\n\n* Rename in DruidCoordinatorRuntimeParams\n\n* Fix compilation\n\n* Residual changes\n","date":"2019-07-17 22:18:48","modifiedFileCount":"77","status":"M","submitter":"Roman Leventov"},{"authorTime":"2019-07-25 08:35:46","codes":[{"authorDate":"2019-03-15 15:22:43","commitOrder":4,"curCode":"  public boolean equals(Object o)\n  {\n    if (this == o) {\n      return true;\n    }\n    if (o == null || getClass() != o.getClass()) {\n      return false;\n    }\n    OrderedSequenceNumber<?> that = (OrderedSequenceNumber<?>) o;\n    return isExclusive == that.isExclusive &&\n           Objects.equals(sequenceNumber, that.sequenceNumber);\n  }\n","date":"2019-03-15 15:22:42","endLine":68,"groupId":"105543","id":5,"instanceNumber":1,"isCurCommit":1,"methodName":"equals","params":"(Objecto)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/74/fd08d445ec65de946ac63239f543ca792b7f5d.src","preCode":"  public boolean equals(Object o)\n  {\n    if (this == o) {\n      return true;\n    }\n    if (o == null || getClass() != o.getClass()) {\n      return false;\n    }\n    OrderedSequenceNumber<?> that = (OrderedSequenceNumber<?>) o;\n    return isExclusive == that.isExclusive &&\n           Objects.equals(sequenceNumber, that.sequenceNumber);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/seekablestream/common/OrderedSequenceNumber.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":57,"status":"N"},{"authorDate":"2019-07-25 08:35:46","commitOrder":4,"curCode":"  public boolean equals(Object o)\n  {\n    if (this == o) {\n      return true;\n    }\n    if (o == null || getClass() != o.getClass()) {\n      return false;\n    }\n    PartitionHolder<?> that = (PartitionHolder<?>) o;\n    return Objects.equals(overshadowableManager, that.overshadowableManager);\n  }\n","date":"2019-07-25 08:35:46","endLine":151,"groupId":"105543","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"equals","params":"(Objecto)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/26/e34dacf76dedb32f7e87a6a931460c155cbcd2.src","preCode":"  public boolean equals(Object o)\n  {\n    if (this == o) {\n      return true;\n    }\n    if (o == null || getClass() != o.getClass()) {\n      return false;\n    }\n\n    PartitionHolder that = (PartitionHolder) o;\n\n    if (!holderMap.equals(that.holderMap)) {\n      return false;\n    }\n\n    return true;\n  }\n","realPath":"core/src/main/java/org/apache/druid/timeline/partition/PartitionHolder.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":141,"status":"M"}],"commitId":"db149462073d59e7563f0d3834e69d44a2bb4011","commitMessage":"@@@Add support minor compaction with segment locking (#7547)\n\n* Segment locking\n\n* Allow both timeChunk and segment lock in the same gruop\n\n* fix it test\n\n* Fix adding same chunk to atomicUpdateGroup\n\n* resolving todos\n\n* Fix segments to lock\n\n* fix segments to lock\n\n* fix kill task\n\n* resolving todos\n\n* resolving todos\n\n* fix teamcity\n\n* remove unused class\n\n* fix single map\n\n* resolving todos\n\n* fix build\n\n* fix SQLMetadataSegmentManager\n\n* fix findInputSegments\n\n* adding more tests\n\n* fixing task lock checks\n\n* add SegmentTransactionalOverwriteAction\n\n* changing publisher\n\n* fixing something\n\n* fix for perfect rollup\n\n* fix test\n\n* adjust package-lock.json\n\n* fix test\n\n* fix style\n\n* adding javadocs\n\n* remove unused classes\n\n* add more javadocs\n\n* unused import\n\n* fix test\n\n* fix test\n\n* Support forceTimeChunk context and force timeChunk lock for parallel index task if intervals are missing\n\n* fix travis\n\n* fix travis\n\n* unused import\n\n* spotbug\n\n* revert getMaxVersion\n\n* address comments\n\n* fix tc\n\n* add missing error handling\n\n* fix backward compatibility\n\n* unused import\n\n* Fix perf of versionedIntervalTimeline\n\n* fix timeline\n\n* fix tc\n\n* remove remaining todos\n\n* add comment for parallel index\n\n* fix javadoc and typos\n\n* typo\n\n* address comments\n","date":"2019-07-25 08:35:46","modifiedFileCount":"130","status":"M","submitter":"Jihoon Son"}]
