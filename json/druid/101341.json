[{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = Maps.newTreeMap();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","date":"2018-08-31 00:56:26","endLine":263,"groupId":"16663","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ec/1039dde39d149f472d5b81d34fe7b69ab06523.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = Maps.newTreeMap();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":117,"status":"B"},{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate(config);\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","date":"2018-08-31 00:56:26","endLine":348,"groupId":"11045","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"ensurePaths","params":"(HadoopDruidIndexerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/25/f695a1caddd808c3e082a55ff1c057eeaff5d1.src","preCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate(config);\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/JobHelper.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":329,"status":"B"}],"commitId":"431d3d8497f9079857c3baa7ae7ab4cb44a22355","commitMessage":"@@@Rename io.druid to org.apache.druid. (#6266)\n\n* Rename io.druid to org.apache.druid.\n\n* Fix META-INF files and remove some benchmark results.\n\n* MonitorsConfig update for metrics package migration.\n\n* Reorder some dimensions in inner queries for some reason.\n\n* Fix protobuf tests.\n","date":"2018-08-31 00:56:26","modifiedFileCount":"5","status":"B","submitter":"Gian Merlino"},{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-10-29 20:02:43","commitOrder":2,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","date":"2018-10-29 20:02:43","endLine":265,"groupId":"16663","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/27/cc9c3c2159e9b9c86560c9ccb6cd61debf6f83.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = Maps.newTreeMap();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":119,"status":"M"},{"authorDate":"2018-08-31 00:56:26","commitOrder":2,"curCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate(config);\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","date":"2018-08-31 00:56:26","endLine":348,"groupId":"11045","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"ensurePaths","params":"(HadoopDruidIndexerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/25/f695a1caddd808c3e082a55ff1c057eeaff5d1.src","preCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate(config);\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/JobHelper.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":329,"status":"N"}],"commitId":"676f5e6d7f184101b8763e4249b18b237bbe0ec7","commitMessage":"@@@Prohibit some guava collection APIs and use JDK collection APIs directly (#6511)\n\n* Prohibit some guava collection APIs and use JDK APIs directly\n\n* reset files that changed by accident\n\n* sort codestyle/druid-forbidden-apis.txt alphabetically\n","date":"2018-10-29 20:02:43","modifiedFileCount":"427","status":"M","submitter":"QiuMM"},{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2019-01-26 07:43:06","commitOrder":3,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","date":"2019-01-26 07:43:06","endLine":277,"groupId":"10997","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d8/c8ae2c5d94e5772a22c203b7af829b75a6b315.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":119,"status":"M"},{"authorDate":"2018-08-31 00:56:26","commitOrder":3,"curCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate(config);\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","date":"2018-08-31 00:56:26","endLine":348,"groupId":"11045","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"ensurePaths","params":"(HadoopDruidIndexerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/25/f695a1caddd808c3e082a55ff1c057eeaff5d1.src","preCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate(config);\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/JobHelper.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":329,"status":"N"}],"commitId":"8492d94f599da1f7851add2a0e7500515abd881d","commitMessage":"@@@Kill Hadoop MR task on kill of Hadoop ingestion task  (#6828)\n\n* KillTask from overlord UI now makes sure that it terminates the underlying MR job.  thus saving unnecessary compute\n\nRun in jobby is now split into 2\n 1. submitAndGetHadoopJobId followed by 2. run\n  submitAndGetHadoopJobId is responsible for submitting the job and returning the jobId as a string.  run monitors this job for completion\n\nJobHelper writes this jobId in the path provided by HadoopIndexTask which in turn is provided by the ForkingTaskRunner\n\nHadoopIndexTask reads this path when kill task is clicked to get hte jobId and fire the kill command via the yarn api. This is taken care in the stopGracefully method which is called in SingleTaskBackgroundRunner. Have enabled `canRestore` method to return `true` for HadoopIndexTask in order for the stopGracefully method to be called\n\nHadoop*Job files have been changed to incorporate the changes to jobby\n\n* Addressing PR comments\n\n* Addressing PR comments - Fix taskDir\n\n* Addressing PR comments - For changing the contract of Task.stopGracefully()\n`SingleTaskBackgroundRunner` calls stopGracefully in stop() and then checks for canRestore condition to return the status of the task\n\n* Addressing PR comments\n 1. Formatting\n 2. Removing `submitAndGetHadoopJobId` from `Jobby` and calling writeJobIdToFile in the job itself\n\n* Addressing PR comments\n 1. POM change. Moving hadoop dependency to indexing-hadoop\n\n* Addressing PR comments\n 1. stopGracefully now accepts TaskConfig as a param\n     Handling isRestoreOnRestart in stopGracefully for `AppenderatorDriverRealtimeIndexTask.  RealtimeIndexTask.  SeekableStreamIndexTask`\n     Changing tests to make TaskConfig param isRestoreOnRestart to true\n","date":"2019-01-26 07:43:06","modifiedFileCount":"20","status":"M","submitter":"Ankit Kothari"},{"authorTime":"2019-03-15 05:28:33","codes":[{"authorDate":"2019-03-15 05:28:33","commitOrder":4,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2019-03-15 05:28:33","endLine":276,"groupId":"10997","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/7d/ad5c6bd840d81bc8ca42df42d8a0185821c6a2.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":118,"status":"M"},{"authorDate":"2019-03-15 05:28:33","commitOrder":4,"curCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate(config);\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2019-03-15 05:28:33","endLine":348,"groupId":"11045","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"ensurePaths","params":"(HadoopDruidIndexerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/19/81099f1af6ff886d7dadd4b69297b27010d7fe.src","preCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate(config);\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/JobHelper.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":329,"status":"M"}],"commitId":"7ada1c49f9735a37808f3ed7656d93ae88b8b925","commitMessage":"@@@Prohibit Throwables.propagate() (#7121)\n\n* Throw caught exception.\n\n* Throw caught exceptions.\n\n* Related checkstyle rule is added to prevent further bugs.\n\n* RuntimeException() is used instead of Throwables.propagate().\n\n* Missing import is added.\n\n* Throwables are propogated if possible.\n\n* Throwables are propogated if possible.\n\n* Throwables are propogated if possible.\n\n* Throwables are propogated if possible.\n\n* * Checkstyle definition is improved.\n* Throwables.propagate() usages are removed.\n\n* Checkstyle pattern is changed for only scanning \"Throwables.propagate(\" instead of checking lookbehind.\n\n* Throwable is kept before firing a Runtime Exception.\n\n* Fix unused assignments.\n","date":"2019-03-15 05:28:33","modifiedFileCount":"228","status":"M","submitter":"Furkan KAMACI"},{"authorTime":"2019-03-15 05:28:33","codes":[{"authorDate":"2019-05-17 04:59:10","commitOrder":5,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2019-05-17 04:59:10","endLine":289,"groupId":"4312","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/11/5a926c2d22fc51da0b1e7c18aeeaa859526eb8.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":117,"status":"M"},{"authorDate":"2019-03-15 05:28:33","commitOrder":5,"curCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate(config);\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2019-03-15 05:28:33","endLine":348,"groupId":"11045","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"ensurePaths","params":"(HadoopDruidIndexerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/19/81099f1af6ff886d7dadd4b69297b27010d7fe.src","preCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate(config);\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/JobHelper.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":329,"status":"N"}],"commitId":"d99f77a01b5f4e0abde0ec85c1a1039de09bbb78","commitMessage":"@@@Add option to use YARN RM as fallback for JobHistory failure (#7673)\n\n* Add option to use YARN RM as fallback for job status\n\n* PR comments\n","date":"2019-05-17 04:59:10","modifiedFileCount":"15","status":"M","submitter":"Jonathan Wei"},{"authorTime":"2019-03-15 05:28:33","codes":[{"authorDate":"2019-07-31 08:24:39","commitOrder":6,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2019-07-31 08:24:39","endLine":289,"groupId":"17160","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/98/4d104a80c0f7b4c5cf13d1ac9a011d67ebb727.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":114,"status":"M"},{"authorDate":"2019-03-15 05:28:33","commitOrder":6,"curCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate(config);\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2019-03-15 05:28:33","endLine":348,"groupId":"11045","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"ensurePaths","params":"(HadoopDruidIndexerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/19/81099f1af6ff886d7dadd4b69297b27010d7fe.src","preCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate(config);\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/JobHelper.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":329,"status":"N"}],"commitId":"385f492a555add279a8c6dd368954fde18c41dcb","commitMessage":"@@@Use PartitionsSpec for all task types (#8141)\n\n* Use partitionsSpec for all task types\n\n* fix doc\n\n* fix typos and revert to use isPushRequired\n\n* address comments\n\n* move partitionsSpec to core\n\n* remove hadoopPartitionsSpec\n","date":"2019-07-31 08:24:39","modifiedFileCount":"29","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-03-15 05:28:33","codes":[{"authorDate":"2019-09-21 04:59:18","commitOrder":7,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2019-09-21 04:59:18","endLine":289,"groupId":"17160","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/59/245a1f5a234fb64eca1cfad90221b9e18f3eb0.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":114,"status":"M"},{"authorDate":"2019-03-15 05:28:33","commitOrder":7,"curCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate(config);\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2019-03-15 05:28:33","endLine":348,"groupId":"11045","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"ensurePaths","params":"(HadoopDruidIndexerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/19/81099f1af6ff886d7dadd4b69297b27010d7fe.src","preCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate(config);\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/JobHelper.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":329,"status":"N"}],"commitId":"99b6eedab59fb15664567cb9a7a40afe6ed93a8a","commitMessage":"@@@Rename partition spec fields (#8507)\n\n* Rename partition spec fields\n\nRename partition spec fields to be consistent across the various types\n(hashed.  single_dim.  dynamic). Specifically.  use targetNumRowsPerSegment\nand maxRowsPerSegment in favor of targetPartitionSize and\nmaxSegmentSize. Consistent and clearer names are easier for users to\nunderstand and use.\n\nAlso fix various IntelliJ inspection warnings and doc spelling mistakes.\n\n* Fix test\n\n* Improve docs\n\n* Add targetRowsPerSegment to HashedPartitionsSpec\n","date":"2019-09-21 04:59:18","modifiedFileCount":"9","status":"M","submitter":"Chi Cao Minh"},{"authorTime":"2020-03-24 14:09:21","codes":[{"authorDate":"2020-03-24 14:09:21","commitOrder":8,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob.getConfiguration(), config);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob.getConfiguration(), config);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2020-03-24 14:09:21","endLine":289,"groupId":"10193","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d9/e6a14ce56a9c9ee5db43cd3764f0397d28eeff.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":114,"status":"M"},{"authorDate":"2020-03-24 14:09:21","commitOrder":8,"curCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate(config);\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job.getConfiguration(), config);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2020-03-24 14:09:21","endLine":370,"groupId":"2558","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"ensurePaths","params":"(HadoopDruidIndexerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/7a/f8983b77e03b22b50fb8c1181f41cc068a0a50.src","preCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate(config);\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/JobHelper.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":351,"status":"M"}],"commitId":"e97695d9da40303c96da9e81cf019f071be36a6f","commitMessage":"@@@fix Hadoop ingestion fails due to error 'JavaScript is disabled' on certain config (#9553)\n\n* fix Hadoop ingestion fails due to error 'JavaScript is disabled'.  if determine partition hadoop job is run\n\n* add test\n\n* fix checkstyle\n\n* address comments\n\n* address comments","date":"2020-03-24 14:09:21","modifiedFileCount":"6","status":"M","submitter":"Maytas Monsereenusorn"},{"authorTime":"2020-11-17 01:59:14","codes":[{"authorDate":"2020-03-24 14:09:21","commitOrder":9,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob.getConfiguration(), config);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob.getConfiguration(), config);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2020-03-24 14:09:21","endLine":289,"groupId":"10193","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d9/e6a14ce56a9c9ee5db43cd3764f0397d28eeff.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob.getConfiguration(), config);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob.getConfiguration(), config);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":114,"status":"N"},{"authorDate":"2020-11-17 01:59:14","commitOrder":9,"curCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate();\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job.getConfiguration(), config);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2020-11-17 01:59:14","endLine":367,"groupId":"2558","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"ensurePaths","params":"(HadoopDruidIndexerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c0/47c2b34e45a72253c9d7350a681b609707077a.src","preCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate(config);\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job.getConfiguration(), config);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/JobHelper.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":348,"status":"M"}],"commitId":"3447934a75f970b5853d7c217f3c861f81316acf","commitMessage":"@@@Ensure Krb auth before killing YARN apps in graceful shutdown (#9785)\n\n","date":"2020-11-17 01:59:14","modifiedFileCount":"2","status":"M","submitter":"Lucas Capistrant"},{"authorTime":"2020-11-17 01:59:14","codes":[{"authorDate":"2021-01-29 22:02:10","commitOrder":10,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob.getConfiguration(), config);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob.getConfiguration(), config);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(Iterators.size(config.getGranularitySpec().sortedBucketIntervals().iterator()));\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2021-01-29 22:02:10","endLine":289,"groupId":"101341","id":19,"instanceNumber":1,"isCurCommit":1,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/eb/e69cd15a7280dce7206c6e4f6c015f83422ac8.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob.getConfiguration(), config);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob.getConfiguration(), config);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":114,"status":"M"},{"authorDate":"2020-11-17 01:59:14","commitOrder":10,"curCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate();\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job.getConfiguration(), config);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2020-11-17 01:59:14","endLine":367,"groupId":"101341","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"ensurePaths","params":"(HadoopDruidIndexerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c0/47c2b34e45a72253c9d7350a681b609707077a.src","preCode":"  public static void ensurePaths(HadoopDruidIndexerConfig config)\n  {\n    authenticate();\n    \r\n    try {\n      Job job = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      job.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n      injectSystemProperties(job.getConfiguration(), config);\n      config.addJobProperties(job);\n\n      config.addInputPaths(job);\n    }\n    catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/JobHelper.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":348,"status":"N"}],"commitId":"0e4750bac208d99f4a07545cc2f401f9bcdc1381","commitMessage":"@@@Granularity interval materialization (#10742)\n\n* Prevent interval materialization for UniformGranularitySpec inside the overlord\n\n* Change API of bucketIntervals in GranularitySpec to return an Iterable<Interval>\n\n* Javadoc update.  respect inputIntervals contract\n\n* Eliminate dependency on wrappedspec (i.e. ArbitraryGranularity) in UniformGranularitySpec\n\n* Added one boundary condition test to UniformGranularityTest and fixed Travis forbidden method errors in IntervalsByGranularity\n\n* Fix Travis style & other checks\n\n* Refactor TreeSet to facilitate re-use in UniformGranularitySpec\n\n* Make sure intervals are unique when there is no segment granularity\n\n* Style/bugspot fixes...\n\n* More travis checks\n\n* Add condensedIntervals method to GranularitySpec and pass it as needed to the lock method\n\n* Style & PR feedback\n\n* Fixed failing test\n\n* Fixed bug in IntervalsByGranularity iterator that it would return repeated elements (see added unit tests that were broken before this change)\n\n* Refactor so that we can get the condensed buckets without materializing the intervals\n\n* Get rid of GranularitySpec::condensedInputIntervals ... not needed\n\n* Travis failures fixes\n\n* Travis checkstyle fix\n\n* Edited/added javadoc comments and a method name (code review feedback)\n\n* Fixed jacoco coverage by moving class and adding more coverage\n\n* Avoid materializing the condensed intervals when locking\n\n* Deal with overlapping intervals\n\n* Remove code and use library code instead\n\n* Refactor intervals by granularity using the FluentIterable.  add sanity checks\n\n* Change !hasNext() to inputIntervals().isEmpty()\n\n* Remove redundant lambda\n\n* Use materialized intervals here since this is outside the overlord (for performance)\n\n* Name refactor to reflect the fact that bucket intervals are sorted.\n\n* Style fixes\n\n* Removed redundant method and have condensedIntervalIterator throw IAE when element is null for consistency with other methods in this class (as well that null interval when condensing does not make sense)\n\n* Remove forbidden api\n\n* Move helper class inside common base class to reduce public space pollution","date":"2021-01-29 22:02:10","modifiedFileCount":"22","status":"M","submitter":"Agustin Gonzalez"}]
