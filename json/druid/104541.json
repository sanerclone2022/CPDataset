[{"authorTime":"2019-12-06 08:50:00","codes":[{"authorDate":"2019-10-19 04:24:14","commitOrder":7,"curCode":"  public void testRunCompactionTwice() throws Exception\n  {\n    runIndexTask();\n\n    final Builder builder = new Builder(\n        DATA_SOURCE,\n        getObjectMapper(),\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        rowIngestionMetersFactory,\n        indexingServiceClient,\n        coordinatorClient,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY,\n        appenderatorsManager\n    );\n\n    final CompactionTask compactionTask1 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    Pair<TaskStatus, List<DataSegment>> resultPair = runTask(compactionTask1);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(PartitionIds.NON_ROOT_GEN_START_PARTITION_ID, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 0), segments.get(i).getShardSpec());\n      }\n    }\n\n    final CompactionTask compactionTask2 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    resultPair = runTask(compactionTask2);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(\n                PartitionIds.NON_ROOT_GEN_START_PARTITION_ID + 1,\n                0,\n                2,\n                (short) 2,\n                (short) 1\n            ),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 0), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","date":"2019-10-19 04:24:14","endLine":309,"groupId":"1002","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunCompactionTwice","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/9a/7fa51bad8e203d62831359efcd7174c29a5d07.src","preCode":"  public void testRunCompactionTwice() throws Exception\n  {\n    runIndexTask();\n\n    final Builder builder = new Builder(\n        DATA_SOURCE,\n        getObjectMapper(),\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        rowIngestionMetersFactory,\n        indexingServiceClient,\n        coordinatorClient,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY,\n        appenderatorsManager\n    );\n\n    final CompactionTask compactionTask1 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    Pair<TaskStatus, List<DataSegment>> resultPair = runTask(compactionTask1);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(PartitionIds.NON_ROOT_GEN_START_PARTITION_ID, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 0), segments.get(i).getShardSpec());\n      }\n    }\n\n    final CompactionTask compactionTask2 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    resultPair = runTask(compactionTask2);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(\n                PartitionIds.NON_ROOT_GEN_START_PARTITION_ID + 1,\n                0,\n                2,\n                (short) 2,\n                (short) 1\n            ),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 0), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskRunTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":233,"status":"NB"},{"authorDate":"2019-12-06 08:50:00","commitOrder":7,"curCode":"  public void testRunRegularIndexTaskWithIngestSegmentFirehose() throws Exception\n  {\n    runIndexTask();\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        new IndexTask.IndexIngestionSpec(\n            new DataSchema(\n                \"test\",\n                getObjectMapper().convertValue(\n                    new StringInputRowParser(\n                        DEFAULT_PARSE_SPEC,\n                        null\n                    ),\n                    Map.class\n                ),\n                new AggregatorFactory[]{\n                    new LongSumAggregatorFactory(\"val\", \"val\")\n                },\n                new UniformGranularitySpec(\n                    Granularities.HOUR,\n                    Granularities.MINUTE,\n                    null\n                ),\n                null,\n                getObjectMapper()\n            ),\n            new IndexTask.IndexIOConfig(\n                new IngestSegmentFirehoseFactory(\n                    DATA_SOURCE,\n                    Intervals.of(\"2014-01-01/2014-01-02\"),\n                    null,\n                    null,\n                    null,\n                    null,\n                    null,\n                    getIndexIO(),\n                    coordinatorClient,\n                    segmentLoaderFactory,\n                    RETRY_POLICY_FACTORY\n                ),\n                false\n            ),\n            IndexTaskTest.createTuningConfig(5000000, null, null, Long.MAX_VALUE, null, null, false, true)\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final Pair<TaskStatus, List<DataSegment>> resultPair = runTask(indexTask);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    final List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(32768, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 0), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","date":"2019-12-06 08:50:00","endLine":757,"groupId":"16729","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testRunRegularIndexTaskWithIngestSegmentFirehose","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3a/18cac2b589922b2e9a05970df4ab9b4b6bfaf7.src","preCode":"  public void testRunRegularIndexTaskWithIngestSegmentFirehose() throws Exception\n  {\n    runIndexTask();\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        new IndexTask.IndexIngestionSpec(\n            new DataSchema(\n                \"test\",\n                getObjectMapper().convertValue(\n                    new StringInputRowParser(\n                        DEFAULT_PARSE_SPEC,\n                        null\n                    ),\n                    Map.class\n                ),\n                new AggregatorFactory[]{\n                    new LongSumAggregatorFactory(\"val\", \"val\")\n                },\n                new UniformGranularitySpec(\n                    Granularities.HOUR,\n                    Granularities.MINUTE,\n                    null\n                ),\n                null,\n                getObjectMapper()\n            ),\n            new IndexTask.IndexIOConfig(\n                new IngestSegmentFirehoseFactory(\n                    DATA_SOURCE,\n                    Intervals.of(\"2014-01-01/2014-01-02\"),\n                    null,\n                    null,\n                    null,\n                    null,\n                    null,\n                    getIndexIO(),\n                    coordinatorClient,\n                    segmentLoaderFactory,\n                    RETRY_POLICY_FACTORY\n                ),\n                false\n            ),\n            IndexTaskTest.createTuningConfig(5000000, null, null, Long.MAX_VALUE, null, null, false, true)\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final Pair<TaskStatus, List<DataSegment>> resultPair = runTask(indexTask);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    final List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(32768, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 0), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskRunTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":682,"status":"B"}],"commitId":"c949a252105a61db64849abdf38648f92b49d30b","commitMessage":"@@@Add DruidInputSource (replacement for IngestSegmentFirehose) (#8982)\n\n* Add Druid input source and format\n\n* Inherit dims/metrics from segment\n\n* Add ingest segment firehose reindexing test\n\n* Remove unnecessary module\n\n* Fix unit tests.  checkstyle\n\n* Add doc entry\n\n* Fix dimensionExclusions handling.  add parallel index integration test\n\n* Add spelling exclusion\n\n* Address some PR comments\n\n* Checkstyle\n\n* wip\n\n* Address rest of PR comments\n\n* Address PR comments\n","date":"2019-12-06 08:50:00","modifiedFileCount":"20","status":"M","submitter":"Jonathan Wei"},{"authorTime":"2020-02-08 08:23:07","codes":[{"authorDate":"2019-10-19 04:24:14","commitOrder":8,"curCode":"  public void testRunCompactionTwice() throws Exception\n  {\n    runIndexTask();\n\n    final Builder builder = new Builder(\n        DATA_SOURCE,\n        getObjectMapper(),\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        rowIngestionMetersFactory,\n        indexingServiceClient,\n        coordinatorClient,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY,\n        appenderatorsManager\n    );\n\n    final CompactionTask compactionTask1 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    Pair<TaskStatus, List<DataSegment>> resultPair = runTask(compactionTask1);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(PartitionIds.NON_ROOT_GEN_START_PARTITION_ID, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 0), segments.get(i).getShardSpec());\n      }\n    }\n\n    final CompactionTask compactionTask2 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    resultPair = runTask(compactionTask2);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(\n                PartitionIds.NON_ROOT_GEN_START_PARTITION_ID + 1,\n                0,\n                2,\n                (short) 2,\n                (short) 1\n            ),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 0), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","date":"2019-10-19 04:24:14","endLine":309,"groupId":"1002","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunCompactionTwice","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/9a/7fa51bad8e203d62831359efcd7174c29a5d07.src","preCode":"  public void testRunCompactionTwice() throws Exception\n  {\n    runIndexTask();\n\n    final Builder builder = new Builder(\n        DATA_SOURCE,\n        getObjectMapper(),\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        rowIngestionMetersFactory,\n        indexingServiceClient,\n        coordinatorClient,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY,\n        appenderatorsManager\n    );\n\n    final CompactionTask compactionTask1 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    Pair<TaskStatus, List<DataSegment>> resultPair = runTask(compactionTask1);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(PartitionIds.NON_ROOT_GEN_START_PARTITION_ID, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 0), segments.get(i).getShardSpec());\n      }\n    }\n\n    final CompactionTask compactionTask2 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    resultPair = runTask(compactionTask2);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(\n                PartitionIds.NON_ROOT_GEN_START_PARTITION_ID + 1,\n                0,\n                2,\n                (short) 2,\n                (short) 1\n            ),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 0), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskRunTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":233,"status":"N"},{"authorDate":"2020-02-08 08:23:07","commitOrder":8,"curCode":"  public void testRunRegularIndexTaskWithIngestSegmentFirehose() throws Exception\n  {\n    runIndexTask();\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        new IndexTask.IndexIngestionSpec(\n            new DataSchema(\n                \"test\",\n                getObjectMapper().convertValue(\n                    new StringInputRowParser(\n                        DEFAULT_PARSE_SPEC,\n                        null\n                    ),\n                    Map.class\n                ),\n                new AggregatorFactory[]{\n                    new LongSumAggregatorFactory(\"val\", \"val\")\n                },\n                new UniformGranularitySpec(\n                    Granularities.HOUR,\n                    Granularities.MINUTE,\n                    null\n                ),\n                null,\n                getObjectMapper()\n            ),\n            new IndexTask.IndexIOConfig(\n                new IngestSegmentFirehoseFactory(\n                    DATA_SOURCE,\n                    Intervals.of(\"2014-01-01/2014-01-02\"),\n                    null,\n                    null,\n                    null,\n                    null,\n                    null,\n                    getIndexIO(),\n                    coordinatorClient,\n                    segmentLoaderFactory,\n                    RETRY_POLICY_FACTORY\n                ),\n                false\n            ),\n            IndexTaskTest.createTuningConfig(5000000, null, null, Long.MAX_VALUE, null, false, true)\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final Pair<TaskStatus, List<DataSegment>> resultPair = runTask(indexTask);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    final List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(32768, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 0), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","date":"2020-02-08 08:23:07","endLine":760,"groupId":"1002","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testRunRegularIndexTaskWithIngestSegmentFirehose","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d3/0229701007dec5332ea0dd159c9fe30d4cc796.src","preCode":"  public void testRunRegularIndexTaskWithIngestSegmentFirehose() throws Exception\n  {\n    runIndexTask();\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        new IndexTask.IndexIngestionSpec(\n            new DataSchema(\n                \"test\",\n                getObjectMapper().convertValue(\n                    new StringInputRowParser(\n                        DEFAULT_PARSE_SPEC,\n                        null\n                    ),\n                    Map.class\n                ),\n                new AggregatorFactory[]{\n                    new LongSumAggregatorFactory(\"val\", \"val\")\n                },\n                new UniformGranularitySpec(\n                    Granularities.HOUR,\n                    Granularities.MINUTE,\n                    null\n                ),\n                null,\n                getObjectMapper()\n            ),\n            new IndexTask.IndexIOConfig(\n                new IngestSegmentFirehoseFactory(\n                    DATA_SOURCE,\n                    Intervals.of(\"2014-01-01/2014-01-02\"),\n                    null,\n                    null,\n                    null,\n                    null,\n                    null,\n                    getIndexIO(),\n                    coordinatorClient,\n                    segmentLoaderFactory,\n                    RETRY_POLICY_FACTORY\n                ),\n                false\n            ),\n            IndexTaskTest.createTuningConfig(5000000, null, null, Long.MAX_VALUE, null, null, false, true)\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final Pair<TaskStatus, List<DataSegment>> resultPair = runTask(indexTask);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    final List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(32768, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 0), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskRunTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":685,"status":"M"}],"commitId":"e81230f9abd092b2ac742258abf534f6e17f35e8","commitMessage":"@@@Refactoring some codes around ingestion (#9274)\n\n* Refactoring codes around ingestion:\n\n- Parallel index task and simple task now use the same segment allocator implementation. This is reusable for the future implementation as well.\n- Added PartitionAnalysis to store the analysis of the partitioning\n- Move some util methods to SegmentLockHelper and rename it to TaskLockHelper\n\n* fix build\n\n* fix SingleDimensionShardSpecFactory\n\n* optimize SingledimensionShardSpecFactory\n\n* fix test\n\n* shard spec builder\n\n* import order\n\n* shardSpecBuilder -> partialShardSpec\n\n* build -> complete\n\n* fix comment; add unit tests for partitionBoundaries\n\n* add more tests and fix javadoc\n\n* fix toString(); add serde tests for HashBasedNumberedPartialShardSpec and SegmentAllocateAction\n\n* fix test\n\n* add equality test for hash and range partial shard specs\n","date":"2020-02-08 08:23:07","modifiedFileCount":"49","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-06-13 12:39:37","codes":[{"authorDate":"2020-06-13 12:39:37","commitOrder":9,"curCode":"  public void testRunCompactionTwice() throws Exception\n  {\n    runIndexTask();\n\n    final Builder builder = new Builder(\n        DATA_SOURCE,\n        getObjectMapper(),\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        rowIngestionMetersFactory,\n        indexingServiceClient,\n        coordinatorClient,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY,\n        appenderatorsManager\n    );\n\n    final CompactionTask compactionTask1 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    Pair<TaskStatus, List<DataSegment>> resultPair = runTask(compactionTask1);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(PartitionIds.NON_ROOT_GEN_START_PARTITION_ID, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n\n    final CompactionTask compactionTask2 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    resultPair = runTask(compactionTask2);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(\n                PartitionIds.NON_ROOT_GEN_START_PARTITION_ID + 1,\n                0,\n                2,\n                (short) 2,\n                (short) 1\n            ),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","date":"2020-06-13 12:39:37","endLine":340,"groupId":"1002","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunCompactionTwice","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e0/8361450fb94d4f141a24bcbea293dff29dc429.src","preCode":"  public void testRunCompactionTwice() throws Exception\n  {\n    runIndexTask();\n\n    final Builder builder = new Builder(\n        DATA_SOURCE,\n        getObjectMapper(),\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        rowIngestionMetersFactory,\n        indexingServiceClient,\n        coordinatorClient,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY,\n        appenderatorsManager\n    );\n\n    final CompactionTask compactionTask1 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    Pair<TaskStatus, List<DataSegment>> resultPair = runTask(compactionTask1);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(PartitionIds.NON_ROOT_GEN_START_PARTITION_ID, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 0), segments.get(i).getShardSpec());\n      }\n    }\n\n    final CompactionTask compactionTask2 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    resultPair = runTask(compactionTask2);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(\n                PartitionIds.NON_ROOT_GEN_START_PARTITION_ID + 1,\n                0,\n                2,\n                (short) 2,\n                (short) 1\n            ),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 0), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskRunTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":264,"status":"M"},{"authorDate":"2020-06-13 12:39:37","commitOrder":9,"curCode":"  public void testRunRegularIndexTaskWithIngestSegmentFirehose() throws Exception\n  {\n    runIndexTask();\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        new IndexTask.IndexIngestionSpec(\n            new DataSchema(\n                \"test\",\n                getObjectMapper().convertValue(\n                    new StringInputRowParser(\n                        DEFAULT_PARSE_SPEC,\n                        null\n                    ),\n                    Map.class\n                ),\n                new AggregatorFactory[]{\n                    new LongSumAggregatorFactory(\"val\", \"val\")\n                },\n                new UniformGranularitySpec(\n                    Granularities.HOUR,\n                    Granularities.MINUTE,\n                    null\n                ),\n                null,\n                getObjectMapper()\n            ),\n            new IndexTask.IndexIOConfig(\n                new IngestSegmentFirehoseFactory(\n                    DATA_SOURCE,\n                    Intervals.of(\"2014-01-01/2014-01-02\"),\n                    null,\n                    null,\n                    null,\n                    null,\n                    null,\n                    getIndexIO(),\n                    coordinatorClient,\n                    segmentLoaderFactory,\n                    RETRY_POLICY_FACTORY\n                ),\n                false\n            ),\n            IndexTaskTest.createTuningConfig(5000000, null, null, Long.MAX_VALUE, null, false, true)\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final Pair<TaskStatus, List<DataSegment>> resultPair = runTask(indexTask);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    final List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(32768, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","date":"2020-06-13 12:39:37","endLine":767,"groupId":"1002","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testRunRegularIndexTaskWithIngestSegmentFirehose","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e0/8361450fb94d4f141a24bcbea293dff29dc429.src","preCode":"  public void testRunRegularIndexTaskWithIngestSegmentFirehose() throws Exception\n  {\n    runIndexTask();\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        new IndexTask.IndexIngestionSpec(\n            new DataSchema(\n                \"test\",\n                getObjectMapper().convertValue(\n                    new StringInputRowParser(\n                        DEFAULT_PARSE_SPEC,\n                        null\n                    ),\n                    Map.class\n                ),\n                new AggregatorFactory[]{\n                    new LongSumAggregatorFactory(\"val\", \"val\")\n                },\n                new UniformGranularitySpec(\n                    Granularities.HOUR,\n                    Granularities.MINUTE,\n                    null\n                ),\n                null,\n                getObjectMapper()\n            ),\n            new IndexTask.IndexIOConfig(\n                new IngestSegmentFirehoseFactory(\n                    DATA_SOURCE,\n                    Intervals.of(\"2014-01-01/2014-01-02\"),\n                    null,\n                    null,\n                    null,\n                    null,\n                    null,\n                    getIndexIO(),\n                    coordinatorClient,\n                    segmentLoaderFactory,\n                    RETRY_POLICY_FACTORY\n                ),\n                false\n            ),\n            IndexTaskTest.createTuningConfig(5000000, null, null, Long.MAX_VALUE, null, false, true)\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final Pair<TaskStatus, List<DataSegment>> resultPair = runTask(indexTask);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    final List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(32768, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 0), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskRunTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":692,"status":"M"}],"commitId":"9a10f8352bd0c8c0fd5199226a4ff3a4cf2e6d46","commitMessage":"@@@Set the core partition set size properly for batch ingestion with dynamic partitioning (#10012)\n\n* Fill in the core partition set size properly for batch ingestion with\ndynamic partitioning\n\n* incomplete javadoc\n\n* Address comments\n\n* fix tests\n\n* fix json serde.  add tests\n\n* checkstyle","date":"2020-06-13 12:39:37","modifiedFileCount":"11","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-08-27 08:08:12","codes":[{"authorDate":"2020-08-27 08:08:12","commitOrder":10,"curCode":"  public void testRunCompactionTwice() throws Exception\n  {\n    runIndexTask();\n\n    final Builder builder = new Builder(\n        DATA_SOURCE,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n\n    final CompactionTask compactionTask1 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    Pair<TaskStatus, List<DataSegment>> resultPair = runTask(compactionTask1);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(PartitionIds.NON_ROOT_GEN_START_PARTITION_ID, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n\n    final CompactionTask compactionTask2 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    resultPair = runTask(compactionTask2);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(\n                PartitionIds.NON_ROOT_GEN_START_PARTITION_ID + 1,\n                0,\n                2,\n                (short) 2,\n                (short) 1\n            ),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","date":"2020-08-27 08:08:12","endLine":402,"groupId":"1002","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunCompactionTwice","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/71/9fba10bbe182731bd0191d5b5847ed7c900e40.src","preCode":"  public void testRunCompactionTwice() throws Exception\n  {\n    runIndexTask();\n\n    final Builder builder = new Builder(\n        DATA_SOURCE,\n        getObjectMapper(),\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        rowIngestionMetersFactory,\n        indexingServiceClient,\n        coordinatorClient,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY,\n        appenderatorsManager\n    );\n\n    final CompactionTask compactionTask1 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    Pair<TaskStatus, List<DataSegment>> resultPair = runTask(compactionTask1);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(PartitionIds.NON_ROOT_GEN_START_PARTITION_ID, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n\n    final CompactionTask compactionTask2 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    resultPair = runTask(compactionTask2);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(\n                PartitionIds.NON_ROOT_GEN_START_PARTITION_ID + 1,\n                0,\n                2,\n                (short) 2,\n                (short) 1\n            ),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskRunTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":333,"status":"M"},{"authorDate":"2020-08-27 08:08:12","commitOrder":10,"curCode":"  public void testRunRegularIndexTaskWithIngestSegmentFirehose() throws Exception\n  {\n    runIndexTask();\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        new IndexTask.IndexIngestionSpec(\n            new DataSchema(\n                \"test\",\n                getObjectMapper().convertValue(\n                    new StringInputRowParser(\n                        DEFAULT_PARSE_SPEC,\n                        null\n                    ),\n                    Map.class\n                ),\n                new AggregatorFactory[]{\n                    new LongSumAggregatorFactory(\"val\", \"val\")\n                },\n                new UniformGranularitySpec(\n                    Granularities.HOUR,\n                    Granularities.MINUTE,\n                    null\n                ),\n                null,\n                getObjectMapper()\n            ),\n            new IndexTask.IndexIOConfig(\n                new IngestSegmentFirehoseFactory(\n                    DATA_SOURCE,\n                    Intervals.of(\"2014-01-01/2014-01-02\"),\n                    null,\n                    null,\n                    null,\n                    null,\n                    null,\n                    getIndexIO(),\n                    coordinatorClient,\n                    segmentLoaderFactory,\n                    RETRY_POLICY_FACTORY\n                ),\n                false\n            ),\n            IndexTaskTest.createTuningConfig(5000000, null, null, Long.MAX_VALUE, null, false, true)\n        ),\n        null\n    );\n\n    final Pair<TaskStatus, List<DataSegment>> resultPair = runTask(indexTask);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    final List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(32768, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","date":"2020-08-27 08:08:12","endLine":786,"groupId":"347","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testRunRegularIndexTaskWithIngestSegmentFirehose","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/71/9fba10bbe182731bd0191d5b5847ed7c900e40.src","preCode":"  public void testRunRegularIndexTaskWithIngestSegmentFirehose() throws Exception\n  {\n    runIndexTask();\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        new IndexTask.IndexIngestionSpec(\n            new DataSchema(\n                \"test\",\n                getObjectMapper().convertValue(\n                    new StringInputRowParser(\n                        DEFAULT_PARSE_SPEC,\n                        null\n                    ),\n                    Map.class\n                ),\n                new AggregatorFactory[]{\n                    new LongSumAggregatorFactory(\"val\", \"val\")\n                },\n                new UniformGranularitySpec(\n                    Granularities.HOUR,\n                    Granularities.MINUTE,\n                    null\n                ),\n                null,\n                getObjectMapper()\n            ),\n            new IndexTask.IndexIOConfig(\n                new IngestSegmentFirehoseFactory(\n                    DATA_SOURCE,\n                    Intervals.of(\"2014-01-01/2014-01-02\"),\n                    null,\n                    null,\n                    null,\n                    null,\n                    null,\n                    getIndexIO(),\n                    coordinatorClient,\n                    segmentLoaderFactory,\n                    RETRY_POLICY_FACTORY\n                ),\n                false\n            ),\n            IndexTaskTest.createTuningConfig(5000000, null, null, Long.MAX_VALUE, null, false, true)\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        new NoopChatHandlerProvider(),\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final Pair<TaskStatus, List<DataSegment>> resultPair = runTask(indexTask);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    final List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(32768, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskRunTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":715,"status":"M"}],"commitId":"f82fd22fa7de175200b7127c34c2eb2900bf7317","commitMessage":"@@@Move tools for indexing to TaskToolbox instead of injecting them in constructor (#10308)\n\n* Move tools for indexing to TaskToolbox instead of injecting them in constructor\n\n* oops.  other changes\n\n* fix test\n\n* unnecessary new file\n\n* fix test\n\n* fix build","date":"2020-08-27 08:08:12","modifiedFileCount":"67","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-09-24 06:29:36","codes":[{"authorDate":"2020-08-27 08:08:12","commitOrder":11,"curCode":"  public void testRunCompactionTwice() throws Exception\n  {\n    runIndexTask();\n\n    final Builder builder = new Builder(\n        DATA_SOURCE,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n\n    final CompactionTask compactionTask1 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    Pair<TaskStatus, List<DataSegment>> resultPair = runTask(compactionTask1);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(PartitionIds.NON_ROOT_GEN_START_PARTITION_ID, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n\n    final CompactionTask compactionTask2 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    resultPair = runTask(compactionTask2);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(\n                PartitionIds.NON_ROOT_GEN_START_PARTITION_ID + 1,\n                0,\n                2,\n                (short) 2,\n                (short) 1\n            ),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","date":"2020-08-27 08:08:12","endLine":402,"groupId":"1002","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunCompactionTwice","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/71/9fba10bbe182731bd0191d5b5847ed7c900e40.src","preCode":"  public void testRunCompactionTwice() throws Exception\n  {\n    runIndexTask();\n\n    final Builder builder = new Builder(\n        DATA_SOURCE,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n\n    final CompactionTask compactionTask1 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    Pair<TaskStatus, List<DataSegment>> resultPair = runTask(compactionTask1);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(PartitionIds.NON_ROOT_GEN_START_PARTITION_ID, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n\n    final CompactionTask compactionTask2 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    resultPair = runTask(compactionTask2);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(\n                PartitionIds.NON_ROOT_GEN_START_PARTITION_ID + 1,\n                0,\n                2,\n                (short) 2,\n                (short) 1\n            ),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskRunTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":333,"status":"N"},{"authorDate":"2020-09-24 06:29:36","commitOrder":11,"curCode":"  public void testRunRegularIndexTaskWithIngestSegmentFirehose() throws Exception\n  {\n    runIndexTask();\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        new IndexTask.IndexIngestionSpec(\n            new DataSchema(\n                \"test\",\n                getObjectMapper().convertValue(\n                    new StringInputRowParser(\n                        DEFAULT_PARSE_SPEC,\n                        null\n                    ),\n                    Map.class\n                ),\n                new AggregatorFactory[]{\n                    new LongSumAggregatorFactory(\"val\", \"val\")\n                },\n                new UniformGranularitySpec(\n                    Granularities.HOUR,\n                    Granularities.MINUTE,\n                    null\n                ),\n                null,\n                getObjectMapper()\n            ),\n            new IndexTask.IndexIOConfig(\n                new IngestSegmentFirehoseFactory(\n                    DATA_SOURCE,\n                    Intervals.of(\"2014-01-01/2014-01-02\"),\n                    null,\n                    null,\n                    null,\n                    null,\n                    null,\n                    getIndexIO(),\n                    coordinatorClient,\n                    segmentLoaderFactory,\n                    RETRY_POLICY_FACTORY\n                ),\n                false\n            ),\n            IndexTaskTest.createTuningConfig(5000000, null, null, Long.MAX_VALUE, null, false, true)\n        ),\n        null\n    );\n\n    \r\n    indexTask.addToContext(Tasks.STORE_COMPACTION_STATE_KEY, true);\n\n    final Pair<TaskStatus, List<DataSegment>> resultPair = runTask(indexTask);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    final List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(32768, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","date":"2020-09-24 06:29:36","endLine":791,"groupId":"347","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testRunRegularIndexTaskWithIngestSegmentFirehose","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/08/cd765a1fb1a27f5080484f51b0013199a24a22.src","preCode":"  public void testRunRegularIndexTaskWithIngestSegmentFirehose() throws Exception\n  {\n    runIndexTask();\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        new IndexTask.IndexIngestionSpec(\n            new DataSchema(\n                \"test\",\n                getObjectMapper().convertValue(\n                    new StringInputRowParser(\n                        DEFAULT_PARSE_SPEC,\n                        null\n                    ),\n                    Map.class\n                ),\n                new AggregatorFactory[]{\n                    new LongSumAggregatorFactory(\"val\", \"val\")\n                },\n                new UniformGranularitySpec(\n                    Granularities.HOUR,\n                    Granularities.MINUTE,\n                    null\n                ),\n                null,\n                getObjectMapper()\n            ),\n            new IndexTask.IndexIOConfig(\n                new IngestSegmentFirehoseFactory(\n                    DATA_SOURCE,\n                    Intervals.of(\"2014-01-01/2014-01-02\"),\n                    null,\n                    null,\n                    null,\n                    null,\n                    null,\n                    getIndexIO(),\n                    coordinatorClient,\n                    segmentLoaderFactory,\n                    RETRY_POLICY_FACTORY\n                ),\n                false\n            ),\n            IndexTaskTest.createTuningConfig(5000000, null, null, Long.MAX_VALUE, null, false, true)\n        ),\n        null\n    );\n\n    final Pair<TaskStatus, List<DataSegment>> resultPair = runTask(indexTask);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    final List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(32768, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskRunTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":717,"status":"M"}],"commitId":"72f1b55f569719b8e0e1920f382bdf98e418924e","commitMessage":"@@@Add last_compaction_state to sys.segments table (#10413)\n\n* Add is_compacted to sys.segments table\n\n* change is_compacted to last_compaction_state\n\n* fix tests\n\n* fix tests\n\n* address comments","date":"2020-09-24 06:29:36","modifiedFileCount":"9","status":"M","submitter":"Maytas Monsereenusorn"},{"authorTime":"2021-02-12 19:03:20","codes":[{"authorDate":"2021-02-12 19:03:20","commitOrder":12,"curCode":"  public void testRunCompactionTwice() throws Exception\n  {\n    runIndexTask();\n\n    final Builder builder = new Builder(\n        DATA_SOURCE,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n\n    final CompactionTask compactionTask1 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    Pair<TaskStatus, List<DataSegment>> resultPair = runTask(compactionTask1);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.NONE, ImmutableList.of(Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1))),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(PartitionIds.NON_ROOT_GEN_START_PARTITION_ID, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n\n    final CompactionTask compactionTask2 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    resultPair = runTask(compactionTask2);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.NONE, ImmutableList.of(Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1))),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(\n                PartitionIds.NON_ROOT_GEN_START_PARTITION_ID + 1,\n                0,\n                2,\n                (short) 2,\n                (short) 1\n            ),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","date":"2021-02-12 19:03:20","endLine":436,"groupId":"1002","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunCompactionTwice","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/bd/1f8193417da8de9a1df24dcc6cc87448fa70ad.src","preCode":"  public void testRunCompactionTwice() throws Exception\n  {\n    runIndexTask();\n\n    final Builder builder = new Builder(\n        DATA_SOURCE,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n\n    final CompactionTask compactionTask1 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    Pair<TaskStatus, List<DataSegment>> resultPair = runTask(compactionTask1);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(PartitionIds.NON_ROOT_GEN_START_PARTITION_ID, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n\n    final CompactionTask compactionTask2 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    resultPair = runTask(compactionTask2);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(\n                PartitionIds.NON_ROOT_GEN_START_PARTITION_ID + 1,\n                0,\n                2,\n                (short) 2,\n                (short) 1\n            ),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskRunTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":361,"status":"M"},{"authorDate":"2021-02-12 19:03:20","commitOrder":12,"curCode":"  public void testRunRegularIndexTaskWithIngestSegmentFirehose() throws Exception\n  {\n    runIndexTask();\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        new IndexTask.IndexIngestionSpec(\n            new DataSchema(\n                \"test\",\n                getObjectMapper().convertValue(\n                    new StringInputRowParser(\n                        DEFAULT_PARSE_SPEC,\n                        null\n                    ),\n                    Map.class\n                ),\n                new AggregatorFactory[]{\n                    new LongSumAggregatorFactory(\"val\", \"val\")\n                },\n                new UniformGranularitySpec(\n                    Granularities.HOUR,\n                    Granularities.MINUTE,\n                    null\n                ),\n                null,\n                getObjectMapper()\n            ),\n            new IndexTask.IndexIOConfig(\n                new IngestSegmentFirehoseFactory(\n                    DATA_SOURCE,\n                    Intervals.of(\"2014-01-01/2014-01-02\"),\n                    null,\n                    null,\n                    null,\n                    null,\n                    null,\n                    getIndexIO(),\n                    coordinatorClient,\n                    segmentLoaderFactory,\n                    RETRY_POLICY_FACTORY\n                ),\n                false\n            ),\n            IndexTaskTest.createTuningConfig(5000000, null, null, Long.MAX_VALUE, null, false, true)\n        ),\n        null\n    );\n\n    \r\n    indexTask.addToContext(Tasks.STORE_COMPACTION_STATE_KEY, true);\n\n    final Pair<TaskStatus, List<DataSegment>> resultPair = runTask(indexTask);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    final List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.MINUTE, ImmutableList.of()),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(32768, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","date":"2021-02-12 19:03:20","endLine":835,"groupId":"347","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testRunRegularIndexTaskWithIngestSegmentFirehose","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/bd/1f8193417da8de9a1df24dcc6cc87448fa70ad.src","preCode":"  public void testRunRegularIndexTaskWithIngestSegmentFirehose() throws Exception\n  {\n    runIndexTask();\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        new IndexTask.IndexIngestionSpec(\n            new DataSchema(\n                \"test\",\n                getObjectMapper().convertValue(\n                    new StringInputRowParser(\n                        DEFAULT_PARSE_SPEC,\n                        null\n                    ),\n                    Map.class\n                ),\n                new AggregatorFactory[]{\n                    new LongSumAggregatorFactory(\"val\", \"val\")\n                },\n                new UniformGranularitySpec(\n                    Granularities.HOUR,\n                    Granularities.MINUTE,\n                    null\n                ),\n                null,\n                getObjectMapper()\n            ),\n            new IndexTask.IndexIOConfig(\n                new IngestSegmentFirehoseFactory(\n                    DATA_SOURCE,\n                    Intervals.of(\"2014-01-01/2014-01-02\"),\n                    null,\n                    null,\n                    null,\n                    null,\n                    null,\n                    getIndexIO(),\n                    coordinatorClient,\n                    segmentLoaderFactory,\n                    RETRY_POLICY_FACTORY\n                ),\n                false\n            ),\n            IndexTaskTest.createTuningConfig(5000000, null, null, Long.MAX_VALUE, null, false, true)\n        ),\n        null\n    );\n\n    \r\n    indexTask.addToContext(Tasks.STORE_COMPACTION_STATE_KEY, true);\n\n    final Pair<TaskStatus, List<DataSegment>> resultPair = runTask(indexTask);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    final List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(DEFAULT_COMPACTION_STATE, segments.get(i).getLastCompactionState());\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(32768, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskRunTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":758,"status":"M"}],"commitId":"6541178c21839530a42af4b4675a9bc680bffca6","commitMessage":"@@@Support segmentGranularity for auto-compaction (#10843)\n\n* Support segmentGranularity for auto-compaction\n\n* Support segmentGranularity for auto-compaction\n\n* Support segmentGranularity for auto-compaction\n\n* Support segmentGranularity for auto-compaction\n\n* resolve conflict\n\n* Support segmentGranularity for auto-compaction\n\n* Support segmentGranularity for auto-compaction\n\n* fix tests\n\n* fix more tests\n\n* fix checkstyle\n\n* add unit tests\n\n* fix checkstyle\n\n* fix checkstyle\n\n* fix checkstyle\n\n* add unit tests\n\n* add integration tests\n\n* fix checkstyle\n\n* fix checkstyle\n\n* fix failing tests\n\n* address comments\n\n* address comments\n\n* fix tests\n\n* fix tests\n\n* fix test\n\n* fix test\n\n* fix test\n\n* fix test\n\n* fix test\n\n* fix test\n\n* fix test\n\n* fix test","date":"2021-02-12 19:03:20","modifiedFileCount":"32","status":"M","submitter":"Maytas Monsereenusorn"},{"authorTime":"2021-02-12 19:03:20","codes":[{"authorDate":"2021-02-18 17:35:10","commitOrder":13,"curCode":"  public void testRunCompactionTwice() throws Exception\n  {\n    runIndexTask();\n\n    final Builder builder = new Builder(\n        DATA_SOURCE,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n\n    final CompactionTask compactionTask1 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    Pair<TaskStatus, List<DataSegment>> resultPair = runTask(compactionTask1);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.MINUTE, ImmutableList.of(Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1))),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(PartitionIds.NON_ROOT_GEN_START_PARTITION_ID, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n\n    final CompactionTask compactionTask2 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    resultPair = runTask(compactionTask2);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.MINUTE, ImmutableList.of(Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1))),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(\n                PartitionIds.NON_ROOT_GEN_START_PARTITION_ID + 1,\n                0,\n                2,\n                (short) 2,\n                (short) 1\n            ),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","date":"2021-02-18 17:35:10","endLine":436,"groupId":"1002","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunCompactionTwice","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0e/287721c4408ce0efb2b96ff33fd4aa78db4f90.src","preCode":"  public void testRunCompactionTwice() throws Exception\n  {\n    runIndexTask();\n\n    final Builder builder = new Builder(\n        DATA_SOURCE,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n\n    final CompactionTask compactionTask1 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    Pair<TaskStatus, List<DataSegment>> resultPair = runTask(compactionTask1);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.NONE, ImmutableList.of(Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1))),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(PartitionIds.NON_ROOT_GEN_START_PARTITION_ID, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n\n    final CompactionTask compactionTask2 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    resultPair = runTask(compactionTask2);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.NONE, ImmutableList.of(Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1))),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(\n                PartitionIds.NON_ROOT_GEN_START_PARTITION_ID + 1,\n                0,\n                2,\n                (short) 2,\n                (short) 1\n            ),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskRunTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":361,"status":"M"},{"authorDate":"2021-02-12 19:03:20","commitOrder":13,"curCode":"  public void testRunRegularIndexTaskWithIngestSegmentFirehose() throws Exception\n  {\n    runIndexTask();\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        new IndexTask.IndexIngestionSpec(\n            new DataSchema(\n                \"test\",\n                getObjectMapper().convertValue(\n                    new StringInputRowParser(\n                        DEFAULT_PARSE_SPEC,\n                        null\n                    ),\n                    Map.class\n                ),\n                new AggregatorFactory[]{\n                    new LongSumAggregatorFactory(\"val\", \"val\")\n                },\n                new UniformGranularitySpec(\n                    Granularities.HOUR,\n                    Granularities.MINUTE,\n                    null\n                ),\n                null,\n                getObjectMapper()\n            ),\n            new IndexTask.IndexIOConfig(\n                new IngestSegmentFirehoseFactory(\n                    DATA_SOURCE,\n                    Intervals.of(\"2014-01-01/2014-01-02\"),\n                    null,\n                    null,\n                    null,\n                    null,\n                    null,\n                    getIndexIO(),\n                    coordinatorClient,\n                    segmentLoaderFactory,\n                    RETRY_POLICY_FACTORY\n                ),\n                false\n            ),\n            IndexTaskTest.createTuningConfig(5000000, null, null, Long.MAX_VALUE, null, false, true)\n        ),\n        null\n    );\n\n    \r\n    indexTask.addToContext(Tasks.STORE_COMPACTION_STATE_KEY, true);\n\n    final Pair<TaskStatus, List<DataSegment>> resultPair = runTask(indexTask);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    final List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.MINUTE, ImmutableList.of()),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(32768, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","date":"2021-02-12 19:03:20","endLine":835,"groupId":"347","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testRunRegularIndexTaskWithIngestSegmentFirehose","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/bd/1f8193417da8de9a1df24dcc6cc87448fa70ad.src","preCode":"  public void testRunRegularIndexTaskWithIngestSegmentFirehose() throws Exception\n  {\n    runIndexTask();\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        new IndexTask.IndexIngestionSpec(\n            new DataSchema(\n                \"test\",\n                getObjectMapper().convertValue(\n                    new StringInputRowParser(\n                        DEFAULT_PARSE_SPEC,\n                        null\n                    ),\n                    Map.class\n                ),\n                new AggregatorFactory[]{\n                    new LongSumAggregatorFactory(\"val\", \"val\")\n                },\n                new UniformGranularitySpec(\n                    Granularities.HOUR,\n                    Granularities.MINUTE,\n                    null\n                ),\n                null,\n                getObjectMapper()\n            ),\n            new IndexTask.IndexIOConfig(\n                new IngestSegmentFirehoseFactory(\n                    DATA_SOURCE,\n                    Intervals.of(\"2014-01-01/2014-01-02\"),\n                    null,\n                    null,\n                    null,\n                    null,\n                    null,\n                    getIndexIO(),\n                    coordinatorClient,\n                    segmentLoaderFactory,\n                    RETRY_POLICY_FACTORY\n                ),\n                false\n            ),\n            IndexTaskTest.createTuningConfig(5000000, null, null, Long.MAX_VALUE, null, false, true)\n        ),\n        null\n    );\n\n    \r\n    indexTask.addToContext(Tasks.STORE_COMPACTION_STATE_KEY, true);\n\n    final Pair<TaskStatus, List<DataSegment>> resultPair = runTask(indexTask);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    final List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.MINUTE, ImmutableList.of()),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(32768, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskRunTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":758,"status":"N"}],"commitId":"eabad0fb352ee7f1f32658edb95d236fe1837662","commitMessage":"@@@Keep query granularity of compacted segments after compaction (#10856)\n\n* Keep query granularity of compacted segments after compaction\n\n* Protect against null isRollup\n\n* Fix bugspot check RC_REF_COMPARISON_BAD_PRACTICE_BOOLEAN & edit an existing comment\n\n* Make sure that NONE is also included when comparing for the finer granularity\n\n* Update integration test check for segment size due to query granularity propagation affecting size\n\n* Minor code cleanup\n\n* Added functional test to verify queryGranlarity after compaction\n\n* Minor style fix\n\n* Update unit tests","date":"2021-02-18 17:35:10","modifiedFileCount":"14","status":"M","submitter":"Agustin Gonzalez"},{"authorTime":"2021-04-02 03:29:36","codes":[{"authorDate":"2021-02-18 17:35:10","commitOrder":14,"curCode":"  public void testRunCompactionTwice() throws Exception\n  {\n    runIndexTask();\n\n    final Builder builder = new Builder(\n        DATA_SOURCE,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n\n    final CompactionTask compactionTask1 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    Pair<TaskStatus, List<DataSegment>> resultPair = runTask(compactionTask1);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.MINUTE, ImmutableList.of(Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1))),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(PartitionIds.NON_ROOT_GEN_START_PARTITION_ID, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n\n    final CompactionTask compactionTask2 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    resultPair = runTask(compactionTask2);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.MINUTE, ImmutableList.of(Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1))),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(\n                PartitionIds.NON_ROOT_GEN_START_PARTITION_ID + 1,\n                0,\n                2,\n                (short) 2,\n                (short) 1\n            ),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","date":"2021-02-18 17:35:10","endLine":436,"groupId":"1002","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunCompactionTwice","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/0e/287721c4408ce0efb2b96ff33fd4aa78db4f90.src","preCode":"  public void testRunCompactionTwice() throws Exception\n  {\n    runIndexTask();\n\n    final Builder builder = new Builder(\n        DATA_SOURCE,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n\n    final CompactionTask compactionTask1 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    Pair<TaskStatus, List<DataSegment>> resultPair = runTask(compactionTask1);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.MINUTE, ImmutableList.of(Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1))),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(PartitionIds.NON_ROOT_GEN_START_PARTITION_ID, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n\n    final CompactionTask compactionTask2 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    resultPair = runTask(compactionTask2);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.MINUTE, ImmutableList.of(Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1))),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(\n                PartitionIds.NON_ROOT_GEN_START_PARTITION_ID + 1,\n                0,\n                2,\n                (short) 2,\n                (short) 1\n            ),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskRunTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":361,"status":"N"},{"authorDate":"2021-04-02 03:29:36","commitOrder":14,"curCode":"  public void testRunRegularIndexTaskWithIngestSegmentFirehose() throws Exception\n  {\n    runIndexTask();\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        new IndexTask.IndexIngestionSpec(\n            new DataSchema(\n                \"test\",\n                getObjectMapper().convertValue(\n                    new StringInputRowParser(\n                        DEFAULT_PARSE_SPEC,\n                        null\n                    ),\n                    Map.class\n                ),\n                new AggregatorFactory[]{\n                    new LongSumAggregatorFactory(\"val\", \"val\")\n                },\n                new UniformGranularitySpec(\n                    Granularities.HOUR,\n                    Granularities.MINUTE,\n                    null\n                ),\n                null,\n                getObjectMapper()\n            ),\n            new IndexTask.IndexIOConfig(\n                new IngestSegmentFirehoseFactory(\n                    DATA_SOURCE,\n                    Intervals.of(\"2014-01-01/2014-01-02\"),\n                    null,\n                    null,\n                    null,\n                    null,\n                    null,\n                    getIndexIO(),\n                    coordinatorClient,\n                    segmentLoaderFactory,\n                    RETRY_POLICY_FACTORY\n                ),\n                false,\n                false\n            ),\n            IndexTaskTest.createTuningConfig(5000000, null, null, Long.MAX_VALUE, null, false, true)\n        ),\n        null\n    );\n\n    \r\n    indexTask.addToContext(Tasks.STORE_COMPACTION_STATE_KEY, true);\n\n    final Pair<TaskStatus, List<DataSegment>> resultPair = runTask(indexTask);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    final List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.MINUTE, ImmutableList.of()),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(32768, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","date":"2021-04-02 03:29:36","endLine":1118,"groupId":"1002","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testRunRegularIndexTaskWithIngestSegmentFirehose","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/5e/adc388db0fe75139a0f090c512cc468d184578.src","preCode":"  public void testRunRegularIndexTaskWithIngestSegmentFirehose() throws Exception\n  {\n    runIndexTask();\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        new IndexTask.IndexIngestionSpec(\n            new DataSchema(\n                \"test\",\n                getObjectMapper().convertValue(\n                    new StringInputRowParser(\n                        DEFAULT_PARSE_SPEC,\n                        null\n                    ),\n                    Map.class\n                ),\n                new AggregatorFactory[]{\n                    new LongSumAggregatorFactory(\"val\", \"val\")\n                },\n                new UniformGranularitySpec(\n                    Granularities.HOUR,\n                    Granularities.MINUTE,\n                    null\n                ),\n                null,\n                getObjectMapper()\n            ),\n            new IndexTask.IndexIOConfig(\n                new IngestSegmentFirehoseFactory(\n                    DATA_SOURCE,\n                    Intervals.of(\"2014-01-01/2014-01-02\"),\n                    null,\n                    null,\n                    null,\n                    null,\n                    null,\n                    getIndexIO(),\n                    coordinatorClient,\n                    segmentLoaderFactory,\n                    RETRY_POLICY_FACTORY\n                ),\n                false\n            ),\n            IndexTaskTest.createTuningConfig(5000000, null, null, Long.MAX_VALUE, null, false, true)\n        ),\n        null\n    );\n\n    \r\n    indexTask.addToContext(Tasks.STORE_COMPACTION_STATE_KEY, true);\n\n    final Pair<TaskStatus, List<DataSegment>> resultPair = runTask(indexTask);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    final List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.MINUTE, ImmutableList.of()),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(32768, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskRunTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1040,"status":"M"}],"commitId":"d7f529336463dad273a742808b49d524bdc4ae11","commitMessage":"@@@Add an option for ingestion task to drop (mark unused) all existing segments that are contained by interval in the ingestionSpec (#11025)\n\n* Auto-Compaction can run indefinitely when segmentGranularity is changed from coarser to finer.\n\n* Add option to drop segments after ingestion\n\n* fix checkstyle\n\n* add tests\n\n* add tests\n\n* add tests\n\n* fix test\n\n* add tests\n\n* fix checkstyle\n\n* fix checkstyle\n\n* add docs\n\n* fix docs\n\n* address comments\n\n* address comments\n\n* fix spelling","date":"2021-04-02 03:29:36","modifiedFileCount":"44","status":"M","submitter":"Maytas Monsereenusorn"},{"authorTime":"2021-07-21 02:44:19","codes":[{"authorDate":"2021-07-21 02:44:19","commitOrder":15,"curCode":"  public void testRunCompactionTwice() throws Exception\n  {\n    runIndexTask();\n\n    final Builder builder = new Builder(\n        DATA_SOURCE,\n        segmentCacheManagerFactory,\n        RETRY_POLICY_FACTORY\n    );\n\n    final CompactionTask compactionTask1 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    Pair<TaskStatus, List<DataSegment>> resultPair = runTask(compactionTask1);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.MINUTE, ImmutableList.of(Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1))),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(PartitionIds.NON_ROOT_GEN_START_PARTITION_ID, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n\n    final CompactionTask compactionTask2 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    resultPair = runTask(compactionTask2);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.MINUTE, ImmutableList.of(Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1))),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(\n                PartitionIds.NON_ROOT_GEN_START_PARTITION_ID + 1,\n                0,\n                2,\n                (short) 2,\n                (short) 1\n            ),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","date":"2021-07-21 02:44:19","endLine":440,"groupId":"104541","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"testRunCompactionTwice","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b6/098ea9854c6e2a2db0c789a1b7ff67c3239486.src","preCode":"  public void testRunCompactionTwice() throws Exception\n  {\n    runIndexTask();\n\n    final Builder builder = new Builder(\n        DATA_SOURCE,\n        segmentLoaderFactory,\n        RETRY_POLICY_FACTORY\n    );\n\n    final CompactionTask compactionTask1 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    Pair<TaskStatus, List<DataSegment>> resultPair = runTask(compactionTask1);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.MINUTE, ImmutableList.of(Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1))),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(PartitionIds.NON_ROOT_GEN_START_PARTITION_ID, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n\n    final CompactionTask compactionTask2 = builder\n        .interval(Intervals.of(\"2014-01-01/2014-01-02\"))\n        .build();\n\n    resultPair = runTask(compactionTask2);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.MINUTE, ImmutableList.of(Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1))),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(\n                PartitionIds.NON_ROOT_GEN_START_PARTITION_ID + 1,\n                0,\n                2,\n                (short) 2,\n                (short) 1\n            ),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskRunTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":365,"status":"M"},{"authorDate":"2021-07-21 02:44:19","commitOrder":15,"curCode":"  public void testRunRegularIndexTaskWithIngestSegmentFirehose() throws Exception\n  {\n    runIndexTask();\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        new IndexTask.IndexIngestionSpec(\n            new DataSchema(\n                \"test\",\n                getObjectMapper().convertValue(\n                    new StringInputRowParser(\n                        DEFAULT_PARSE_SPEC,\n                        null\n                    ),\n                    Map.class\n                ),\n                new AggregatorFactory[]{\n                    new LongSumAggregatorFactory(\"val\", \"val\")\n                },\n                new UniformGranularitySpec(\n                    Granularities.HOUR,\n                    Granularities.MINUTE,\n                    null\n                ),\n                null,\n                getObjectMapper()\n            ),\n            new IndexTask.IndexIOConfig(\n                new IngestSegmentFirehoseFactory(\n                    DATA_SOURCE,\n                    Intervals.of(\"2014-01-01/2014-01-02\"),\n                    null,\n                    null,\n                    null,\n                    null,\n                    null,\n                    getIndexIO(),\n                    coordinatorClient,\n                    segmentCacheManagerFactory,\n                    RETRY_POLICY_FACTORY\n                ),\n                false,\n                false\n            ),\n            IndexTaskTest.createTuningConfig(5000000, null, null, Long.MAX_VALUE, null, false, true)\n        ),\n        null\n    );\n\n    \r\n    indexTask.addToContext(Tasks.STORE_COMPACTION_STATE_KEY, true);\n\n    final Pair<TaskStatus, List<DataSegment>> resultPair = runTask(indexTask);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    final List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.MINUTE, ImmutableList.of()),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(32768, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","date":"2021-07-21 02:44:19","endLine":1196,"groupId":"104541","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testRunRegularIndexTaskWithIngestSegmentFirehose","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b6/098ea9854c6e2a2db0c789a1b7ff67c3239486.src","preCode":"  public void testRunRegularIndexTaskWithIngestSegmentFirehose() throws Exception\n  {\n    runIndexTask();\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        new IndexTask.IndexIngestionSpec(\n            new DataSchema(\n                \"test\",\n                getObjectMapper().convertValue(\n                    new StringInputRowParser(\n                        DEFAULT_PARSE_SPEC,\n                        null\n                    ),\n                    Map.class\n                ),\n                new AggregatorFactory[]{\n                    new LongSumAggregatorFactory(\"val\", \"val\")\n                },\n                new UniformGranularitySpec(\n                    Granularities.HOUR,\n                    Granularities.MINUTE,\n                    null\n                ),\n                null,\n                getObjectMapper()\n            ),\n            new IndexTask.IndexIOConfig(\n                new IngestSegmentFirehoseFactory(\n                    DATA_SOURCE,\n                    Intervals.of(\"2014-01-01/2014-01-02\"),\n                    null,\n                    null,\n                    null,\n                    null,\n                    null,\n                    getIndexIO(),\n                    coordinatorClient,\n                    segmentLoaderFactory,\n                    RETRY_POLICY_FACTORY\n                ),\n                false,\n                false\n            ),\n            IndexTaskTest.createTuningConfig(5000000, null, null, Long.MAX_VALUE, null, false, true)\n        ),\n        null\n    );\n\n    \r\n    indexTask.addToContext(Tasks.STORE_COMPACTION_STATE_KEY, true);\n\n    final Pair<TaskStatus, List<DataSegment>> resultPair = runTask(indexTask);\n\n    Assert.assertTrue(resultPair.lhs.isSuccess());\n\n    final List<DataSegment> segments = resultPair.rhs;\n    Assert.assertEquals(3, segments.size());\n\n    for (int i = 0; i < 3; i++) {\n      Assert.assertEquals(\n          Intervals.of(\"2014-01-01T0%d:00:00/2014-01-01T0%d:00:00\", i, i + 1),\n          segments.get(i).getInterval()\n      );\n      Assert.assertEquals(\n          getDefaultCompactionState(Granularities.HOUR, Granularities.MINUTE, ImmutableList.of()),\n          segments.get(i).getLastCompactionState()\n      );\n      if (lockGranularity == LockGranularity.SEGMENT) {\n        Assert.assertEquals(\n            new NumberedOverwriteShardSpec(32768, 0, 2, (short) 1, (short) 1),\n            segments.get(i).getShardSpec()\n        );\n      } else {\n        Assert.assertEquals(new NumberedShardSpec(0, 1), segments.get(i).getShardSpec());\n      }\n    }\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/CompactionTaskRunTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":1118,"status":"M"}],"commitId":"94c1671eaf7b050972602fdedcb1971cdbde692d","commitMessage":"@@@Split SegmentLoader into SegmentLoader and SegmentCacheManager (#11466)\n\nThis PR splits current SegmentLoader into SegmentLoader and SegmentCacheManager.\n\nSegmentLoader - this class is responsible for building the segment object but does not expose any methods for downloading.  cache space management.  etc. Default implementation delegates the download operations to SegmentCacheManager and only contains the logic for building segments once downloaded. . This class will be used in SegmentManager to construct Segment objects.\n\nSegmentCacheManager - this class manages the segment cache on the local disk. It fetches the segment files to the local disk.  can clean up the cache.  and in the future.  support reserve and release on cache space. [See https://github.com/Make SegmentLoader extensible and customizable #11398]. This class will be used in ingestion tasks such as compaction.  re-indexing where segment files need to be downloaded locally.","date":"2021-07-21 02:44:19","modifiedFileCount":"41","status":"M","submitter":"Abhishek Agarwal"}]
