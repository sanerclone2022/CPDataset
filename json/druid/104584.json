[{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        createIngestionSpec(\n            tmpDir,\n            new CSVParseSpec(\n                new TimestampSpec(\n                    \"time\",\n                    \"auto\",\n                    null\n                ),\n                new DimensionsSpec(\n                    null,\n                    Lists.newArrayList(),\n                    Lists.newArrayList()\n                ),\n                null,\n                Arrays.asList(\"time\", \"dim\", \"val\"),\n                true,\n                0\n            ),\n            null,\n            createTuningConfig(2, null, false, true),\n            false\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2018-08-31 00:56:26","endLine":615,"groupId":"4873","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testCSVFileWithHeaderColumnOverride","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3e/288c5d2f7786d294be8827c9e930d730fb69d8.src","preCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        createIngestionSpec(\n            tmpDir,\n            new CSVParseSpec(\n                new TimestampSpec(\n                    \"time\",\n                    \"auto\",\n                    null\n                ),\n                new DimensionsSpec(\n                    null,\n                    Lists.newArrayList(),\n                    Lists.newArrayList()\n                ),\n                null,\n                Arrays.asList(\"time\", \"dim\", \"val\"),\n                true,\n                0\n            ),\n            null,\n            createTuningConfig(2, null, false, true),\n            false\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":566,"status":"B"},{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec = createIngestionSpec(\n        tmpDir,\n        new CSVParseSpec(\n            new TimestampSpec(\n                \"time\",\n                \"auto\",\n                null\n            ),\n            new DimensionsSpec(\n                null,\n                Lists.newArrayList(),\n                Lists.newArrayList()\n            ),\n            null,\n            Arrays.asList(\"time\", \"dim\", \"val\"),\n            true,\n            0\n        ),\n        null,\n        createTuningConfig(2, null, null, null, null, false, false, false), \r\n        false\n    );\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2018-08-31 00:56:26","endLine":827,"groupId":"4876","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testIgnoreParseException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3e/288c5d2f7786d294be8827c9e930d730fb69d8.src","preCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec = createIngestionSpec(\n        tmpDir,\n        new CSVParseSpec(\n            new TimestampSpec(\n                \"time\",\n                \"auto\",\n                null\n            ),\n            new DimensionsSpec(\n                null,\n                Lists.newArrayList(),\n                Lists.newArrayList()\n            ),\n            null,\n            Arrays.asList(\"time\", \"dim\", \"val\"),\n            true,\n            0\n        ),\n        null,\n        createTuningConfig(2, null, null, null, null, false, false, false), \r\n        false\n    );\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":775,"status":"B"}],"commitId":"431d3d8497f9079857c3baa7ae7ab4cb44a22355","commitMessage":"@@@Rename io.druid to org.apache.druid. (#6266)\n\n* Rename io.druid to org.apache.druid.\n\n* Fix META-INF files and remove some benchmark results.\n\n* MonitorsConfig update for metrics package migration.\n\n* Reorder some dimensions in inner queries for some reason.\n\n* Fix protobuf tests.\n","date":"2018-08-31 00:56:26","modifiedFileCount":"5","status":"B","submitter":"Gian Merlino"},{"authorTime":"2018-10-07 07:45:07","codes":[{"authorDate":"2018-10-07 07:45:07","commitOrder":2,"curCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        createIngestionSpec(\n            tmpDir,\n            new CSVParseSpec(\n                new TimestampSpec(\n                    \"time\",\n                    \"auto\",\n                    null\n                ),\n                new DimensionsSpec(\n                    null,\n                    Lists.newArrayList(),\n                    Lists.newArrayList()\n                ),\n                null,\n                Arrays.asList(\"time\", \"dim\", \"val\"),\n                true,\n                0\n            ),\n            null,\n            createTuningConfigWithTargetPartitionSize(2, false, true),\n            false\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2018-10-07 07:45:07","endLine":752,"groupId":"19638","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testCSVFileWithHeaderColumnOverride","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/9d/4f6e7fb752711ed07a23a68ba15d56ebdcb044.src","preCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        createIngestionSpec(\n            tmpDir,\n            new CSVParseSpec(\n                new TimestampSpec(\n                    \"time\",\n                    \"auto\",\n                    null\n                ),\n                new DimensionsSpec(\n                    null,\n                    Lists.newArrayList(),\n                    Lists.newArrayList()\n                ),\n                null,\n                Arrays.asList(\"time\", \"dim\", \"val\"),\n                true,\n                0\n            ),\n            null,\n            createTuningConfig(2, null, false, true),\n            false\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":703,"status":"M"},{"authorDate":"2018-10-07 07:45:07","commitOrder":2,"curCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec = createIngestionSpec(\n        tmpDir,\n        new CSVParseSpec(\n            new TimestampSpec(\n                \"time\",\n                \"auto\",\n                null\n            ),\n            new DimensionsSpec(\n                null,\n                Lists.newArrayList(),\n                Lists.newArrayList()\n            ),\n            null,\n            Arrays.asList(\"time\", \"dim\", \"val\"),\n            true,\n            0\n        ),\n        null,\n        createTuningConfig(2, null, null, null, null, null, false, false, false), \r\n        false\n    );\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2018-10-07 07:45:07","endLine":964,"groupId":"19640","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testIgnoreParseException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/9d/4f6e7fb752711ed07a23a68ba15d56ebdcb044.src","preCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec = createIngestionSpec(\n        tmpDir,\n        new CSVParseSpec(\n            new TimestampSpec(\n                \"time\",\n                \"auto\",\n                null\n            ),\n            new DimensionsSpec(\n                null,\n                Lists.newArrayList(),\n                Lists.newArrayList()\n            ),\n            null,\n            Arrays.asList(\"time\", \"dim\", \"val\"),\n            true,\n            0\n        ),\n        null,\n        createTuningConfig(2, null, null, null, null, false, false, false), \r\n        false\n    );\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":912,"status":"M"}],"commitId":"45aa51a00c642a501834e2dfe54d68cbab8e0464","commitMessage":"@@@Add support hash partitioning by a subset of dimensions to indexTask (#6326)\n\n* Add support hash partitioning by a subset of dimensions to indexTask\n\n* add doc\n\n* fix style\n\n* fix test\n\n* fix doc\n\n* fix build\n","date":"2018-10-07 07:45:07","modifiedFileCount":"9","status":"M","submitter":"Jihoon Son"},{"authorTime":"2018-10-29 20:02:43","codes":[{"authorDate":"2018-10-29 20:02:43","commitOrder":3,"curCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        createIngestionSpec(\n            tmpDir,\n            new CSVParseSpec(\n                new TimestampSpec(\n                    \"time\",\n                    \"auto\",\n                    null\n                ),\n                new DimensionsSpec(\n                    null,\n                    new ArrayList<>(),\n                    new ArrayList<>()\n                ),\n                null,\n                Arrays.asList(\"time\", \"dim\", \"val\"),\n                true,\n                0\n            ),\n            null,\n            createTuningConfigWithTargetPartitionSize(2, false, true),\n            false\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2018-10-29 20:02:43","endLine":751,"groupId":"12639","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testCSVFileWithHeaderColumnOverride","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/a9/616bf930fbe7f8db06831692eff0ac35101a76.src","preCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        createIngestionSpec(\n            tmpDir,\n            new CSVParseSpec(\n                new TimestampSpec(\n                    \"time\",\n                    \"auto\",\n                    null\n                ),\n                new DimensionsSpec(\n                    null,\n                    Lists.newArrayList(),\n                    Lists.newArrayList()\n                ),\n                null,\n                Arrays.asList(\"time\", \"dim\", \"val\"),\n                true,\n                0\n            ),\n            null,\n            createTuningConfigWithTargetPartitionSize(2, false, true),\n            false\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":702,"status":"M"},{"authorDate":"2018-10-29 20:02:43","commitOrder":3,"curCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec = createIngestionSpec(\n        tmpDir,\n        new CSVParseSpec(\n            new TimestampSpec(\n                \"time\",\n                \"auto\",\n                null\n            ),\n            new DimensionsSpec(\n                null,\n                new ArrayList<>(),\n                new ArrayList<>()\n            ),\n            null,\n            Arrays.asList(\"time\", \"dim\", \"val\"),\n            true,\n            0\n        ),\n        null,\n        createTuningConfig(2, null, null, null, null, null, false, false, false), \r\n        false\n    );\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2018-10-29 20:02:43","endLine":963,"groupId":"3550","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testIgnoreParseException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/a9/616bf930fbe7f8db06831692eff0ac35101a76.src","preCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec = createIngestionSpec(\n        tmpDir,\n        new CSVParseSpec(\n            new TimestampSpec(\n                \"time\",\n                \"auto\",\n                null\n            ),\n            new DimensionsSpec(\n                null,\n                Lists.newArrayList(),\n                Lists.newArrayList()\n            ),\n            null,\n            Arrays.asList(\"time\", \"dim\", \"val\"),\n            true,\n            0\n        ),\n        null,\n        createTuningConfig(2, null, null, null, null, null, false, false, false), \r\n        false\n    );\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":911,"status":"M"}],"commitId":"676f5e6d7f184101b8763e4249b18b237bbe0ec7","commitMessage":"@@@Prohibit some guava collection APIs and use JDK collection APIs directly (#6511)\n\n* Prohibit some guava collection APIs and use JDK APIs directly\n\n* reset files that changed by accident\n\n* sort codestyle/druid-forbidden-apis.txt alphabetically\n","date":"2018-10-29 20:02:43","modifiedFileCount":"427","status":"M","submitter":"QiuMM"},{"authorTime":"2018-10-29 20:02:43","codes":[{"authorDate":"2019-01-11 01:50:14","commitOrder":4,"curCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        createIngestionSpec(\n            tmpDir,\n            new CSVParseSpec(\n                new TimestampSpec(\n                    \"time\",\n                    \"auto\",\n                    null\n                ),\n                new DimensionsSpec(\n                    null,\n                    new ArrayList<>(),\n                    new ArrayList<>()\n                ),\n                null,\n                Arrays.asList(\"time\", \"dim\", \"val\"),\n                true,\n                0\n            ),\n            null,\n            createTuningConfigWithMaxRowsPerSegment(2, false, true),\n            false\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2019-01-11 01:50:14","endLine":751,"groupId":"3548","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testCSVFileWithHeaderColumnOverride","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/cd/05698d82b78c3ad78b961169f4954f015c333b.src","preCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        createIngestionSpec(\n            tmpDir,\n            new CSVParseSpec(\n                new TimestampSpec(\n                    \"time\",\n                    \"auto\",\n                    null\n                ),\n                new DimensionsSpec(\n                    null,\n                    new ArrayList<>(),\n                    new ArrayList<>()\n                ),\n                null,\n                Arrays.asList(\"time\", \"dim\", \"val\"),\n                true,\n                0\n            ),\n            null,\n            createTuningConfigWithTargetPartitionSize(2, false, true),\n            false\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":702,"status":"M"},{"authorDate":"2018-10-29 20:02:43","commitOrder":4,"curCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec = createIngestionSpec(\n        tmpDir,\n        new CSVParseSpec(\n            new TimestampSpec(\n                \"time\",\n                \"auto\",\n                null\n            ),\n            new DimensionsSpec(\n                null,\n                new ArrayList<>(),\n                new ArrayList<>()\n            ),\n            null,\n            Arrays.asList(\"time\", \"dim\", \"val\"),\n            true,\n            0\n        ),\n        null,\n        createTuningConfig(2, null, null, null, null, null, false, false, false), \r\n        false\n    );\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2018-10-29 20:02:43","endLine":963,"groupId":"3550","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testIgnoreParseException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/a9/616bf930fbe7f8db06831692eff0ac35101a76.src","preCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec = createIngestionSpec(\n        tmpDir,\n        new CSVParseSpec(\n            new TimestampSpec(\n                \"time\",\n                \"auto\",\n                null\n            ),\n            new DimensionsSpec(\n                null,\n                new ArrayList<>(),\n                new ArrayList<>()\n            ),\n            null,\n            Arrays.asList(\"time\", \"dim\", \"val\"),\n            true,\n            0\n        ),\n        null,\n        createTuningConfig(2, null, null, null, null, null, false, false, false), \r\n        false\n    );\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":911,"status":"N"}],"commitId":"c35a39d70bf705aa49c3a3c97bab87959bb80a4e","commitMessage":"@@@Add support maxRowsPerSegment for auto compaction (#6780)\n\n* Add support maxRowsPerSegment for auto compaction\n\n* fix build\n\n* fix build\n\n* fix teamcity\n\n* add test\n\n* fix test\n\n* address comment\n","date":"2019-01-11 01:50:14","modifiedFileCount":"33","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-03-16 14:29:25","codes":[{"authorDate":"2019-03-16 14:29:25","commitOrder":5,"curCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        createIngestionSpec(\n            tmpDir,\n            new CSVParseSpec(\n                new TimestampSpec(\n                    \"time\",\n                    \"auto\",\n                    null\n                ),\n                new DimensionsSpec(\n                    null,\n                    new ArrayList<>(),\n                    new ArrayList<>()\n                ),\n                null,\n                Arrays.asList(\"time\", \"dim\", \"val\"),\n                true,\n                0\n            ),\n            null,\n            createTuningConfigWithMaxRowsPerSegment(2, true),\n            false\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2019-03-16 14:29:25","endLine":703,"groupId":"6204","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testCSVFileWithHeaderColumnOverride","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/97/279f94b6435aa72db32093179f35e7e99e99fd.src","preCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        createIngestionSpec(\n            tmpDir,\n            new CSVParseSpec(\n                new TimestampSpec(\n                    \"time\",\n                    \"auto\",\n                    null\n                ),\n                new DimensionsSpec(\n                    null,\n                    new ArrayList<>(),\n                    new ArrayList<>()\n                ),\n                null,\n                Arrays.asList(\"time\", \"dim\", \"val\"),\n                true,\n                0\n            ),\n            null,\n            createTuningConfigWithMaxRowsPerSegment(2, false, true),\n            false\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":654,"status":"M"},{"authorDate":"2019-03-16 14:29:25","commitOrder":5,"curCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec = createIngestionSpec(\n        tmpDir,\n        new CSVParseSpec(\n            new TimestampSpec(\n                \"time\",\n                \"auto\",\n                null\n            ),\n            new DimensionsSpec(\n                null,\n                new ArrayList<>(),\n                new ArrayList<>()\n            ),\n            null,\n            Arrays.asList(\"time\", \"dim\", \"val\"),\n            true,\n            0\n        ),\n        null,\n        createTuningConfig(2, null, null, null, null, null, false, false), \r\n        false\n    );\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2019-03-16 14:29:25","endLine":915,"groupId":"6206","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testIgnoreParseException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/97/279f94b6435aa72db32093179f35e7e99e99fd.src","preCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec = createIngestionSpec(\n        tmpDir,\n        new CSVParseSpec(\n            new TimestampSpec(\n                \"time\",\n                \"auto\",\n                null\n            ),\n            new DimensionsSpec(\n                null,\n                new ArrayList<>(),\n                new ArrayList<>()\n            ),\n            null,\n            Arrays.asList(\"time\", \"dim\", \"val\"),\n            true,\n            0\n        ),\n        null,\n        createTuningConfig(2, null, null, null, null, null, false, false, false), \r\n        false\n    );\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":863,"status":"M"}],"commitId":"892d1d35d6cc00487d583f05f1cc138782180ef9","commitMessage":"@@@Deprecate NoneShardSpec and drop support for automatic segment merge (#6883)\n\n* Deprecate noneShardSpec\n\n* clean up noneShardSpec constructor\n\n* revert unnecessary change\n\n* Deprecate mergeTask\n\n* add more doc\n\n* remove convert from indexMerger\n\n* Remove mergeTask\n\n* remove HadoopDruidConverterConfig\n\n* fix build\n\n* fix build\n\n* fix teamcity\n\n* fix teamcity\n\n* fix ServerModule\n\n* fix compilation\n\n* fix compilation\n","date":"2019-03-16 14:29:25","modifiedFileCount":"40","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-07-25 08:35:46","codes":[{"authorDate":"2019-07-25 08:35:46","commitOrder":6,"curCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        createIngestionSpec(\n            jsonMapper,\n            tmpDir,\n            new CSVParseSpec(\n                new TimestampSpec(\n                    \"time\",\n                    \"auto\",\n                    null\n                ),\n                new DimensionsSpec(\n                    null,\n                    new ArrayList<>(),\n                    new ArrayList<>()\n                ),\n                null,\n                Arrays.asList(\"time\", \"dim\", \"val\"),\n                true,\n                0\n            ),\n            null,\n            createTuningConfigWithMaxRowsPerSegment(2, true),\n            false\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2019-07-25 08:35:46","endLine":667,"groupId":"6204","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testCSVFileWithHeaderColumnOverride","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/45/ced1688e1327635091e00a3a7d366dfc7b9133.src","preCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        createIngestionSpec(\n            tmpDir,\n            new CSVParseSpec(\n                new TimestampSpec(\n                    \"time\",\n                    \"auto\",\n                    null\n                ),\n                new DimensionsSpec(\n                    null,\n                    new ArrayList<>(),\n                    new ArrayList<>()\n                ),\n                null,\n                Arrays.asList(\"time\", \"dim\", \"val\"),\n                true,\n                0\n            ),\n            null,\n            createTuningConfigWithMaxRowsPerSegment(2, true),\n            false\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":617,"status":"M"},{"authorDate":"2019-07-25 08:35:46","commitOrder":6,"curCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec = createIngestionSpec(\n        jsonMapper,\n        tmpDir,\n        new CSVParseSpec(\n            new TimestampSpec(\n                \"time\",\n                \"auto\",\n                null\n            ),\n            new DimensionsSpec(\n                null,\n                new ArrayList<>(),\n                new ArrayList<>()\n            ),\n            null,\n            Arrays.asList(\"time\", \"dim\", \"val\"),\n            true,\n            0\n        ),\n        null,\n        createTuningConfig(2, null, null, null, null, null, false, false), \r\n        false\n    );\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2019-07-25 08:35:46","endLine":883,"groupId":"6206","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testIgnoreParseException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/45/ced1688e1327635091e00a3a7d366dfc7b9133.src","preCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec = createIngestionSpec(\n        tmpDir,\n        new CSVParseSpec(\n            new TimestampSpec(\n                \"time\",\n                \"auto\",\n                null\n            ),\n            new DimensionsSpec(\n                null,\n                new ArrayList<>(),\n                new ArrayList<>()\n            ),\n            null,\n            Arrays.asList(\"time\", \"dim\", \"val\"),\n            true,\n            0\n        ),\n        null,\n        createTuningConfig(2, null, null, null, null, null, false, false), \r\n        false\n    );\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":830,"status":"M"}],"commitId":"db149462073d59e7563f0d3834e69d44a2bb4011","commitMessage":"@@@Add support minor compaction with segment locking (#7547)\n\n* Segment locking\n\n* Allow both timeChunk and segment lock in the same gruop\n\n* fix it test\n\n* Fix adding same chunk to atomicUpdateGroup\n\n* resolving todos\n\n* Fix segments to lock\n\n* fix segments to lock\n\n* fix kill task\n\n* resolving todos\n\n* resolving todos\n\n* fix teamcity\n\n* remove unused class\n\n* fix single map\n\n* resolving todos\n\n* fix build\n\n* fix SQLMetadataSegmentManager\n\n* fix findInputSegments\n\n* adding more tests\n\n* fixing task lock checks\n\n* add SegmentTransactionalOverwriteAction\n\n* changing publisher\n\n* fixing something\n\n* fix for perfect rollup\n\n* fix test\n\n* adjust package-lock.json\n\n* fix test\n\n* fix style\n\n* adding javadocs\n\n* remove unused classes\n\n* add more javadocs\n\n* unused import\n\n* fix test\n\n* fix test\n\n* Support forceTimeChunk context and force timeChunk lock for parallel index task if intervals are missing\n\n* fix travis\n\n* fix travis\n\n* unused import\n\n* spotbug\n\n* revert getMaxVersion\n\n* address comments\n\n* fix tc\n\n* add missing error handling\n\n* fix backward compatibility\n\n* unused import\n\n* Fix perf of versionedIntervalTimeline\n\n* fix timeline\n\n* fix tc\n\n* remove remaining todos\n\n* add comment for parallel index\n\n* fix javadoc and typos\n\n* typo\n\n* address comments\n","date":"2019-07-25 08:35:46","modifiedFileCount":"130","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-07-30 08:06:33","codes":[{"authorDate":"2019-07-30 08:06:33","commitOrder":7,"curCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        createIngestionSpec(\n            jsonMapper,\n            tmpDir,\n            new CSVParseSpec(\n                new TimestampSpec(\n                    \"time\",\n                    \"auto\",\n                    null\n                ),\n                new DimensionsSpec(\n                    null,\n                    new ArrayList<>(),\n                    new ArrayList<>()\n                ),\n                null,\n                Arrays.asList(\"time\", \"dim\", \"val\"),\n                true,\n                0\n            ),\n            null,\n            createTuningConfigWithMaxRowsPerSegment(2, true),\n            false\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2019-07-30 08:06:33","endLine":681,"groupId":"6204","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testCSVFileWithHeaderColumnOverride","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d3/150aeb5249e06c4f038c9972b2d9b93ea8fa7f.src","preCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        createIngestionSpec(\n            jsonMapper,\n            tmpDir,\n            new CSVParseSpec(\n                new TimestampSpec(\n                    \"time\",\n                    \"auto\",\n                    null\n                ),\n                new DimensionsSpec(\n                    null,\n                    new ArrayList<>(),\n                    new ArrayList<>()\n                ),\n                null,\n                Arrays.asList(\"time\", \"dim\", \"val\"),\n                true,\n                0\n            ),\n            null,\n            createTuningConfigWithMaxRowsPerSegment(2, true),\n            false\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":630,"status":"M"},{"authorDate":"2019-07-30 08:06:33","commitOrder":7,"curCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec = createIngestionSpec(\n        jsonMapper,\n        tmpDir,\n        new CSVParseSpec(\n            new TimestampSpec(\n                \"time\",\n                \"auto\",\n                null\n            ),\n            new DimensionsSpec(\n                null,\n                new ArrayList<>(),\n                new ArrayList<>()\n            ),\n            null,\n            Arrays.asList(\"time\", \"dim\", \"val\"),\n            true,\n            0\n        ),\n        null,\n        createTuningConfig(2, null, null, null, null, null, false, false), \r\n        false\n    );\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2019-07-30 08:06:33","endLine":901,"groupId":"6206","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testIgnoreParseException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d3/150aeb5249e06c4f038c9972b2d9b93ea8fa7f.src","preCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec = createIngestionSpec(\n        jsonMapper,\n        tmpDir,\n        new CSVParseSpec(\n            new TimestampSpec(\n                \"time\",\n                \"auto\",\n                null\n            ),\n            new DimensionsSpec(\n                null,\n                new ArrayList<>(),\n                new ArrayList<>()\n            ),\n            null,\n            Arrays.asList(\"time\", \"dim\", \"val\"),\n            true,\n            0\n        ),\n        null,\n        createTuningConfig(2, null, null, null, null, null, false, false), \r\n        false\n    );\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":847,"status":"M"}],"commitId":"640b7afc1cee911a27de7bf938dda24a85ba1510","commitMessage":"@@@Add CliIndexer process type and initial task runner implementation (#8107)\n\n* Add CliIndexer process type and initial task runner implementation\n\n* Fix HttpRemoteTaskRunnerTest\n\n* Remove batch sanity check on PeonAppenderatorsManager\n\n* Fix paralle index tests\n\n* PR comments\n\n* Adjust Jersey resource logging\n\n* Additional cleanup\n\n* Fix SystemSchemaTest\n\n* Add comment to LocalDataSegmentPusherTest absolute path test\n\n* More PR comments\n\n* Use Server annotated with RemoteChatHandler\n\n* More PR comments\n\n* Checkstyle\n\n* PR comments\n\n* Add task shutdown to stopGracefully\n\n* Small cleanup\n\n* Compile fix\n\n* Address PR comments\n\n* Adjust TaskReportFileWriter and fix nits\n\n* Remove unnecessary closer\n\n* More PR comments\n\n* Minor adjustments\n\n* PR comments\n\n* ThreadingTaskRunner: cancel  task run future not shutdownFuture and remove thread from workitem\n","date":"2019-07-30 08:06:33","modifiedFileCount":"64","status":"M","submitter":"Jonathan Wei"},{"authorTime":"2019-11-16 01:22:09","codes":[{"authorDate":"2019-11-16 01:22:09","commitOrder":8,"curCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        createIngestionSpec(\n            useInputFormatApi,\n            jsonMapper,\n            tmpDir,\n            new CSVParseSpec(\n                new TimestampSpec(\"time\", \"auto\", null),\n                DimensionsSpec.EMPTY,\n                null,\n                Arrays.asList(\"time\", \"dim\", \"val\"),\n                true,\n                0\n            ),\n            null,\n            createTuningConfigWithMaxRowsPerSegment(2, true),\n            false\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2019-11-16 01:22:09","endLine":756,"groupId":"5240","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testCSVFileWithHeaderColumnOverride","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f0/594724d325bc8485ada69fbe8de155449ee328.src","preCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        createIngestionSpec(\n            jsonMapper,\n            tmpDir,\n            new CSVParseSpec(\n                new TimestampSpec(\n                    \"time\",\n                    \"auto\",\n                    null\n                ),\n                new DimensionsSpec(\n                    null,\n                    new ArrayList<>(),\n                    new ArrayList<>()\n                ),\n                null,\n                Arrays.asList(\"time\", \"dim\", \"val\"),\n                true,\n                0\n            ),\n            null,\n            createTuningConfigWithMaxRowsPerSegment(2, true),\n            false\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":712,"status":"M"},{"authorDate":"2019-11-16 01:22:09","commitOrder":8,"curCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec = createIngestionSpec(\n        useInputFormatApi,\n        jsonMapper,\n        tmpDir,\n        new CSVParseSpec(\n            new TimestampSpec(\"time\", \"auto\", null),\n            DimensionsSpec.EMPTY,\n            null,\n            Arrays.asList(\"time\", \"dim\", \"val\"),\n            true,\n            0\n        ),\n        null,\n        createTuningConfig(2, null, null, null, null, null, false, false), \r\n        false\n    );\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2019-11-16 01:22:09","endLine":972,"groupId":"5245","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testIgnoreParseException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f0/594724d325bc8485ada69fbe8de155449ee328.src","preCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec = createIngestionSpec(\n        jsonMapper,\n        tmpDir,\n        new CSVParseSpec(\n            new TimestampSpec(\n                \"time\",\n                \"auto\",\n                null\n            ),\n            new DimensionsSpec(\n                null,\n                new ArrayList<>(),\n                new ArrayList<>()\n            ),\n            null,\n            Arrays.asList(\"time\", \"dim\", \"val\"),\n            true,\n            0\n        ),\n        null,\n        createTuningConfig(2, null, null, null, null, null, false, false), \r\n        false\n    );\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":925,"status":"M"}],"commitId":"1611792855ad9def8b6f5b1375862d05d1acca0a","commitMessage":"@@@Add InputSource and InputFormat interfaces (#8823)\n\n* Add InputSource and InputFormat interfaces\n\n* revert orc dependency\n\n* fix dimension exclusions and failing unit tests\n\n* fix tests\n\n* fix test\n\n* fix test\n\n* fix firehose and inputSource for parallel indexing task\n\n* fix tc\n\n* fix tc: remove unused method\n\n* Formattable\n\n* add needsFormat(); renamed to ObjectSource; pass metricsName for reader\n\n* address comments\n\n* fix closing resource\n\n* fix checkstyle\n\n* fix tests\n\n* remove verify from csv\n\n* Revert \"remove verify from csv\"\n\nThis reverts commit 1ea7758489cc8c9d708bd691fd48e62085fd9455.\n\n* address comments\n\n* fix import order and javadoc\n\n* flatMap\n\n* sampleLine\n\n* Add IntermediateRowParsingReader\n\n* Address comments\n\n* move csv reader test\n\n* remove test for verify\n\n* adjust comments\n\n* Fix InputEntityIteratingReader\n\n* rename source -> entity\n\n* address comments\n","date":"2019-11-16 01:22:09","modifiedFileCount":"72","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-02-08 08:23:07","codes":[{"authorDate":"2019-11-16 01:22:09","commitOrder":9,"curCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        createIngestionSpec(\n            useInputFormatApi,\n            jsonMapper,\n            tmpDir,\n            new CSVParseSpec(\n                new TimestampSpec(\"time\", \"auto\", null),\n                DimensionsSpec.EMPTY,\n                null,\n                Arrays.asList(\"time\", \"dim\", \"val\"),\n                true,\n                0\n            ),\n            null,\n            createTuningConfigWithMaxRowsPerSegment(2, true),\n            false\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2019-11-16 01:22:09","endLine":756,"groupId":"5240","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"testCSVFileWithHeaderColumnOverride","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/f0/594724d325bc8485ada69fbe8de155449ee328.src","preCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        createIngestionSpec(\n            useInputFormatApi,\n            jsonMapper,\n            tmpDir,\n            new CSVParseSpec(\n                new TimestampSpec(\"time\", \"auto\", null),\n                DimensionsSpec.EMPTY,\n                null,\n                Arrays.asList(\"time\", \"dim\", \"val\"),\n                true,\n                0\n            ),\n            null,\n            createTuningConfigWithMaxRowsPerSegment(2, true),\n            false\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":712,"status":"N"},{"authorDate":"2020-02-08 08:23:07","commitOrder":9,"curCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec = createIngestionSpec(\n        useInputFormatApi,\n        jsonMapper,\n        tmpDir,\n        new CSVParseSpec(\n            new TimestampSpec(\"time\", \"auto\", null),\n            DimensionsSpec.EMPTY,\n            null,\n            Arrays.asList(\"time\", \"dim\", \"val\"),\n            true,\n            0\n        ),\n        null,\n        createTuningConfig(2, null, null, null, null, false, false), \r\n        false\n    );\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2020-02-08 08:23:07","endLine":975,"groupId":"22807","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testIgnoreParseException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/e3/93d7536af99bd8b5187981c3440cf665b59187.src","preCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec = createIngestionSpec(\n        useInputFormatApi,\n        jsonMapper,\n        tmpDir,\n        new CSVParseSpec(\n            new TimestampSpec(\"time\", \"auto\", null),\n            DimensionsSpec.EMPTY,\n            null,\n            Arrays.asList(\"time\", \"dim\", \"val\"),\n            true,\n            0\n        ),\n        null,\n        createTuningConfig(2, null, null, null, null, null, false, false), \r\n        false\n    );\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":928,"status":"M"}],"commitId":"e81230f9abd092b2ac742258abf534f6e17f35e8","commitMessage":"@@@Refactoring some codes around ingestion (#9274)\n\n* Refactoring codes around ingestion:\n\n- Parallel index task and simple task now use the same segment allocator implementation. This is reusable for the future implementation as well.\n- Added PartitionAnalysis to store the analysis of the partitioning\n- Move some util methods to SegmentLockHelper and rename it to TaskLockHelper\n\n* fix build\n\n* fix SingleDimensionShardSpecFactory\n\n* optimize SingledimensionShardSpecFactory\n\n* fix test\n\n* shard spec builder\n\n* import order\n\n* shardSpecBuilder -> partialShardSpec\n\n* build -> complete\n\n* fix comment; add unit tests for partitionBoundaries\n\n* add more tests and fix javadoc\n\n* fix toString(); add serde tests for HashBasedNumberedPartialShardSpec and SegmentAllocateAction\n\n* fix test\n\n* add equality test for hash and range partial shard specs\n","date":"2020-02-08 08:23:07","modifiedFileCount":"49","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-05-06 02:17:57","codes":[{"authorDate":"2020-05-06 02:17:57","commitOrder":10,"curCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    final TimestampSpec timestampSpec = new TimestampSpec(\"time\", \"auto\", null);\n    final List<String> columns = Arrays.asList(\"time\", \"dim\", \"val\");\n    final IndexTuningConfig tuningConfig = createTuningConfigWithMaxRowsPerSegment(2, true);\n    final IndexIngestionSpec ingestionSpec;\n    if (useInputFormatApi) {\n      ingestionSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          new CSVParseSpec(timestampSpec, DimensionsSpec.EMPTY, null, columns, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false\n      );\n    } else {\n      ingestionSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          timestampSpec,\n          DimensionsSpec.EMPTY,\n          new CsvInputFormat(columns, null, null, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false\n      );\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        ingestionSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2020-05-06 02:17:57","endLine":803,"groupId":"141","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"testCSVFileWithHeaderColumnOverride","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/07/1862ea678d0bff086612c51f6ad333fc119125.src","preCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        createIngestionSpec(\n            useInputFormatApi,\n            jsonMapper,\n            tmpDir,\n            new CSVParseSpec(\n                new TimestampSpec(\"time\", \"auto\", null),\n                DimensionsSpec.EMPTY,\n                null,\n                Arrays.asList(\"time\", \"dim\", \"val\"),\n                true,\n                0\n            ),\n            null,\n            createTuningConfigWithMaxRowsPerSegment(2, true),\n            false\n        ),\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":746,"status":"M"},{"authorDate":"2020-05-06 02:17:57","commitOrder":10,"curCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    final TimestampSpec timestampSpec = new TimestampSpec(\"time\", \"auto\", null);\n    final List<String> columns = Arrays.asList(\"time\", \"dim\", \"val\");\n    \r\n    final IndexTuningConfig tuningConfig = createTuningConfig(2, null, null, null, null, false, false);\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec;\n    if (useInputFormatApi) {\n      parseExceptionIgnoreSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          timestampSpec,\n          DimensionsSpec.EMPTY,\n          new CsvInputFormat(columns, null, null, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false\n      );\n    } else {\n      parseExceptionIgnoreSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          new CSVParseSpec(timestampSpec, DimensionsSpec.EMPTY, null, columns, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false\n      );\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2020-05-06 02:17:57","endLine":1029,"groupId":"7467","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"testIgnoreParseException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/07/1862ea678d0bff086612c51f6ad333fc119125.src","preCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec = createIngestionSpec(\n        useInputFormatApi,\n        jsonMapper,\n        tmpDir,\n        new CSVParseSpec(\n            new TimestampSpec(\"time\", \"auto\", null),\n            DimensionsSpec.EMPTY,\n            null,\n            Arrays.asList(\"time\", \"dim\", \"val\"),\n            true,\n            0\n        ),\n        null,\n        createTuningConfig(2, null, null, null, null, false, false), \r\n        false\n    );\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":969,"status":"M"}],"commitId":"964a1fc9df69520e54e10edd33e0404ed6e8330d","commitMessage":"@@@Remove ParseSpec.toInputFormat() (#9815)\n\n* Remove toInputFormat() from ParseSpec\n\n* fix test","date":"2020-05-06 02:17:57","modifiedFileCount":"24","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-08-27 08:08:12","codes":[{"authorDate":"2020-08-27 08:08:12","commitOrder":11,"curCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    final TimestampSpec timestampSpec = new TimestampSpec(\"time\", \"auto\", null);\n    final List<String> columns = Arrays.asList(\"time\", \"dim\", \"val\");\n    final IndexTuningConfig tuningConfig = createTuningConfigWithMaxRowsPerSegment(2, true);\n    final IndexIngestionSpec ingestionSpec;\n    if (useInputFormatApi) {\n      ingestionSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          new CSVParseSpec(timestampSpec, DimensionsSpec.EMPTY, null, columns, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false\n      );\n    } else {\n      ingestionSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          timestampSpec,\n          DimensionsSpec.EMPTY,\n          new CsvInputFormat(columns, null, null, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false\n      );\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        ingestionSpec,\n        null\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2020-08-27 08:08:12","endLine":764,"groupId":"141","id":21,"instanceNumber":1,"isCurCommit":0,"methodName":"testCSVFileWithHeaderColumnOverride","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/7b/b25791a130e5c1990f4b27d5733cac8ecc4a14.src","preCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    final TimestampSpec timestampSpec = new TimestampSpec(\"time\", \"auto\", null);\n    final List<String> columns = Arrays.asList(\"time\", \"dim\", \"val\");\n    final IndexTuningConfig tuningConfig = createTuningConfigWithMaxRowsPerSegment(2, true);\n    final IndexIngestionSpec ingestionSpec;\n    if (useInputFormatApi) {\n      ingestionSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          new CSVParseSpec(timestampSpec, DimensionsSpec.EMPTY, null, columns, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false\n      );\n    } else {\n      ingestionSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          timestampSpec,\n          DimensionsSpec.EMPTY,\n          new CsvInputFormat(columns, null, null, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false\n      );\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        ingestionSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":711,"status":"M"},{"authorDate":"2020-08-27 08:08:12","commitOrder":11,"curCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    final TimestampSpec timestampSpec = new TimestampSpec(\"time\", \"auto\", null);\n    final List<String> columns = Arrays.asList(\"time\", \"dim\", \"val\");\n    \r\n    final IndexTuningConfig tuningConfig = createTuningConfig(2, null, null, null, null, false, false);\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec;\n    if (useInputFormatApi) {\n      parseExceptionIgnoreSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          timestampSpec,\n          DimensionsSpec.EMPTY,\n          new CsvInputFormat(columns, null, null, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false\n      );\n    } else {\n      parseExceptionIgnoreSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          new CSVParseSpec(timestampSpec, DimensionsSpec.EMPTY, null, columns, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false\n      );\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2020-08-27 08:08:12","endLine":974,"groupId":"7467","id":22,"instanceNumber":2,"isCurCommit":0,"methodName":"testIgnoreParseException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/7b/b25791a130e5c1990f4b27d5733cac8ecc4a14.src","preCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    final TimestampSpec timestampSpec = new TimestampSpec(\"time\", \"auto\", null);\n    final List<String> columns = Arrays.asList(\"time\", \"dim\", \"val\");\n    \r\n    final IndexTuningConfig tuningConfig = createTuningConfig(2, null, null, null, null, false, false);\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec;\n    if (useInputFormatApi) {\n      parseExceptionIgnoreSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          timestampSpec,\n          DimensionsSpec.EMPTY,\n          new CsvInputFormat(columns, null, null, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false\n      );\n    } else {\n      parseExceptionIgnoreSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          new CSVParseSpec(timestampSpec, DimensionsSpec.EMPTY, null, columns, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false\n      );\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null,\n        AuthTestUtils.TEST_AUTHORIZER_MAPPER,\n        null,\n        rowIngestionMetersFactory,\n        appenderatorsManager\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":918,"status":"M"}],"commitId":"f82fd22fa7de175200b7127c34c2eb2900bf7317","commitMessage":"@@@Move tools for indexing to TaskToolbox instead of injecting them in constructor (#10308)\n\n* Move tools for indexing to TaskToolbox instead of injecting them in constructor\n\n* oops.  other changes\n\n* fix test\n\n* unnecessary new file\n\n* fix test\n\n* fix build","date":"2020-08-27 08:08:12","modifiedFileCount":"67","status":"M","submitter":"Jihoon Son"},{"authorTime":"2021-04-02 03:29:36","codes":[{"authorDate":"2021-04-02 03:29:36","commitOrder":12,"curCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    final TimestampSpec timestampSpec = new TimestampSpec(\"time\", \"auto\", null);\n    final List<String> columns = Arrays.asList(\"time\", \"dim\", \"val\");\n    final IndexTuningConfig tuningConfig = createTuningConfigWithMaxRowsPerSegment(2, true);\n    final IndexIngestionSpec ingestionSpec;\n    if (useInputFormatApi) {\n      ingestionSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          new CSVParseSpec(timestampSpec, DimensionsSpec.EMPTY, null, columns, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false,\n          false\n      );\n    } else {\n      ingestionSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          timestampSpec,\n          DimensionsSpec.EMPTY,\n          new CsvInputFormat(columns, null, null, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false,\n          false\n      );\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        ingestionSpec,\n        null\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2021-04-02 03:29:36","endLine":835,"groupId":"104584","id":23,"instanceNumber":1,"isCurCommit":0,"methodName":"testCSVFileWithHeaderColumnOverride","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/33/64c422440acebe917c9fa96f04d4197ede7d97.src","preCode":"  public void testCSVFileWithHeaderColumnOverride() throws Exception\n  {\n    File tmpDir = temporaryFolder.newFolder();\n\n    File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    final TimestampSpec timestampSpec = new TimestampSpec(\"time\", \"auto\", null);\n    final List<String> columns = Arrays.asList(\"time\", \"dim\", \"val\");\n    final IndexTuningConfig tuningConfig = createTuningConfigWithMaxRowsPerSegment(2, true);\n    final IndexIngestionSpec ingestionSpec;\n    if (useInputFormatApi) {\n      ingestionSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          new CSVParseSpec(timestampSpec, DimensionsSpec.EMPTY, null, columns, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false\n      );\n    } else {\n      ingestionSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          timestampSpec,\n          DimensionsSpec.EMPTY,\n          new CsvInputFormat(columns, null, null, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false\n      );\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        ingestionSpec,\n        null\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(1, segments.size());\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":780,"status":"M"},{"authorDate":"2021-04-02 03:29:36","commitOrder":12,"curCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    final TimestampSpec timestampSpec = new TimestampSpec(\"time\", \"auto\", null);\n    final List<String> columns = Arrays.asList(\"time\", \"dim\", \"val\");\n    \r\n    final IndexTuningConfig tuningConfig = createTuningConfig(2, null, null, null, null, false, false);\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec;\n    if (useInputFormatApi) {\n      parseExceptionIgnoreSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          timestampSpec,\n          DimensionsSpec.EMPTY,\n          new CsvInputFormat(columns, null, null, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false,\n          false\n      );\n    } else {\n      parseExceptionIgnoreSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          new CSVParseSpec(timestampSpec, DimensionsSpec.EMPTY, null, columns, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false,\n          false\n      );\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","date":"2021-04-02 03:29:36","endLine":1050,"groupId":"104584","id":24,"instanceNumber":2,"isCurCommit":0,"methodName":"testIgnoreParseException","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/33/64c422440acebe917c9fa96f04d4197ede7d97.src","preCode":"  public void testIgnoreParseException() throws Exception\n  {\n    final File tmpDir = temporaryFolder.newFolder();\n\n    final File tmpFile = File.createTempFile(\"druid\", \"index\", tmpDir);\n\n    try (BufferedWriter writer = Files.newWriter(tmpFile, StandardCharsets.UTF_8)) {\n      writer.write(\"time,d,val\\n\");\n      writer.write(\"unparseable,a,1\\n\");\n      writer.write(\"2014-01-01T00:00:10Z,a,1\\n\");\n    }\n\n    final TimestampSpec timestampSpec = new TimestampSpec(\"time\", \"auto\", null);\n    final List<String> columns = Arrays.asList(\"time\", \"dim\", \"val\");\n    \r\n    final IndexTuningConfig tuningConfig = createTuningConfig(2, null, null, null, null, false, false);\n\n    \r\n    \r\n    final IndexIngestionSpec parseExceptionIgnoreSpec;\n    if (useInputFormatApi) {\n      parseExceptionIgnoreSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          timestampSpec,\n          DimensionsSpec.EMPTY,\n          new CsvInputFormat(columns, null, null, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false\n      );\n    } else {\n      parseExceptionIgnoreSpec = createIngestionSpec(\n          jsonMapper,\n          tmpDir,\n          new CSVParseSpec(timestampSpec, DimensionsSpec.EMPTY, null, columns, true, 0),\n          null,\n          null,\n          tuningConfig,\n          false\n      );\n    }\n\n    IndexTask indexTask = new IndexTask(\n        null,\n        null,\n        parseExceptionIgnoreSpec,\n        null\n    );\n\n    final List<DataSegment> segments = runTask(indexTask).rhs;\n\n    Assert.assertEquals(Collections.singletonList(\"d\"), segments.get(0).getDimensions());\n    Assert.assertEquals(Collections.singletonList(\"val\"), segments.get(0).getMetrics());\n    Assert.assertEquals(Intervals.of(\"2014/P1D\"), segments.get(0).getInterval());\n  }\n","realPath":"indexing-service/src/test/java/org/apache/druid/indexing/common/task/IndexTaskTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":992,"status":"M"}],"commitId":"d7f529336463dad273a742808b49d524bdc4ae11","commitMessage":"@@@Add an option for ingestion task to drop (mark unused) all existing segments that are contained by interval in the ingestionSpec (#11025)\n\n* Auto-Compaction can run indefinitely when segmentGranularity is changed from coarser to finer.\n\n* Add option to drop segments after ingestion\n\n* fix checkstyle\n\n* add tests\n\n* add tests\n\n* add tests\n\n* fix test\n\n* add tests\n\n* fix checkstyle\n\n* fix checkstyle\n\n* add docs\n\n* fix docs\n\n* address comments\n\n* address comments\n\n* fix spelling","date":"2021-04-02 03:29:36","modifiedFileCount":"44","status":"M","submitter":"Maytas Monsereenusorn"}]
