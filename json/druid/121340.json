[{"authorTime":"2018-08-31 00:56:26","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = Maps.newTreeMap();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","date":"2018-08-31 00:56:26","endLine":263,"groupId":"16663","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ec/1039dde39d149f472d5b81d34fe7b69ab06523.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = Maps.newTreeMap();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":117,"status":"B"},{"authorDate":"2018-08-31 00:56:26","commitOrder":1,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      final long startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      if (!groupByJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", groupByJob.getJobID());\n        failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = config.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = Maps.newTreeMap(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              new TypeReference<Long>() {}\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          if (numberOfShards == 1) {\n            actualSpecs.add(new HadoopyShardSpec(NoneShardSpec.instance(), shardCount++));\n          } else {\n            for (int i = 0; i < numberOfShards; ++i) {\n              actualSpecs.add(\n                  new HadoopyShardSpec(\n                      new HashBasedNumberedShardSpec(\n                          i,\n                          numberOfShards,\n                          null,\n                          HadoopDruidIndexerConfig.JSON_MAPPER\n                      ),\n                      shardCount++\n                  )\n              );\n              log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n            }\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","date":"2018-08-31 00:56:26","endLine":217,"groupId":"7096","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/6f/b8a81bf1282b5c17eae33d855e457ffa1582c3.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      final long startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      if (!groupByJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", groupByJob.getJobID());\n        failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = config.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = Maps.newTreeMap(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              new TypeReference<Long>() {}\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          if (numberOfShards == 1) {\n            actualSpecs.add(new HadoopyShardSpec(NoneShardSpec.instance(), shardCount++));\n          } else {\n            for (int i = 0; i < numberOfShards; ++i) {\n              actualSpecs.add(\n                  new HadoopyShardSpec(\n                      new HashBasedNumberedShardSpec(\n                          i,\n                          numberOfShards,\n                          null,\n                          HadoopDruidIndexerConfig.JSON_MAPPER\n                      ),\n                      shardCount++\n                  )\n              );\n              log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n            }\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DetermineHashedPartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":85,"status":"B"}],"commitId":"431d3d8497f9079857c3baa7ae7ab4cb44a22355","commitMessage":"@@@Rename io.druid to org.apache.druid. (#6266)\n\n* Rename io.druid to org.apache.druid.\n\n* Fix META-INF files and remove some benchmark results.\n\n* MonitorsConfig update for metrics package migration.\n\n* Reorder some dimensions in inner queries for some reason.\n\n* Fix protobuf tests.\n","date":"2018-08-31 00:56:26","modifiedFileCount":"5","status":"B","submitter":"Gian Merlino"},{"authorTime":"2018-10-23 18:17:38","codes":[{"authorDate":"2018-08-31 00:56:26","commitOrder":2,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = Maps.newTreeMap();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","date":"2018-08-31 00:56:26","endLine":263,"groupId":"16663","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/ec/1039dde39d149f472d5b81d34fe7b69ab06523.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = Maps.newTreeMap();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":117,"status":"N"},{"authorDate":"2018-10-23 18:17:38","commitOrder":2,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      final long startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      if (!groupByJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", groupByJob.getJobID());\n        failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = config.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = Maps.newTreeMap(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          if (numberOfShards == 1) {\n            actualSpecs.add(new HadoopyShardSpec(NoneShardSpec.instance(), shardCount++));\n          } else {\n            for (int i = 0; i < numberOfShards; ++i) {\n              actualSpecs.add(\n                  new HadoopyShardSpec(\n                      new HashBasedNumberedShardSpec(\n                          i,\n                          numberOfShards,\n                          null,\n                          HadoopDruidIndexerConfig.JSON_MAPPER\n                      ),\n                      shardCount++\n                  )\n              );\n              log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n            }\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","date":"2018-10-23 18:17:38","endLine":217,"groupId":"7096","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/12/d4c487c1fa0565dda0e93ff48b747e61d37dbd.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      final long startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      if (!groupByJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", groupByJob.getJobID());\n        failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = config.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = Maps.newTreeMap(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              new TypeReference<Long>() {}\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          if (numberOfShards == 1) {\n            actualSpecs.add(new HadoopyShardSpec(NoneShardSpec.instance(), shardCount++));\n          } else {\n            for (int i = 0; i < numberOfShards; ++i) {\n              actualSpecs.add(\n                  new HadoopyShardSpec(\n                      new HashBasedNumberedShardSpec(\n                          i,\n                          numberOfShards,\n                          null,\n                          HadoopDruidIndexerConfig.JSON_MAPPER\n                      ),\n                      shardCount++\n                  )\n              );\n              log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n            }\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DetermineHashedPartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":85,"status":"M"}],"commitId":"84ac18dc1bce14afe88ebcccd46da21baefae73d","commitMessage":"@@@Catch some incorrect method parameter or call argument formatting patterns with checkstyle (#6461)\n\n* Catch some incorrect method parameter or call argument formatting patterns with checkstyle\n\n* Fix DiscoveryModule\n\n* Inline parameters_and_arguments.txt\n\n* Fix a bug in PolyBind\n\n* Fix formatting\n","date":"2018-10-23 18:17:38","modifiedFileCount":"339","status":"M","submitter":"Roman Leventov"},{"authorTime":"2018-10-29 20:02:43","codes":[{"authorDate":"2018-10-29 20:02:43","commitOrder":3,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","date":"2018-10-29 20:02:43","endLine":265,"groupId":"16663","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/27/cc9c3c2159e9b9c86560c9ccb6cd61debf6f83.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = Maps.newTreeMap();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":119,"status":"M"},{"authorDate":"2018-10-29 20:02:43","commitOrder":3,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      final long startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      if (!groupByJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", groupByJob.getJobID());\n        failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = config.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          if (numberOfShards == 1) {\n            actualSpecs.add(new HadoopyShardSpec(NoneShardSpec.instance(), shardCount++));\n          } else {\n            for (int i = 0; i < numberOfShards; ++i) {\n              actualSpecs.add(\n                  new HadoopyShardSpec(\n                      new HashBasedNumberedShardSpec(\n                          i,\n                          numberOfShards,\n                          null,\n                          HadoopDruidIndexerConfig.JSON_MAPPER\n                      ),\n                      shardCount++\n                  )\n              );\n              log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n            }\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","date":"2018-10-29 20:02:43","endLine":219,"groupId":"7096","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c8/696b563a7051968d544dce1b8a9b91fe6dc656.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      final long startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      if (!groupByJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", groupByJob.getJobID());\n        failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = config.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = Maps.newTreeMap(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          if (numberOfShards == 1) {\n            actualSpecs.add(new HadoopyShardSpec(NoneShardSpec.instance(), shardCount++));\n          } else {\n            for (int i = 0; i < numberOfShards; ++i) {\n              actualSpecs.add(\n                  new HadoopyShardSpec(\n                      new HashBasedNumberedShardSpec(\n                          i,\n                          numberOfShards,\n                          null,\n                          HadoopDruidIndexerConfig.JSON_MAPPER\n                      ),\n                      shardCount++\n                  )\n              );\n              log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n            }\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DetermineHashedPartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":87,"status":"M"}],"commitId":"676f5e6d7f184101b8763e4249b18b237bbe0ec7","commitMessage":"@@@Prohibit some guava collection APIs and use JDK collection APIs directly (#6511)\n\n* Prohibit some guava collection APIs and use JDK APIs directly\n\n* reset files that changed by accident\n\n* sort codestyle/druid-forbidden-apis.txt alphabetically\n","date":"2018-10-29 20:02:43","modifiedFileCount":"427","status":"M","submitter":"QiuMM"},{"authorTime":"2019-01-26 07:43:06","codes":[{"authorDate":"2019-01-26 07:43:06","commitOrder":4,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","date":"2019-01-26 07:43:06","endLine":277,"groupId":"10997","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d8/c8ae2c5d94e5772a22c203b7af829b75a6b315.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":119,"status":"M"},{"authorDate":"2019-01-26 07:43:06","commitOrder":4,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      if (!groupByJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", groupByJob.getJobID());\n        failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = config.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          if (numberOfShards == 1) {\n            actualSpecs.add(new HadoopyShardSpec(NoneShardSpec.instance(), shardCount++));\n          } else {\n            for (int i = 0; i < numberOfShards; ++i) {\n              actualSpecs.add(\n                  new HadoopyShardSpec(\n                      new HashBasedNumberedShardSpec(\n                          i,\n                          numberOfShards,\n                          null,\n                          HadoopDruidIndexerConfig.JSON_MAPPER\n                      ),\n                      shardCount++\n                  )\n              );\n              log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n            }\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","date":"2019-01-26 07:43:06","endLine":225,"groupId":"15415","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/97/6b78d718a20d9b07d6ff5344ca276d4eab2167.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      final long startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      if (!groupByJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", groupByJob.getJobID());\n        failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = config.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          if (numberOfShards == 1) {\n            actualSpecs.add(new HadoopyShardSpec(NoneShardSpec.instance(), shardCount++));\n          } else {\n            for (int i = 0; i < numberOfShards; ++i) {\n              actualSpecs.add(\n                  new HadoopyShardSpec(\n                      new HashBasedNumberedShardSpec(\n                          i,\n                          numberOfShards,\n                          null,\n                          HadoopDruidIndexerConfig.JSON_MAPPER\n                      ),\n                      shardCount++\n                  )\n              );\n              log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n            }\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DetermineHashedPartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":88,"status":"M"}],"commitId":"8492d94f599da1f7851add2a0e7500515abd881d","commitMessage":"@@@Kill Hadoop MR task on kill of Hadoop ingestion task  (#6828)\n\n* KillTask from overlord UI now makes sure that it terminates the underlying MR job.  thus saving unnecessary compute\n\nRun in jobby is now split into 2\n 1. submitAndGetHadoopJobId followed by 2. run\n  submitAndGetHadoopJobId is responsible for submitting the job and returning the jobId as a string.  run monitors this job for completion\n\nJobHelper writes this jobId in the path provided by HadoopIndexTask which in turn is provided by the ForkingTaskRunner\n\nHadoopIndexTask reads this path when kill task is clicked to get hte jobId and fire the kill command via the yarn api. This is taken care in the stopGracefully method which is called in SingleTaskBackgroundRunner. Have enabled `canRestore` method to return `true` for HadoopIndexTask in order for the stopGracefully method to be called\n\nHadoop*Job files have been changed to incorporate the changes to jobby\n\n* Addressing PR comments\n\n* Addressing PR comments - Fix taskDir\n\n* Addressing PR comments - For changing the contract of Task.stopGracefully()\n`SingleTaskBackgroundRunner` calls stopGracefully in stop() and then checks for canRestore condition to return the status of the task\n\n* Addressing PR comments\n 1. Formatting\n 2. Removing `submitAndGetHadoopJobId` from `Jobby` and calling writeJobIdToFile in the job itself\n\n* Addressing PR comments\n 1. POM change. Moving hadoop dependency to indexing-hadoop\n\n* Addressing PR comments\n 1. stopGracefully now accepts TaskConfig as a param\n     Handling isRestoreOnRestart in stopGracefully for `AppenderatorDriverRealtimeIndexTask.  RealtimeIndexTask.  SeekableStreamIndexTask`\n     Changing tests to make TaskConfig param isRestoreOnRestart to true\n","date":"2019-01-26 07:43:06","modifiedFileCount":"20","status":"M","submitter":"Ankit Kothari"},{"authorTime":"2019-03-15 05:28:33","codes":[{"authorDate":"2019-03-15 05:28:33","commitOrder":5,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2019-03-15 05:28:33","endLine":276,"groupId":"10997","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/7d/ad5c6bd840d81bc8ca42df42d8a0185821c6a2.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":118,"status":"M"},{"authorDate":"2019-03-15 05:28:33","commitOrder":5,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      if (!groupByJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", groupByJob.getJobID());\n        failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = config.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          if (numberOfShards == 1) {\n            actualSpecs.add(new HadoopyShardSpec(NoneShardSpec.instance(), shardCount++));\n          } else {\n            for (int i = 0; i < numberOfShards; ++i) {\n              actualSpecs.add(\n                  new HadoopyShardSpec(\n                      new HashBasedNumberedShardSpec(\n                          i,\n                          numberOfShards,\n                          null,\n                          HadoopDruidIndexerConfig.JSON_MAPPER\n                      ),\n                      shardCount++\n                  )\n              );\n              log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n            }\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2019-03-15 05:28:33","endLine":224,"groupId":"15415","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b4/b0d8ee17eea80c31d97e7e2828e6ee77311513.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      if (!groupByJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", groupByJob.getJobID());\n        failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = config.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          if (numberOfShards == 1) {\n            actualSpecs.add(new HadoopyShardSpec(NoneShardSpec.instance(), shardCount++));\n          } else {\n            for (int i = 0; i < numberOfShards; ++i) {\n              actualSpecs.add(\n                  new HadoopyShardSpec(\n                      new HashBasedNumberedShardSpec(\n                          i,\n                          numberOfShards,\n                          null,\n                          HadoopDruidIndexerConfig.JSON_MAPPER\n                      ),\n                      shardCount++\n                  )\n              );\n              log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n            }\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw Throwables.propagate(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DetermineHashedPartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":87,"status":"M"}],"commitId":"7ada1c49f9735a37808f3ed7656d93ae88b8b925","commitMessage":"@@@Prohibit Throwables.propagate() (#7121)\n\n* Throw caught exception.\n\n* Throw caught exceptions.\n\n* Related checkstyle rule is added to prevent further bugs.\n\n* RuntimeException() is used instead of Throwables.propagate().\n\n* Missing import is added.\n\n* Throwables are propogated if possible.\n\n* Throwables are propogated if possible.\n\n* Throwables are propogated if possible.\n\n* Throwables are propogated if possible.\n\n* * Checkstyle definition is improved.\n* Throwables.propagate() usages are removed.\n\n* Checkstyle pattern is changed for only scanning \"Throwables.propagate(\" instead of checking lookbehind.\n\n* Throwable is kept before firing a Runtime Exception.\n\n* Fix unused assignments.\n","date":"2019-03-15 05:28:33","modifiedFileCount":"228","status":"M","submitter":"Furkan KAMACI"},{"authorTime":"2019-03-16 14:29:25","codes":[{"authorDate":"2019-03-15 05:28:33","commitOrder":6,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2019-03-15 05:28:33","endLine":276,"groupId":"10997","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/7d/ad5c6bd840d81bc8ca42df42d8a0185821c6a2.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":118,"status":"N"},{"authorDate":"2019-03-16 14:29:25","commitOrder":6,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      if (!groupByJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", groupByJob.getJobID());\n        failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = config.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          for (int i = 0; i < numberOfShards; ++i) {\n            actualSpecs.add(\n                new HadoopyShardSpec(\n                    new HashBasedNumberedShardSpec(\n                        i,\n                        numberOfShards,\n                        null,\n                        HadoopDruidIndexerConfig.JSON_MAPPER\n                    ),\n                    shardCount++\n                )\n            );\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2019-03-16 14:29:25","endLine":219,"groupId":"15415","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d3/0899cccc2664330df481cd24d3e8361d52a4d3.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      if (!groupByJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", groupByJob.getJobID());\n        failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = config.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          if (numberOfShards == 1) {\n            actualSpecs.add(new HadoopyShardSpec(NoneShardSpec.instance(), shardCount++));\n          } else {\n            for (int i = 0; i < numberOfShards; ++i) {\n              actualSpecs.add(\n                  new HadoopyShardSpec(\n                      new HashBasedNumberedShardSpec(\n                          i,\n                          numberOfShards,\n                          null,\n                          HadoopDruidIndexerConfig.JSON_MAPPER\n                      ),\n                      shardCount++\n                  )\n              );\n              log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n            }\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DetermineHashedPartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"M"}],"commitId":"892d1d35d6cc00487d583f05f1cc138782180ef9","commitMessage":"@@@Deprecate NoneShardSpec and drop support for automatic segment merge (#6883)\n\n* Deprecate noneShardSpec\n\n* clean up noneShardSpec constructor\n\n* revert unnecessary change\n\n* Deprecate mergeTask\n\n* add more doc\n\n* remove convert from indexMerger\n\n* Remove mergeTask\n\n* remove HadoopDruidConverterConfig\n\n* fix build\n\n* fix build\n\n* fix teamcity\n\n* fix teamcity\n\n* fix ServerModule\n\n* fix compilation\n\n* fix compilation\n","date":"2019-03-16 14:29:25","modifiedFileCount":"40","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-05-17 04:59:10","codes":[{"authorDate":"2019-05-17 04:59:10","commitOrder":7,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2019-05-17 04:59:10","endLine":289,"groupId":"4312","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/11/5a926c2d22fc51da0b1e7c18aeeaa859526eb8.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      if (!dimSelectionJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n        failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":117,"status":"M"},{"authorDate":"2019-05-17 04:59:10","commitOrder":7,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      try {\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = config.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          for (int i = 0; i < numberOfShards; ++i) {\n            actualSpecs.add(\n                new HadoopyShardSpec(\n                    new HashBasedNumberedShardSpec(\n                        i,\n                        numberOfShards,\n                        null,\n                        HadoopDruidIndexerConfig.JSON_MAPPER\n                    ),\n                    shardCount++\n                )\n            );\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2019-05-17 04:59:10","endLine":226,"groupId":"4951","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c8/3bc085a2f60da65485e123382229a2aa3c2867.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      if (!groupByJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\", groupByJob.getJobID());\n        failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n        return false;\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = config.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          for (int i = 0; i < numberOfShards; ++i) {\n            actualSpecs.add(\n                new HadoopyShardSpec(\n                    new HashBasedNumberedShardSpec(\n                        i,\n                        numberOfShards,\n                        null,\n                        HadoopDruidIndexerConfig.JSON_MAPPER\n                    ),\n                    shardCount++\n                )\n            );\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DetermineHashedPartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"M"}],"commitId":"d99f77a01b5f4e0abde0ec85c1a1039de09bbb78","commitMessage":"@@@Add option to use YARN RM as fallback for JobHistory failure (#7673)\n\n* Add option to use YARN RM as fallback for job status\n\n* PR comments\n","date":"2019-05-17 04:59:10","modifiedFileCount":"15","status":"M","submitter":"Jonathan Wei"},{"authorTime":"2019-05-17 04:59:10","codes":[{"authorDate":"2019-07-31 08:24:39","commitOrder":8,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2019-07-31 08:24:39","endLine":289,"groupId":"17160","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/98/4d104a80c0f7b4c5cf13d1ac9a011d67ebb727.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!config.getPartitionsSpec().isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":114,"status":"M"},{"authorDate":"2019-05-17 04:59:10","commitOrder":8,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      try {\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = config.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          for (int i = 0; i < numberOfShards; ++i) {\n            actualSpecs.add(\n                new HadoopyShardSpec(\n                    new HashBasedNumberedShardSpec(\n                        i,\n                        numberOfShards,\n                        null,\n                        HadoopDruidIndexerConfig.JSON_MAPPER\n                    ),\n                    shardCount++\n                )\n            );\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2019-05-17 04:59:10","endLine":226,"groupId":"4951","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c8/3bc085a2f60da65485e123382229a2aa3c2867.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      try {\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = config.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          for (int i = 0; i < numberOfShards; ++i) {\n            actualSpecs.add(\n                new HadoopyShardSpec(\n                    new HashBasedNumberedShardSpec(\n                        i,\n                        numberOfShards,\n                        null,\n                        HadoopDruidIndexerConfig.JSON_MAPPER\n                    ),\n                    shardCount++\n                )\n            );\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DetermineHashedPartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"N"}],"commitId":"385f492a555add279a8c6dd368954fde18c41dcb","commitMessage":"@@@Use PartitionsSpec for all task types (#8141)\n\n* Use partitionsSpec for all task types\n\n* fix doc\n\n* fix typos and revert to use isPushRequired\n\n* address comments\n\n* move partitionsSpec to core\n\n* remove hadoopPartitionsSpec\n","date":"2019-07-31 08:24:39","modifiedFileCount":"29","status":"M","submitter":"Jihoon Son"},{"authorTime":"2019-05-17 04:59:10","codes":[{"authorDate":"2019-09-21 04:59:18","commitOrder":9,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2019-09-21 04:59:18","endLine":289,"groupId":"17160","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/59/245a1f5a234fb64eca1cfad90221b9e18f3eb0.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, config.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":114,"status":"M"},{"authorDate":"2019-05-17 04:59:10","commitOrder":9,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      try {\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = config.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          for (int i = 0; i < numberOfShards; ++i) {\n            actualSpecs.add(\n                new HadoopyShardSpec(\n                    new HashBasedNumberedShardSpec(\n                        i,\n                        numberOfShards,\n                        null,\n                        HadoopDruidIndexerConfig.JSON_MAPPER\n                    ),\n                    shardCount++\n                )\n            );\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2019-05-17 04:59:10","endLine":226,"groupId":"4951","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/c8/3bc085a2f60da65485e123382229a2aa3c2867.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      try {\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = config.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          for (int i = 0; i < numberOfShards; ++i) {\n            actualSpecs.add(\n                new HadoopyShardSpec(\n                    new HashBasedNumberedShardSpec(\n                        i,\n                        numberOfShards,\n                        null,\n                        HadoopDruidIndexerConfig.JSON_MAPPER\n                    ),\n                    shardCount++\n                )\n            );\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DetermineHashedPartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"N"}],"commitId":"99b6eedab59fb15664567cb9a7a40afe6ed93a8a","commitMessage":"@@@Rename partition spec fields (#8507)\n\n* Rename partition spec fields\n\nRename partition spec fields to be consistent across the various types\n(hashed.  single_dim.  dynamic). Specifically.  use targetNumRowsPerSegment\nand maxRowsPerSegment in favor of targetPartitionSize and\nmaxSegmentSize. Consistent and clearer names are easier for users to\nunderstand and use.\n\nAlso fix various IntelliJ inspection warnings and doc spelling mistakes.\n\n* Fix test\n\n* Improve docs\n\n* Add targetRowsPerSegment to HashedPartitionsSpec\n","date":"2019-09-21 04:59:18","modifiedFileCount":"9","status":"M","submitter":"Chi Cao Minh"},{"authorTime":"2020-01-30 03:50:52","codes":[{"authorDate":"2019-09-21 04:59:18","commitOrder":10,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2019-09-21 04:59:18","endLine":289,"groupId":"17160","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/59/245a1f5a234fb64eca1cfad90221b9e18f3eb0.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":114,"status":"N"},{"authorDate":"2020-01-30 03:50:52","commitOrder":10,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      try {\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          for (int i = 0; i < numberOfShards; ++i) {\n            actualSpecs.add(\n                new HadoopyShardSpec(\n                    new HashBasedNumberedShardSpec(\n                        i,\n                        numberOfShards,\n                        null,\n                        HadoopDruidIndexerConfig.JSON_MAPPER\n                    ),\n                    shardCount++\n                )\n            );\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2020-01-30 03:50:52","endLine":226,"groupId":"4951","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d6/6b42eef866e254922c04c0aa640e8fdf27a4d8.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      try {\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, config.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = config.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = config.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          for (int i = 0; i < numberOfShards; ++i) {\n            actualSpecs.add(\n                new HadoopyShardSpec(\n                    new HashBasedNumberedShardSpec(\n                        i,\n                        numberOfShards,\n                        null,\n                        HadoopDruidIndexerConfig.JSON_MAPPER\n                    ),\n                    shardCount++\n                )\n            );\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DetermineHashedPartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"M"}],"commitId":"303b02eba16bb639e5adee37cc7ecbab89b00b64","commitMessage":"@@@intelliJ inspections cleanup (#9260)\n\n* intelliJ inspections cleanup\n\n- remove redundant escapes\n- performance warnings\n- access static member via instance reference\n- static method declared final\n- inner class may be static\n\nMost of these changes are aesthetic.  however.  they will allow inspections to\nbe enabled as part of CI checks going forward\n\nThe valuable changes in this delta are:\n- using StringBuilder instead of string addition in a loop\n    indexing-hadoop/.../Utils.java\n    processing/.../ByteBufferMinMaxOffsetHeap.java\n- Use class variables instead of static variables for parameterized test\n    processing/src/.../ScanQueryLimitRowIteratorTest.java\n\n* Add intelliJ inspection warnings as errors to druid profile\n\n* one more static inner class\n","date":"2020-01-30 03:50:52","modifiedFileCount":"43","status":"M","submitter":"Suneet Saldanha"},{"authorTime":"2020-03-24 14:09:21","codes":[{"authorDate":"2020-03-24 14:09:21","commitOrder":11,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob.getConfiguration(), config);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob.getConfiguration(), config);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2020-03-24 14:09:21","endLine":289,"groupId":"10193","id":21,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d9/e6a14ce56a9c9ee5db43cd3764f0397d28eeff.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":114,"status":"M"},{"authorDate":"2020-03-24 14:09:21","commitOrder":11,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob.getConfiguration(), config);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      try {\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          for (int i = 0; i < numberOfShards; ++i) {\n            actualSpecs.add(\n                new HadoopyShardSpec(\n                    new HashBasedNumberedShardSpec(\n                        i,\n                        numberOfShards,\n                        null,\n                        HadoopDruidIndexerConfig.JSON_MAPPER\n                    ),\n                    shardCount++\n                )\n            );\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2020-03-24 14:09:21","endLine":226,"groupId":"4951","id":22,"instanceNumber":2,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/9f/20f171fb7b5e456e047c31f81496523b9ac7ef.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      try {\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          for (int i = 0; i < numberOfShards; ++i) {\n            actualSpecs.add(\n                new HadoopyShardSpec(\n                    new HashBasedNumberedShardSpec(\n                        i,\n                        numberOfShards,\n                        null,\n                        HadoopDruidIndexerConfig.JSON_MAPPER\n                    ),\n                    shardCount++\n                )\n            );\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DetermineHashedPartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"M"}],"commitId":"e97695d9da40303c96da9e81cf019f071be36a6f","commitMessage":"@@@fix Hadoop ingestion fails due to error 'JavaScript is disabled' on certain config (#9553)\n\n* fix Hadoop ingestion fails due to error 'JavaScript is disabled'.  if determine partition hadoop job is run\n\n* add test\n\n* fix checkstyle\n\n* address comments\n\n* address comments","date":"2020-03-24 14:09:21","modifiedFileCount":"6","status":"M","submitter":"Maytas Monsereenusorn"},{"authorTime":"2020-06-19 09:40:43","codes":[{"authorDate":"2020-03-24 14:09:21","commitOrder":12,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob.getConfiguration(), config);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob.getConfiguration(), config);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2020-03-24 14:09:21","endLine":289,"groupId":"10193","id":23,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d9/e6a14ce56a9c9ee5db43cd3764f0397d28eeff.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob.getConfiguration(), config);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob.getConfiguration(), config);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":114,"status":"N"},{"authorDate":"2020-06-19 09:40:43","commitOrder":12,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob.getConfiguration(), config);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      try {\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          for (int i = 0; i < numberOfShards; ++i) {\n            actualSpecs.add(\n                new HadoopyShardSpec(\n                    new HashBasedNumberedShardSpec(\n                        i,\n                        numberOfShards,\n                        i,\n                        numberOfShards,\n                        null,\n                        HadoopDruidIndexerConfig.JSON_MAPPER\n                    ),\n                    shardCount++\n                )\n            );\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2020-06-19 09:40:43","endLine":228,"groupId":"4951","id":24,"instanceNumber":2,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/62/a13d3f9fc48c2cc2a8011b24d66a04918be11b.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob.getConfiguration(), config);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      try {\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          for (int i = 0; i < numberOfShards; ++i) {\n            actualSpecs.add(\n                new HadoopyShardSpec(\n                    new HashBasedNumberedShardSpec(\n                        i,\n                        numberOfShards,\n                        null,\n                        HadoopDruidIndexerConfig.JSON_MAPPER\n                    ),\n                    shardCount++\n                )\n            );\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DetermineHashedPartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"M"}],"commitId":"d644a27f1a545105a4b1a4110f3ed83d7c46a46f","commitMessage":"@@@Create packed core partitions for hash/range-partitioned segments in native batch ingestion (#10025)\n\n* Fill in the core partition set size properly for batch ingestion with\ndynamic partitioning\n\n* incomplete javadoc\n\n* Address comments\n\n* fix tests\n\n* fix json serde.  add tests\n\n* checkstyle\n\n* Set core partition set size for hash-partitioned segments properly in\nbatch ingestion\n\n* test for both parallel and single-threaded task\n\n* unused variables\n\n* fix test\n\n* unused imports\n\n* add hash/range buckets\n\n* some test adjustment and missing json serde\n\n* centralized partition id allocation in parallel and simple tasks\n\n* remove string partition chunk\n\n* revive string partition chunk\n\n* fill numCorePartitions for hadoop\n\n* clean up hash stuffs\n\n* resolved todos\n\n* javadocs\n\n* Fix tests\n\n* add more tests\n\n* doc\n\n* unused imports","date":"2020-06-19 09:40:43","modifiedFileCount":"78","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-09-25 07:32:56","codes":[{"authorDate":"2020-03-24 14:09:21","commitOrder":13,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob.getConfiguration(), config);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob.getConfiguration(), config);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2020-03-24 14:09:21","endLine":289,"groupId":"10193","id":25,"instanceNumber":1,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d9/e6a14ce56a9c9ee5db43cd3764f0397d28eeff.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob.getConfiguration(), config);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob.getConfiguration(), config);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":114,"status":"N"},{"authorDate":"2020-09-25 07:32:56","commitOrder":13,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob.getConfiguration(), config);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      try {\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      PartitionsSpec partitionsSpec = config.getPartitionsSpec();\n      if (!(partitionsSpec instanceof HashedPartitionsSpec)) {\n        throw new ISE(\n            \"%s is expected, but got %s\",\n            HashedPartitionsSpec.class.getName(),\n            partitionsSpec.getClass().getName()\n        );\n      }\n      HashPartitionFunction partitionFunction = ((HashedPartitionsSpec) partitionsSpec).getPartitionFunction();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          for (int i = 0; i < numberOfShards; ++i) {\n            actualSpecs.add(\n                new HadoopyShardSpec(\n                    new HashBasedNumberedShardSpec(\n                        i,\n                        numberOfShards,\n                        i,\n                        numberOfShards,\n                        null,\n                        partitionFunction,\n                        HadoopDruidIndexerConfig.JSON_MAPPER\n                    ),\n                    shardCount++\n                )\n            );\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2020-09-25 07:32:56","endLine":241,"groupId":"4951","id":26,"instanceNumber":2,"isCurCommit":0,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b1/ec54609935cb93e05c5791c2e18a734fa98761.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob.getConfiguration(), config);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      try {\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          for (int i = 0; i < numberOfShards; ++i) {\n            actualSpecs.add(\n                new HadoopyShardSpec(\n                    new HashBasedNumberedShardSpec(\n                        i,\n                        numberOfShards,\n                        i,\n                        numberOfShards,\n                        null,\n                        HadoopDruidIndexerConfig.JSON_MAPPER\n                    ),\n                    shardCount++\n                )\n            );\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DetermineHashedPartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":89,"status":"M"}],"commitId":"0cc9eb4903e7bddb4c1484984bf87c8fab7648df","commitMessage":"@@@Store hash partition function in dataSegment and allow segment pruning only when hash partition function is provided (#10288)\n\n* Store hash partition function in dataSegment and allow segment pruning only when hash partition function is provided\n\n* query context\n\n* fix tests; add more test\n\n* javadoc\n\n* docs and more tests\n\n* remove default and hadoop tests\n\n* consistent name and fix javadoc\n\n* spelling and field name\n\n* default function for partitionsSpec\n\n* other comments\n\n* address comments\n\n* fix tests and spelling\n\n* test\n\n* doc","date":"2020-09-25 07:32:56","modifiedFileCount":"50","status":"M","submitter":"Jihoon Son"},{"authorTime":"2021-01-29 22:02:10","codes":[{"authorDate":"2021-01-29 22:02:10","commitOrder":14,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob.getConfiguration(), config);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob.getConfiguration(), config);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(Iterators.size(config.getGranularitySpec().sortedBucketIntervals().iterator()));\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2021-01-29 22:02:10","endLine":289,"groupId":"121340","id":27,"instanceNumber":1,"isCurCommit":1,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/eb/e69cd15a7280dce7206c6e4f6c015f83422ac8.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n\n      if (!(config.getPartitionsSpec() instanceof SingleDimensionPartitionsSpec)) {\n        throw new ISE(\n            \"DeterminePartitionsJob can only be run for SingleDimensionPartitionsSpec, partitionSpec found [%s]\",\n            config.getPartitionsSpec()\n        );\n      }\n\n      final SingleDimensionPartitionsSpec partitionsSpec =\n          (SingleDimensionPartitionsSpec) config.getPartitionsSpec();\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        groupByJob = Job.getInstance(\n            new Configuration(),\n            StringUtils.format(\"%s-determine_partitions_groupby-%s\", config.getDataSource(), config.getIntervals())\n        );\n\n        JobHelper.injectSystemProperties(groupByJob.getConfiguration(), config);\n        config.addJobProperties(groupByJob);\n\n        groupByJob.setMapperClass(DeterminePartitionsGroupByMapper.class);\n        groupByJob.setMapOutputKeyClass(BytesWritable.class);\n        groupByJob.setMapOutputValueClass(NullWritable.class);\n        groupByJob.setCombinerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setReducerClass(DeterminePartitionsGroupByReducer.class);\n        groupByJob.setOutputKeyClass(BytesWritable.class);\n        groupByJob.setOutputValueClass(NullWritable.class);\n        groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n        JobHelper.setupClasspath(\n            JobHelper.distributedClassPath(config.getWorkingPath()),\n            JobHelper.distributedClassPath(config.makeIntermediatePath()),\n            groupByJob\n        );\n\n        config.addInputPaths(groupByJob);\n        config.intoConfiguration(groupByJob);\n        FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n        groupByJob.submit();\n        log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n        \r\n        if (groupByJob.getJobID() != null) {\n          JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n        }\n\n\n        try {\n          if (!groupByJob.waitForCompletion(true)) {\n            log.error(\"Job failed: %s\", groupByJob.getJobID());\n            failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n            return false;\n          }\n        }\n        catch (IOException ioe) {\n          if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n            throw ioe;\n          }\n        }\n      } else {\n        log.info(\"Skipping group-by job.\");\n      }\n\n      \r\n\r\n\n      final Job dimSelectionJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_dimselection-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      dimSelectionJob.getConfiguration().set(\"io.sort.record.percent\", \"0.19\");\n\n      JobHelper.injectSystemProperties(dimSelectionJob.getConfiguration(), config);\n      config.addJobProperties(dimSelectionJob);\n\n      if (!partitionsSpec.isAssumeGrouped()) {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionPostGroupByMapper.class);\n        dimSelectionJob.setInputFormatClass(SequenceFileInputFormat.class);\n        FileInputFormat.addInputPath(dimSelectionJob, config.makeGroupedDataDir());\n      } else {\n        \r\n        dimSelectionJob.setMapperClass(DeterminePartitionsDimSelectionAssumeGroupedMapper.class);\n        config.addInputPaths(dimSelectionJob);\n      }\n\n      SortableBytes.useSortableBytesAsMapOutputKey(dimSelectionJob, DeterminePartitionsDimSelectionPartitioner.class);\n      dimSelectionJob.setMapOutputValueClass(Text.class);\n      dimSelectionJob.setCombinerClass(DeterminePartitionsDimSelectionCombiner.class);\n      dimSelectionJob.setReducerClass(DeterminePartitionsDimSelectionReducer.class);\n      dimSelectionJob.setOutputKeyClass(BytesWritable.class);\n      dimSelectionJob.setOutputValueClass(Text.class);\n      dimSelectionJob.setOutputFormatClass(DeterminePartitionsDimSelectionOutputFormat.class);\n      dimSelectionJob.setNumReduceTasks(config.getGranularitySpec().bucketIntervals().get().size());\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          dimSelectionJob\n      );\n\n      config.intoConfiguration(dimSelectionJob);\n      FileOutputFormat.setOutputPath(dimSelectionJob, config.makeIntermediatePath());\n\n      dimSelectionJob.submit();\n      log.info(\n          \"Job %s submitted, status available at: %s\",\n          dimSelectionJob.getJobName(),\n          dimSelectionJob.getTrackingURL()\n      );\n\n      \r\n      if (dimSelectionJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), dimSelectionJob.getJobID().toString());\n      }\n\n\n      try {\n        if (!dimSelectionJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", dimSelectionJob.getJobID().toString());\n          failureCause = Utils.getFailureMessage(dimSelectionJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, dimSelectionJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(dimSelectionJob.getConfiguration());\n        }\n        if (Utils.exists(dimSelectionJob, fileSystem, partitionInfoPath)) {\n          List<ShardSpec> specs = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(dimSelectionJob, partitionInfoPath), new TypeReference<List<ShardSpec>>()\n              {\n              }\n          );\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(specs.size());\n          for (int i = 0; i < specs.size(); ++i) {\n            actualSpecs.add(new HadoopyShardSpec(specs.get(i), shardCount++));\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", segmentGranularity, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(segmentGranularity.getStartMillis(), actualSpecs);\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n      config.setShardSpecs(shardSpecs);\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DeterminePartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":114,"status":"M"},{"authorDate":"2021-01-29 22:02:10","commitOrder":14,"curCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob.getConfiguration(), config);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (config.getInputIntervals().isEmpty()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(Iterators.size(config.getSegmentGranularIntervals().iterator()));\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      try {\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (config.getInputIntervals().isEmpty()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>()\n            {\n            }\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      PartitionsSpec partitionsSpec = config.getPartitionsSpec();\n      if (!(partitionsSpec instanceof HashedPartitionsSpec)) {\n        throw new ISE(\n            \"%s is expected, but got %s\",\n            HashedPartitionsSpec.class.getName(),\n            partitionsSpec.getClass().getName()\n        );\n      }\n      HashPartitionFunction partitionFunction = ((HashedPartitionsSpec) partitionsSpec).getPartitionFunction();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          for (int i = 0; i < numberOfShards; ++i) {\n            actualSpecs.add(\n                new HadoopyShardSpec(\n                    new HashBasedNumberedShardSpec(\n                        i,\n                        numberOfShards,\n                        i,\n                        numberOfShards,\n                        null,\n                        partitionFunction,\n                        HadoopDruidIndexerConfig.JSON_MAPPER\n                    ),\n                    shardCount++\n                )\n            );\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2021-01-29 22:02:10","endLine":243,"groupId":"121340","id":28,"instanceNumber":2,"isCurCommit":1,"methodName":"run","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/1d/0c3c4b14fc9953502959cc16871e1431775bb0.src","preCode":"  public boolean run()\n  {\n    try {\n      \r\n\r\n\r\n\n      startTime = System.currentTimeMillis();\n      groupByJob = Job.getInstance(\n          new Configuration(),\n          StringUtils.format(\"%s-determine_partitions_hashed-%s\", config.getDataSource(), config.getIntervals())\n      );\n\n      JobHelper.injectSystemProperties(groupByJob.getConfiguration(), config);\n      config.addJobProperties(groupByJob);\n      groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n      groupByJob.setMapOutputKeyClass(LongWritable.class);\n      groupByJob.setMapOutputValueClass(BytesWritable.class);\n      groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n      groupByJob.setOutputKeyClass(NullWritable.class);\n      groupByJob.setOutputValueClass(NullWritable.class);\n      groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n      groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        groupByJob.setNumReduceTasks(1);\n      } else {\n        groupByJob.setNumReduceTasks(config.getSegmentGranularIntervals().get().size());\n      }\n      JobHelper.setupClasspath(\n          JobHelper.distributedClassPath(config.getWorkingPath()),\n          JobHelper.distributedClassPath(config.makeIntermediatePath()),\n          groupByJob\n      );\n\n      config.addInputPaths(groupByJob);\n      config.intoConfiguration(groupByJob);\n      FileOutputFormat.setOutputPath(groupByJob, config.makeGroupedDataDir());\n\n      groupByJob.submit();\n      log.info(\"Job %s submitted, status available at: %s\", groupByJob.getJobName(), groupByJob.getTrackingURL());\n\n      \r\n      if (groupByJob.getJobID() != null) {\n        JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(), groupByJob.getJobID().toString());\n      }\n\n      try {\n        if (!groupByJob.waitForCompletion(true)) {\n          log.error(\"Job failed: %s\", groupByJob.getJobID());\n          failureCause = Utils.getFailureMessage(groupByJob, HadoopDruidIndexerConfig.JSON_MAPPER);\n          return false;\n        }\n      }\n      catch (IOException ioe) {\n        if (!Utils.checkAppSuccessForJobIOException(ioe, groupByJob, config.isUseYarnRMJobStatusFallback())) {\n          throw ioe;\n        }\n      }\n\n      \r\n\r\n\n\n      log.info(\"Job completed, loading up partitions for intervals[%s].\", config.getSegmentGranularIntervals());\n      FileSystem fileSystem = null;\n      if (!config.getSegmentGranularIntervals().isPresent()) {\n        final Path intervalInfoPath = config.makeIntervalInfoPath();\n        fileSystem = intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n        if (!Utils.exists(groupByJob, fileSystem, intervalInfoPath)) {\n          throw new ISE(\"Path[%s] didn't exist!?\", intervalInfoPath);\n        }\n        List<Interval> intervals = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n            Utils.openInputStream(groupByJob, intervalInfoPath),\n            new TypeReference<List<Interval>>() {}\n        );\n        config.setGranularitySpec(\n            new UniformGranularitySpec(\n                config.getGranularitySpec().getSegmentGranularity(),\n                config.getGranularitySpec().getQueryGranularity(),\n                config.getGranularitySpec().isRollup(),\n                intervals\n            )\n        );\n        log.info(\"Determined Intervals for Job [%s].\", config.getSegmentGranularIntervals());\n      }\n      Map<Long, List<HadoopyShardSpec>> shardSpecs = new TreeMap<>(DateTimeComparator.getInstance());\n      PartitionsSpec partitionsSpec = config.getPartitionsSpec();\n      if (!(partitionsSpec instanceof HashedPartitionsSpec)) {\n        throw new ISE(\n            \"%s is expected, but got %s\",\n            HashedPartitionsSpec.class.getName(),\n            partitionsSpec.getClass().getName()\n        );\n      }\n      HashPartitionFunction partitionFunction = ((HashedPartitionsSpec) partitionsSpec).getPartitionFunction();\n      int shardCount = 0;\n      for (Interval segmentGranularity : config.getSegmentGranularIntervals().get()) {\n        DateTime bucket = segmentGranularity.getStart();\n\n        final Path partitionInfoPath = config.makeSegmentPartitionInfoPath(segmentGranularity);\n        if (fileSystem == null) {\n          fileSystem = partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n        }\n        if (Utils.exists(groupByJob, fileSystem, partitionInfoPath)) {\n          final Long numRows = HadoopDruidIndexerConfig.JSON_MAPPER.readValue(\n              Utils.openInputStream(groupByJob, partitionInfoPath),\n              Long.class\n          );\n\n          log.info(\"Found approximately [%,d] rows in data.\", numRows);\n\n          final int numberOfShards = (int) Math.ceil((double) numRows / config.getTargetPartitionSize());\n\n          log.info(\"Creating [%,d] shards\", numberOfShards);\n\n          List<HadoopyShardSpec> actualSpecs = Lists.newArrayListWithExpectedSize(numberOfShards);\n          for (int i = 0; i < numberOfShards; ++i) {\n            actualSpecs.add(\n                new HadoopyShardSpec(\n                    new HashBasedNumberedShardSpec(\n                        i,\n                        numberOfShards,\n                        i,\n                        numberOfShards,\n                        null,\n                        partitionFunction,\n                        HadoopDruidIndexerConfig.JSON_MAPPER\n                    ),\n                    shardCount++\n                )\n            );\n            log.info(\"DateTime[%s], partition[%d], spec[%s]\", bucket, i, actualSpecs.get(i));\n          }\n\n          shardSpecs.put(bucket.getMillis(), actualSpecs);\n\n        } else {\n          log.info(\"Path[%s] didn't exist!?\", partitionInfoPath);\n        }\n      }\n\n      config.setShardSpecs(shardSpecs);\n      log.info(\n          \"DetermineHashedPartitionsJob took %d millis\",\n          (System.currentTimeMillis() - startTime)\n      );\n\n      return true;\n    }\n    catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"indexing-hadoop/src/main/java/org/apache/druid/indexer/DetermineHashedPartitionsJob.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":89,"status":"M"}],"commitId":"0e4750bac208d99f4a07545cc2f401f9bcdc1381","commitMessage":"@@@Granularity interval materialization (#10742)\n\n* Prevent interval materialization for UniformGranularitySpec inside the overlord\n\n* Change API of bucketIntervals in GranularitySpec to return an Iterable<Interval>\n\n* Javadoc update.  respect inputIntervals contract\n\n* Eliminate dependency on wrappedspec (i.e. ArbitraryGranularity) in UniformGranularitySpec\n\n* Added one boundary condition test to UniformGranularityTest and fixed Travis forbidden method errors in IntervalsByGranularity\n\n* Fix Travis style & other checks\n\n* Refactor TreeSet to facilitate re-use in UniformGranularitySpec\n\n* Make sure intervals are unique when there is no segment granularity\n\n* Style/bugspot fixes...\n\n* More travis checks\n\n* Add condensedIntervals method to GranularitySpec and pass it as needed to the lock method\n\n* Style & PR feedback\n\n* Fixed failing test\n\n* Fixed bug in IntervalsByGranularity iterator that it would return repeated elements (see added unit tests that were broken before this change)\n\n* Refactor so that we can get the condensed buckets without materializing the intervals\n\n* Get rid of GranularitySpec::condensedInputIntervals ... not needed\n\n* Travis failures fixes\n\n* Travis checkstyle fix\n\n* Edited/added javadoc comments and a method name (code review feedback)\n\n* Fixed jacoco coverage by moving class and adding more coverage\n\n* Avoid materializing the condensed intervals when locking\n\n* Deal with overlapping intervals\n\n* Remove code and use library code instead\n\n* Refactor intervals by granularity using the FluentIterable.  add sanity checks\n\n* Change !hasNext() to inputIntervals().isEmpty()\n\n* Remove redundant lambda\n\n* Use materialized intervals here since this is outside the overlord (for performance)\n\n* Name refactor to reflect the fact that bucket intervals are sorted.\n\n* Style fixes\n\n* Removed redundant method and have condensedIntervalIterator throw IAE when element is null for consistency with other methods in this class (as well that null interval when condensing does not make sense)\n\n* Remove forbidden api\n\n* Move helper class inside common base class to reduce public space pollution","date":"2021-01-29 22:02:10","modifiedFileCount":"22","status":"M","submitter":"Agustin Gonzalez"}]
