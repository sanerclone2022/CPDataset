[{"authorTime":"2019-11-21 09:24:12","codes":[{"authorDate":"2019-08-16 08:43:35","commitOrder":2,"curCode":"  private TaskStatus runSinglePhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    final ParallelIndexTaskRunner<SinglePhaseSubTask, PushedSegmentsReport> runner = createRunner(\n        toolbox,\n        this::createSinglePhaseTaskRunner\n    );\n\n    final TaskState state = runNextPhase(runner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, runner.getReports());\n    }\n    return TaskStatus.fromCode(getId(), state);\n  }\n","date":"2019-08-16 08:43:35","endLine":453,"groupId":"2879","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"runSinglePhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/2f/a30e34270cbf73e77f8b0d52fcc889b0794b21.src","preCode":"  private TaskStatus runSinglePhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    final ParallelIndexTaskRunner<SinglePhaseSubTask, PushedSegmentsReport> runner = createRunner(\n        toolbox,\n        this::createSinglePhaseTaskRunner\n    );\n\n    final TaskState state = runNextPhase(runner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, runner.getReports());\n    }\n    return TaskStatus.fromCode(getId(), state);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":440,"status":"NB"},{"authorDate":"2019-11-21 09:24:12","commitOrder":2,"curCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    \r\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedHashPartitionsReport> indexingRunner = createRunner(\n        toolbox,\n        this::createPartialHashSegmentGenerateRunner\n    );\n\n    TaskState state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n\n    \r\n    Map<Pair<Interval, Integer>, List<HashPartitionLocation>> partitionToLocations =\n        groupHashPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialHashSegmentMergeIOConfig> ioConfigs = createHashMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexTaskRunner<PartialHashSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialHashSegmentMergeRunner(tb, ioConfigs)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), state);\n  }\n","date":"2019-11-21 09:24:12","endLine":520,"groupId":"2879","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"runHashPartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/28/bfc7c421b1728933e09761ab1b2d45b4aa5ab4.src","preCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    \r\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedHashPartitionsReport> indexingRunner = createRunner(\n        toolbox,\n        this::createPartialHashSegmentGenerateRunner\n    );\n\n    TaskState state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n\n    \r\n    Map<Pair<Interval, Integer>, List<HashPartitionLocation>> partitionToLocations =\n        groupHashPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialHashSegmentMergeIOConfig> ioConfigs = createHashMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexTaskRunner<PartialHashSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialHashSegmentMergeRunner(tb, ioConfigs)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), state);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":486,"status":"B"}],"commitId":"ff6217365bb5803dae364dddbe7bfc870bd406bc","commitMessage":"@@@Refactor parallel indexing perfect rollup partitioning (#8852)\n\n* Refactor parallel indexing perfect rollup partitioning\n\nRefactoring to make it easier to later add range partitioning for\nperfect rollup parallel indexing. This is accomplished by adding several\nnew base classes (e.g..  PerfectRollupWorkerTask) and new classes for\nencapsulating logic that needs to be changed for different partitioning\nstrategies (e.g..  IndexTaskInputRowIteratorBuilder).\n\nThe code is functionally equivalent to before except for the following\nsmall behavior changes:\n\n1) PartialSegmentMergeTask: Previously.  this task had a priority of\n   DEFAULT_TASK_PRIORITY. It now has a priority of\n   DEFAULT_BATCH_INDEX_TASK_PRIORITY (via the new PerfectRollupWorkerTask\n   base class).  since it is a batch index task.\n\n2) ParallelIndexPhaseRunner: A decorator was added to\n   subTaskSpecIterator to ensure the subtasks are generated with unique\n   ids. Previously.  only tests (i.e..  MultiPhaseParallelIndexingTest)\n   would have this decorator.  but this behavior is desired for non-test\n   code as well.\n\n* Fix forbidden apis and pmd warnings\n\n* Fix analyze dependencies warnings\n\n* Fix IndexTask json and add IT diags\n\n* Fix parallel index supervisor<->worker serde\n\n* Fix TeamCity inspection errors/warnings\n\n* Fix TeamCity inspection errors/warnings again\n\n* Integrate changes with those from #8823\n\n* Address review comments\n\n* Address more review comments\n\n* Fix forbidden apis\n\n* Address more review comments\n","date":"2019-11-21 09:24:12","modifiedFileCount":"34","status":"M","submitter":"Chi Cao Minh"},{"authorTime":"2020-06-19 09:40:43","codes":[{"authorDate":"2019-08-16 08:43:35","commitOrder":3,"curCode":"  private TaskStatus runSinglePhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    final ParallelIndexTaskRunner<SinglePhaseSubTask, PushedSegmentsReport> runner = createRunner(\n        toolbox,\n        this::createSinglePhaseTaskRunner\n    );\n\n    final TaskState state = runNextPhase(runner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, runner.getReports());\n    }\n    return TaskStatus.fromCode(getId(), state);\n  }\n","date":"2019-08-16 08:43:35","endLine":453,"groupId":"2879","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"runSinglePhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/2f/a30e34270cbf73e77f8b0d52fcc889b0794b21.src","preCode":"  private TaskStatus runSinglePhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    final ParallelIndexTaskRunner<SinglePhaseSubTask, PushedSegmentsReport> runner = createRunner(\n        toolbox,\n        this::createSinglePhaseTaskRunner\n    );\n\n    final TaskState state = runNextPhase(runner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, runner.getReports());\n    }\n    return TaskStatus.fromCode(getId(), state);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":440,"status":"N"},{"authorDate":"2020-06-19 09:40:43","commitOrder":3,"curCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    \r\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner\n        = createRunner(toolbox, this::createPartialHashSegmentGenerateRunner);\n\n    TaskState state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), state);\n  }\n","date":"2020-06-19 09:40:43","endLine":561,"groupId":"2879","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"runHashPartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/be/d85dea6926fce5c22c4ce268c98a7ec15fcaf4.src","preCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    \r\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedHashPartitionsReport> indexingRunner = createRunner(\n        toolbox,\n        this::createPartialHashSegmentGenerateRunner\n    );\n\n    TaskState state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n\n    \r\n    Map<Pair<Interval, Integer>, List<HashPartitionLocation>> partitionToLocations =\n        groupHashPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialHashSegmentMergeIOConfig> ioConfigs = createHashMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexTaskRunner<PartialHashSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialHashSegmentMergeRunner(tb, ioConfigs)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), state);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":529,"status":"M"}],"commitId":"d644a27f1a545105a4b1a4110f3ed83d7c46a46f","commitMessage":"@@@Create packed core partitions for hash/range-partitioned segments in native batch ingestion (#10025)\n\n* Fill in the core partition set size properly for batch ingestion with\ndynamic partitioning\n\n* incomplete javadoc\n\n* Address comments\n\n* fix tests\n\n* fix json serde.  add tests\n\n* checkstyle\n\n* Set core partition set size for hash-partitioned segments properly in\nbatch ingestion\n\n* test for both parallel and single-threaded task\n\n* unused variables\n\n* fix test\n\n* unused imports\n\n* add hash/range buckets\n\n* some test adjustment and missing json serde\n\n* centralized partition id allocation in parallel and simple tasks\n\n* remove string partition chunk\n\n* revive string partition chunk\n\n* fill numCorePartitions for hadoop\n\n* clean up hash stuffs\n\n* resolved todos\n\n* javadocs\n\n* Fix tests\n\n* add more tests\n\n* doc\n\n* unused imports","date":"2020-06-19 09:40:43","modifiedFileCount":"78","status":"M","submitter":"Jihoon Son"},{"authorTime":"2020-09-25 04:47:53","codes":[{"authorDate":"2019-08-16 08:43:35","commitOrder":4,"curCode":"  private TaskStatus runSinglePhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    final ParallelIndexTaskRunner<SinglePhaseSubTask, PushedSegmentsReport> runner = createRunner(\n        toolbox,\n        this::createSinglePhaseTaskRunner\n    );\n\n    final TaskState state = runNextPhase(runner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, runner.getReports());\n    }\n    return TaskStatus.fromCode(getId(), state);\n  }\n","date":"2019-08-16 08:43:35","endLine":453,"groupId":"2879","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"runSinglePhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/2f/a30e34270cbf73e77f8b0d52fcc889b0794b21.src","preCode":"  private TaskStatus runSinglePhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    final ParallelIndexTaskRunner<SinglePhaseSubTask, PushedSegmentsReport> runner = createRunner(\n        toolbox,\n        this::createSinglePhaseTaskRunner\n    );\n\n    final TaskState state = runNextPhase(runner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, runner.getReports());\n    }\n    return TaskStatus.fromCode(getId(), state);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":440,"status":"N"},{"authorDate":"2020-09-25 04:47:53","commitOrder":4,"curCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Integer numShardsOverride;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    if (partitionsSpec.getNumShards() == null) {\n      \r\n      LOG.info(\"numShards is unspecified, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      if (cardinalityRunner == null) {\n        throw new ISE(\"Could not create cardinality runner for hash partitioning.\");\n      }\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        return TaskStatus.failure(getId());\n      }\n\n      int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                       ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                       : partitionsSpec.getMaxRowsPerSegment();\n      LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n      if (cardinalityRunner.getReports() == null) {\n        throw new ISE(\"Could not determine cardinalities for hash partitioning.\");\n      }\n      numShardsOverride = determineNumShardsFromCardinalityReport(\n          cardinalityRunner.getReports().values(),\n          effectiveMaxRowsPerSegment\n      );\n\n      LOG.info(\"Automatically determined numShards: \" + numShardsOverride);\n    } else {\n      numShardsOverride = null;\n    }\n\n    \r\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, numShardsOverride)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), state);\n  }\n","date":"2020-09-25 04:47:53","endLine":602,"groupId":"2879","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"runHashPartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/dd/0e75980c9ca76bbe3de2e249cb0b6e0658ee2d.src","preCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    \r\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner\n        = createRunner(toolbox, this::createPartialHashSegmentGenerateRunner);\n\n    TaskState state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), state);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":520,"status":"M"}],"commitId":"cb30b1fe2353dc28601c50984def9e83adb89571","commitMessage":"@@@Automatically determine numShards for parallel ingestion hash partitioning (#10419)\n\n* Automatically determine numShards for parallel ingestion hash partitioning\n\n* Fix inspection.  tests.  coverage\n\n* Docs and some PR comments\n\n* Adjust locking\n\n* Use HllSketch instead of HyperLogLogCollector\n\n* Fix tests\n\n* Address some PR comments\n\n* Fix granularity bug\n\n* Small doc fix","date":"2020-09-25 04:47:53","modifiedFileCount":"14","status":"M","submitter":"Jonathan Wei"},{"authorTime":"2020-11-26 06:50:22","codes":[{"authorDate":"2019-08-16 08:43:35","commitOrder":5,"curCode":"  private TaskStatus runSinglePhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    final ParallelIndexTaskRunner<SinglePhaseSubTask, PushedSegmentsReport> runner = createRunner(\n        toolbox,\n        this::createSinglePhaseTaskRunner\n    );\n\n    final TaskState state = runNextPhase(runner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, runner.getReports());\n    }\n    return TaskStatus.fromCode(getId(), state);\n  }\n","date":"2019-08-16 08:43:35","endLine":453,"groupId":"2879","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"runSinglePhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/2f/a30e34270cbf73e77f8b0d52fcc889b0794b21.src","preCode":"  private TaskStatus runSinglePhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    final ParallelIndexTaskRunner<SinglePhaseSubTask, PushedSegmentsReport> runner = createRunner(\n        toolbox,\n        this::createSinglePhaseTaskRunner\n    );\n\n    final TaskState state = runNextPhase(runner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, runner.getReports());\n    }\n    return TaskStatus.fromCode(getId(), state);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":440,"status":"N"},{"authorDate":"2020-11-26 06:50:22","commitOrder":5,"curCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Map<Interval, Integer> intervalToNumShards;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    final boolean needsInputSampling =\n        partitionsSpec.getNumShards() == null\n        || ingestionSchemaToUse.getDataSchema().getGranularitySpec().inputIntervals().isEmpty();\n    if (needsInputSampling) {\n      \r\n      LOG.info(\"Needs to determine intervals or numShards, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        return TaskStatus.failure(getId());\n      }\n\n      if (cardinalityRunner.getReports().isEmpty()) {\n        String msg = \"No valid rows for hash partitioning.\"\n                     + \" All rows may have invalid timestamps or have been filtered out.\";\n        LOG.warn(msg);\n        return TaskStatus.success(getId(), msg);\n      }\n\n      if (partitionsSpec.getNumShards() == null) {\n        int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                         ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                         : partitionsSpec.getMaxRowsPerSegment();\n        LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n        intervalToNumShards = determineNumShardsFromCardinalityReport(\n            cardinalityRunner.getReports().values(),\n            effectiveMaxRowsPerSegment\n        );\n      } else {\n        intervalToNumShards = CollectionUtils.mapValues(\n            mergeCardinalityReports(cardinalityRunner.getReports().values()),\n            k -> partitionsSpec.getNumShards()\n        );\n      }\n\n      ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n          ingestionSchemaToUse,\n          intervalToNumShards.keySet()\n      );\n    } else {\n      \r\n      intervalToNumShards = null;\n    }\n\n    \r\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, segmentCreateIngestionSpec, intervalToNumShards)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), state);\n  }\n","date":"2020-11-26 06:50:22","endLine":650,"groupId":"2879","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"runHashPartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/3d/272b43813c4d222e918d7c69cdad0ae3e419df.src","preCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Integer numShardsOverride;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    if (partitionsSpec.getNumShards() == null) {\n      \r\n      LOG.info(\"numShards is unspecified, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      if (cardinalityRunner == null) {\n        throw new ISE(\"Could not create cardinality runner for hash partitioning.\");\n      }\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        return TaskStatus.failure(getId());\n      }\n\n      int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                       ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                       : partitionsSpec.getMaxRowsPerSegment();\n      LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n      if (cardinalityRunner.getReports() == null) {\n        throw new ISE(\"Could not determine cardinalities for hash partitioning.\");\n      }\n      numShardsOverride = determineNumShardsFromCardinalityReport(\n          cardinalityRunner.getReports().values(),\n          effectiveMaxRowsPerSegment\n      );\n\n      LOG.info(\"Automatically determined numShards: \" + numShardsOverride);\n    } else {\n      numShardsOverride = null;\n    }\n\n    \r\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, numShardsOverride)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), state);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":551,"status":"M"}],"commitId":"7462b0b953e87890a683a0bee7b7480465450342","commitMessage":"@@@Allow missing intervals for Parallel task with hash/range partitioning (#10592)\n\n* Allow missing intervals for Parallel task\n\n* fix row filter\n\n* fix tests\n\n* fix log","date":"2020-11-26 06:50:22","modifiedFileCount":"22","status":"M","submitter":"Jihoon Son"},{"authorTime":"2021-04-09 12:03:00","codes":[{"authorDate":"2021-04-09 12:03:00","commitOrder":6,"curCode":"  private TaskStatus runSinglePhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    final ParallelIndexTaskRunner<SinglePhaseSubTask, PushedSegmentsReport> runner = createRunner(\n        toolbox,\n        this::createSinglePhaseTaskRunner\n    );\n\n    final TaskState state = runNextPhase(runner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, runner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(runner.getReports());\n      }\n    }\n    TaskStatus taskStatus = TaskStatus.fromCode(getId(), state);\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","date":"2021-04-09 12:03:00","endLine":547,"groupId":"2879","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"runSinglePhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/60/984e8a9dbb447af7d2cec5781c4c746fcb2eff.src","preCode":"  private TaskStatus runSinglePhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    final ParallelIndexTaskRunner<SinglePhaseSubTask, PushedSegmentsReport> runner = createRunner(\n        toolbox,\n        this::createSinglePhaseTaskRunner\n    );\n\n    final TaskState state = runNextPhase(runner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, runner.getReports());\n    }\n    return TaskStatus.fromCode(getId(), state);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":526,"status":"M"},{"authorDate":"2021-04-09 12:03:00","commitOrder":6,"curCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Map<Interval, Integer> intervalToNumShards;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    final boolean needsInputSampling =\n        partitionsSpec.getNumShards() == null\n        || ingestionSchemaToUse.getDataSchema().getGranularitySpec().inputIntervals().isEmpty();\n    if (needsInputSampling) {\n      \r\n      LOG.info(\"Needs to determine intervals or numShards, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        return TaskStatus.failure(getId());\n      }\n\n      if (cardinalityRunner.getReports().isEmpty()) {\n        String msg = \"No valid rows for hash partitioning.\"\n                     + \" All rows may have invalid timestamps or have been filtered out.\";\n        LOG.warn(msg);\n        return TaskStatus.success(getId(), msg);\n      }\n\n      if (partitionsSpec.getNumShards() == null) {\n        int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                         ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                         : partitionsSpec.getMaxRowsPerSegment();\n        LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n        intervalToNumShards = determineNumShardsFromCardinalityReport(\n            cardinalityRunner.getReports().values(),\n            effectiveMaxRowsPerSegment\n        );\n      } else {\n        intervalToNumShards = CollectionUtils.mapValues(\n            mergeCardinalityReports(cardinalityRunner.getReports().values()),\n            k -> partitionsSpec.getNumShards()\n        );\n      }\n\n      ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n          ingestionSchemaToUse,\n          intervalToNumShards.keySet()\n      );\n    } else {\n      \r\n      intervalToNumShards = null;\n    }\n\n    \r\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, segmentCreateIngestionSpec, intervalToNumShards)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(mergeRunner.getReports());\n      }\n    }\n\n    TaskStatus taskStatus = TaskStatus.fromCode(getId(), state);\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","date":"2021-04-09 12:03:00","endLine":694,"groupId":"2879","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"runHashPartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/60/984e8a9dbb447af7d2cec5781c4c746fcb2eff.src","preCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Map<Interval, Integer> intervalToNumShards;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    final boolean needsInputSampling =\n        partitionsSpec.getNumShards() == null\n        || ingestionSchemaToUse.getDataSchema().getGranularitySpec().inputIntervals().isEmpty();\n    if (needsInputSampling) {\n      \r\n      LOG.info(\"Needs to determine intervals or numShards, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        return TaskStatus.failure(getId());\n      }\n\n      if (cardinalityRunner.getReports().isEmpty()) {\n        String msg = \"No valid rows for hash partitioning.\"\n                     + \" All rows may have invalid timestamps or have been filtered out.\";\n        LOG.warn(msg);\n        return TaskStatus.success(getId(), msg);\n      }\n\n      if (partitionsSpec.getNumShards() == null) {\n        int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                         ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                         : partitionsSpec.getMaxRowsPerSegment();\n        LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n        intervalToNumShards = determineNumShardsFromCardinalityReport(\n            cardinalityRunner.getReports().values(),\n            effectiveMaxRowsPerSegment\n        );\n      } else {\n        intervalToNumShards = CollectionUtils.mapValues(\n            mergeCardinalityReports(cardinalityRunner.getReports().values()),\n            k -> partitionsSpec.getNumShards()\n        );\n      }\n\n      ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n          ingestionSchemaToUse,\n          intervalToNumShards.keySet()\n      );\n    } else {\n      \r\n      intervalToNumShards = null;\n    }\n\n    \r\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, segmentCreateIngestionSpec, intervalToNumShards)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n    }\n\n    return TaskStatus.fromCode(getId(), state);\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":587,"status":"M"}],"commitId":"8264203cee688607091232897749e959e7706010","commitMessage":"@@@Allow client to configure batch ingestion task to wait to complete until segments are confirmed to be available by other (#10676)\n\n* Add ability to wait for segment availability for batch jobs\n\n* IT updates\n\n* fix queries in legacy hadoop IT\n\n* Fix broken indexing integration tests\n\n* address an lgtm flag\n\n* spell checker still flagging for hadoop doc. adding under that file header too\n\n* fix compaction IT\n\n* Updates to wait for availability method\n\n* improve unit testing for patch\n\n* fix bad indentation\n\n* refactor waitForSegmentAvailability\n\n* Fixes based off of review comments\n\n* cleanup to get compile after merging with master\n\n* fix failing test after previous logic update\n\n* add back code that must have gotten deleted during conflict resolution\n\n* update some logging code\n\n* fixes to get compilation working after merge with master\n\n* reset interrupt flag in catch block after code review pointed it out\n\n* small changes following self-review\n\n* fixup some issues brought on by merge with master\n\n* small changes after review\n\n* cleanup a little bit after merge with master\n\n* Fix potential resource leak in AbstractBatchIndexTask\n\n* syntax fix\n\n* Add a Compcation TuningConfig type\n\n* add docs stipulating the lack of support by Compaction tasks for the new config\n\n* Fixup compilation errors after merge with master\n\n* Remove erreneous newline","date":"2021-04-09 12:03:00","modifiedFileCount":"106","status":"M","submitter":"Lucas Capistrant"},{"authorTime":"2021-08-03 03:11:28","codes":[{"authorDate":"2021-08-03 03:11:28","commitOrder":7,"curCode":"  private TaskStatus runSinglePhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    final ParallelIndexTaskRunner<SinglePhaseSubTask, PushedSegmentsReport> runner = createRunner(\n        toolbox,\n        this::createSinglePhaseTaskRunner\n    );\n\n    final TaskState state = runNextPhase(runner);\n    TaskStatus taskStatus;\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, runner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(runner.getReports());\n      }\n      taskStatus = TaskStatus.success(getId());\n    } else {\n      \r\n      Preconditions.checkState(state.isFailure(), \"Unrecognized state after task is complete[%s]\", state);\n      final String errorMessage = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          runner.getName()\n      );\n      taskStatus = TaskStatus.failure(getId(), errorMessage);\n    }\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","date":"2021-08-03 03:11:28","endLine":584,"groupId":"10656","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"runSinglePhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d2/a3ca7819df3dc7fa78aaaf3e72308e8fdbff1d.src","preCode":"  private TaskStatus runSinglePhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    final ParallelIndexTaskRunner<SinglePhaseSubTask, PushedSegmentsReport> runner = createRunner(\n        toolbox,\n        this::createSinglePhaseTaskRunner\n    );\n\n    final TaskState state = runNextPhase(runner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, runner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(runner.getReports());\n      }\n    }\n    TaskStatus taskStatus = TaskStatus.fromCode(getId(), state);\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":554,"status":"M"},{"authorDate":"2021-08-03 03:11:28","commitOrder":7,"curCode":"  TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Map<Interval, Integer> intervalToNumShards;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    final boolean needsInputSampling =\n        partitionsSpec.getNumShards() == null\n        || ingestionSchemaToUse.getDataSchema().getGranularitySpec().inputIntervals().isEmpty();\n    if (needsInputSampling) {\n      \r\n      LOG.info(\"Needs to determine intervals or numShards, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        String errMsg = StringUtils.format(\n            TASK_PHASE_FAILURE_MSG,\n            cardinalityRunner.getName()\n        );\n        return TaskStatus.failure(getId(), errMsg);\n      }\n\n      if (cardinalityRunner.getReports().isEmpty()) {\n        String msg = \"No valid rows for hash partitioning.\"\n                     + \" All rows may have invalid timestamps or have been filtered out.\";\n        LOG.warn(msg);\n        return TaskStatus.success(getId(), msg);\n      }\n\n      if (partitionsSpec.getNumShards() == null) {\n        int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                         ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                         : partitionsSpec.getMaxRowsPerSegment();\n        LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n        intervalToNumShards = determineNumShardsFromCardinalityReport(\n            cardinalityRunner.getReports().values(),\n            effectiveMaxRowsPerSegment\n        );\n      } else {\n        intervalToNumShards = CollectionUtils.mapValues(\n            mergeCardinalityReports(cardinalityRunner.getReports().values()),\n            k -> partitionsSpec.getNumShards()\n        );\n      }\n\n      ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n          ingestionSchemaToUse,\n          intervalToNumShards.keySet()\n      );\n    } else {\n      \r\n      intervalToNumShards = null;\n    }\n\n    \r\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, segmentCreateIngestionSpec, intervalToNumShards)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          indexingRunner.getName()\n      );\n      return TaskStatus.failure(getId(), errMsg);\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    state = runNextPhase(mergeRunner);\n    TaskStatus taskStatus;\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(mergeRunner.getReports());\n      }\n      taskStatus = TaskStatus.success(getId());\n    } else {\n      \r\n      Preconditions.checkState(state.isFailure(), \"Unrecognized state after task is complete[%s]\", state);\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          mergeRunner.getName()\n      );\n      taskStatus = TaskStatus.failure(getId(), errMsg);\n    }\n\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","date":"2021-08-03 03:11:28","endLine":749,"groupId":"10658","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"runHashPartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d2/a3ca7819df3dc7fa78aaaf3e72308e8fdbff1d.src","preCode":"  private TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Map<Interval, Integer> intervalToNumShards;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    final boolean needsInputSampling =\n        partitionsSpec.getNumShards() == null\n        || ingestionSchemaToUse.getDataSchema().getGranularitySpec().inputIntervals().isEmpty();\n    if (needsInputSampling) {\n      \r\n      LOG.info(\"Needs to determine intervals or numShards, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        return TaskStatus.failure(getId());\n      }\n\n      if (cardinalityRunner.getReports().isEmpty()) {\n        String msg = \"No valid rows for hash partitioning.\"\n                     + \" All rows may have invalid timestamps or have been filtered out.\";\n        LOG.warn(msg);\n        return TaskStatus.success(getId(), msg);\n      }\n\n      if (partitionsSpec.getNumShards() == null) {\n        int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                         ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                         : partitionsSpec.getMaxRowsPerSegment();\n        LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n        intervalToNumShards = determineNumShardsFromCardinalityReport(\n            cardinalityRunner.getReports().values(),\n            effectiveMaxRowsPerSegment\n        );\n      } else {\n        intervalToNumShards = CollectionUtils.mapValues(\n            mergeCardinalityReports(cardinalityRunner.getReports().values()),\n            k -> partitionsSpec.getNumShards()\n        );\n      }\n\n      ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n          ingestionSchemaToUse,\n          intervalToNumShards.keySet()\n      );\n    } else {\n      \r\n      intervalToNumShards = null;\n    }\n\n    \r\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, segmentCreateIngestionSpec, intervalToNumShards)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      return TaskStatus.failure(getId());\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    state = runNextPhase(mergeRunner);\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(mergeRunner.getReports());\n      }\n    }\n\n    TaskStatus taskStatus = TaskStatus.fromCode(getId(), state);\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":625,"status":"M"}],"commitId":"a2da407b704db8eed85b97cd7c1d22541f2b1102","commitMessage":"@@@Add error msg to parallel task's TaskStatus (#11486)\n\n* Add error msg to parallel task's TaskStatus\n\n* Consolidate failure block\n\n* Add failure test\n\n* Make it fail\n\n* Add fail while stopped\n\n* Simplify hash task test using a runner that fails after so many runs (parameter)\n\n* Remove unthrown exception\n\n* Use runner names to identify phase\n\n* Added range partition kill test & fixed a timing bug with the custom runner\n\n* Forbidden api\n\n* Style\n\n* Unit test code cleanup\n\n* Added message to invalid state exception and improved readability  of the phase error messages for the parallel task failure unit tests","date":"2021-08-03 03:11:28","modifiedFileCount":"19","status":"M","submitter":"Agustin Gonzalez"},{"authorTime":"2021-08-14 04:40:25","codes":[{"authorDate":"2021-08-03 03:11:28","commitOrder":8,"curCode":"  private TaskStatus runSinglePhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    final ParallelIndexTaskRunner<SinglePhaseSubTask, PushedSegmentsReport> runner = createRunner(\n        toolbox,\n        this::createSinglePhaseTaskRunner\n    );\n\n    final TaskState state = runNextPhase(runner);\n    TaskStatus taskStatus;\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, runner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(runner.getReports());\n      }\n      taskStatus = TaskStatus.success(getId());\n    } else {\n      \r\n      Preconditions.checkState(state.isFailure(), \"Unrecognized state after task is complete[%s]\", state);\n      final String errorMessage = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          runner.getName()\n      );\n      taskStatus = TaskStatus.failure(getId(), errorMessage);\n    }\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","date":"2021-08-03 03:11:28","endLine":584,"groupId":"10656","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"runSinglePhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/d2/a3ca7819df3dc7fa78aaaf3e72308e8fdbff1d.src","preCode":"  private TaskStatus runSinglePhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    final ParallelIndexTaskRunner<SinglePhaseSubTask, PushedSegmentsReport> runner = createRunner(\n        toolbox,\n        this::createSinglePhaseTaskRunner\n    );\n\n    final TaskState state = runNextPhase(runner);\n    TaskStatus taskStatus;\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, runner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(runner.getReports());\n      }\n      taskStatus = TaskStatus.success(getId());\n    } else {\n      \r\n      Preconditions.checkState(state.isFailure(), \"Unrecognized state after task is complete[%s]\", state);\n      final String errorMessage = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          runner.getName()\n      );\n      taskStatus = TaskStatus.failure(getId(), errorMessage);\n    }\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":554,"status":"N"},{"authorDate":"2021-08-14 04:40:25","commitOrder":8,"curCode":"  TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Map<Interval, Integer> intervalToNumShards;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    final boolean needsInputSampling =\n        partitionsSpec.getNumShards() == null\n        || ingestionSchemaToUse.getDataSchema().getGranularitySpec().inputIntervals().isEmpty();\n    if (needsInputSampling) {\n      \r\n      LOG.info(\"Needs to determine intervals or numShards, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        String errMsg = StringUtils.format(\n            TASK_PHASE_FAILURE_MSG,\n            cardinalityRunner.getName()\n        );\n        return TaskStatus.failure(getId(), errMsg);\n      }\n\n      if (cardinalityRunner.getReports().isEmpty()) {\n        String msg = \"No valid rows for hash partitioning.\"\n                     + \" All rows may have invalid timestamps or have been filtered out.\";\n        LOG.warn(msg);\n        return TaskStatus.success(getId(), msg);\n      }\n\n      if (partitionsSpec.getNumShards() == null) {\n        int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                         ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                         : partitionsSpec.getMaxRowsPerSegment();\n        LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n        intervalToNumShards = determineNumShardsFromCardinalityReport(\n            cardinalityRunner.getReports().values(),\n            effectiveMaxRowsPerSegment\n        );\n      } else {\n        intervalToNumShards = CollectionUtils.mapValues(\n            mergeCardinalityReports(cardinalityRunner.getReports().values()),\n            k -> partitionsSpec.getNumShards()\n        );\n      }\n\n      ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n          ingestionSchemaToUse,\n          intervalToNumShards.keySet()\n      );\n    } else {\n      \r\n      intervalToNumShards = null;\n    }\n\n    \r\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, segmentCreateIngestionSpec, intervalToNumShards)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          indexingRunner.getName()\n      );\n      return TaskStatus.failure(getId(), errMsg);\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<PartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    state = runNextPhase(mergeRunner);\n    TaskStatus taskStatus;\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(mergeRunner.getReports());\n      }\n      taskStatus = TaskStatus.success(getId());\n    } else {\n      \r\n      Preconditions.checkState(state.isFailure(), \"Unrecognized state after task is complete[%s]\", state);\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          mergeRunner.getName()\n      );\n      taskStatus = TaskStatus.failure(getId(), errMsg);\n    }\n\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","date":"2021-08-14 04:40:25","endLine":749,"groupId":"10658","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"runHashPartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b6/6f49f7a6c8a7b15f150b69f2416af8f16759e2.src","preCode":"  TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Map<Interval, Integer> intervalToNumShards;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    final boolean needsInputSampling =\n        partitionsSpec.getNumShards() == null\n        || ingestionSchemaToUse.getDataSchema().getGranularitySpec().inputIntervals().isEmpty();\n    if (needsInputSampling) {\n      \r\n      LOG.info(\"Needs to determine intervals or numShards, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        String errMsg = StringUtils.format(\n            TASK_PHASE_FAILURE_MSG,\n            cardinalityRunner.getName()\n        );\n        return TaskStatus.failure(getId(), errMsg);\n      }\n\n      if (cardinalityRunner.getReports().isEmpty()) {\n        String msg = \"No valid rows for hash partitioning.\"\n                     + \" All rows may have invalid timestamps or have been filtered out.\";\n        LOG.warn(msg);\n        return TaskStatus.success(getId(), msg);\n      }\n\n      if (partitionsSpec.getNumShards() == null) {\n        int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                         ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                         : partitionsSpec.getMaxRowsPerSegment();\n        LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n        intervalToNumShards = determineNumShardsFromCardinalityReport(\n            cardinalityRunner.getReports().values(),\n            effectiveMaxRowsPerSegment\n        );\n      } else {\n        intervalToNumShards = CollectionUtils.mapValues(\n            mergeCardinalityReports(cardinalityRunner.getReports().values()),\n            k -> partitionsSpec.getNumShards()\n        );\n      }\n\n      ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n          ingestionSchemaToUse,\n          intervalToNumShards.keySet()\n      );\n    } else {\n      \r\n      intervalToNumShards = null;\n    }\n\n    \r\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport<GenericPartitionStat>> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, segmentCreateIngestionSpec, intervalToNumShards)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          indexingRunner.getName()\n      );\n      return TaskStatus.failure(getId(), errMsg);\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<GenericPartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialGenericSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    state = runNextPhase(mergeRunner);\n    TaskStatus taskStatus;\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(mergeRunner.getReports());\n      }\n      taskStatus = TaskStatus.success(getId());\n    } else {\n      \r\n      Preconditions.checkState(state.isFailure(), \"Unrecognized state after task is complete[%s]\", state);\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          mergeRunner.getName()\n      );\n      taskStatus = TaskStatus.failure(getId(), errMsg);\n    }\n\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":625,"status":"M"}],"commitId":"c7b46671b311e841f7ddc3596dc4e9b4cf96b24d","commitMessage":"@@@option to use deep storage for storing shuffle data (#11507)\n\nFixes #11297.\nDescription\n\nDescription and design in the proposal #11297\nKey changed/added classes in this PR\n\n    *DataSegmentPusher\n    *ShuffleClient\n    *PartitionStat\n    *PartitionLocation\n    *IntermediaryDataManager\n","date":"2021-08-14 04:40:25","modifiedFileCount":"47","status":"M","submitter":"Parag Jain"},{"authorTime":"2021-08-14 04:40:25","codes":[{"authorDate":"2021-09-17 02:58:11","commitOrder":9,"curCode":"  private TaskStatus runSinglePhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    ingestionState = IngestionState.BUILD_SEGMENTS;\n    ParallelIndexTaskRunner<SinglePhaseSubTask, PushedSegmentsReport> parallelSinglePhaseRunner = createRunner(\n        toolbox,\n        this::createSinglePhaseTaskRunner\n    );\n\n    final TaskState state = runNextPhase(parallelSinglePhaseRunner);\n    TaskStatus taskStatus;\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, parallelSinglePhaseRunner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(parallelSinglePhaseRunner.getReports());\n      }\n      taskStatus = TaskStatus.success(getId());\n    } else {\n      \r\n      Preconditions.checkState(state.isFailure(), \"Unrecognized state after task is complete[%s]\", state);\n      final String errorMessage = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          parallelSinglePhaseRunner.getName()\n      );\n      taskStatus = TaskStatus.failure(getId(), errorMessage);\n    }\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","date":"2021-09-17 02:58:11","endLine":593,"groupId":"104780","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"runSinglePhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/19/b965c2e57ae115d3ecb994fd967a8fe88fbf38.src","preCode":"  private TaskStatus runSinglePhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    final ParallelIndexTaskRunner<SinglePhaseSubTask, PushedSegmentsReport> runner = createRunner(\n        toolbox,\n        this::createSinglePhaseTaskRunner\n    );\n\n    final TaskState state = runNextPhase(runner);\n    TaskStatus taskStatus;\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, runner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(runner.getReports());\n      }\n      taskStatus = TaskStatus.success(getId());\n    } else {\n      \r\n      Preconditions.checkState(state.isFailure(), \"Unrecognized state after task is complete[%s]\", state);\n      final String errorMessage = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          runner.getName()\n      );\n      taskStatus = TaskStatus.failure(getId(), errorMessage);\n    }\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":562,"status":"M"},{"authorDate":"2021-08-14 04:40:25","commitOrder":9,"curCode":"  TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Map<Interval, Integer> intervalToNumShards;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    final boolean needsInputSampling =\n        partitionsSpec.getNumShards() == null\n        || ingestionSchemaToUse.getDataSchema().getGranularitySpec().inputIntervals().isEmpty();\n    if (needsInputSampling) {\n      \r\n      LOG.info(\"Needs to determine intervals or numShards, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        String errMsg = StringUtils.format(\n            TASK_PHASE_FAILURE_MSG,\n            cardinalityRunner.getName()\n        );\n        return TaskStatus.failure(getId(), errMsg);\n      }\n\n      if (cardinalityRunner.getReports().isEmpty()) {\n        String msg = \"No valid rows for hash partitioning.\"\n                     + \" All rows may have invalid timestamps or have been filtered out.\";\n        LOG.warn(msg);\n        return TaskStatus.success(getId(), msg);\n      }\n\n      if (partitionsSpec.getNumShards() == null) {\n        int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                         ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                         : partitionsSpec.getMaxRowsPerSegment();\n        LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n        intervalToNumShards = determineNumShardsFromCardinalityReport(\n            cardinalityRunner.getReports().values(),\n            effectiveMaxRowsPerSegment\n        );\n      } else {\n        intervalToNumShards = CollectionUtils.mapValues(\n            mergeCardinalityReports(cardinalityRunner.getReports().values()),\n            k -> partitionsSpec.getNumShards()\n        );\n      }\n\n      ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n          ingestionSchemaToUse,\n          intervalToNumShards.keySet()\n      );\n    } else {\n      \r\n      intervalToNumShards = null;\n    }\n\n    \r\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, segmentCreateIngestionSpec, intervalToNumShards)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          indexingRunner.getName()\n      );\n      return TaskStatus.failure(getId(), errMsg);\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<PartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    state = runNextPhase(mergeRunner);\n    TaskStatus taskStatus;\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(mergeRunner.getReports());\n      }\n      taskStatus = TaskStatus.success(getId());\n    } else {\n      \r\n      Preconditions.checkState(state.isFailure(), \"Unrecognized state after task is complete[%s]\", state);\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          mergeRunner.getName()\n      );\n      taskStatus = TaskStatus.failure(getId(), errMsg);\n    }\n\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","date":"2021-08-14 04:40:25","endLine":749,"groupId":"104780","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"runHashPartitionMultiPhaseParallel","params":"(TaskToolboxtoolbox)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/b6/6f49f7a6c8a7b15f150b69f2416af8f16759e2.src","preCode":"  TaskStatus runHashPartitionMultiPhaseParallel(TaskToolbox toolbox) throws Exception\n  {\n    TaskState state;\n    ParallelIndexIngestionSpec ingestionSchemaToUse = ingestionSchema;\n\n    if (!(ingestionSchema.getTuningConfig().getPartitionsSpec() instanceof HashedPartitionsSpec)) {\n      \r\n      throw new ISE(\n          \"forceGuaranteedRollup is set but partitionsSpec [%s] is not a single_dim or hash partition spec.\",\n          ingestionSchema.getTuningConfig().getPartitionsSpec()\n      );\n    }\n\n    final Map<Interval, Integer> intervalToNumShards;\n    HashedPartitionsSpec partitionsSpec = (HashedPartitionsSpec) ingestionSchema.getTuningConfig().getPartitionsSpec();\n    final boolean needsInputSampling =\n        partitionsSpec.getNumShards() == null\n        || ingestionSchemaToUse.getDataSchema().getGranularitySpec().inputIntervals().isEmpty();\n    if (needsInputSampling) {\n      \r\n      LOG.info(\"Needs to determine intervals or numShards, beginning %s phase.\", PartialDimensionCardinalityTask.TYPE);\n      ParallelIndexTaskRunner<PartialDimensionCardinalityTask, DimensionCardinalityReport> cardinalityRunner =\n          createRunner(\n              toolbox,\n              this::createPartialDimensionCardinalityRunner\n          );\n\n      state = runNextPhase(cardinalityRunner);\n      if (state.isFailure()) {\n        String errMsg = StringUtils.format(\n            TASK_PHASE_FAILURE_MSG,\n            cardinalityRunner.getName()\n        );\n        return TaskStatus.failure(getId(), errMsg);\n      }\n\n      if (cardinalityRunner.getReports().isEmpty()) {\n        String msg = \"No valid rows for hash partitioning.\"\n                     + \" All rows may have invalid timestamps or have been filtered out.\";\n        LOG.warn(msg);\n        return TaskStatus.success(getId(), msg);\n      }\n\n      if (partitionsSpec.getNumShards() == null) {\n        int effectiveMaxRowsPerSegment = partitionsSpec.getMaxRowsPerSegment() == null\n                                         ? PartitionsSpec.DEFAULT_MAX_ROWS_PER_SEGMENT\n                                         : partitionsSpec.getMaxRowsPerSegment();\n        LOG.info(\"effective maxRowsPerSegment is: \" + effectiveMaxRowsPerSegment);\n\n        intervalToNumShards = determineNumShardsFromCardinalityReport(\n            cardinalityRunner.getReports().values(),\n            effectiveMaxRowsPerSegment\n        );\n      } else {\n        intervalToNumShards = CollectionUtils.mapValues(\n            mergeCardinalityReports(cardinalityRunner.getReports().values()),\n            k -> partitionsSpec.getNumShards()\n        );\n      }\n\n      ingestionSchemaToUse = rewriteIngestionSpecWithIntervalsIfMissing(\n          ingestionSchemaToUse,\n          intervalToNumShards.keySet()\n      );\n    } else {\n      \r\n      intervalToNumShards = null;\n    }\n\n    \r\n    final ParallelIndexIngestionSpec segmentCreateIngestionSpec = ingestionSchemaToUse;\n    ParallelIndexTaskRunner<PartialHashSegmentGenerateTask, GeneratedPartitionsReport> indexingRunner =\n        createRunner(\n            toolbox,\n            f -> createPartialHashSegmentGenerateRunner(toolbox, segmentCreateIngestionSpec, intervalToNumShards)\n        );\n\n    state = runNextPhase(indexingRunner);\n    if (state.isFailure()) {\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          indexingRunner.getName()\n      );\n      return TaskStatus.failure(getId(), errMsg);\n    }\n\n    \r\n    \r\n    Map<Pair<Interval, Integer>, List<PartitionLocation>> partitionToLocations =\n        groupGenericPartitionLocationsPerPartition(indexingRunner.getReports());\n    final List<PartialSegmentMergeIOConfig> ioConfigs = createGenericMergeIOConfigs(\n        ingestionSchema.getTuningConfig().getTotalNumMergeTasks(),\n        partitionToLocations\n    );\n\n    final ParallelIndexIngestionSpec segmentMergeIngestionSpec = ingestionSchemaToUse;\n    final ParallelIndexTaskRunner<PartialGenericSegmentMergeTask, PushedSegmentsReport> mergeRunner = createRunner(\n        toolbox,\n        tb -> createPartialGenericSegmentMergeRunner(tb, ioConfigs, segmentMergeIngestionSpec)\n    );\n    state = runNextPhase(mergeRunner);\n    TaskStatus taskStatus;\n    if (state.isSuccess()) {\n      \r\n      publishSegments(toolbox, mergeRunner.getReports());\n      if (awaitSegmentAvailabilityTimeoutMillis > 0) {\n        waitForSegmentAvailability(mergeRunner.getReports());\n      }\n      taskStatus = TaskStatus.success(getId());\n    } else {\n      \r\n      Preconditions.checkState(state.isFailure(), \"Unrecognized state after task is complete[%s]\", state);\n      String errMsg = StringUtils.format(\n          TASK_PHASE_FAILURE_MSG,\n          mergeRunner.getName()\n      );\n      taskStatus = TaskStatus.failure(getId(), errMsg);\n    }\n\n    toolbox.getTaskReportFileWriter().write(\n        getId(),\n        getTaskCompletionReports(taskStatus, segmentAvailabilityConfirmationCompleted)\n    );\n    return taskStatus;\n  }\n","realPath":"indexing-service/src/main/java/org/apache/druid/indexing/common/task/batch/parallel/ParallelIndexSupervisorTask.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":625,"status":"N"}],"commitId":"22b41ddbbfe2b07b085e295ba171bcdc07e04900","commitMessage":"@@@Task reports for parallel task: single phase and sequential mode (#11688)\n\n* Task reports for parallel task: single phase and sequential mode\n\n* Address comments\n\n* Add null check for currentSubTaskHolder","date":"2021-09-17 02:58:11","modifiedFileCount":"13","status":"M","submitter":"Jonathan Wei"}]
