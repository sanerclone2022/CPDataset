[{"authorTime":"2019-04-10 00:03:26","codes":[{"authorDate":"2019-04-10 00:03:26","commitOrder":1,"curCode":"  private static OrcStruct getFirstRow(Job job, String orcPath) throws IOException, InterruptedException\n  {\n    File testFile = new File(orcPath);\n    Path path = new Path(testFile.getAbsoluteFile().toURI());\n    FileSplit split = new FileSplit(path, 0, testFile.length(), null);\n\n    InputFormat inputFormat = ReflectionUtils.newInstance(\n        OrcInputFormat.class,\n        job.getConfiguration()\n    );\n    TaskAttemptContext context = new TaskAttemptContextImpl(job.getConfiguration(), new TaskAttemptID());\n\n    try (RecordReader reader = inputFormat.createRecordReader(split, context)) {\n\n      reader.initialize(split, context);\n      reader.nextKeyValue();\n      return (OrcStruct) reader.getCurrentValue();\n    }\n  }\n","date":"2019-04-10 00:03:26","endLine":239,"groupId":"6021","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"getFirstRow","params":"(Jobjob@StringorcPath)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/8d/d399fa0dd12d4cb5f0ae40c6e8c06c506ede65.src","preCode":"  private static OrcStruct getFirstRow(Job job, String orcPath) throws IOException, InterruptedException\n  {\n    File testFile = new File(orcPath);\n    Path path = new Path(testFile.getAbsoluteFile().toURI());\n    FileSplit split = new FileSplit(path, 0, testFile.length(), null);\n\n    InputFormat inputFormat = ReflectionUtils.newInstance(\n        OrcInputFormat.class,\n        job.getConfiguration()\n    );\n    TaskAttemptContext context = new TaskAttemptContextImpl(job.getConfiguration(), new TaskAttemptID());\n\n    try (RecordReader reader = inputFormat.createRecordReader(split, context)) {\n\n      reader.initialize(split, context);\n      reader.nextKeyValue();\n      return (OrcStruct) reader.getCurrentValue();\n    }\n  }\n","realPath":"extensions-core/orc-extensions/src/test/java/org/apache/druid/data/input/orc/OrcHadoopInputRowParserTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":221,"status":"B"},{"authorDate":"2019-04-10 00:03:26","commitOrder":1,"curCode":"  private static List<InputRow> getAllRows(HadoopDruidIndexerConfig config)\n      throws IOException, InterruptedException\n  {\n    Job job = Job.getInstance(new Configuration());\n    config.intoConfiguration(job);\n\n    File testFile = new File(((StaticPathSpec) config.getPathSpec()).getPaths());\n    Path path = new Path(testFile.getAbsoluteFile().toURI());\n    FileSplit split = new FileSplit(path, 0, testFile.length(), null);\n\n    InputFormat inputFormat = ReflectionUtils.newInstance(\n        OrcInputFormat.class,\n        job.getConfiguration()\n    );\n    TaskAttemptContext context = new TaskAttemptContextImpl(job.getConfiguration(), new TaskAttemptID());\n\n    try (RecordReader reader = inputFormat.createRecordReader(split, context)) {\n      List<InputRow> records = new ArrayList<>();\n      InputRowParser parser = config.getParser();\n\n      reader.initialize(split, context);\n      while (reader.nextKeyValue()) {\n        reader.nextKeyValue();\n        Object data = reader.getCurrentValue();\n        records.add(((List<InputRow>) parser.parseBatch(data)).get(0));\n      }\n\n      return records;\n    }\n  }\n","date":"2019-04-10 00:03:26","endLine":270,"groupId":"14014","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"getAllRows","params":"(HadoopDruidIndexerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/8d/d399fa0dd12d4cb5f0ae40c6e8c06c506ede65.src","preCode":"  private static List<InputRow> getAllRows(HadoopDruidIndexerConfig config)\n      throws IOException, InterruptedException\n  {\n    Job job = Job.getInstance(new Configuration());\n    config.intoConfiguration(job);\n\n    File testFile = new File(((StaticPathSpec) config.getPathSpec()).getPaths());\n    Path path = new Path(testFile.getAbsoluteFile().toURI());\n    FileSplit split = new FileSplit(path, 0, testFile.length(), null);\n\n    InputFormat inputFormat = ReflectionUtils.newInstance(\n        OrcInputFormat.class,\n        job.getConfiguration()\n    );\n    TaskAttemptContext context = new TaskAttemptContextImpl(job.getConfiguration(), new TaskAttemptID());\n\n    try (RecordReader reader = inputFormat.createRecordReader(split, context)) {\n      List<InputRow> records = new ArrayList<>();\n      InputRowParser parser = config.getParser();\n\n      reader.initialize(split, context);\n      while (reader.nextKeyValue()) {\n        reader.nextKeyValue();\n        Object data = reader.getCurrentValue();\n        records.add(((List<InputRow>) parser.parseBatch(data)).get(0));\n      }\n\n      return records;\n    }\n  }\n","realPath":"extensions-core/orc-extensions/src/test/java/org/apache/druid/data/input/orc/OrcHadoopInputRowParserTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":241,"status":"B"}],"commitId":"89bb43f382643e7bad05681956c6a08778cf0165","commitMessage":"@@@'core' ORC extension (#7138)\n\n* orc extension reworked to use apache orc map-reduce lib.  moved to core extensions.  support for flattenSpec.  tests.  docs\n\n* change binary handling to be compatible with avro and parquet.  Rows.objectToStrings now converts byte[] to base64.  change date handling\n\n* better docs and tests\n\n* fix it\n\n* formatting\n\n* doc fix\n\n* fix it\n\n* exclude redundant dependencies\n\n* use latest orc-mapreduce.  add hadoop jobProperties recommendations to docs\n\n* doc fix\n\n* review stuff and fix binaryAsString\n\n* cache for root level fields\n\n* more better\n","date":"2019-04-10 00:03:26","modifiedFileCount":"1","status":"B","submitter":"Clint Wylie"},{"authorTime":"2019-11-29 04:45:24","codes":[{"authorDate":"2019-11-29 04:45:24","commitOrder":2,"curCode":"  private static OrcStruct getFirstRow(Job job, String orcPath) throws IOException\n  {\n    File testFile = new File(orcPath);\n    Path path = new Path(testFile.getAbsoluteFile().toURI());\n    FileSplit split = new FileSplit(path, 0, testFile.length(), new String[]{\"host\"});\n\n    InputFormat<NullWritable, OrcStruct> inputFormat = ReflectionUtils.newInstance(\n        OrcInputFormat.class,\n        job.getConfiguration()\n    );\n    RecordReader<NullWritable, OrcStruct> reader = inputFormat.getRecordReader(\n        split,\n        new JobConf(job.getConfiguration()),\n        null\n    );\n    try {\n      final NullWritable key = reader.createKey();\n      final OrcStruct value = reader.createValue();\n      if (reader.next(key, value)) {\n        return value;\n      } else {\n        throw new NoSuchElementException();\n      }\n    }\n    finally {\n      reader.close();\n    }\n  }\n","date":"2019-11-29 04:45:24","endLine":247,"groupId":"102122","id":3,"instanceNumber":1,"isCurCommit":1,"methodName":"getFirstRow","params":"(Jobjob@StringorcPath)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/38/eb5da8d5b39c764c845b9a44970d15cdb906c8.src","preCode":"  private static OrcStruct getFirstRow(Job job, String orcPath) throws IOException, InterruptedException\n  {\n    File testFile = new File(orcPath);\n    Path path = new Path(testFile.getAbsoluteFile().toURI());\n    FileSplit split = new FileSplit(path, 0, testFile.length(), null);\n\n    InputFormat inputFormat = ReflectionUtils.newInstance(\n        OrcInputFormat.class,\n        job.getConfiguration()\n    );\n    TaskAttemptContext context = new TaskAttemptContextImpl(job.getConfiguration(), new TaskAttemptID());\n\n    try (RecordReader reader = inputFormat.createRecordReader(split, context)) {\n\n      reader.initialize(split, context);\n      reader.nextKeyValue();\n      return (OrcStruct) reader.getCurrentValue();\n    }\n  }\n","realPath":"extensions-core/orc-extensions/src/test/java/org/apache/druid/data/input/orc/OrcHadoopInputRowParserTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":220,"status":"M"},{"authorDate":"2019-11-29 04:45:24","commitOrder":2,"curCode":"  private static List<InputRow> getAllRows(HadoopDruidIndexerConfig config) throws IOException\n  {\n    Job job = Job.getInstance(new Configuration());\n    config.intoConfiguration(job);\n\n    File testFile = new File(((StaticPathSpec) config.getPathSpec()).getPaths());\n    Path path = new Path(testFile.getAbsoluteFile().toURI());\n    FileSplit split = new FileSplit(path, 0, testFile.length(), new String[]{\"host\"});\n\n    InputFormat<NullWritable, OrcStruct> inputFormat = ReflectionUtils.newInstance(\n        OrcInputFormat.class,\n        job.getConfiguration()\n    );\n    RecordReader<NullWritable, OrcStruct> reader = inputFormat.getRecordReader(\n        split,\n        new JobConf(job.getConfiguration()),\n        null\n    );\n    try {\n      List<InputRow> records = new ArrayList<>();\n      InputRowParser parser = config.getParser();\n      final NullWritable key = reader.createKey();\n      OrcStruct value = reader.createValue();\n\n      while (reader.next(key, value)) {\n        records.add(((List<InputRow>) parser.parseBatch(value)).get(0));\n        value = reader.createValue();\n      }\n\n      return records;\n    }\n    finally {\n      reader.close();\n    }\n  }\n","date":"2019-11-29 04:45:24","endLine":283,"groupId":"102122","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"getAllRows","params":"(HadoopDruidIndexerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-druid-10-0.7/blobInfo/CC_OUT/blobs/38/eb5da8d5b39c764c845b9a44970d15cdb906c8.src","preCode":"  private static List<InputRow> getAllRows(HadoopDruidIndexerConfig config)\n      throws IOException, InterruptedException\n  {\n    Job job = Job.getInstance(new Configuration());\n    config.intoConfiguration(job);\n\n    File testFile = new File(((StaticPathSpec) config.getPathSpec()).getPaths());\n    Path path = new Path(testFile.getAbsoluteFile().toURI());\n    FileSplit split = new FileSplit(path, 0, testFile.length(), null);\n\n    InputFormat inputFormat = ReflectionUtils.newInstance(\n        OrcInputFormat.class,\n        job.getConfiguration()\n    );\n    TaskAttemptContext context = new TaskAttemptContextImpl(job.getConfiguration(), new TaskAttemptID());\n\n    try (RecordReader reader = inputFormat.createRecordReader(split, context)) {\n      List<InputRow> records = new ArrayList<>();\n      InputRowParser parser = config.getParser();\n\n      reader.initialize(split, context);\n      while (reader.nextKeyValue()) {\n        reader.nextKeyValue();\n        Object data = reader.getCurrentValue();\n        records.add(((List<InputRow>) parser.parseBatch(data)).get(0));\n      }\n\n      return records;\n    }\n  }\n","realPath":"extensions-core/orc-extensions/src/test/java/org/apache/druid/data/input/orc/OrcHadoopInputRowParserTest.java","repoName":"druid","snippetEndLine":0,"snippetStartLine":0,"startLine":249,"status":"M"}],"commitId":"86e8903523fc3d2d177963d2f95fd56441605102","commitMessage":"@@@Support orc format for native batch ingestion (#8950)\n\n* Support orc format for native batch ingestion\n\n* fix pom and remove wrong comment\n\n* fix unnecessary condition check\n\n* use flatMap back to handle exception properly\n\n* move exceptionThrowingIterator to intermediateRowParsingReader\n\n* runtime\n","date":"2019-11-29 04:45:24","modifiedFileCount":"5","status":"M","submitter":"Jihoon Son"}]
