[{"authorTime":"2020-05-07 08:20:26","codes":[{"authorDate":"2020-05-07 08:20:26","commitOrder":1,"curCode":"    public void testDataNodeRestartWithBusyMasterDuringSnapshot() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        internalCluster().startDataOnlyNodes(2);\n        logger.info(\"-->  creating repository\");\n        assertAcked(client().admin().cluster().preparePutRepository(\"test-repo\")\n            .setType(\"mock\").setSettings(Settings.builder()\n                .put(\"location\", randomRepoPath())\n                .put(\"compress\", randomBoolean())\n                .put(\"chunk_size\", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));\n        assertAcked(prepareCreate(\"test-idx\", 0, Settings.builder()\n            .put(\"number_of_shards\", 5).put(\"number_of_replicas\", 0)));\n        ensureGreen();\n        logger.info(\"-->  indexing some data\");\n        final int numdocs = randomIntBetween(50, 100);\n        IndexRequestBuilder[] builders = new IndexRequestBuilder[numdocs];\n        for (int i = 0; i < builders.length; i++) {\n            builders[i] = client().prepareIndex(\"test-idx\").setId(Integer.toString(i)).setSource(\"field1\", \"bar \" + i);\n        }\n        indexRandom(true, builders);\n        flushAndRefresh();\n        final String dataNode = blockNodeWithIndex(\"test-repo\", \"test-idx\");\n        logger.info(\"-->  snapshot\");\n        ServiceDisruptionScheme disruption = new BusyMasterServiceDisruption(random(), Priority.HIGH);\n        setDisruptionScheme(disruption);\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        disruption.startDisrupting();\n        logger.info(\"-->  restarting data node, which should cause primary shards to be failed\");\n        internalCluster().restartNode(dataNode, InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshots to show as failed\");\n        assertBusy(() -> assertThat(\n            client().admin().cluster().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), greaterThanOrEqualTo(1)), 60L, TimeUnit.SECONDS);\n\n        unblockNode(\"test-repo\", dataNode);\n        disruption.stopDisrupting();\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n        }, 60L, TimeUnit.SECONDS);\n    }\n","date":"2020-05-07 08:20:26","endLine":1215,"groupId":"38208","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testDataNodeRestartWithBusyMasterDuringSnapshot","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/65/a783e727c2fb590f5744ca0c7c366d8c7f5252.src","preCode":"    public void testDataNodeRestartWithBusyMasterDuringSnapshot() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        internalCluster().startDataOnlyNodes(2);\n        logger.info(\"-->  creating repository\");\n        assertAcked(client().admin().cluster().preparePutRepository(\"test-repo\")\n            .setType(\"mock\").setSettings(Settings.builder()\n                .put(\"location\", randomRepoPath())\n                .put(\"compress\", randomBoolean())\n                .put(\"chunk_size\", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));\n        assertAcked(prepareCreate(\"test-idx\", 0, Settings.builder()\n            .put(\"number_of_shards\", 5).put(\"number_of_replicas\", 0)));\n        ensureGreen();\n        logger.info(\"-->  indexing some data\");\n        final int numdocs = randomIntBetween(50, 100);\n        IndexRequestBuilder[] builders = new IndexRequestBuilder[numdocs];\n        for (int i = 0; i < builders.length; i++) {\n            builders[i] = client().prepareIndex(\"test-idx\").setId(Integer.toString(i)).setSource(\"field1\", \"bar \" + i);\n        }\n        indexRandom(true, builders);\n        flushAndRefresh();\n        final String dataNode = blockNodeWithIndex(\"test-repo\", \"test-idx\");\n        logger.info(\"-->  snapshot\");\n        ServiceDisruptionScheme disruption = new BusyMasterServiceDisruption(random(), Priority.HIGH);\n        setDisruptionScheme(disruption);\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        disruption.startDisrupting();\n        logger.info(\"-->  restarting data node, which should cause primary shards to be failed\");\n        internalCluster().restartNode(dataNode, InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshots to show as failed\");\n        assertBusy(() -> assertThat(\n            client().admin().cluster().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), greaterThanOrEqualTo(1)), 60L, TimeUnit.SECONDS);\n\n        unblockNode(\"test-repo\", dataNode);\n        disruption.stopDisrupting();\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n        }, 60L, TimeUnit.SECONDS);\n    }\n","realPath":"server/src/internalClusterTest/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1169,"status":"B"},{"authorDate":"2020-05-07 08:20:26","commitOrder":1,"curCode":"    public void testDataNodeRestartAfterShardSnapshotFailure() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        final List<String> dataNodes = internalCluster().startDataOnlyNodes(2);\n        logger.info(\"-->  creating repository\");\n        assertAcked(client().admin().cluster().preparePutRepository(\"test-repo\")\n            .setType(\"mock\").setSettings(Settings.builder()\n                .put(\"location\", randomRepoPath())\n                .put(\"compress\", randomBoolean())\n                .put(\"chunk_size\", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));\n        assertAcked(prepareCreate(\"test-idx\", 0, Settings.builder()\n            .put(\"number_of_shards\", 2).put(\"number_of_replicas\", 0)));\n        ensureGreen();\n        logger.info(\"-->  indexing some data\");\n        final int numdocs = randomIntBetween(50, 100);\n        IndexRequestBuilder[] builders = new IndexRequestBuilder[numdocs];\n        for (int i = 0; i < builders.length; i++) {\n            builders[i] = client().prepareIndex(\"test-idx\").setId(Integer.toString(i)).setSource(\"field1\", \"bar \" + i);\n        }\n        indexRandom(true, builders);\n        flushAndRefresh();\n        blockAllDataNodes(\"test-repo\");\n        logger.info(\"-->  snapshot\");\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        logger.info(\"-->  restarting first data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(0), InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshot of first primary to show as failed\");\n        assertBusy(() -> assertThat(\n            client().admin().cluster().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), is(1)), 60L, TimeUnit.SECONDS);\n\n        logger.info(\"-->  restarting second data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(1), InternalTestCluster.EMPTY_CALLBACK);\n\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n            assertThat(snapshotInfo.totalShards(), is(2));\n            assertThat(snapshotInfo.shardFailures(), hasSize(2));\n        }, 60L, TimeUnit.SECONDS);\n    }\n","date":"2020-05-07 08:20:26","endLine":1263,"groupId":"38208","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testDataNodeRestartAfterShardSnapshotFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/65/a783e727c2fb590f5744ca0c7c366d8c7f5252.src","preCode":"    public void testDataNodeRestartAfterShardSnapshotFailure() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        final List<String> dataNodes = internalCluster().startDataOnlyNodes(2);\n        logger.info(\"-->  creating repository\");\n        assertAcked(client().admin().cluster().preparePutRepository(\"test-repo\")\n            .setType(\"mock\").setSettings(Settings.builder()\n                .put(\"location\", randomRepoPath())\n                .put(\"compress\", randomBoolean())\n                .put(\"chunk_size\", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));\n        assertAcked(prepareCreate(\"test-idx\", 0, Settings.builder()\n            .put(\"number_of_shards\", 2).put(\"number_of_replicas\", 0)));\n        ensureGreen();\n        logger.info(\"-->  indexing some data\");\n        final int numdocs = randomIntBetween(50, 100);\n        IndexRequestBuilder[] builders = new IndexRequestBuilder[numdocs];\n        for (int i = 0; i < builders.length; i++) {\n            builders[i] = client().prepareIndex(\"test-idx\").setId(Integer.toString(i)).setSource(\"field1\", \"bar \" + i);\n        }\n        indexRandom(true, builders);\n        flushAndRefresh();\n        blockAllDataNodes(\"test-repo\");\n        logger.info(\"-->  snapshot\");\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        logger.info(\"-->  restarting first data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(0), InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshot of first primary to show as failed\");\n        assertBusy(() -> assertThat(\n            client().admin().cluster().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), is(1)), 60L, TimeUnit.SECONDS);\n\n        logger.info(\"-->  restarting second data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(1), InternalTestCluster.EMPTY_CALLBACK);\n\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n            assertThat(snapshotInfo.totalShards(), is(2));\n            assertThat(snapshotInfo.shardFailures(), hasSize(2));\n        }, 60L, TimeUnit.SECONDS);\n    }\n","realPath":"server/src/internalClusterTest/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1217,"status":"B"}],"commitId":"cabe6a330514f0f80f3b95be8763082b25efce89","commitMessage":"@@@Create plugin for internalClusterTest task (#55896)\n\nThis commit creates a new gradle plugin to provide a separate task name\nand source set for running ESIntegTestCase tests. The only project\nconverted to use the new plugin in this PR is server.  as an example. The\nremaining cases in x-pack will be handled in followups.","date":"2020-05-07 08:20:26","modifiedFileCount":"5","status":"B","submitter":"Ryan Ernst"},{"authorTime":"2020-06-10 16:44:19","codes":[{"authorDate":"2020-06-10 16:44:19","commitOrder":2,"curCode":"    public void testDataNodeRestartWithBusyMasterDuringSnapshot() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        internalCluster().startDataOnlyNodes(2);\n        logger.info(\"-->  creating repository\");\n        final Path repoPath = randomRepoPath();\n        assertAcked(client().admin().cluster().preparePutRepository(\"test-repo\")\n            .setType(\"mock\").setSettings(Settings.builder()\n                .put(\"location\", repoPath)\n                .put(\"compress\", randomBoolean())\n                .put(\"chunk_size\", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));\n        maybeInitWithOldSnapshotVersion(\"test-repo\", repoPath);\n\n        assertAcked(prepareCreate(\"test-idx\", 0, Settings.builder()\n            .put(\"number_of_shards\", 5).put(\"number_of_replicas\", 0)));\n        ensureGreen();\n        logger.info(\"-->  indexing some data\");\n        final int numdocs = randomIntBetween(50, 100);\n        IndexRequestBuilder[] builders = new IndexRequestBuilder[numdocs];\n        for (int i = 0; i < builders.length; i++) {\n            builders[i] = client().prepareIndex(\"test-idx\").setId(Integer.toString(i)).setSource(\"field1\", \"bar \" + i);\n        }\n        indexRandom(true, builders);\n        flushAndRefresh();\n        final String dataNode = blockNodeWithIndex(\"test-repo\", \"test-idx\");\n        logger.info(\"-->  snapshot\");\n        ServiceDisruptionScheme disruption = new BusyMasterServiceDisruption(random(), Priority.HIGH);\n        setDisruptionScheme(disruption);\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        disruption.startDisrupting();\n        logger.info(\"-->  restarting data node, which should cause primary shards to be failed\");\n        internalCluster().restartNode(dataNode, InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshots to show as failed\");\n        assertBusy(() -> assertThat(\n            client().admin().cluster().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), greaterThanOrEqualTo(1)), 60L, TimeUnit.SECONDS);\n\n        unblockNode(\"test-repo\", dataNode);\n        disruption.stopDisrupting();\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n        }, 60L, TimeUnit.SECONDS);\n    }\n","date":"2020-06-10 16:44:19","endLine":1292,"groupId":"38208","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testDataNodeRestartWithBusyMasterDuringSnapshot","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/fd/7b98a2365e61257ad5187be9351fbabcd86890.src","preCode":"    public void testDataNodeRestartWithBusyMasterDuringSnapshot() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        internalCluster().startDataOnlyNodes(2);\n        logger.info(\"-->  creating repository\");\n        assertAcked(client().admin().cluster().preparePutRepository(\"test-repo\")\n            .setType(\"mock\").setSettings(Settings.builder()\n                .put(\"location\", randomRepoPath())\n                .put(\"compress\", randomBoolean())\n                .put(\"chunk_size\", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));\n        assertAcked(prepareCreate(\"test-idx\", 0, Settings.builder()\n            .put(\"number_of_shards\", 5).put(\"number_of_replicas\", 0)));\n        ensureGreen();\n        logger.info(\"-->  indexing some data\");\n        final int numdocs = randomIntBetween(50, 100);\n        IndexRequestBuilder[] builders = new IndexRequestBuilder[numdocs];\n        for (int i = 0; i < builders.length; i++) {\n            builders[i] = client().prepareIndex(\"test-idx\").setId(Integer.toString(i)).setSource(\"field1\", \"bar \" + i);\n        }\n        indexRandom(true, builders);\n        flushAndRefresh();\n        final String dataNode = blockNodeWithIndex(\"test-repo\", \"test-idx\");\n        logger.info(\"-->  snapshot\");\n        ServiceDisruptionScheme disruption = new BusyMasterServiceDisruption(random(), Priority.HIGH);\n        setDisruptionScheme(disruption);\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        disruption.startDisrupting();\n        logger.info(\"-->  restarting data node, which should cause primary shards to be failed\");\n        internalCluster().restartNode(dataNode, InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshots to show as failed\");\n        assertBusy(() -> assertThat(\n            client().admin().cluster().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), greaterThanOrEqualTo(1)), 60L, TimeUnit.SECONDS);\n\n        unblockNode(\"test-repo\", dataNode);\n        disruption.stopDisrupting();\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n        }, 60L, TimeUnit.SECONDS);\n    }\n","realPath":"server/src/internalClusterTest/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1243,"status":"M"},{"authorDate":"2020-06-10 16:44:19","commitOrder":2,"curCode":"    public void testDataNodeRestartAfterShardSnapshotFailure() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        final List<String> dataNodes = internalCluster().startDataOnlyNodes(2);\n        logger.info(\"-->  creating repository\");\n        final Path repoPath = randomRepoPath();\n        assertAcked(client().admin().cluster().preparePutRepository(\"test-repo\")\n            .setType(\"mock\").setSettings(Settings.builder()\n                .put(\"location\", repoPath)\n                .put(\"compress\", randomBoolean())\n                .put(\"chunk_size\", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));\n        maybeInitWithOldSnapshotVersion(\"test-repo\", repoPath);\n\n        assertAcked(prepareCreate(\"test-idx\", 0, Settings.builder()\n            .put(\"number_of_shards\", 2).put(\"number_of_replicas\", 0)));\n        ensureGreen();\n        logger.info(\"-->  indexing some data\");\n        final int numdocs = randomIntBetween(50, 100);\n        IndexRequestBuilder[] builders = new IndexRequestBuilder[numdocs];\n        for (int i = 0; i < builders.length; i++) {\n            builders[i] = client().prepareIndex(\"test-idx\").setId(Integer.toString(i)).setSource(\"field1\", \"bar \" + i);\n        }\n        indexRandom(true, builders);\n        flushAndRefresh();\n        blockAllDataNodes(\"test-repo\");\n        logger.info(\"-->  snapshot\");\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        logger.info(\"-->  restarting first data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(0), InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshot of first primary to show as failed\");\n        assertBusy(() -> assertThat(\n            client().admin().cluster().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), is(1)), 60L, TimeUnit.SECONDS);\n\n        logger.info(\"-->  restarting second data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(1), InternalTestCluster.EMPTY_CALLBACK);\n\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n            assertThat(snapshotInfo.totalShards(), is(2));\n            assertThat(snapshotInfo.shardFailures(), hasSize(2));\n        }, 60L, TimeUnit.SECONDS);\n    }\n","date":"2020-06-10 16:44:19","endLine":1343,"groupId":"38208","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testDataNodeRestartAfterShardSnapshotFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/fd/7b98a2365e61257ad5187be9351fbabcd86890.src","preCode":"    public void testDataNodeRestartAfterShardSnapshotFailure() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        final List<String> dataNodes = internalCluster().startDataOnlyNodes(2);\n        logger.info(\"-->  creating repository\");\n        assertAcked(client().admin().cluster().preparePutRepository(\"test-repo\")\n            .setType(\"mock\").setSettings(Settings.builder()\n                .put(\"location\", randomRepoPath())\n                .put(\"compress\", randomBoolean())\n                .put(\"chunk_size\", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));\n        assertAcked(prepareCreate(\"test-idx\", 0, Settings.builder()\n            .put(\"number_of_shards\", 2).put(\"number_of_replicas\", 0)));\n        ensureGreen();\n        logger.info(\"-->  indexing some data\");\n        final int numdocs = randomIntBetween(50, 100);\n        IndexRequestBuilder[] builders = new IndexRequestBuilder[numdocs];\n        for (int i = 0; i < builders.length; i++) {\n            builders[i] = client().prepareIndex(\"test-idx\").setId(Integer.toString(i)).setSource(\"field1\", \"bar \" + i);\n        }\n        indexRandom(true, builders);\n        flushAndRefresh();\n        blockAllDataNodes(\"test-repo\");\n        logger.info(\"-->  snapshot\");\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        logger.info(\"-->  restarting first data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(0), InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshot of first primary to show as failed\");\n        assertBusy(() -> assertThat(\n            client().admin().cluster().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), is(1)), 60L, TimeUnit.SECONDS);\n\n        logger.info(\"-->  restarting second data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(1), InternalTestCluster.EMPTY_CALLBACK);\n\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n            assertThat(snapshotInfo.totalShards(), is(2));\n            assertThat(snapshotInfo.shardFailures(), hasSize(2));\n        }, 60L, TimeUnit.SECONDS);\n    }\n","realPath":"server/src/internalClusterTest/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1294,"status":"M"}],"commitId":"f3b53d9dfed0dbca1ff10194ca0d433494ec6701","commitMessage":"@@@Improve Test Coverage for Old Repository Metadata Formats (#57915)\n\nUse the the hack used in `CorruptedBlobStoreRepositoryIT` in more snapshot\nfailure tests to verify that BwC repository metadata is handled properly\nin these so far not-test-covered scenarios.\nAlso.  some minor related dry-up of snapshot tests.\n\nRelates #57798","date":"2020-06-10 16:44:19","modifiedFileCount":"5","status":"M","submitter":"Armin Braun"},{"authorTime":"2020-07-06 16:42:47","codes":[{"authorDate":"2020-06-10 16:44:19","commitOrder":3,"curCode":"    public void testDataNodeRestartWithBusyMasterDuringSnapshot() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        internalCluster().startDataOnlyNodes(2);\n        logger.info(\"-->  creating repository\");\n        final Path repoPath = randomRepoPath();\n        assertAcked(client().admin().cluster().preparePutRepository(\"test-repo\")\n            .setType(\"mock\").setSettings(Settings.builder()\n                .put(\"location\", repoPath)\n                .put(\"compress\", randomBoolean())\n                .put(\"chunk_size\", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));\n        maybeInitWithOldSnapshotVersion(\"test-repo\", repoPath);\n\n        assertAcked(prepareCreate(\"test-idx\", 0, Settings.builder()\n            .put(\"number_of_shards\", 5).put(\"number_of_replicas\", 0)));\n        ensureGreen();\n        logger.info(\"-->  indexing some data\");\n        final int numdocs = randomIntBetween(50, 100);\n        IndexRequestBuilder[] builders = new IndexRequestBuilder[numdocs];\n        for (int i = 0; i < builders.length; i++) {\n            builders[i] = client().prepareIndex(\"test-idx\").setId(Integer.toString(i)).setSource(\"field1\", \"bar \" + i);\n        }\n        indexRandom(true, builders);\n        flushAndRefresh();\n        final String dataNode = blockNodeWithIndex(\"test-repo\", \"test-idx\");\n        logger.info(\"-->  snapshot\");\n        ServiceDisruptionScheme disruption = new BusyMasterServiceDisruption(random(), Priority.HIGH);\n        setDisruptionScheme(disruption);\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        disruption.startDisrupting();\n        logger.info(\"-->  restarting data node, which should cause primary shards to be failed\");\n        internalCluster().restartNode(dataNode, InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshots to show as failed\");\n        assertBusy(() -> assertThat(\n            client().admin().cluster().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), greaterThanOrEqualTo(1)), 60L, TimeUnit.SECONDS);\n\n        unblockNode(\"test-repo\", dataNode);\n        disruption.stopDisrupting();\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n        }, 60L, TimeUnit.SECONDS);\n    }\n","date":"2020-06-10 16:44:19","endLine":1292,"groupId":"38208","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testDataNodeRestartWithBusyMasterDuringSnapshot","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/fd/7b98a2365e61257ad5187be9351fbabcd86890.src","preCode":"    public void testDataNodeRestartWithBusyMasterDuringSnapshot() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        internalCluster().startDataOnlyNodes(2);\n        logger.info(\"-->  creating repository\");\n        final Path repoPath = randomRepoPath();\n        assertAcked(client().admin().cluster().preparePutRepository(\"test-repo\")\n            .setType(\"mock\").setSettings(Settings.builder()\n                .put(\"location\", repoPath)\n                .put(\"compress\", randomBoolean())\n                .put(\"chunk_size\", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));\n        maybeInitWithOldSnapshotVersion(\"test-repo\", repoPath);\n\n        assertAcked(prepareCreate(\"test-idx\", 0, Settings.builder()\n            .put(\"number_of_shards\", 5).put(\"number_of_replicas\", 0)));\n        ensureGreen();\n        logger.info(\"-->  indexing some data\");\n        final int numdocs = randomIntBetween(50, 100);\n        IndexRequestBuilder[] builders = new IndexRequestBuilder[numdocs];\n        for (int i = 0; i < builders.length; i++) {\n            builders[i] = client().prepareIndex(\"test-idx\").setId(Integer.toString(i)).setSource(\"field1\", \"bar \" + i);\n        }\n        indexRandom(true, builders);\n        flushAndRefresh();\n        final String dataNode = blockNodeWithIndex(\"test-repo\", \"test-idx\");\n        logger.info(\"-->  snapshot\");\n        ServiceDisruptionScheme disruption = new BusyMasterServiceDisruption(random(), Priority.HIGH);\n        setDisruptionScheme(disruption);\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        disruption.startDisrupting();\n        logger.info(\"-->  restarting data node, which should cause primary shards to be failed\");\n        internalCluster().restartNode(dataNode, InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshots to show as failed\");\n        assertBusy(() -> assertThat(\n            client().admin().cluster().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), greaterThanOrEqualTo(1)), 60L, TimeUnit.SECONDS);\n\n        unblockNode(\"test-repo\", dataNode);\n        disruption.stopDisrupting();\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n        }, 60L, TimeUnit.SECONDS);\n    }\n","realPath":"server/src/internalClusterTest/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1243,"status":"N"},{"authorDate":"2020-07-06 16:42:47","commitOrder":3,"curCode":"    public void testDataNodeRestartAfterShardSnapshotFailure() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        final List<String> dataNodes = internalCluster().startDataOnlyNodes(2);\n        final Path repoPath = randomRepoPath();\n        createRepository(\"test-repo\", \"mock\", Settings.builder()\n            .put(\"location\", repoPath).put(\"compress\", randomBoolean())\n            .put(\"chunk_size\", randomIntBetween(100, 1000), ByteSizeUnit.BYTES));\n        maybeInitWithOldSnapshotVersion(\"test-repo\", repoPath);\n\n        assertAcked(prepareCreate(\"test-idx\", 0, Settings.builder()\n            .put(\"number_of_shards\", 2).put(\"number_of_replicas\", 0)));\n        ensureGreen();\n        logger.info(\"-->  indexing some data\");\n        final int numdocs = randomIntBetween(50, 100);\n        IndexRequestBuilder[] builders = new IndexRequestBuilder[numdocs];\n        for (int i = 0; i < builders.length; i++) {\n            builders[i] = client().prepareIndex(\"test-idx\").setId(Integer.toString(i)).setSource(\"field1\", \"bar \" + i);\n        }\n        indexRandom(true, builders);\n        flushAndRefresh();\n        blockAllDataNodes(\"test-repo\");\n        logger.info(\"-->  snapshot\");\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        logger.info(\"-->  restarting first data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(0), InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshot of first primary to show as failed\");\n        assertBusy(() -> assertThat(\n            client().admin().cluster().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), is(1)), 60L, TimeUnit.SECONDS);\n\n        logger.info(\"-->  restarting second data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(1), InternalTestCluster.EMPTY_CALLBACK);\n\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n            assertThat(snapshotInfo.totalShards(), is(2));\n            assertThat(snapshotInfo.shardFailures(), hasSize(2));\n        }, 60L, TimeUnit.SECONDS);\n    }\n","date":"2020-07-06 16:42:47","endLine":1290,"groupId":"38208","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testDataNodeRestartAfterShardSnapshotFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/0c/87caac7efacb5aa29e5fa998c87c4bb8e613bd.src","preCode":"    public void testDataNodeRestartAfterShardSnapshotFailure() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        final List<String> dataNodes = internalCluster().startDataOnlyNodes(2);\n        logger.info(\"-->  creating repository\");\n        final Path repoPath = randomRepoPath();\n        assertAcked(client().admin().cluster().preparePutRepository(\"test-repo\")\n            .setType(\"mock\").setSettings(Settings.builder()\n                .put(\"location\", repoPath)\n                .put(\"compress\", randomBoolean())\n                .put(\"chunk_size\", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));\n        maybeInitWithOldSnapshotVersion(\"test-repo\", repoPath);\n\n        assertAcked(prepareCreate(\"test-idx\", 0, Settings.builder()\n            .put(\"number_of_shards\", 2).put(\"number_of_replicas\", 0)));\n        ensureGreen();\n        logger.info(\"-->  indexing some data\");\n        final int numdocs = randomIntBetween(50, 100);\n        IndexRequestBuilder[] builders = new IndexRequestBuilder[numdocs];\n        for (int i = 0; i < builders.length; i++) {\n            builders[i] = client().prepareIndex(\"test-idx\").setId(Integer.toString(i)).setSource(\"field1\", \"bar \" + i);\n        }\n        indexRandom(true, builders);\n        flushAndRefresh();\n        blockAllDataNodes(\"test-repo\");\n        logger.info(\"-->  snapshot\");\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        logger.info(\"-->  restarting first data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(0), InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshot of first primary to show as failed\");\n        assertBusy(() -> assertThat(\n            client().admin().cluster().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), is(1)), 60L, TimeUnit.SECONDS);\n\n        logger.info(\"-->  restarting second data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(1), InternalTestCluster.EMPTY_CALLBACK);\n\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n            assertThat(snapshotInfo.totalShards(), is(2));\n            assertThat(snapshotInfo.shardFailures(), hasSize(2));\n        }, 60L, TimeUnit.SECONDS);\n    }\n","realPath":"server/src/internalClusterTest/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1244,"status":"M"}],"commitId":"2ba8e2e98bf4b7c54929c0a19046deba61951817","commitMessage":"@@@Merge branch 'master' into feature/runtime_fields\n","date":"2020-07-06 16:42:47","modifiedFileCount":"330","status":"M","submitter":"Luca Cavanna"},{"authorTime":"2020-07-10 21:44:56","codes":[{"authorDate":"2020-07-10 21:44:56","commitOrder":4,"curCode":"    public void testDataNodeRestartWithBusyMasterDuringSnapshot() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        internalCluster().startDataOnlyNodes(2);\n        final Path repoPath = randomRepoPath();\n        createRepository(\"test-repo\", \"mock\", repoPath);\n        maybeInitWithOldSnapshotVersion(\"test-repo\", repoPath);\n\n        assertAcked(prepareCreate(\"test-idx\", 0, indexSettingsNoReplicas(5)));\n        ensureGreen();\n        indexRandomDocs(\"test-idx\", randomIntBetween(50, 100));\n\n        final String dataNode = blockNodeWithIndex(\"test-repo\", \"test-idx\");\n        logger.info(\"-->  snapshot\");\n        ServiceDisruptionScheme disruption = new BusyMasterServiceDisruption(random(), Priority.HIGH);\n        setDisruptionScheme(disruption);\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        disruption.startDisrupting();\n        logger.info(\"-->  restarting data node, which should cause primary shards to be failed\");\n        internalCluster().restartNode(dataNode, InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshots to show as failed\");\n        assertBusy(() -> assertThat(\n            client().admin().cluster().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), greaterThanOrEqualTo(1)), 60L, TimeUnit.SECONDS);\n\n        unblockNode(\"test-repo\", dataNode);\n        disruption.stopDisrupting();\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n        }, 60L, TimeUnit.SECONDS);\n    }\n","date":"2020-07-10 21:44:56","endLine":1090,"groupId":"38208","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testDataNodeRestartWithBusyMasterDuringSnapshot","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/37/eed98ad8532ce7638e68604a18be13a449398e.src","preCode":"    public void testDataNodeRestartWithBusyMasterDuringSnapshot() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        internalCluster().startDataOnlyNodes(2);\n        logger.info(\"-->  creating repository\");\n        final Path repoPath = randomRepoPath();\n        assertAcked(client().admin().cluster().preparePutRepository(\"test-repo\")\n            .setType(\"mock\").setSettings(Settings.builder()\n                .put(\"location\", repoPath)\n                .put(\"compress\", randomBoolean())\n                .put(\"chunk_size\", randomIntBetween(100, 1000), ByteSizeUnit.BYTES)));\n        maybeInitWithOldSnapshotVersion(\"test-repo\", repoPath);\n\n        assertAcked(prepareCreate(\"test-idx\", 0, Settings.builder()\n            .put(\"number_of_shards\", 5).put(\"number_of_replicas\", 0)));\n        ensureGreen();\n        logger.info(\"-->  indexing some data\");\n        final int numdocs = randomIntBetween(50, 100);\n        IndexRequestBuilder[] builders = new IndexRequestBuilder[numdocs];\n        for (int i = 0; i < builders.length; i++) {\n            builders[i] = client().prepareIndex(\"test-idx\").setId(Integer.toString(i)).setSource(\"field1\", \"bar \" + i);\n        }\n        indexRandom(true, builders);\n        flushAndRefresh();\n        final String dataNode = blockNodeWithIndex(\"test-repo\", \"test-idx\");\n        logger.info(\"-->  snapshot\");\n        ServiceDisruptionScheme disruption = new BusyMasterServiceDisruption(random(), Priority.HIGH);\n        setDisruptionScheme(disruption);\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        disruption.startDisrupting();\n        logger.info(\"-->  restarting data node, which should cause primary shards to be failed\");\n        internalCluster().restartNode(dataNode, InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshots to show as failed\");\n        assertBusy(() -> assertThat(\n            client().admin().cluster().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), greaterThanOrEqualTo(1)), 60L, TimeUnit.SECONDS);\n\n        unblockNode(\"test-repo\", dataNode);\n        disruption.stopDisrupting();\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n        }, 60L, TimeUnit.SECONDS);\n    }\n","realPath":"server/src/internalClusterTest/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1053,"status":"M"},{"authorDate":"2020-07-10 21:44:56","commitOrder":4,"curCode":"    public void testDataNodeRestartAfterShardSnapshotFailure() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        final List<String> dataNodes = internalCluster().startDataOnlyNodes(2);\n        final Path repoPath = randomRepoPath();\n        createRepository(\"test-repo\", \"mock\", repoPath);\n        maybeInitWithOldSnapshotVersion(\"test-repo\", repoPath);\n\n        assertAcked(prepareCreate(\"test-idx\", 0, indexSettingsNoReplicas(2)));\n        ensureGreen();\n        indexRandomDocs(\"test-idx\", randomIntBetween(50, 100));\n\n        blockAllDataNodes(\"test-repo\");\n        logger.info(\"-->  snapshot\");\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        logger.info(\"-->  restarting first data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(0), InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshot of first primary to show as failed\");\n        assertBusy(() -> assertThat(\n            client().admin().cluster().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), is(1)), 60L, TimeUnit.SECONDS);\n\n        logger.info(\"-->  restarting second data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(1), InternalTestCluster.EMPTY_CALLBACK);\n\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n            assertThat(snapshotInfo.totalShards(), is(2));\n            assertThat(snapshotInfo.shardFailures(), hasSize(2));\n        }, 60L, TimeUnit.SECONDS);\n    }\n","date":"2020-07-10 21:44:56","endLine":1129,"groupId":"38208","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testDataNodeRestartAfterShardSnapshotFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/37/eed98ad8532ce7638e68604a18be13a449398e.src","preCode":"    public void testDataNodeRestartAfterShardSnapshotFailure() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        final List<String> dataNodes = internalCluster().startDataOnlyNodes(2);\n        final Path repoPath = randomRepoPath();\n        createRepository(\"test-repo\", \"mock\", Settings.builder()\n            .put(\"location\", repoPath).put(\"compress\", randomBoolean())\n            .put(\"chunk_size\", randomIntBetween(100, 1000), ByteSizeUnit.BYTES));\n        maybeInitWithOldSnapshotVersion(\"test-repo\", repoPath);\n\n        assertAcked(prepareCreate(\"test-idx\", 0, Settings.builder()\n            .put(\"number_of_shards\", 2).put(\"number_of_replicas\", 0)));\n        ensureGreen();\n        logger.info(\"-->  indexing some data\");\n        final int numdocs = randomIntBetween(50, 100);\n        IndexRequestBuilder[] builders = new IndexRequestBuilder[numdocs];\n        for (int i = 0; i < builders.length; i++) {\n            builders[i] = client().prepareIndex(\"test-idx\").setId(Integer.toString(i)).setSource(\"field1\", \"bar \" + i);\n        }\n        indexRandom(true, builders);\n        flushAndRefresh();\n        blockAllDataNodes(\"test-repo\");\n        logger.info(\"-->  snapshot\");\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        logger.info(\"-->  restarting first data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(0), InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshot of first primary to show as failed\");\n        assertBusy(() -> assertThat(\n            client().admin().cluster().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), is(1)), 60L, TimeUnit.SECONDS);\n\n        logger.info(\"-->  restarting second data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(1), InternalTestCluster.EMPTY_CALLBACK);\n\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n            assertThat(snapshotInfo.totalShards(), is(2));\n            assertThat(snapshotInfo.shardFailures(), hasSize(2));\n        }, 60L, TimeUnit.SECONDS);\n    }\n","realPath":"server/src/internalClusterTest/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1092,"status":"M"}],"commitId":"366ca378a8644d6654417988820c67f4feb714fe","commitMessage":"@@@Merge branch 'master' into feature/runtime_fields\n","date":"2020-07-10 21:44:56","modifiedFileCount":"565","status":"M","submitter":"Luca Cavanna"},{"authorTime":"2020-09-24 22:01:27","codes":[{"authorDate":"2020-09-24 22:01:27","commitOrder":5,"curCode":"    public void testDataNodeRestartWithBusyMasterDuringSnapshot() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        internalCluster().startDataOnlyNodes(2);\n        final Path repoPath = randomRepoPath();\n        createRepository(\"test-repo\", \"mock\", repoPath);\n        maybeInitWithOldSnapshotVersion(\"test-repo\", repoPath);\n\n        assertAcked(prepareCreate(\"test-idx\", 0, indexSettingsNoReplicas(5)));\n        ensureGreen();\n        indexRandomDocs(\"test-idx\", randomIntBetween(50, 100));\n\n        final String dataNode = blockNodeWithIndex(\"test-repo\", \"test-idx\");\n        logger.info(\"-->  snapshot\");\n        ServiceDisruptionScheme disruption = new BusyMasterServiceDisruption(random(), Priority.HIGH);\n        setDisruptionScheme(disruption);\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        disruption.startDisrupting();\n        logger.info(\"-->  restarting data node, which should cause primary shards to be failed\");\n        internalCluster().restartNode(dataNode, InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshots to show as failed\");\n        assertBusy(() -> assertThat(\n            clusterAdmin().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), greaterThanOrEqualTo(1)), 60L, TimeUnit.SECONDS);\n\n        unblockNode(\"test-repo\", dataNode);\n        disruption.stopDisrupting();\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = clusterAdmin()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n        }, 60L, TimeUnit.SECONDS);\n    }\n","date":"2020-09-24 22:01:27","endLine":1024,"groupId":"38208","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testDataNodeRestartWithBusyMasterDuringSnapshot","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/19/c5e52b8279309e64993410cd5bef26425fc773.src","preCode":"    public void testDataNodeRestartWithBusyMasterDuringSnapshot() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        internalCluster().startDataOnlyNodes(2);\n        final Path repoPath = randomRepoPath();\n        createRepository(\"test-repo\", \"mock\", repoPath);\n        maybeInitWithOldSnapshotVersion(\"test-repo\", repoPath);\n\n        assertAcked(prepareCreate(\"test-idx\", 0, indexSettingsNoReplicas(5)));\n        ensureGreen();\n        indexRandomDocs(\"test-idx\", randomIntBetween(50, 100));\n\n        final String dataNode = blockNodeWithIndex(\"test-repo\", \"test-idx\");\n        logger.info(\"-->  snapshot\");\n        ServiceDisruptionScheme disruption = new BusyMasterServiceDisruption(random(), Priority.HIGH);\n        setDisruptionScheme(disruption);\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        disruption.startDisrupting();\n        logger.info(\"-->  restarting data node, which should cause primary shards to be failed\");\n        internalCluster().restartNode(dataNode, InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshots to show as failed\");\n        assertBusy(() -> assertThat(\n            client().admin().cluster().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), greaterThanOrEqualTo(1)), 60L, TimeUnit.SECONDS);\n\n        unblockNode(\"test-repo\", dataNode);\n        disruption.stopDisrupting();\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n        }, 60L, TimeUnit.SECONDS);\n    }\n","realPath":"server/src/internalClusterTest/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":987,"status":"M"},{"authorDate":"2020-09-24 22:01:27","commitOrder":5,"curCode":"    public void testDataNodeRestartAfterShardSnapshotFailure() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        final List<String> dataNodes = internalCluster().startDataOnlyNodes(2);\n        final Path repoPath = randomRepoPath();\n        createRepository(\"test-repo\", \"mock\", repoPath);\n        maybeInitWithOldSnapshotVersion(\"test-repo\", repoPath);\n\n        assertAcked(prepareCreate(\"test-idx\", 0, indexSettingsNoReplicas(2)));\n        ensureGreen();\n        indexRandomDocs(\"test-idx\", randomIntBetween(50, 100));\n\n        blockAllDataNodes(\"test-repo\");\n        logger.info(\"-->  snapshot\");\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        logger.info(\"-->  restarting first data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(0), InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshot of first primary to show as failed\");\n        assertBusy(() -> assertThat(\n            clusterAdmin().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), is(1)), 60L, TimeUnit.SECONDS);\n\n        logger.info(\"-->  restarting second data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(1), InternalTestCluster.EMPTY_CALLBACK);\n\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = clusterAdmin()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n            assertThat(snapshotInfo.totalShards(), is(2));\n            assertThat(snapshotInfo.shardFailures(), hasSize(2));\n        }, 60L, TimeUnit.SECONDS);\n    }\n","date":"2020-09-24 22:01:27","endLine":1063,"groupId":"38208","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testDataNodeRestartAfterShardSnapshotFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/19/c5e52b8279309e64993410cd5bef26425fc773.src","preCode":"    public void testDataNodeRestartAfterShardSnapshotFailure() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        final List<String> dataNodes = internalCluster().startDataOnlyNodes(2);\n        final Path repoPath = randomRepoPath();\n        createRepository(\"test-repo\", \"mock\", repoPath);\n        maybeInitWithOldSnapshotVersion(\"test-repo\", repoPath);\n\n        assertAcked(prepareCreate(\"test-idx\", 0, indexSettingsNoReplicas(2)));\n        ensureGreen();\n        indexRandomDocs(\"test-idx\", randomIntBetween(50, 100));\n\n        blockAllDataNodes(\"test-repo\");\n        logger.info(\"-->  snapshot\");\n        client(internalCluster().getMasterName()).admin().cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\").setWaitForCompletion(false).setIndices(\"test-idx\").get();\n        logger.info(\"-->  restarting first data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(0), InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshot of first primary to show as failed\");\n        assertBusy(() -> assertThat(\n            client().admin().cluster().prepareSnapshotStatus(\"test-repo\").setSnapshots(\"test-snap\").get().getSnapshots()\n                .get(0).getShardsStats().getFailedShards(), is(1)), 60L, TimeUnit.SECONDS);\n\n        logger.info(\"-->  restarting second data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(1), InternalTestCluster.EMPTY_CALLBACK);\n\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = client().admin().cluster()\n                .prepareGetSnapshots(\"test-repo\").setSnapshots(\"test-snap\").setIgnoreUnavailable(true).get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n            assertThat(snapshotInfo.totalShards(), is(2));\n            assertThat(snapshotInfo.shardFailures(), hasSize(2));\n        }, 60L, TimeUnit.SECONDS);\n    }\n","realPath":"server/src/internalClusterTest/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1026,"status":"M"}],"commitId":"952c13c61955268611d443386aab10d70e8cdbe3","commitMessage":"@@@Dry up Snapshot Integ Tests some More (#62856)\n\nJust some obvious drying up of these super complex tests.","date":"2020-09-24 22:01:27","modifiedFileCount":"9","status":"M","submitter":"Armin Braun"},{"authorTime":"2021-06-24 22:58:33","codes":[{"authorDate":"2021-06-24 22:58:33","commitOrder":6,"curCode":"    public void testDataNodeRestartWithBusyMasterDuringSnapshot() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        internalCluster().startDataOnlyNodes(2);\n        final Path repoPath = randomRepoPath();\n        createRepository(\"test-repo\", \"mock\", repoPath);\n        maybeInitWithOldSnapshotVersion(\"test-repo\", repoPath);\n\n        assertAcked(prepareCreate(\"test-idx\", 0, indexSettingsNoReplicas(5)));\n        ensureGreen();\n        indexRandomDocs(\"test-idx\", randomIntBetween(50, 100));\n\n        final String dataNode = blockNodeWithIndex(\"test-repo\", \"test-idx\");\n        logger.info(\"-->  snapshot\");\n        ServiceDisruptionScheme disruption = new BusyMasterServiceDisruption(random(), Priority.HIGH);\n        setDisruptionScheme(disruption);\n        client(internalCluster().getMasterName()).admin()\n            .cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\")\n            .setWaitForCompletion(false)\n            .setIndices(\"test-idx\")\n            .get();\n        disruption.startDisrupting();\n        logger.info(\"-->  restarting data node, which should cause primary shards to be failed\");\n        internalCluster().restartNode(dataNode, InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshots to show as failed\");\n        assertBusy(\n            () -> assertThat(\n                clusterAdmin().prepareSnapshotStatus(\"test-repo\")\n                    .setSnapshots(\"test-snap\")\n                    .get()\n                    .getSnapshots()\n                    .get(0)\n                    .getShardsStats()\n                    .getFailedShards(),\n                greaterThanOrEqualTo(1)\n            ),\n            60L,\n            TimeUnit.SECONDS\n        );\n\n        unblockNode(\"test-repo\", dataNode);\n        disruption.stopDisrupting();\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = clusterAdmin().prepareGetSnapshots(\"test-repo\")\n                .setSnapshots(\"test-snap\")\n                .setIgnoreUnavailable(true)\n                .get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots().size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots().get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n        }, 60L, TimeUnit.SECONDS);\n    }\n","date":"2021-06-24 22:58:33","endLine":896,"groupId":"106303","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testDataNodeRestartWithBusyMasterDuringSnapshot","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/1f/5308718a4e837c752ca66f19143b76f012a8fb.src","preCode":"    public void testDataNodeRestartWithBusyMasterDuringSnapshot() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        internalCluster().startDataOnlyNodes(2);\n        final Path repoPath = randomRepoPath();\n        createRepository(\"test-repo\", \"mock\", repoPath);\n        maybeInitWithOldSnapshotVersion(\"test-repo\", repoPath);\n\n        assertAcked(prepareCreate(\"test-idx\", 0, indexSettingsNoReplicas(5)));\n        ensureGreen();\n        indexRandomDocs(\"test-idx\", randomIntBetween(50, 100));\n\n        final String dataNode = blockNodeWithIndex(\"test-repo\", \"test-idx\");\n        logger.info(\"-->  snapshot\");\n        ServiceDisruptionScheme disruption = new BusyMasterServiceDisruption(random(), Priority.HIGH);\n        setDisruptionScheme(disruption);\n        client(internalCluster().getMasterName()).admin()\n            .cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\")\n            .setWaitForCompletion(false)\n            .setIndices(\"test-idx\")\n            .get();\n        disruption.startDisrupting();\n        logger.info(\"-->  restarting data node, which should cause primary shards to be failed\");\n        internalCluster().restartNode(dataNode, InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshots to show as failed\");\n        assertBusy(\n            () -> assertThat(\n                clusterAdmin().prepareSnapshotStatus(\"test-repo\")\n                    .setSnapshots(\"test-snap\")\n                    .get()\n                    .getSnapshots()\n                    .get(0)\n                    .getShardsStats()\n                    .getFailedShards(),\n                greaterThanOrEqualTo(1)\n            ),\n            60L,\n            TimeUnit.SECONDS\n        );\n\n        unblockNode(\"test-repo\", dataNode);\n        disruption.stopDisrupting();\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = clusterAdmin().prepareGetSnapshots(\"test-repo\")\n                .setSnapshots(\"test-snap\")\n                .setIgnoreUnavailable(true)\n                .get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n        }, 60L, TimeUnit.SECONDS);\n    }\n","realPath":"server/src/internalClusterTest/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":842,"status":"M"},{"authorDate":"2021-06-24 22:58:33","commitOrder":6,"curCode":"    public void testDataNodeRestartAfterShardSnapshotFailure() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        final List<String> dataNodes = internalCluster().startDataOnlyNodes(2);\n        final Path repoPath = randomRepoPath();\n        createRepository(\"test-repo\", \"mock\", repoPath);\n        maybeInitWithOldSnapshotVersion(\"test-repo\", repoPath);\n\n        assertAcked(prepareCreate(\"test-idx\", 0, indexSettingsNoReplicas(2)));\n        ensureGreen();\n        indexRandomDocs(\"test-idx\", randomIntBetween(50, 100));\n\n        blockAllDataNodes(\"test-repo\");\n        logger.info(\"-->  snapshot\");\n        client(internalCluster().getMasterName()).admin()\n            .cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\")\n            .setWaitForCompletion(false)\n            .setIndices(\"test-idx\")\n            .get();\n        logger.info(\"-->  restarting first data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(0), InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshot of first primary to show as failed\");\n        assertBusy(\n            () -> assertThat(\n                clusterAdmin().prepareSnapshotStatus(\"test-repo\")\n                    .setSnapshots(\"test-snap\")\n                    .get()\n                    .getSnapshots()\n                    .get(0)\n                    .getShardsStats()\n                    .getFailedShards(),\n                is(1)\n            ),\n            60L,\n            TimeUnit.SECONDS\n        );\n\n        logger.info(\"-->  restarting second data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(1), InternalTestCluster.EMPTY_CALLBACK);\n\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = clusterAdmin().prepareGetSnapshots(\"test-repo\")\n                .setSnapshots(\"test-snap\")\n                .setIgnoreUnavailable(true)\n                .get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots().size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots().get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n            assertThat(snapshotInfo.totalShards(), is(2));\n            assertThat(snapshotInfo.shardFailures(), hasSize(2));\n        }, 60L, TimeUnit.SECONDS);\n    }\n","date":"2021-06-24 22:58:33","endLine":952,"groupId":"106303","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testDataNodeRestartAfterShardSnapshotFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/1f/5308718a4e837c752ca66f19143b76f012a8fb.src","preCode":"    public void testDataNodeRestartAfterShardSnapshotFailure() throws Exception {\n        logger.info(\"-->  starting a master node and two data nodes\");\n        internalCluster().startMasterOnlyNode();\n        final List<String> dataNodes = internalCluster().startDataOnlyNodes(2);\n        final Path repoPath = randomRepoPath();\n        createRepository(\"test-repo\", \"mock\", repoPath);\n        maybeInitWithOldSnapshotVersion(\"test-repo\", repoPath);\n\n        assertAcked(prepareCreate(\"test-idx\", 0, indexSettingsNoReplicas(2)));\n        ensureGreen();\n        indexRandomDocs(\"test-idx\", randomIntBetween(50, 100));\n\n        blockAllDataNodes(\"test-repo\");\n        logger.info(\"-->  snapshot\");\n        client(internalCluster().getMasterName()).admin()\n            .cluster()\n            .prepareCreateSnapshot(\"test-repo\", \"test-snap\")\n            .setWaitForCompletion(false)\n            .setIndices(\"test-idx\")\n            .get();\n        logger.info(\"-->  restarting first data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(0), InternalTestCluster.EMPTY_CALLBACK);\n\n        logger.info(\"-->  wait for shard snapshot of first primary to show as failed\");\n        assertBusy(\n            () -> assertThat(\n                clusterAdmin().prepareSnapshotStatus(\"test-repo\")\n                    .setSnapshots(\"test-snap\")\n                    .get()\n                    .getSnapshots()\n                    .get(0)\n                    .getShardsStats()\n                    .getFailedShards(),\n                is(1)\n            ),\n            60L,\n            TimeUnit.SECONDS\n        );\n\n        logger.info(\"-->  restarting second data node, which should cause the primary shard on it to be failed\");\n        internalCluster().restartNode(dataNodes.get(1), InternalTestCluster.EMPTY_CALLBACK);\n\n        \r\n        assertBusy(() -> {\n            GetSnapshotsResponse snapshotsStatusResponse = clusterAdmin().prepareGetSnapshots(\"test-repo\")\n                .setSnapshots(\"test-snap\")\n                .setIgnoreUnavailable(true)\n                .get();\n            assertEquals(1, snapshotsStatusResponse.getSnapshots(\"test-repo\").size());\n            SnapshotInfo snapshotInfo = snapshotsStatusResponse.getSnapshots(\"test-repo\").get(0);\n            assertTrue(snapshotInfo.state().toString(), snapshotInfo.state().completed());\n            assertThat(snapshotInfo.totalShards(), is(2));\n            assertThat(snapshotInfo.shardFailures(), hasSize(2));\n        }, 60L, TimeUnit.SECONDS);\n    }\n","realPath":"server/src/internalClusterTest/java/org/elasticsearch/snapshots/DedicatedClusterSnapshotRestoreIT.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":898,"status":"M"}],"commitId":"cbf48e06330c07b838cef50a6f8b9305d4f3516b","commitMessage":"@@@Flatten Get Snapshots Response (#74451)\n\nThis PR returns the get snapshots API to the 7.x format (and transport client behavior) and enhances it for requests that ask for multiple repositories.\nThe changes for requests that target multiple repositories are:\n* Add `repository` field to `SnapshotInfo` and REST response\n* Add `failures` map alongside `snapshots` list instead of returning just an exception response as done for single repo requests\n* Pagination now works across repositories instead of being per repository for multi-repository requests\n\ncloses #69108\ncloses #43462","date":"2021-06-24 22:58:33","modifiedFileCount":"53","status":"M","submitter":"Armin Braun"}]
