[{"authorTime":"2018-07-16 16:30:07","codes":[{"authorDate":"2018-07-16 16:30:07","commitOrder":1,"curCode":"    public void testIntervalMinute() throws IOException {\n        testSearchCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:35.000Z\",\n                        \"2017-02-01T09:02:59.000Z\",\n                        \"2017-02-01T09:15:37.000Z\",\n                        \"2017-02-01T09:16:04.000Z\",\n                        \"2017-02-01T09:16:42.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(4).field(DATE_FIELD),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(5, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T09:02:35.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T09:02:59.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T09:15:37.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T09:16:04.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T09:16:42.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n        );\n        testSearchAndReduceCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:35.000Z\",\n                        \"2017-02-01T09:02:59.000Z\",\n                        \"2017-02-01T09:15:37.000Z\",\n                        \"2017-02-01T09:16:04.000Z\",\n                        \"2017-02-01T09:16:42.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(15).field(DATE_FIELD),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(15, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T09:02:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T09:03:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T09:04:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T09:05:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T09:06:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T09:07:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T09:08:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T09:09:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(8);\n                    assertEquals(\"2017-02-01T09:10:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(9);\n                    assertEquals(\"2017-02-01T09:11:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(10);\n                    assertEquals(\"2017-02-01T09:12:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(11);\n                    assertEquals(\"2017-02-01T09:13:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(12);\n                    assertEquals(\"2017-02-01T09:14:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(13);\n                    assertEquals(\"2017-02-01T09:15:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(14);\n                    assertEquals(\"2017-02-01T09:16:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n                }\n        );\n    }\n","date":"2018-07-16 16:30:07","endLine":1185,"groupId":"65636","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testIntervalMinute","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/7c/f29e3aa9cc540ec5b371f56f24a3654b10f0c1.src","preCode":"    public void testIntervalMinute() throws IOException {\n        testSearchCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:35.000Z\",\n                        \"2017-02-01T09:02:59.000Z\",\n                        \"2017-02-01T09:15:37.000Z\",\n                        \"2017-02-01T09:16:04.000Z\",\n                        \"2017-02-01T09:16:42.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(4).field(DATE_FIELD),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(5, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T09:02:35.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T09:02:59.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T09:15:37.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T09:16:04.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T09:16:42.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n        );\n        testSearchAndReduceCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:35.000Z\",\n                        \"2017-02-01T09:02:59.000Z\",\n                        \"2017-02-01T09:15:37.000Z\",\n                        \"2017-02-01T09:16:04.000Z\",\n                        \"2017-02-01T09:16:42.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(15).field(DATE_FIELD),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(15, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T09:02:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T09:03:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T09:04:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T09:05:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T09:06:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T09:07:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T09:08:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T09:09:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(8);\n                    assertEquals(\"2017-02-01T09:10:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(9);\n                    assertEquals(\"2017-02-01T09:11:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(10);\n                    assertEquals(\"2017-02-01T09:12:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(11);\n                    assertEquals(\"2017-02-01T09:13:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(12);\n                    assertEquals(\"2017-02-01T09:14:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(13);\n                    assertEquals(\"2017-02-01T09:15:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(14);\n                    assertEquals(\"2017-02-01T09:16:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n                }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1076,"status":"B"},{"authorDate":"2018-07-16 16:30:07","commitOrder":1,"curCode":"    public void testIntervalSecond() throws IOException {\n        testSearchCase(new MatchAllDocsQuery(),\n                Arrays.asList(\"2017-02-01T00:00:05.015Z\", \"2017-02-01T00:00:07.299Z\", \"2017-02-01T00:00:07.074Z\",\n                        \"2017-02-01T00:00:11.688Z\", \"2017-02-01T00:00:11.210Z\", \"2017-02-01T00:00:11.380Z\"),\n                aggregation -> aggregation.setNumBuckets(7).field(DATE_FIELD), histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(3, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T00:00:05.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T00:00:07.000Z\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T00:00:11.000Z\", bucket.getKeyAsString());\n                    assertEquals(3, bucket.getDocCount());\n                });\n        testSearchAndReduceCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T00:00:05.015Z\",\n                        \"2017-02-01T00:00:07.299Z\",\n                        \"2017-02-01T00:00:07.074Z\",\n                        \"2017-02-01T00:00:11.688Z\",\n                        \"2017-02-01T00:00:11.210Z\",\n                        \"2017-02-01T00:00:11.380Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(7).field(DATE_FIELD),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(7, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T00:00:05.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T00:00:06.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T00:00:07.000Z\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T00:00:08.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T00:00:09.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T00:00:10.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T00:00:11.000Z\", bucket.getKeyAsString());\n                    assertEquals(3, bucket.getDocCount());\n                }\n        );\n    }\n","date":"2018-07-16 16:30:07","endLine":1250,"groupId":"65636","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testIntervalSecond","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/7c/f29e3aa9cc540ec5b371f56f24a3654b10f0c1.src","preCode":"    public void testIntervalSecond() throws IOException {\n        testSearchCase(new MatchAllDocsQuery(),\n                Arrays.asList(\"2017-02-01T00:00:05.015Z\", \"2017-02-01T00:00:07.299Z\", \"2017-02-01T00:00:07.074Z\",\n                        \"2017-02-01T00:00:11.688Z\", \"2017-02-01T00:00:11.210Z\", \"2017-02-01T00:00:11.380Z\"),\n                aggregation -> aggregation.setNumBuckets(7).field(DATE_FIELD), histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(3, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T00:00:05.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T00:00:07.000Z\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T00:00:11.000Z\", bucket.getKeyAsString());\n                    assertEquals(3, bucket.getDocCount());\n                });\n        testSearchAndReduceCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T00:00:05.015Z\",\n                        \"2017-02-01T00:00:07.299Z\",\n                        \"2017-02-01T00:00:07.074Z\",\n                        \"2017-02-01T00:00:11.688Z\",\n                        \"2017-02-01T00:00:11.210Z\",\n                        \"2017-02-01T00:00:11.380Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(7).field(DATE_FIELD),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(7, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T00:00:05.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T00:00:06.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T00:00:07.000Z\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T00:00:08.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T00:00:09.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T00:00:10.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T00:00:11.000Z\", bucket.getKeyAsString());\n                    assertEquals(3, bucket.getDocCount());\n                }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1187,"status":"B"}],"commitId":"5d3a53843a21098de3d1a342482db555eb660976","commitMessage":"@@@Merge branch 'master' into index-lifecycle\n","date":"2018-07-16 16:30:07","modifiedFileCount":"183","status":"B","submitter":"Colin Goodheart-Smithe"},{"authorTime":"2018-11-19 22:21:01","codes":[{"authorDate":"2018-11-19 22:21:01","commitOrder":2,"curCode":"    public void testIntervalMinute() throws IOException {\n        final List<DateTime> datesForMinuteInterval = Arrays.asList(\n            new DateTime(2017, 2, 1, 9, 2, 35, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 9, 2, 59, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 9, 15, 37, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 9, 16, 4, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 9, 16, 42, DateTimeZone.UTC));\n\n        testSearchCase(DEFAULT_QUERY, datesForMinuteInterval,\n            aggregation -> aggregation.setNumBuckets(4).field(DATE_FIELD),\n            histogram -> {\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(datesForMinuteInterval.size(), buckets.size());\n                for (int i = 0; i < buckets.size(); i++) {\n                    final Histogram.Bucket bucket = buckets.get(i);\n                    assertEquals(datesForMinuteInterval.get(i), bucket.getKey());\n                    assertEquals(1, bucket.getDocCount());\n                }\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForMinuteInterval,\n            aggregation -> aggregation.setNumBuckets(15).field(DATE_FIELD),\n            histogram -> {\n                final Map<DateTime, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(datesForMinuteInterval.get(0).withSecondOfMinute(0), 2);\n                expectedDocCount.put(datesForMinuteInterval.get(2).withSecondOfMinute(0), 1);\n                expectedDocCount.put(datesForMinuteInterval.get(3).withSecondOfMinute(0), 2);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(15, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","date":"2018-11-19 22:21:01","endLine":650,"groupId":"39071","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testIntervalMinute","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/2d/5109405dc1c23b27b6f3f7dd3e8280cc59df6f.src","preCode":"    public void testIntervalMinute() throws IOException {\n        testSearchCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:35.000Z\",\n                        \"2017-02-01T09:02:59.000Z\",\n                        \"2017-02-01T09:15:37.000Z\",\n                        \"2017-02-01T09:16:04.000Z\",\n                        \"2017-02-01T09:16:42.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(4).field(DATE_FIELD),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(5, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T09:02:35.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T09:02:59.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T09:15:37.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T09:16:04.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T09:16:42.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n        );\n        testSearchAndReduceCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:35.000Z\",\n                        \"2017-02-01T09:02:59.000Z\",\n                        \"2017-02-01T09:15:37.000Z\",\n                        \"2017-02-01T09:16:04.000Z\",\n                        \"2017-02-01T09:16:42.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(15).field(DATE_FIELD),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(15, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T09:02:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T09:03:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T09:04:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T09:05:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T09:06:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T09:07:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T09:08:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T09:09:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(8);\n                    assertEquals(\"2017-02-01T09:10:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(9);\n                    assertEquals(\"2017-02-01T09:11:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(10);\n                    assertEquals(\"2017-02-01T09:12:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(11);\n                    assertEquals(\"2017-02-01T09:13:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(12);\n                    assertEquals(\"2017-02-01T09:14:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(13);\n                    assertEquals(\"2017-02-01T09:15:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(14);\n                    assertEquals(\"2017-02-01T09:16:00.000Z\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n                }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":617,"status":"M"},{"authorDate":"2018-11-19 22:21:01","commitOrder":2,"curCode":"    public void testIntervalSecond() throws IOException {\n        final List<DateTime> datesForSecondInterval = Arrays.asList(\n            new DateTime(2017, 2, 1, 0, 0, 5, 15, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 0, 0, 7, 299, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 0, 0, 7, 74, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 0, 0, 11, 688, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 0, 0, 11, 210, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 0, 0, 11, 380, DateTimeZone.UTC));\n        final DateTime startDate = datesForSecondInterval.get(0).withMillisOfSecond(0);\n        final Map<DateTime, Integer> expectedDocCount = new HashMap<>();\n        expectedDocCount.put(startDate, 1);\n        expectedDocCount.put(startDate.plusSeconds(2), 2);\n        expectedDocCount.put(startDate.plusSeconds(6), 3);\n\n        testSearchCase(DEFAULT_QUERY, datesForSecondInterval,\n            aggregation -> aggregation.setNumBuckets(7).field(DATE_FIELD), histogram -> {\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(expectedDocCount.size(), buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            });\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForSecondInterval,\n            aggregation -> aggregation.setNumBuckets(7).field(DATE_FIELD),\n            histogram -> {\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(7, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","date":"2018-11-19 22:21:01","endLine":682,"groupId":"39075","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testIntervalSecond","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/2d/5109405dc1c23b27b6f3f7dd3e8280cc59df6f.src","preCode":"    public void testIntervalSecond() throws IOException {\n        testSearchCase(new MatchAllDocsQuery(),\n                Arrays.asList(\"2017-02-01T00:00:05.015Z\", \"2017-02-01T00:00:07.299Z\", \"2017-02-01T00:00:07.074Z\",\n                        \"2017-02-01T00:00:11.688Z\", \"2017-02-01T00:00:11.210Z\", \"2017-02-01T00:00:11.380Z\"),\n                aggregation -> aggregation.setNumBuckets(7).field(DATE_FIELD), histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(3, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T00:00:05.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T00:00:07.000Z\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T00:00:11.000Z\", bucket.getKeyAsString());\n                    assertEquals(3, bucket.getDocCount());\n                });\n        testSearchAndReduceCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T00:00:05.015Z\",\n                        \"2017-02-01T00:00:07.299Z\",\n                        \"2017-02-01T00:00:07.074Z\",\n                        \"2017-02-01T00:00:11.688Z\",\n                        \"2017-02-01T00:00:11.210Z\",\n                        \"2017-02-01T00:00:11.380Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(7).field(DATE_FIELD),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(7, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T00:00:05.000Z\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T00:00:06.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T00:00:07.000Z\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T00:00:08.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T00:00:09.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T00:00:10.000Z\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T00:00:11.000Z\", bucket.getKeyAsString());\n                    assertEquals(3, bucket.getDocCount());\n                }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":652,"status":"M"}],"commitId":"450db7fcf7b99066b42507be15727856765729ff","commitMessage":"@@@[Tests] Fix slowness of AutoDateHistogramAggregatorTests (#35072)\n\nRandomize test assertion and test set size instead of asserting on an\nexhaustive list of dates with fixed test set size. Also refactor common \nobjects used to avoid recreating them.  avoid date to string conversion\nand reduce duplicate test code\n\nCloses #33181","date":"2018-11-19 22:21:01","modifiedFileCount":"1","status":"M","submitter":"Ekal Golas"},{"authorTime":"2019-01-23 17:40:05","codes":[{"authorDate":"2019-01-23 17:40:05","commitOrder":3,"curCode":"    public void testIntervalMinute() throws IOException {\n        final List<ZonedDateTime> datesForMinuteInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 35, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 59, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 15, 37, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 16, 4, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 16, 42, 0, ZoneOffset.UTC));\n\n        testSearchCase(DEFAULT_QUERY, datesForMinuteInterval,\n            aggregation -> aggregation.setNumBuckets(4).field(DATE_FIELD),\n            histogram -> {\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(datesForMinuteInterval.size(), buckets.size());\n                for (int i = 0; i < buckets.size(); i++) {\n                    final Histogram.Bucket bucket = buckets.get(i);\n                    assertEquals(datesForMinuteInterval.get(i), bucket.getKey());\n                    assertEquals(1, bucket.getDocCount());\n                }\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForMinuteInterval,\n            aggregation -> aggregation.setNumBuckets(15).field(DATE_FIELD),\n            histogram -> {\n                final Map<ZonedDateTime, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(datesForMinuteInterval.get(0).withSecond(0), 2);\n                expectedDocCount.put(datesForMinuteInterval.get(2).withSecond(0), 1);\n                expectedDocCount.put(datesForMinuteInterval.get(3).withSecond(0), 2);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(15, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","date":"2019-01-23 17:40:05","endLine":687,"groupId":"39071","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testIntervalMinute","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/92/93b33e22f4361b1e80b1addfd49310f176fabb.src","preCode":"    public void testIntervalMinute() throws IOException {\n        final List<DateTime> datesForMinuteInterval = Arrays.asList(\n            new DateTime(2017, 2, 1, 9, 2, 35, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 9, 2, 59, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 9, 15, 37, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 9, 16, 4, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 9, 16, 42, DateTimeZone.UTC));\n\n        testSearchCase(DEFAULT_QUERY, datesForMinuteInterval,\n            aggregation -> aggregation.setNumBuckets(4).field(DATE_FIELD),\n            histogram -> {\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(datesForMinuteInterval.size(), buckets.size());\n                for (int i = 0; i < buckets.size(); i++) {\n                    final Histogram.Bucket bucket = buckets.get(i);\n                    assertEquals(datesForMinuteInterval.get(i), bucket.getKey());\n                    assertEquals(1, bucket.getDocCount());\n                }\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForMinuteInterval,\n            aggregation -> aggregation.setNumBuckets(15).field(DATE_FIELD),\n            histogram -> {\n                final Map<DateTime, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(datesForMinuteInterval.get(0).withSecondOfMinute(0), 2);\n                expectedDocCount.put(datesForMinuteInterval.get(2).withSecondOfMinute(0), 1);\n                expectedDocCount.put(datesForMinuteInterval.get(3).withSecondOfMinute(0), 2);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(15, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":654,"status":"M"},{"authorDate":"2019-01-23 17:40:05","commitOrder":3,"curCode":"    public void testIntervalSecond() throws IOException {\n        final List<ZonedDateTime> datesForSecondInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 5, 15, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 7, 299, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 7, 74, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 11, 688, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 11, 210, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 11, 380, ZoneOffset.UTC));\n        final ZonedDateTime startDate = datesForSecondInterval.get(0).withNano(0);\n        final Map<ZonedDateTime, Integer> expectedDocCount = new HashMap<>();\n        expectedDocCount.put(startDate, 1);\n        expectedDocCount.put(startDate.plusSeconds(2), 2);\n        expectedDocCount.put(startDate.plusSeconds(6), 3);\n\n        testSearchCase(DEFAULT_QUERY, datesForSecondInterval,\n            aggregation -> aggregation.setNumBuckets(7).field(DATE_FIELD), histogram -> {\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(expectedDocCount.size(), buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            });\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForSecondInterval,\n            aggregation -> aggregation.setNumBuckets(7).field(DATE_FIELD),\n            histogram -> {\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(7, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","date":"2019-01-23 17:40:05","endLine":719,"groupId":"8744","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testIntervalSecond","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/92/93b33e22f4361b1e80b1addfd49310f176fabb.src","preCode":"    public void testIntervalSecond() throws IOException {\n        final List<DateTime> datesForSecondInterval = Arrays.asList(\n            new DateTime(2017, 2, 1, 0, 0, 5, 15, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 0, 0, 7, 299, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 0, 0, 7, 74, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 0, 0, 11, 688, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 0, 0, 11, 210, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 0, 0, 11, 380, DateTimeZone.UTC));\n        final DateTime startDate = datesForSecondInterval.get(0).withMillisOfSecond(0);\n        final Map<DateTime, Integer> expectedDocCount = new HashMap<>();\n        expectedDocCount.put(startDate, 1);\n        expectedDocCount.put(startDate.plusSeconds(2), 2);\n        expectedDocCount.put(startDate.plusSeconds(6), 3);\n\n        testSearchCase(DEFAULT_QUERY, datesForSecondInterval,\n            aggregation -> aggregation.setNumBuckets(7).field(DATE_FIELD), histogram -> {\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(expectedDocCount.size(), buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            });\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForSecondInterval,\n            aggregation -> aggregation.setNumBuckets(7).field(DATE_FIELD),\n            histogram -> {\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(7, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":689,"status":"M"}],"commitId":"daa2ec8a605d385a65b9ab3e89d016b3fd0dffe2","commitMessage":"@@@Switch mapping/aggregations over to java time (#36363)\n\nThis commit moves the aggregation and mapping code from joda time to\njava time. This includes field mappers.  root object mappers.  aggregations with date\nhistograms.  query builders and a lot of changes within tests.\n\nThe cut-over to java time is a requirement so that we can support nanoseconds\nproperly in a future field mapper.\n\nRelates #27330","date":"2019-01-23 17:40:05","modifiedFileCount":"154","status":"M","submitter":"Alexander Reelsen"},{"authorTime":"2020-06-16 02:33:31","codes":[{"authorDate":"2020-06-16 02:33:31","commitOrder":4,"curCode":"    public void testIntervalMinute() throws IOException {\n        final List<ZonedDateTime> datesForMinuteInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 35, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 59, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 15, 37, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 16, 4, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 16, 42, 0, ZoneOffset.UTC));\n        Map<String, Integer> skeletonDocCount = new TreeMap<>();\n        skeletonDocCount.put(\"2017-02-01T09:02:00.000Z\", 2);\n        skeletonDocCount.put(\"2017-02-01T09:15:00.000Z\", 1);\n        skeletonDocCount.put(\"2017-02-01T09:16:00.000Z\", 2);\n        testSearchCase(DEFAULT_QUERY, datesForMinuteInterval,\n            aggregation -> aggregation.setNumBuckets(4).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(skeletonDocCount))\n        );\n        Map<String, Integer> fullDocCount = new TreeMap<>();\n        fullDocCount.put(\"2017-02-01T09:02:00.000Z\", 2);\n        fullDocCount.put(\"2017-02-01T09:07:00.000Z\", 0);\n        fullDocCount.put(\"2017-02-01T09:12:00.000Z\", 3);\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForMinuteInterval,\n            aggregation -> aggregation.setNumBuckets(4).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(fullDocCount))\n        );\n\n        testSearchCase(DEFAULT_QUERY, datesForMinuteInterval,\n            aggregation -> aggregation.setNumBuckets(15).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(skeletonDocCount))\n        );\n        fullDocCount.clear();\n        fullDocCount.putAll(skeletonDocCount);\n        for (int minute = 3; minute < 15; minute++) {\n            fullDocCount.put(String.format(Locale.ROOT, \"2017-02-01T09:%02d:00.000Z\", minute), 0);    \n        }\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForMinuteInterval,\n            aggregation -> aggregation.setNumBuckets(15).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(fullDocCount))\n        );\n    }\n","date":"2020-06-16 02:33:31","endLine":826,"groupId":"10690","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testIntervalMinute","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/47/7df9e59163c957ddad7d1665b8af353b9129c9.src","preCode":"    public void testIntervalMinute() throws IOException {\n        final List<ZonedDateTime> datesForMinuteInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 35, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 59, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 15, 37, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 16, 4, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 16, 42, 0, ZoneOffset.UTC));\n\n        testSearchCase(DEFAULT_QUERY, datesForMinuteInterval,\n            aggregation -> aggregation.setNumBuckets(4).field(DATE_FIELD),\n            histogram -> {\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(datesForMinuteInterval.size(), buckets.size());\n                for (int i = 0; i < buckets.size(); i++) {\n                    final Histogram.Bucket bucket = buckets.get(i);\n                    assertEquals(datesForMinuteInterval.get(i), bucket.getKey());\n                    assertEquals(1, bucket.getDocCount());\n                }\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForMinuteInterval,\n            aggregation -> aggregation.setNumBuckets(15).field(DATE_FIELD),\n            histogram -> {\n                final Map<ZonedDateTime, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(datesForMinuteInterval.get(0).withSecond(0), 2);\n                expectedDocCount.put(datesForMinuteInterval.get(2).withSecond(0), 1);\n                expectedDocCount.put(datesForMinuteInterval.get(3).withSecond(0), 2);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(15, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":789,"status":"M"},{"authorDate":"2020-06-16 02:33:31","commitOrder":4,"curCode":"    public void testIntervalSecond() throws IOException {\n        final List<ZonedDateTime> datesForSecondInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 5, 15, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 7, 299, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 7, 74, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 11, 688, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 11, 210, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 11, 380, ZoneOffset.UTC));\n        Map<String, Integer> expectedDocCount = new TreeMap<>();\n        expectedDocCount.put(\"2017-02-01T00:00:05.000Z\", 1);\n        expectedDocCount.put(\"2017-02-01T00:00:07.000Z\", 2);\n        expectedDocCount.put(\"2017-02-01T00:00:11.000Z\", 3);\n        testSearchCase(DEFAULT_QUERY, datesForSecondInterval,\n            aggregation -> aggregation.setNumBuckets(7).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n        expectedDocCount.put(\"2017-02-01T00:00:06.000Z\", 0);\n        expectedDocCount.put(\"2017-02-01T00:00:08.000Z\", 0);\n        expectedDocCount.put(\"2017-02-01T00:00:09.000Z\", 0);\n        expectedDocCount.put(\"2017-02-01T00:00:10.000Z\", 0);\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForSecondInterval,\n            aggregation -> aggregation.setNumBuckets(7).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n    }\n","date":"2020-06-16 02:33:31","endLine":852,"groupId":"10690","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testIntervalSecond","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/47/7df9e59163c957ddad7d1665b8af353b9129c9.src","preCode":"    public void testIntervalSecond() throws IOException {\n        final List<ZonedDateTime> datesForSecondInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 5, 15, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 7, 299, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 7, 74, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 11, 688, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 11, 210, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 11, 380, ZoneOffset.UTC));\n        final ZonedDateTime startDate = datesForSecondInterval.get(0).withNano(0);\n        final Map<ZonedDateTime, Integer> expectedDocCount = new HashMap<>();\n        expectedDocCount.put(startDate, 1);\n        expectedDocCount.put(startDate.plusSeconds(2), 2);\n        expectedDocCount.put(startDate.plusSeconds(6), 3);\n\n        testSearchCase(DEFAULT_QUERY, datesForSecondInterval,\n            aggregation -> aggregation.setNumBuckets(7).field(DATE_FIELD), histogram -> {\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(expectedDocCount.size(), buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            });\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForSecondInterval,\n            aggregation -> aggregation.setNumBuckets(7).field(DATE_FIELD),\n            histogram -> {\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(7, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKey(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":828,"status":"M"}],"commitId":"7c7fe0152d43a19379942a144dd4904b3d1db570","commitMessage":"@@@Save memory when auto_date_histogram is not on top (#57304)\n\nThis builds an `auto_date_histogram` aggregator that natively aggregates\nfrom many buckets and uses it when the `auto_date_histogram` used to use\n`asMultiBucketAggregator` which should save a significant amount of\nmemory in those cases. In particular.  this happens when\n`auto_date_histogram` is a sub-aggregator of a multi-bucketing aggregator\nlike `terms` or `histogram` or `filters`. For the most part we preserve\nthe original implementation when `auto_date_histogram` only collects from\na single bucket.\n\nIt isn't possible to \"just port the aggregator\" without taking a pretty\nsignificant performance hit because we used to rewrite all of the\nbuckets every time we switched to a coarser and coarser rounding\nconfiguration. Without some major surgery to how to delay sub-aggs\nwe'd end up rewriting the delay list zillions of time if there are many\nbuckets.\n\nThe multi-bucket version of the aggregator has a \"budget\" of \"wasted\"\nbuckets and only rewrites all of the buckets when we exceed that budget.\nNow that we don't rebucket every time we increase the rounding we can no\nlonger get an accurate count of the number of buckets! So instead the\naggregator uses an estimate of the number of buckets to trigger switching\nto a coarser rounding. This estimate is likely to be *terrible* when\nbuckets are far apart compared to the rounding. So it also uses the\ndifference between the first and last bucket to trigger switching to a\ncoarser rounding. Which covers for the shortcomings of the bucket\nestimation technique pretty well. It also causes the aggregator to emit\nfewer buckets in cases where they'd be reduced together on the\ncoordinating node. This is wonderful! But probably fairly rare.\n\nAll of that does buy us some speed improvements when the aggregator is\na child of multi-bucket aggregator:\nWithout metrics or time zone: 25% faster\nWith metrics: 15% faster\nWith time zone: 22% faster\n\nRelates to #56487\n","date":"2020-06-16 02:33:31","modifiedFileCount":"14","status":"M","submitter":"Nik Everett"},{"authorTime":"2020-08-07 05:14:20","codes":[{"authorDate":"2020-08-07 05:14:20","commitOrder":5,"curCode":"    public void testIntervalMinute() throws IOException {\n        final List<ZonedDateTime> datesForMinuteInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 35, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 59, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 15, 37, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 16, 4, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 16, 42, 0, ZoneOffset.UTC));\n        Map<String, Integer> skeletonDocCount = new TreeMap<>();\n        skeletonDocCount.put(\"2017-02-01T09:02:00.000Z\", 2);\n        skeletonDocCount.put(\"2017-02-01T09:15:00.000Z\", 1);\n        skeletonDocCount.put(\"2017-02-01T09:16:00.000Z\", 2);\n        Map<String, Integer> fullDocCount = new TreeMap<>();\n        fullDocCount.put(\"2017-02-01T09:02:00.000Z\", 2);\n        fullDocCount.put(\"2017-02-01T09:07:00.000Z\", 0);\n        fullDocCount.put(\"2017-02-01T09:12:00.000Z\", 3);\n        testSearchCase(DEFAULT_QUERY, datesForMinuteInterval,\n            aggregation -> aggregation.setNumBuckets(4).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(fullDocCount))\n        );\n        fullDocCount.clear();\n        fullDocCount.putAll(skeletonDocCount);\n        for (int minute = 3; minute < 15; minute++) {\n            fullDocCount.put(String.format(Locale.ROOT, \"2017-02-01T09:%02d:00.000Z\", minute), 0);\n        }\n        testSearchCase(DEFAULT_QUERY, datesForMinuteInterval,\n            aggregation -> aggregation.setNumBuckets(15).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(fullDocCount))\n        );\n    }\n","date":"2020-08-07 05:14:20","endLine":797,"groupId":"105016","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testIntervalMinute","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/4c/b34acab2449fdaa8d6149b4effef11af92b7f1.src","preCode":"    public void testIntervalMinute() throws IOException {\n        final List<ZonedDateTime> datesForMinuteInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 35, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 59, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 15, 37, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 16, 4, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 16, 42, 0, ZoneOffset.UTC));\n        Map<String, Integer> skeletonDocCount = new TreeMap<>();\n        skeletonDocCount.put(\"2017-02-01T09:02:00.000Z\", 2);\n        skeletonDocCount.put(\"2017-02-01T09:15:00.000Z\", 1);\n        skeletonDocCount.put(\"2017-02-01T09:16:00.000Z\", 2);\n        testSearchCase(DEFAULT_QUERY, datesForMinuteInterval,\n            aggregation -> aggregation.setNumBuckets(4).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(skeletonDocCount))\n        );\n        Map<String, Integer> fullDocCount = new TreeMap<>();\n        fullDocCount.put(\"2017-02-01T09:02:00.000Z\", 2);\n        fullDocCount.put(\"2017-02-01T09:07:00.000Z\", 0);\n        fullDocCount.put(\"2017-02-01T09:12:00.000Z\", 3);\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForMinuteInterval,\n            aggregation -> aggregation.setNumBuckets(4).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(fullDocCount))\n        );\n\n        testSearchCase(DEFAULT_QUERY, datesForMinuteInterval,\n            aggregation -> aggregation.setNumBuckets(15).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(skeletonDocCount))\n        );\n        fullDocCount.clear();\n        fullDocCount.putAll(skeletonDocCount);\n        for (int minute = 3; minute < 15; minute++) {\n            fullDocCount.put(String.format(Locale.ROOT, \"2017-02-01T09:%02d:00.000Z\", minute), 0);    \n        }\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForMinuteInterval,\n            aggregation -> aggregation.setNumBuckets(15).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(fullDocCount))\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":769,"status":"M"},{"authorDate":"2020-08-07 05:14:20","commitOrder":5,"curCode":"    public void testIntervalSecond() throws IOException {\n        final List<ZonedDateTime> datesForSecondInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 5, 15, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 7, 299, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 7, 74, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 11, 688, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 11, 210, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 11, 380, ZoneOffset.UTC));\n        Map<String, Integer> expectedDocCount = new TreeMap<>();\n        expectedDocCount.put(\"2017-02-01T00:00:05.000Z\", 1);\n        expectedDocCount.put(\"2017-02-01T00:00:07.000Z\", 2);\n        expectedDocCount.put(\"2017-02-01T00:00:11.000Z\", 3);\n        expectedDocCount.put(\"2017-02-01T00:00:06.000Z\", 0);\n        expectedDocCount.put(\"2017-02-01T00:00:08.000Z\", 0);\n        expectedDocCount.put(\"2017-02-01T00:00:09.000Z\", 0);\n        expectedDocCount.put(\"2017-02-01T00:00:10.000Z\", 0);\n        testSearchCase(DEFAULT_QUERY, datesForSecondInterval,\n            aggregation -> aggregation.setNumBuckets(7).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n    }\n","date":"2020-08-07 05:14:20","endLine":819,"groupId":"105016","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testIntervalSecond","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/4c/b34acab2449fdaa8d6149b4effef11af92b7f1.src","preCode":"    public void testIntervalSecond() throws IOException {\n        final List<ZonedDateTime> datesForSecondInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 5, 15, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 7, 299, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 7, 74, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 11, 688, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 11, 210, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 0, 0, 11, 380, ZoneOffset.UTC));\n        Map<String, Integer> expectedDocCount = new TreeMap<>();\n        expectedDocCount.put(\"2017-02-01T00:00:05.000Z\", 1);\n        expectedDocCount.put(\"2017-02-01T00:00:07.000Z\", 2);\n        expectedDocCount.put(\"2017-02-01T00:00:11.000Z\", 3);\n        testSearchCase(DEFAULT_QUERY, datesForSecondInterval,\n            aggregation -> aggregation.setNumBuckets(7).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n        expectedDocCount.put(\"2017-02-01T00:00:06.000Z\", 0);\n        expectedDocCount.put(\"2017-02-01T00:00:08.000Z\", 0);\n        expectedDocCount.put(\"2017-02-01T00:00:09.000Z\", 0);\n        expectedDocCount.put(\"2017-02-01T00:00:10.000Z\", 0);\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForSecondInterval,\n            aggregation -> aggregation.setNumBuckets(7).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":799,"status":"M"}],"commitId":"5e3ea6eb11c68bdcc9adda51715a6e1fea9186d6","commitMessage":"@@@Merge branch 'master' into feature/runtime_fields\n","date":"2020-08-07 05:14:20","modifiedFileCount":"73","status":"M","submitter":"Nik Everett"}]
