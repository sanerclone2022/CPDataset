[{"authorTime":"2018-08-31 21:45:22","codes":[{"authorDate":"2018-08-31 21:45:22","commitOrder":1,"curCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null, null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.type(), doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","date":"2018-08-31 21:45:22","endLine":1443,"groupId":"24470","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetention","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/d3/aead9e44e163c6e697952cfa04b2f64b44b143.src","preCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null, null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.type(), doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1373,"status":"B"},{"authorDate":"2018-08-31 21:45:22","commitOrder":1,"curCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null, null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null, useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.type(), doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(engine.getLocalCheckpoint());\n            engine.syncTranslog();\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","date":"2018-08-31 21:45:22","endLine":1530,"groupId":"14889","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetentionAndRecoverySource","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/d3/aead9e44e163c6e697952cfa04b2f64b44b143.src","preCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null, null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null, useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.type(), doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(engine.getLocalCheckpoint());\n            engine.syncTranslog();\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1445,"status":"B"}],"commitId":"3eef74d5d582c19c29d7a297a120004bf4d38930","commitMessage":"@@@Merge branch 'master' into index-lifecycle\n","date":"2018-08-31 21:45:22","modifiedFileCount":"108","status":"B","submitter":"Colin Goodheart-Smithe"},{"authorTime":"2018-09-11 04:30:44","codes":[{"authorDate":"2018-08-31 21:45:22","commitOrder":2,"curCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null, null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.type(), doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","date":"2018-08-31 21:45:22","endLine":1443,"groupId":"24470","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetention","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/d3/aead9e44e163c6e697952cfa04b2f64b44b143.src","preCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null, null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.type(), doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1373,"status":"N"},{"authorDate":"2018-09-11 04:30:44","commitOrder":2,"curCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null, null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null, useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.type(), doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            \r\n            if (globalCheckpoint.get() == engine.getLocalCheckpoint() && liveDocs.isEmpty() == false) {\n                String deleteId = randomFrom(liveDocs);\n                engine.delete(new Engine.Delete(\"test\", deleteId, newUid(deleteId), primaryTerm.get()));\n                liveDocsWithSource.remove(deleteId);\n                liveDocs.remove(deleteId);\n                engine.flush();\n            }\n            globalCheckpoint.set(engine.getLocalCheckpoint());\n            engine.syncTranslog();\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","date":"2018-09-11 04:30:44","endLine":1540,"groupId":"14889","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetentionAndRecoverySource","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/a2/6fd72468b48c8ae552488eae2f29e456b4e937.src","preCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null, null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null, useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.type(), doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(engine.getLocalCheckpoint());\n            engine.syncTranslog();\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1445,"status":"M"}],"commitId":"cdc4f57a773b588fe872f247ee62c3d5539e2faa","commitMessage":"@@@Merge branch 'master' into index-lifecycle\n","date":"2018-09-11 04:30:44","modifiedFileCount":"186","status":"M","submitter":"Colin Goodheart-Smithe"},{"authorTime":"2019-06-20 14:46:30","codes":[{"authorDate":"2019-06-20 14:46:30","commitOrder":3,"curCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.type(), doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","date":"2019-06-20 14:46:30","endLine":1653,"groupId":"24470","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetention","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/59/bbee9f1bbf562675dfebca2ad92907f3ffcc1e.src","preCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.type(), doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1582,"status":"M"},{"authorDate":"2019-06-20 14:46:30","commitOrder":3,"curCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.type(), doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            \r\n            if (globalCheckpoint.get() == engine.getPersistedLocalCheckpoint() && liveDocs.isEmpty() == false) {\n                String deleteId = randomFrom(liveDocs);\n                engine.delete(new Engine.Delete(\"test\", deleteId, newUid(deleteId), primaryTerm.get()));\n                liveDocsWithSource.remove(deleteId);\n                liveDocs.remove(deleteId);\n                engine.flush();\n            }\n            globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n            engine.syncTranslog();\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","date":"2019-06-20 14:46:30","endLine":1753,"groupId":"14889","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetentionAndRecoverySource","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/59/bbee9f1bbf562675dfebca2ad92907f3ffcc1e.src","preCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.type(), doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            \r\n            if (globalCheckpoint.get() == engine.getLocalCheckpoint() && liveDocs.isEmpty() == false) {\n                String deleteId = randomFrom(liveDocs);\n                engine.delete(new Engine.Delete(\"test\", deleteId, newUid(deleteId), primaryTerm.get()));\n                liveDocsWithSource.remove(deleteId);\n                liveDocs.remove(deleteId);\n                engine.flush();\n            }\n            globalCheckpoint.set(engine.getLocalCheckpoint());\n            engine.syncTranslog();\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1655,"status":"M"}],"commitId":"f27e808c1457ddbda1c69b4422d9a10ee6ec299d","commitMessage":"@@@Advance checkpoints only after persisting ops (#43205)\n\nLocal and global checkpoints currently do not correctly reflect what's persisted to disk. The issue is\nthat the local checkpoint is adapted as soon as an operation is processed (but not fsynced yet). This\nleaves room for the history below the global checkpoint to still change in case of a crash. As we rely\non global checkpoints for CCR as well as operation-based recoveries.  this has the risk of shard\ncopies / follower clusters going out of sync.\n\nThis commit required changing some core classes in the system:\n\n- The LocalCheckpointTracker keeps track now not only of the information whether an operation has\nbeen processed.  but also whether that operation has been persisted to disk.\n- TranslogWriter now keeps track of the sequence numbers that have not been fsynced yet. Once\nthey are fsynced.  TranslogWriter notifies LocalCheckpointTracker of this.\n- ReplicationTracker now keeps track of the persisted local and persisted global checkpoints of all\nshard copies when in primary mode. The computed global checkpoint (which represents the\nminimum of all persisted local checkpoints of all in-sync shard copies).  which was previously stored\nin the checkpoint entry for the local shard copy.  has been moved to an extra field.\n- The periodic global checkpoint sync now also takes async durability into account.  where the local\ncheckpoints on shards only advance when the translog is asynchronously fsynced. This means that\nthe previous condition to detect inactivity (max sequence number is equal to global checkpoint) is\nnot sufficient anymore.\n- The new index closing API does not work when combined with async durability. The shard\nverification step is now requires an additional pre-flight step to fsync the translog.  so that the main\nverify shard step has the most up-to-date global checkpoint at disposition.","date":"2019-06-20 14:46:30","modifiedFileCount":"56","status":"M","submitter":"Yannick Welsch"},{"authorTime":"2019-10-15 16:05:29","codes":[{"authorDate":"2019-10-15 16:05:29","commitOrder":4,"curCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","date":"2019-10-15 16:05:29","endLine":1629,"groupId":"24470","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetention","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/95/bfa4a03c6989ccf471184a5a4d19ca304b6c11.src","preCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.type(), doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1558,"status":"M"},{"authorDate":"2019-10-15 16:05:29","commitOrder":4,"curCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            \r\n            if (globalCheckpoint.get() == engine.getPersistedLocalCheckpoint() && liveDocs.isEmpty() == false) {\n                String deleteId = randomFrom(liveDocs);\n                engine.delete(new Engine.Delete(deleteId, newUid(deleteId), primaryTerm.get()));\n                liveDocsWithSource.remove(deleteId);\n                liveDocs.remove(deleteId);\n                engine.flush();\n            }\n            globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n            engine.syncTranslog();\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","date":"2019-10-15 16:05:29","endLine":1729,"groupId":"48407","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetentionAndRecoverySource","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/95/bfa4a03c6989ccf471184a5a4d19ca304b6c11.src","preCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.type(), doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            \r\n            if (globalCheckpoint.get() == engine.getPersistedLocalCheckpoint() && liveDocs.isEmpty() == false) {\n                String deleteId = randomFrom(liveDocs);\n                engine.delete(new Engine.Delete(\"test\", deleteId, newUid(deleteId), primaryTerm.get()));\n                liveDocsWithSource.remove(deleteId);\n                liveDocs.remove(deleteId);\n                engine.flush();\n            }\n            globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n            engine.syncTranslog();\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1631,"status":"M"}],"commitId":"6531369f11d159896957b75a7eae0b7da214d01a","commitMessage":"@@@Don't persist type information to translog (#47229)\n\nWe no longer need to store type information in the translog.  given that an index\ncan only have a single type.\n\nRelates to #41059","date":"2019-10-15 16:05:29","modifiedFileCount":"100","status":"M","submitter":"Alan Woodward"},{"authorTime":"2019-11-01 21:41:45","codes":[{"authorDate":"2019-10-15 16:05:29","commitOrder":5,"curCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","date":"2019-10-15 16:05:29","endLine":1629,"groupId":"24470","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetention","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/95/bfa4a03c6989ccf471184a5a4d19ca304b6c11.src","preCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1558,"status":"N"},{"authorDate":"2019-11-01 21:41:45","commitOrder":5,"curCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","date":"2019-11-01 21:41:45","endLine":1742,"groupId":"48407","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetentionAndRecoverySource","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/9d/5e500c42901a5a4fe027ddf424ef101eb3a449.src","preCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            \r\n            if (globalCheckpoint.get() == engine.getPersistedLocalCheckpoint() && liveDocs.isEmpty() == false) {\n                String deleteId = randomFrom(liveDocs);\n                engine.delete(new Engine.Delete(deleteId, newUid(deleteId), primaryTerm.get()));\n                liveDocsWithSource.remove(deleteId);\n                liveDocs.remove(deleteId);\n                engine.flush();\n            }\n            globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n            engine.syncTranslog();\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1636,"status":"M"}],"commitId":"3a359291fc64a8f0ec07b0e8e93991e299ca3416","commitMessage":"@@@Fix testForceMergeWithSoftDeletesRetentionAndRecoverySource (#48766)\n\nThis test failure manifests the limitation of the recovery source merge \npolicy explained in #41628. If we already merge down to a single segment\nthen subsequent force merges will be noop although they can prune\nrecovery source. We need to adjust this test until we have a fix for the\nmerge policy.\n\nRelates #41628\nCloses #48735","date":"2019-11-01 21:41:45","modifiedFileCount":"1","status":"M","submitter":"Nhat Nguyen"},{"authorTime":"2019-11-14 18:44:43","codes":[{"authorDate":"2019-11-14 18:44:43","commitOrder":6,"curCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","date":"2019-11-14 18:44:43","endLine":1634,"groupId":"24470","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetention","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/dc/37022c130fe51778a9c1ed027424703af3bb52.src","preCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1563,"status":"M"},{"authorDate":"2019-11-14 18:44:43","commitOrder":6,"curCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","date":"2019-11-14 18:44:43","endLine":1742,"groupId":"48407","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetentionAndRecoverySource","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/dc/37022c130fe51778a9c1ed027424703af3bb52.src","preCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService(\"test\");\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1636,"status":"M"}],"commitId":"6acb70a0ab6f08f3be4b81260c9712c14c1941f2","commitMessage":"@@@IndexMetaData should allow only one mapping per index (#48952)\n\nWe removed IndexMetaData.getMappings() in #47344.  and since then it\nhas only been possible to associate a single mapping with an index within the\ncluster state. However.  it is still possible to add multiple mappings against\ndifferent types when building metadata.  even though this just has a last-applied-wins\neffect.\n\nThis commit removes the type parameter from IndexMetaData.Builder.putMapping()\nand alters various tests and consumers to reflect this.","date":"2019-11-14 18:44:43","modifiedFileCount":"26","status":"M","submitter":"Alan Woodward"},{"authorTime":"2019-12-14 02:56:50","codes":[{"authorDate":"2019-12-14 02:56:50","commitOrder":7,"curCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged(indexSettings.getTranslogRetentionAge(), indexSettings.getTranslogRetentionSize(),\n                indexSettings.getSoftDeleteRetentionOperations());\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","date":"2019-12-14 02:56:50","endLine":1636,"groupId":"24470","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetention","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/c3/8f426dbfb59d5ebb57113f6aaade5848f1cc89.src","preCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1564,"status":"M"},{"authorDate":"2019-12-14 02:56:50","commitOrder":7,"curCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged(indexSettings.getTranslogRetentionAge(), indexSettings.getTranslogRetentionSize(),\n                indexSettings.getSoftDeleteRetentionOperations());\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","date":"2019-12-14 02:56:50","endLine":1745,"groupId":"48407","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetentionAndRecoverySource","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/c3/8f426dbfb59d5ebb57113f6aaade5848f1cc89.src","preCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1638,"status":"M"}],"commitId":"b9fbc8dc74826329f0bb00fc4f531507a12ec327","commitMessage":"@@@Migrate peer recovery from translog to retention lease (#49448)\n\nSince 7.4.  we switch from translog to Lucene as the source of history \nfor peer recoveries. However.  we reduce the likelihood of\noperation-based recoveries when performing a full cluster restart from\npre-7.4 because existing copies do not have PPRL.\n\nTo remedy this issue.  we fallback using translog in peer recoveries if \nthe recovering replica does not have a peer recovery retention lease. \nand the replication group hasn't fully migrated to PRRL.\n\nRelates #45136","date":"2019-12-14 02:56:50","modifiedFileCount":"20","status":"M","submitter":"Nhat Nguyen"},{"authorTime":"2019-12-26 22:02:02","codes":[{"authorDate":"2019-12-26 22:02:02","commitOrder":8,"curCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged(indexSettings.getTranslogRetentionAge(), indexSettings.getTranslogRetentionSize(),\n                indexSettings.getSoftDeleteRetentionOperations());\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","date":"2019-12-26 22:02:02","endLine":1384,"groupId":"24470","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetention","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/9a/2f79b7530a77385650dc011ce9ec2f774783de.src","preCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged(indexSettings.getTranslogRetentionAge(), indexSettings.getTranslogRetentionSize(),\n                indexSettings.getSoftDeleteRetentionOperations());\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1313,"status":"M"},{"authorDate":"2019-12-26 22:02:02","commitOrder":8,"curCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged(indexSettings.getTranslogRetentionAge(), indexSettings.getTranslogRetentionSize(),\n                indexSettings.getSoftDeleteRetentionOperations());\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","date":"2019-12-26 22:02:02","endLine":1492,"groupId":"48407","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetentionAndRecoverySource","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/9a/2f79b7530a77385650dc011ce9ec2f774783de.src","preCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_SETTING.getKey(), true)\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged(indexSettings.getTranslogRetentionAge(), indexSettings.getTranslogRetentionSize(),\n                indexSettings.getSoftDeleteRetentionOperations());\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1386,"status":"M"}],"commitId":"424ed93e38b0aadb6dec17c0544602e71f77bd45","commitMessage":"@@@Always use soft-deletes in InternalEngine (#50415)\n\nPeer recoveries become faster and use less storage (i.e..  no more extra \ntranslog) with soft-deletes. Soft-deletes has been enabled by default\nsince 7.0. We should make it mandatory in 8.0.  so we can simplify the\nlogic in the engine.  recoveries.  and other components.\n\nWith this change.  InternalEngine will always use soft-deletes regardless \nof the soft_deletes settings.","date":"2019-12-26 22:02:02","modifiedFileCount":"7","status":"M","submitter":"Nhat Nguyen"},{"authorTime":"2020-01-25 04:13:53","codes":[{"authorDate":"2020-01-25 04:13:53","commitOrder":9,"curCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","date":"2020-01-25 04:13:53","endLine":1384,"groupId":"24470","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetention","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/04/15941a48baba2c9c02768f971f5ce7031af164.src","preCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged(indexSettings.getTranslogRetentionAge(), indexSettings.getTranslogRetentionSize(),\n                indexSettings.getSoftDeleteRetentionOperations());\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1314,"status":"M"},{"authorDate":"2020-01-25 04:13:53","commitOrder":9,"curCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","date":"2020-01-25 04:13:53","endLine":1491,"groupId":"48407","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetentionAndRecoverySource","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/04/15941a48baba2c9c02768f971f5ce7031af164.src","preCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged(indexSettings.getTranslogRetentionAge(), indexSettings.getTranslogRetentionSize(),\n                indexSettings.getSoftDeleteRetentionOperations());\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1386,"status":"M"}],"commitId":"b034d1e2ef81bcf150ea7d7c115009d2e71545f6","commitMessage":"@@@Remove translog retention policy (#51417)\n\nWe no longer need to retain the extra translog for peer recovery as we \nhave switched using Lucene index exclusively in 8.0. This change removes\nthe translog retention policy.\n\nRelates #50775","date":"2020-01-25 04:13:53","modifiedFileCount":"20","status":"M","submitter":"Nhat Nguyen"},{"authorTime":"2020-03-11 05:00:32","codes":[{"authorDate":"2020-03-11 05:00:32","commitOrder":10,"curCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","date":"2020-03-11 05:00:32","endLine":1307,"groupId":"9539","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetention","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/97/6f4d4cd41428f3b235e4bfbbaca714208ad10e.src","preCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1237,"status":"M"},{"authorDate":"2020-03-11 05:00:32","commitOrder":10,"curCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","date":"2020-03-11 05:00:32","endLine":1414,"groupId":"5742","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetentionAndRecoverySource","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/97/6f4d4cd41428f3b235e4bfbbaca714208ad10e.src","preCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false);\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1309,"status":"M"}],"commitId":"713e931df4c1eaaa28a024b562ff6f2632c72f55","commitMessage":"@@@Record Force Merges in Live Commit Data (#52694)\n\n* Record Force Merges in live commit data\n\nPrerequisite of #52182. Record force merges in the live commit data\nso two shard states with the same sequence number that differ only in whether\nor not they have been force merged can be distinguished when creating snapshots.\n","date":"2020-03-11 05:00:32","modifiedFileCount":"10","status":"M","submitter":"Armin Braun"},{"authorTime":"2020-04-01 03:52:01","codes":[{"authorDate":"2020-04-01 03:52:01","commitOrder":11,"curCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetadata indexMetadata = IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetadata);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetadata(IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","date":"2020-04-01 03:52:01","endLine":1307,"groupId":"9539","id":21,"instanceNumber":1,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetention","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/53/58f63325f963260acb4ca15f3ac0bb4ef12587.src","preCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1237,"status":"M"},{"authorDate":"2020-04-01 03:52:01","commitOrder":11,"curCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetadata indexMetadata = IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetadata);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetadata(IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","date":"2020-04-01 03:52:01","endLine":1414,"groupId":"5742","id":22,"instanceNumber":2,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetentionAndRecoverySource","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/53/58f63325f963260acb4ca15f3ac0bb4ef12587.src","preCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetaData indexMetaData = IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetaData);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetaData(IndexMetaData.builder(defaultSettings.getIndexMetaData()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1309,"status":"M"}],"commitId":"95a7eed9aa35f47b228e402508709b5bd6703cf4","commitMessage":"@@@Rename MetaData to Metadata in all of the places (#54519)\n\nThis is a simple naming change PR.  to fix the fact that \"metadata\" is a\nsingle English word.  and for too long we have not followed general\nnaming conventions for it. We are also not consistent about it.  for\nexample.  METADATA instead of META_DATA if we were trying to be\nconsistent with MetaData (although METADATA is correct when considered\nin the context of \"metadata\"). This was a simple find and replace across\nthe code base.  only taking a few minutes to fix this naming issue\nforever.","date":"2020-04-01 03:52:01","modifiedFileCount":"1712","status":"M","submitter":"Jason Tedor"},{"authorTime":"2020-04-16 04:31:51","codes":[{"authorDate":"2020-04-16 04:31:51","commitOrder":12,"curCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetadata indexMetadata = IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetadata);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), is(in(liveDocs)));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetadata(IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","date":"2020-04-16 04:31:51","endLine":1345,"groupId":"9539","id":23,"instanceNumber":1,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetention","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/c9/283863dd18a7240eb7e2050cf6369b8e9391a6.src","preCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetadata indexMetadata = IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetadata);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetadata(IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1275,"status":"M"},{"authorDate":"2020-04-16 04:31:51","commitOrder":12,"curCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetadata indexMetadata = IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetadata);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), is(in(liveDocs)));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetadata(IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","date":"2020-04-16 04:31:51","endLine":1452,"groupId":"5742","id":24,"instanceNumber":2,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetentionAndRecoverySource","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/c9/283863dd18a7240eb7e2050cf6369b8e9391a6.src","preCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetadata indexMetadata = IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetadata);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), isIn(liveDocs));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetadata(IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1347,"status":"M"}],"commitId":"38cd668ad045ae451ba895b7e41929b86d340cec","commitMessage":"@@@Remove deprecated third-party methods from tests (#55255)\n\nI've noticed that a lot of our tests are using deprecated static methods\nfrom the Hamcrest matchers. While this is not a big deal in any\nobjective sense.  it seems like a small good thing to reduce compilation\nwarnings and be ready for a new release of the matcher library if we\nneed to upgrade. I've also switched a few other methods in tests that\nhave drop-in replacements.","date":"2020-04-16 04:31:51","modifiedFileCount":"56","status":"M","submitter":"William Brafford"},{"authorTime":"2020-10-06 00:02:55","codes":[{"authorDate":"2020-10-06 00:02:55","commitOrder":13,"curCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetadata indexMetadata = IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetadata);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService::fieldType)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), is(in(liveDocs)));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetadata(IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService::fieldType), hasSize(liveDocs.size()));\n        }\n    }\n","date":"2020-10-06 00:02:55","endLine":1348,"groupId":"9539","id":25,"instanceNumber":1,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetention","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/ed/2127d8aa16b4ff3d3014a712c80776a11e8494.src","preCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetadata indexMetadata = IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetadata);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), is(in(liveDocs)));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetadata(IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocs.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1278,"status":"M"},{"authorDate":"2020-10-06 00:02:55","commitOrder":13,"curCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetadata indexMetadata = IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetadata);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService::fieldType)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), is(in(liveDocs)));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetadata(IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService::fieldType), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","date":"2020-10-06 00:02:55","endLine":1455,"groupId":"5742","id":26,"instanceNumber":2,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetentionAndRecoverySource","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/ed/2127d8aa16b4ff3d3014a712c80776a11e8494.src","preCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetadata indexMetadata = IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetadata);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), is(in(liveDocs)));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetadata(IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1350,"status":"M"}],"commitId":"cf130f386e5a83e35508f08a1c75352d3b071a08","commitMessage":"@@@Replace some usages of QueryShardContext#getMapperService (#63239)\n\nQueryShardContext has a getter that allows to have access to MapperService. In many cases.  it is misused to lookup field types which QueryShardContext has a specific method for. This commit replaces those usages with a function String -> MappedFieldType.\n\nThere are also a few other places where MapperService is retrieved to call methods that are also available directly on QueryShardContext.  which are replaced as part of this commit too.","date":"2020-10-06 00:02:55","modifiedFileCount":"41","status":"M","submitter":"Luca Cavanna"},{"authorTime":"2021-01-04 20:02:24","codes":[{"authorDate":"2021-01-04 20:02:24","commitOrder":14,"curCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetadata indexMetadata = IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetadata);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), is(in(liveDocs)));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetadata(IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine);\n            assertThat(readAllOperationsInLucene(engine), hasSize(liveDocs.size()));\n        }\n    }\n","date":"2021-01-04 20:02:24","endLine":1394,"groupId":"9539","id":27,"instanceNumber":1,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetention","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/60/36d9fa62e50fa55fa89fe027160eb6d275f336.src","preCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetadata indexMetadata = IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetadata);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService::fieldType)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), is(in(liveDocs)));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetadata(IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService::fieldType), hasSize(liveDocs.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1325,"status":"M"},{"authorDate":"2021-01-04 20:02:24","commitOrder":14,"curCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetadata indexMetadata = IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetadata);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), is(in(liveDocs)));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetadata(IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine);\n            assertThat(readAllOperationsInLucene(engine), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","date":"2021-01-04 20:02:24","endLine":1500,"groupId":"5742","id":28,"instanceNumber":2,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetentionAndRecoverySource","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/60/36d9fa62e50fa55fa89fe027160eb6d275f336.src","preCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetadata indexMetadata = IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetadata);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final MapperService mapperService = createMapperService();\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine, mapperService::fieldType)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), is(in(liveDocs)));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetadata(IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine, mapperService);\n            assertThat(readAllOperationsInLucene(engine, mapperService::fieldType), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1396,"status":"M"}],"commitId":"fa7bce3dba2a0591f04d6d0bf4be0ca53a025517","commitMessage":"@@@LuceneChangesSnapshot doesn't need reference to MapperService (#66335)\n\nLuceneChangesSnapshot currently takes a field type lookup lambda so that\nit can call `FieldsVisitor#postProcess()` once it has read a document from\nits lucene index. However.  this `postProcess()` call is unnecessary - the only\nfields loaded for the document are `_source`.  `_id` and `_routing`.  none of\nwhich require post-processing. Deleting this call allows us to remove the\nfieldtype lookup from a number of places.  with the result that various Engine\ntests no longer need to create a MapperService (quite a heavy operation) to\ntest the transaction log.","date":"2021-01-04 20:02:24","modifiedFileCount":"10","status":"M","submitter":"Alan Woodward"},{"authorTime":"2021-01-20 03:10:50","codes":[{"authorDate":"2021-01-20 03:10:50","commitOrder":15,"curCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetadata indexMetadata = IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetadata);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), is(in(liveDocs)));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetadata(IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine);\n            assertThat(readAllOperationsInLucene(engine), hasSize(liveDocs.size()));\n        }\n    }\n","date":"2021-01-20 03:10:50","endLine":1394,"groupId":"102498","id":29,"instanceNumber":1,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetention","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/8e/ef4cd7b1bee7d499876da130a49cc3ab13d573.src","preCode":"    public void testForceMergeWithSoftDeletesRetention() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetadata indexMetadata = IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetadata);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final Set<String> liveDocs = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null, globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n            }\n            for (int i = 0; i < numDocs; i++) {\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n\n            long localCheckpoint = engine.getProcessedLocalCheckpoint();\n            globalCheckpoint.set(randomLongBetween(0, localCheckpoint));\n            engine.syncTranslog();\n            final long safeCommitCheckpoint;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                safeCommitCheckpoint = Long.parseLong(safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= localCheckpoint; seqno++) {\n                long minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitCheckpoint + 1);\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), is(in(liveDocs)));\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                } else {\n                    assertThat(msg, ops.get(seqno), notNullValue());\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetadata(IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build());\n            engine.onSettingsChanged();\n            globalCheckpoint.set(localCheckpoint);\n            engine.syncTranslog();\n\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine);\n            assertThat(readAllOperationsInLucene(engine), hasSize(liveDocs.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1325,"status":"M"},{"authorDate":"2021-01-20 03:10:50","commitOrder":15,"curCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetadata indexMetadata = IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetadata);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), is(in(liveDocs)));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetadata(IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine);\n            assertThat(readAllOperationsInLucene(engine), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","date":"2021-01-20 03:10:50","endLine":1500,"groupId":"102498","id":30,"instanceNumber":2,"isCurCommit":0,"methodName":"testForceMergeWithSoftDeletesRetentionAndRecoverySource","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/8e/ef4cd7b1bee7d499876da130a49cc3ab13d573.src","preCode":"    public void testForceMergeWithSoftDeletesRetentionAndRecoverySource() throws Exception {\n        final long retainedExtraOps = randomLongBetween(0, 10);\n        Settings.Builder settings = Settings.builder()\n            .put(defaultSettings.getSettings())\n            .put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), retainedExtraOps);\n        final IndexMetadata indexMetadata = IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build();\n        final IndexSettings indexSettings = IndexSettingsModule.newIndexSettings(indexMetadata);\n        final AtomicLong globalCheckpoint = new AtomicLong(SequenceNumbers.NO_OPS_PERFORMED);\n        final boolean omitSourceAllTheTime = randomBoolean();\n        final Set<String> liveDocs = new HashSet<>();\n        final Set<String> liveDocsWithSource = new HashSet<>();\n        try (Store store = createStore();\n             InternalEngine engine = createEngine(config(indexSettings, store, createTempDir(), newMergePolicy(), null,\n                 null,\n                 globalCheckpoint::get))) {\n            int numDocs = scaledRandomIntBetween(10, 100);\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                engine.index(indexForDoc(doc));\n                liveDocs.add(doc.id());\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(Integer.toString(i));\n                }\n            }\n            for (int i = 0; i < numDocs; i++) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(Integer.toString(i), null, testDocument(), B_1, null,\n                    useRecoverySource);\n                if (randomBoolean()) {\n                    engine.delete(new Engine.Delete(doc.id(), newUid(doc.id()), primaryTerm.get()));\n                    liveDocs.remove(doc.id());\n                    liveDocsWithSource.remove(doc.id());\n                }\n                if (randomBoolean()) {\n                    engine.index(indexForDoc(doc));\n                    liveDocs.add(doc.id());\n                    if (useRecoverySource == false) {\n                        liveDocsWithSource.add(doc.id());\n                    } else {\n                        liveDocsWithSource.remove(doc.id());\n                    }\n                }\n                if (randomBoolean()) {\n                    engine.flush(randomBoolean(), true);\n                }\n            }\n            engine.flush();\n            globalCheckpoint.set(randomLongBetween(0, engine.getPersistedLocalCheckpoint()));\n            engine.syncTranslog();\n            final long minSeqNoToRetain;\n            try (Engine.IndexCommitRef safeCommit = engine.acquireSafeIndexCommit()) {\n                long safeCommitLocalCheckpoint = Long.parseLong(\n                    safeCommit.getIndexCommit().getUserData().get(SequenceNumbers.LOCAL_CHECKPOINT_KEY));\n                minSeqNoToRetain = Math.min(globalCheckpoint.get() + 1 - retainedExtraOps, safeCommitLocalCheckpoint + 1);\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine);\n            Map<Long, Translog.Operation> ops = readAllOperationsInLucene(engine)\n                .stream().collect(Collectors.toMap(Translog.Operation::seqNo, Function.identity()));\n            for (long seqno = 0; seqno <= engine.getPersistedLocalCheckpoint(); seqno++) {\n                String msg = \"seq# [\" + seqno + \"], global checkpoint [\" + globalCheckpoint + \"], retained-ops [\" + retainedExtraOps + \"]\";\n                if (seqno < minSeqNoToRetain) {\n                    Translog.Operation op = ops.get(seqno);\n                    if (op != null) {\n                        assertThat(op, instanceOf(Translog.Index.class));\n                        assertThat(msg, ((Translog.Index) op).id(), is(in(liveDocs)));\n                    }\n                } else {\n                    Translog.Operation op = ops.get(seqno);\n                    assertThat(msg, op, notNullValue());\n                    if (op instanceof Translog.Index) {\n                        assertEquals(msg, ((Translog.Index) op).source(), B_1);\n                    }\n                }\n            }\n            settings.put(IndexSettings.INDEX_SOFT_DELETES_RETENTION_OPERATIONS_SETTING.getKey(), 0);\n            indexSettings.updateIndexMetadata(IndexMetadata.builder(defaultSettings.getIndexMetadata()).settings(settings).build());\n            engine.onSettingsChanged();\n            \r\n            \r\n            final int numSegments;\n            try (Engine.Searcher searcher = engine.acquireSearcher(\"test\", Engine.SearcherScope.INTERNAL)) {\n                numSegments = searcher.getDirectoryReader().leaves().size();\n            }\n            if (numSegments == 1) {\n                boolean useRecoverySource = randomBoolean() || omitSourceAllTheTime;\n                ParsedDocument doc = testParsedDocument(\"dummy\", null, testDocument(), B_1, null, useRecoverySource);\n                engine.index(indexForDoc(doc));\n                if (useRecoverySource == false) {\n                    liveDocsWithSource.add(doc.id());\n                }\n                engine.syncTranslog();\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.flush(randomBoolean(), true);\n            } else {\n                globalCheckpoint.set(engine.getPersistedLocalCheckpoint());\n                engine.syncTranslog();\n            }\n            engine.forceMerge(true, 1, false, false, false, UUIDs.randomBase64UUID());\n            assertConsistentHistoryBetweenTranslogAndLuceneIndex(engine);\n            assertThat(readAllOperationsInLucene(engine), hasSize(liveDocsWithSource.size()));\n        }\n    }\n","realPath":"server/src/test/java/org/elasticsearch/index/engine/InternalEngineTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":1396,"status":"M"}],"commitId":"3454a094f73e7696446dbd2c0525041293dd4460","commitMessage":"@@@Remove support for upgrading segments on merge. (#67548)\n\nIn earlier Elasticsearch versions.  we had an 'upgrade API' which attempted to\nupgrade an index to the current major version. This action performed a merge to\nupgrade Lucene segments. The upgrade API has not worked since 5.x and was\nrecently deprecated and removed. So the logic for upgrading Lucene segments is\nunused and can also be removed.\n\nThis PR is part of an effort to clarify our approach to index compatibility by\nremoving old upgrade strategies that are no longer relevant.","date":"2021-01-20 03:10:50","modifiedFileCount":"9","status":"M","submitter":"Julie Tibshirani"}]
