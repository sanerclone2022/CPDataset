[{"authorTime":"2018-07-16 16:30:07","codes":[{"authorDate":"2018-07-16 16:30:07","commitOrder":1,"curCode":"    public void testMatchAllDocs() throws IOException {\n        Query query = new MatchAllDocsQuery();\n\n        testSearchCase(query, dataset,\n                aggregation -> aggregation.setNumBuckets(6).field(DATE_FIELD),\n                histogram -> assertEquals(10, histogram.getBuckets().size())\n        );\n        testSearchAndReduceCase(query, dataset,\n                aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n                histogram -> assertEquals(8, histogram.getBuckets().size())\n        );\n    }\n","date":"2018-07-16 16:30:07","endLine":93,"groupId":"5619","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testMatchAllDocs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/7c/f29e3aa9cc540ec5b371f56f24a3654b10f0c1.src","preCode":"    public void testMatchAllDocs() throws IOException {\n        Query query = new MatchAllDocsQuery();\n\n        testSearchCase(query, dataset,\n                aggregation -> aggregation.setNumBuckets(6).field(DATE_FIELD),\n                histogram -> assertEquals(10, histogram.getBuckets().size())\n        );\n        testSearchAndReduceCase(query, dataset,\n                aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n                histogram -> assertEquals(8, histogram.getBuckets().size())\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":82,"status":"B"},{"authorDate":"2018-07-16 16:30:07","commitOrder":1,"curCode":"    public void testIntervalHourWithTZ() throws IOException {\n        testSearchCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:00.000Z\",\n                        \"2017-02-01T09:35:00.000Z\",\n                        \"2017-02-01T10:15:00.000Z\",\n                        \"2017-02-01T13:06:00.000Z\",\n                        \"2017-02-01T14:04:00.000Z\",\n                        \"2017-02-01T14:05:00.000Z\",\n                        \"2017-02-01T15:59:00.000Z\",\n                        \"2017-02-01T16:06:00.000Z\",\n                        \"2017-02-01T16:48:00.000Z\",\n                        \"2017-02-01T16:59:00.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(10, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T08:02:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T08:35:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T09:15:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T12:06:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T13:04:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T13:05:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T14:59:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T15:06:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(8);\n                    assertEquals(\"2017-02-01T15:48:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(9);\n                    assertEquals(\"2017-02-01T15:59:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n        );\n        testSearchAndReduceCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:00.000Z\",\n                        \"2017-02-01T09:35:00.000Z\",\n                        \"2017-02-01T10:15:00.000Z\",\n                        \"2017-02-01T13:06:00.000Z\",\n                        \"2017-02-01T14:04:00.000Z\",\n                        \"2017-02-01T14:05:00.000Z\",\n                        \"2017-02-01T15:59:00.000Z\",\n                        \"2017-02-01T16:06:00.000Z\",\n                        \"2017-02-01T16:48:00.000Z\",\n                        \"2017-02-01T16:59:00.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(8, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T08:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T09:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T10:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T11:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T12:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T13:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T14:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T15:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(3, bucket.getDocCount());\n                }\n        );\n    }\n","date":"2018-07-16 16:30:07","endLine":639,"groupId":"48920","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testIntervalHourWithTZ","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/7c/f29e3aa9cc540ec5b371f56f24a3654b10f0c1.src","preCode":"    public void testIntervalHourWithTZ() throws IOException {\n        testSearchCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:00.000Z\",\n                        \"2017-02-01T09:35:00.000Z\",\n                        \"2017-02-01T10:15:00.000Z\",\n                        \"2017-02-01T13:06:00.000Z\",\n                        \"2017-02-01T14:04:00.000Z\",\n                        \"2017-02-01T14:05:00.000Z\",\n                        \"2017-02-01T15:59:00.000Z\",\n                        \"2017-02-01T16:06:00.000Z\",\n                        \"2017-02-01T16:48:00.000Z\",\n                        \"2017-02-01T16:59:00.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(10, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T08:02:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T08:35:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T09:15:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T12:06:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T13:04:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T13:05:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T14:59:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T15:06:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(8);\n                    assertEquals(\"2017-02-01T15:48:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(9);\n                    assertEquals(\"2017-02-01T15:59:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n        );\n        testSearchAndReduceCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:00.000Z\",\n                        \"2017-02-01T09:35:00.000Z\",\n                        \"2017-02-01T10:15:00.000Z\",\n                        \"2017-02-01T13:06:00.000Z\",\n                        \"2017-02-01T14:04:00.000Z\",\n                        \"2017-02-01T14:05:00.000Z\",\n                        \"2017-02-01T15:59:00.000Z\",\n                        \"2017-02-01T16:06:00.000Z\",\n                        \"2017-02-01T16:48:00.000Z\",\n                        \"2017-02-01T16:59:00.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(8, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T08:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T09:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T10:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T11:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T12:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T13:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T14:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T15:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(3, bucket.getDocCount());\n                }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":528,"status":"B"}],"commitId":"5d3a53843a21098de3d1a342482db555eb660976","commitMessage":"@@@Merge branch 'master' into index-lifecycle\n","date":"2018-07-16 16:30:07","modifiedFileCount":"183","status":"B","submitter":"Colin Goodheart-Smithe"},{"authorTime":"2018-11-19 22:21:01","codes":[{"authorDate":"2018-11-19 22:21:01","commitOrder":2,"curCode":"    public void testMatchAllDocs() throws IOException {\n        testSearchCase(DEFAULT_QUERY, DATES_WITH_TIME,\n            aggregation -> aggregation.setNumBuckets(6).field(DATE_FIELD),\n            histogram -> assertEquals(10, histogram.getBuckets().size())\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, DATES_WITH_TIME,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n            histogram -> assertEquals(8, histogram.getBuckets().size())\n        );\n    }\n","date":"2018-11-19 22:21:01","endLine":91,"groupId":"5619","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testMatchAllDocs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/2d/5109405dc1c23b27b6f3f7dd3e8280cc59df6f.src","preCode":"    public void testMatchAllDocs() throws IOException {\n        Query query = new MatchAllDocsQuery();\n\n        testSearchCase(query, dataset,\n                aggregation -> aggregation.setNumBuckets(6).field(DATE_FIELD),\n                histogram -> assertEquals(10, histogram.getBuckets().size())\n        );\n        testSearchAndReduceCase(query, dataset,\n                aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n                histogram -> assertEquals(8, histogram.getBuckets().size())\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":82,"status":"M"},{"authorDate":"2018-11-19 22:21:01","commitOrder":2,"curCode":"    public void testIntervalHourWithTZ() throws IOException {\n        final List<DateTime> datesForHourInterval = Arrays.asList(\n            new DateTime(2017, 2, 1, 9, 2, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 9, 35, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 10, 15, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 13, 6, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 14, 4, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 14, 5, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 15, 59, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 6, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 48, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 59, 0, DateTimeZone.UTC));\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n            histogram -> {\n                final List<String> dateStrings = datesForHourInterval.stream()\n                    .map(dateTime -> dateTime.withZone(DateTimeZone.forOffsetHours(-1)).toString()).collect(Collectors.toList());\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(datesForHourInterval.size(), buckets.size());\n                for (int i = 0; i < buckets.size(); i++) {\n                    final Histogram.Bucket bucket = buckets.get(i);\n                    assertEquals(dateStrings.get(i), bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n            histogram -> {\n                final Map<String, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(\"2017-02-01T08:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T09:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T12:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T13:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T14:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T15:00:00.000-01:00\", 3);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(8, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKeyAsString(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","date":"2018-11-19 22:21:01","endLine":426,"groupId":"39066","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testIntervalHourWithTZ","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/2d/5109405dc1c23b27b6f3f7dd3e8280cc59df6f.src","preCode":"    public void testIntervalHourWithTZ() throws IOException {\n        testSearchCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:00.000Z\",\n                        \"2017-02-01T09:35:00.000Z\",\n                        \"2017-02-01T10:15:00.000Z\",\n                        \"2017-02-01T13:06:00.000Z\",\n                        \"2017-02-01T14:04:00.000Z\",\n                        \"2017-02-01T14:05:00.000Z\",\n                        \"2017-02-01T15:59:00.000Z\",\n                        \"2017-02-01T16:06:00.000Z\",\n                        \"2017-02-01T16:48:00.000Z\",\n                        \"2017-02-01T16:59:00.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(10, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T08:02:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T08:35:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T09:15:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T12:06:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T13:04:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T13:05:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T14:59:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T15:06:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(8);\n                    assertEquals(\"2017-02-01T15:48:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(9);\n                    assertEquals(\"2017-02-01T15:59:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n        );\n        testSearchAndReduceCase(new MatchAllDocsQuery(),\n                Arrays.asList(\n                        \"2017-02-01T09:02:00.000Z\",\n                        \"2017-02-01T09:35:00.000Z\",\n                        \"2017-02-01T10:15:00.000Z\",\n                        \"2017-02-01T13:06:00.000Z\",\n                        \"2017-02-01T14:04:00.000Z\",\n                        \"2017-02-01T14:05:00.000Z\",\n                        \"2017-02-01T15:59:00.000Z\",\n                        \"2017-02-01T16:06:00.000Z\",\n                        \"2017-02-01T16:48:00.000Z\",\n                        \"2017-02-01T16:59:00.000Z\"\n                ),\n                aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n                histogram -> {\n                    List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                    assertEquals(8, buckets.size());\n\n                    Histogram.Bucket bucket = buckets.get(0);\n                    assertEquals(\"2017-02-01T08:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(1);\n                    assertEquals(\"2017-02-01T09:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(2);\n                    assertEquals(\"2017-02-01T10:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(3);\n                    assertEquals(\"2017-02-01T11:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(0, bucket.getDocCount());\n\n                    bucket = buckets.get(4);\n                    assertEquals(\"2017-02-01T12:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(5);\n                    assertEquals(\"2017-02-01T13:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(2, bucket.getDocCount());\n\n                    bucket = buckets.get(6);\n                    assertEquals(\"2017-02-01T14:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n\n                    bucket = buckets.get(7);\n                    assertEquals(\"2017-02-01T15:00:00.000-01:00\", bucket.getKeyAsString());\n                    assertEquals(3, bucket.getDocCount());\n                }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":384,"status":"M"}],"commitId":"450db7fcf7b99066b42507be15727856765729ff","commitMessage":"@@@[Tests] Fix slowness of AutoDateHistogramAggregatorTests (#35072)\n\nRandomize test assertion and test set size instead of asserting on an\nexhaustive list of dates with fixed test set size. Also refactor common \nobjects used to avoid recreating them.  avoid date to string conversion\nand reduce duplicate test code\n\nCloses #33181","date":"2018-11-19 22:21:01","modifiedFileCount":"1","status":"M","submitter":"Ekal Golas"},{"authorTime":"2018-11-19 22:21:01","codes":[{"authorDate":"2019-01-23 01:38:55","commitOrder":3,"curCode":"    public void testMatchAllDocs() throws IOException {\n        testSearchCase(DEFAULT_QUERY, DATES_WITH_TIME,\n            aggregation -> aggregation.setNumBuckets(6).field(DATE_FIELD),\n            histogram -> {\n                assertEquals(10, histogram.getBuckets().size());\n                assertTrue(AggregationInspectionHelper.hasValue(histogram));\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, DATES_WITH_TIME,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n            histogram -> {\n                assertEquals(8, histogram.getBuckets().size());\n                assertTrue(AggregationInspectionHelper.hasValue(histogram));\n            }\n        );\n    }\n","date":"2019-01-23 01:38:55","endLine":101,"groupId":"5619","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testMatchAllDocs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/6b/4d1482adb5e0aa9ddf8030804d353ea8466a03.src","preCode":"    public void testMatchAllDocs() throws IOException {\n        testSearchCase(DEFAULT_QUERY, DATES_WITH_TIME,\n            aggregation -> aggregation.setNumBuckets(6).field(DATE_FIELD),\n            histogram -> assertEquals(10, histogram.getBuckets().size())\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, DATES_WITH_TIME,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n            histogram -> assertEquals(8, histogram.getBuckets().size())\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"M"},{"authorDate":"2018-11-19 22:21:01","commitOrder":3,"curCode":"    public void testIntervalHourWithTZ() throws IOException {\n        final List<DateTime> datesForHourInterval = Arrays.asList(\n            new DateTime(2017, 2, 1, 9, 2, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 9, 35, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 10, 15, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 13, 6, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 14, 4, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 14, 5, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 15, 59, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 6, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 48, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 59, 0, DateTimeZone.UTC));\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n            histogram -> {\n                final List<String> dateStrings = datesForHourInterval.stream()\n                    .map(dateTime -> dateTime.withZone(DateTimeZone.forOffsetHours(-1)).toString()).collect(Collectors.toList());\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(datesForHourInterval.size(), buckets.size());\n                for (int i = 0; i < buckets.size(); i++) {\n                    final Histogram.Bucket bucket = buckets.get(i);\n                    assertEquals(dateStrings.get(i), bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n            histogram -> {\n                final Map<String, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(\"2017-02-01T08:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T09:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T12:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T13:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T14:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T15:00:00.000-01:00\", 3);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(8, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKeyAsString(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","date":"2018-11-19 22:21:01","endLine":426,"groupId":"39066","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testIntervalHourWithTZ","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/2d/5109405dc1c23b27b6f3f7dd3e8280cc59df6f.src","preCode":"    public void testIntervalHourWithTZ() throws IOException {\n        final List<DateTime> datesForHourInterval = Arrays.asList(\n            new DateTime(2017, 2, 1, 9, 2, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 9, 35, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 10, 15, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 13, 6, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 14, 4, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 14, 5, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 15, 59, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 6, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 48, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 59, 0, DateTimeZone.UTC));\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n            histogram -> {\n                final List<String> dateStrings = datesForHourInterval.stream()\n                    .map(dateTime -> dateTime.withZone(DateTimeZone.forOffsetHours(-1)).toString()).collect(Collectors.toList());\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(datesForHourInterval.size(), buckets.size());\n                for (int i = 0; i < buckets.size(); i++) {\n                    final Histogram.Bucket bucket = buckets.get(i);\n                    assertEquals(dateStrings.get(i), bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n            histogram -> {\n                final Map<String, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(\"2017-02-01T08:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T09:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T12:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T13:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T14:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T15:00:00.000-01:00\", 3);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(8, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKeyAsString(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":384,"status":"N"}],"commitId":"2ba9e361aba7df6e9c0b5fa8fae3d219d50ddf09","commitMessage":"@@@Add helper classes to determine if aggs have a value (#36020)\n\nThis adds a set of helper classes to determine if an agg \"has a value\". \nThis is needed because InternalAggs represent \"empty\" in different \nmanners according to convention. Some use `NaN`.  `+/- Inf`.  `0.0`.  etc.\n\nA user can pass the Internal agg type to one of these helper methods\nand it will report if the agg contains a value or not.  which allows the\nuser to differentiate \"empty\" from a real `NaN`.\n\nThese helpers are best-effort in some cases.  For example.  several\npipeline aggs share a single return class but use different conventions\nto mark \"empty\".  so the helper uses the loosest definition that applies\nto all the aggs that use the class.\n\nSums in particular are unreliable.  The InternalSum simply returns 0.0\nif the agg is empty (which is correct.  no values == sum of zero).  But this\nalso means the helper cannot differentiate from \"empty\" and `+1 + -1`.","date":"2019-01-23 01:38:55","modifiedFileCount":"45","status":"M","submitter":"Zachary Tong"},{"authorTime":"2019-01-23 17:40:05","codes":[{"authorDate":"2019-01-23 01:38:55","commitOrder":4,"curCode":"    public void testMatchAllDocs() throws IOException {\n        testSearchCase(DEFAULT_QUERY, DATES_WITH_TIME,\n            aggregation -> aggregation.setNumBuckets(6).field(DATE_FIELD),\n            histogram -> {\n                assertEquals(10, histogram.getBuckets().size());\n                assertTrue(AggregationInspectionHelper.hasValue(histogram));\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, DATES_WITH_TIME,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n            histogram -> {\n                assertEquals(8, histogram.getBuckets().size());\n                assertTrue(AggregationInspectionHelper.hasValue(histogram));\n            }\n        );\n    }\n","date":"2019-01-23 01:38:55","endLine":101,"groupId":"5619","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testMatchAllDocs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/6b/4d1482adb5e0aa9ddf8030804d353ea8466a03.src","preCode":"    public void testMatchAllDocs() throws IOException {\n        testSearchCase(DEFAULT_QUERY, DATES_WITH_TIME,\n            aggregation -> aggregation.setNumBuckets(6).field(DATE_FIELD),\n            histogram -> {\n                assertEquals(10, histogram.getBuckets().size());\n                assertTrue(AggregationInspectionHelper.hasValue(histogram));\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, DATES_WITH_TIME,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n            histogram -> {\n                assertEquals(8, histogram.getBuckets().size());\n                assertTrue(AggregationInspectionHelper.hasValue(histogram));\n            }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"N"},{"authorDate":"2019-01-23 17:40:05","commitOrder":4,"curCode":"    public void testIntervalHourWithTZ() throws IOException {\n        final List<ZonedDateTime> datesForHourInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 35, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 10, 15, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 13, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 4, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 5, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 15, 59, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 48, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 59, 0, 0, ZoneOffset.UTC));\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(ZoneOffset.ofHours(-1)),\n            histogram -> {\n                final List<String> dateStrings = datesForHourInterval.stream()\n                    .map(dateTime -> DateFormatter.forPattern(\"strict_date_time\")\n                        .format(dateTime.withZoneSameInstant(ZoneOffset.ofHours(-1)))).collect(Collectors.toList());\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(datesForHourInterval.size(), buckets.size());\n                for (int i = 0; i < buckets.size(); i++) {\n                    final Histogram.Bucket bucket = buckets.get(i);\n                    assertEquals(dateStrings.get(i), bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(ZoneOffset.ofHours(-1)),\n            histogram -> {\n                final Map<String, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(\"2017-02-01T08:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T09:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T12:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T13:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T14:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T15:00:00.000-01:00\", 3);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(8, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKeyAsString(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","date":"2019-01-23 17:40:05","endLine":463,"groupId":"8738","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testIntervalHourWithTZ","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/92/93b33e22f4361b1e80b1addfd49310f176fabb.src","preCode":"    public void testIntervalHourWithTZ() throws IOException {\n        final List<DateTime> datesForHourInterval = Arrays.asList(\n            new DateTime(2017, 2, 1, 9, 2, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 9, 35, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 10, 15, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 13, 6, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 14, 4, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 14, 5, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 15, 59, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 6, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 48, 0, DateTimeZone.UTC),\n            new DateTime(2017, 2, 1, 16, 59, 0, DateTimeZone.UTC));\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n            histogram -> {\n                final List<String> dateStrings = datesForHourInterval.stream()\n                    .map(dateTime -> dateTime.withZone(DateTimeZone.forOffsetHours(-1)).toString()).collect(Collectors.toList());\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(datesForHourInterval.size(), buckets.size());\n                for (int i = 0; i < buckets.size(); i++) {\n                    final Histogram.Bucket bucket = buckets.get(i);\n                    assertEquals(dateStrings.get(i), bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(DateTimeZone.forOffsetHours(-1)),\n            histogram -> {\n                final Map<String, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(\"2017-02-01T08:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T09:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T12:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T13:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T14:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T15:00:00.000-01:00\", 3);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(8, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKeyAsString(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":420,"status":"M"}],"commitId":"daa2ec8a605d385a65b9ab3e89d016b3fd0dffe2","commitMessage":"@@@Switch mapping/aggregations over to java time (#36363)\n\nThis commit moves the aggregation and mapping code from joda time to\njava time. This includes field mappers.  root object mappers.  aggregations with date\nhistograms.  query builders and a lot of changes within tests.\n\nThe cut-over to java time is a requirement so that we can support nanoseconds\nproperly in a future field mapper.\n\nRelates #27330","date":"2019-01-23 17:40:05","modifiedFileCount":"154","status":"M","submitter":"Alexander Reelsen"},{"authorTime":"2020-06-16 02:33:31","codes":[{"authorDate":"2020-06-16 02:33:31","commitOrder":5,"curCode":"    public void testMatchAllDocs() throws IOException {\n        Map<String, Integer> expectedDocCount = new TreeMap<>();\n        expectedDocCount.put(\"2010-01-01T00:00:00.000Z\", 2);\n        expectedDocCount.put(\"2012-01-01T00:00:00.000Z\", 1);\n        expectedDocCount.put(\"2013-01-01T00:00:00.000Z\", 2);\n        expectedDocCount.put(\"2015-01-01T00:00:00.000Z\", 3);\n        expectedDocCount.put(\"2016-01-01T00:00:00.000Z\", 1);\n        expectedDocCount.put(\"2017-01-01T00:00:00.000Z\", 1);\n        testSearchCase(DEFAULT_QUERY, DATES_WITH_TIME,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n        expectedDocCount.put(\"2011-01-01T00:00:00.000Z\", 0);\n        expectedDocCount.put(\"2014-01-01T00:00:00.000Z\", 0);\n        testSearchAndReduceCase(DEFAULT_QUERY, DATES_WITH_TIME,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n    }\n","date":"2020-06-16 02:33:31","endLine":128,"groupId":"61175","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testMatchAllDocs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/47/7df9e59163c957ddad7d1665b8af353b9129c9.src","preCode":"    public void testMatchAllDocs() throws IOException {\n        testSearchCase(DEFAULT_QUERY, DATES_WITH_TIME,\n            aggregation -> aggregation.setNumBuckets(6).field(DATE_FIELD),\n            histogram -> {\n                assertEquals(10, histogram.getBuckets().size());\n                assertTrue(AggregationInspectionHelper.hasValue(histogram));\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, DATES_WITH_TIME,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n            histogram -> {\n                assertEquals(8, histogram.getBuckets().size());\n                assertTrue(AggregationInspectionHelper.hasValue(histogram));\n            }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":110,"status":"M"},{"authorDate":"2020-06-16 02:33:31","commitOrder":5,"curCode":"    public void testIntervalHourWithTZ() throws IOException {\n        List<ZonedDateTime> datesForHourInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 35, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 10, 15, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 13, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 4, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 5, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 15, 59, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 48, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 59, 0, 0, ZoneOffset.UTC));\n        Map<String, Integer> expectedDocCount = new TreeMap<>();\n        expectedDocCount.put(\"2017-02-01T08:00:00.000-01:00\", 2);\n        expectedDocCount.put(\"2017-02-01T09:00:00.000-01:00\", 1);\n        expectedDocCount.put(\"2017-02-01T12:00:00.000-01:00\", 1);\n        expectedDocCount.put(\"2017-02-01T13:00:00.000-01:00\", 2);\n        expectedDocCount.put(\"2017-02-01T14:00:00.000-01:00\", 1);\n        expectedDocCount.put(\"2017-02-01T15:00:00.000-01:00\", 3);\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(ZoneOffset.ofHours(-1)),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n        expectedDocCount.put(\"2017-02-01T10:00:00.000-01:00\", 0);\n        expectedDocCount.put(\"2017-02-01T11:00:00.000-01:00\", 0);\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(ZoneOffset.ofHours(-1)),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n    }\n","date":"2020-06-16 02:33:31","endLine":598,"groupId":"10681","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testIntervalHourWithTZ","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/47/7df9e59163c957ddad7d1665b8af353b9129c9.src","preCode":"    public void testIntervalHourWithTZ() throws IOException {\n        final List<ZonedDateTime> datesForHourInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 35, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 10, 15, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 13, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 4, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 5, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 15, 59, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 48, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 59, 0, 0, ZoneOffset.UTC));\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(ZoneOffset.ofHours(-1)),\n            histogram -> {\n                final List<String> dateStrings = datesForHourInterval.stream()\n                    .map(dateTime -> DateFormatter.forPattern(\"strict_date_time\")\n                        .format(dateTime.withZoneSameInstant(ZoneOffset.ofHours(-1)))).collect(Collectors.toList());\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(datesForHourInterval.size(), buckets.size());\n                for (int i = 0; i < buckets.size(); i++) {\n                    final Histogram.Bucket bucket = buckets.get(i);\n                    assertEquals(dateStrings.get(i), bucket.getKeyAsString());\n                    assertEquals(1, bucket.getDocCount());\n                }\n            }\n        );\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(ZoneOffset.ofHours(-1)),\n            histogram -> {\n                final Map<String, Integer> expectedDocCount = new HashMap<>();\n                expectedDocCount.put(\"2017-02-01T08:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T09:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T12:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T13:00:00.000-01:00\", 2);\n                expectedDocCount.put(\"2017-02-01T14:00:00.000-01:00\", 1);\n                expectedDocCount.put(\"2017-02-01T15:00:00.000-01:00\", 3);\n                final List<? extends Histogram.Bucket> buckets = histogram.getBuckets();\n                assertEquals(8, buckets.size());\n                buckets.forEach(bucket ->\n                    assertEquals(expectedDocCount.getOrDefault(bucket.getKeyAsString(), 0).longValue(), bucket.getDocCount()));\n            }\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":569,"status":"M"}],"commitId":"7c7fe0152d43a19379942a144dd4904b3d1db570","commitMessage":"@@@Save memory when auto_date_histogram is not on top (#57304)\n\nThis builds an `auto_date_histogram` aggregator that natively aggregates\nfrom many buckets and uses it when the `auto_date_histogram` used to use\n`asMultiBucketAggregator` which should save a significant amount of\nmemory in those cases. In particular.  this happens when\n`auto_date_histogram` is a sub-aggregator of a multi-bucketing aggregator\nlike `terms` or `histogram` or `filters`. For the most part we preserve\nthe original implementation when `auto_date_histogram` only collects from\na single bucket.\n\nIt isn't possible to \"just port the aggregator\" without taking a pretty\nsignificant performance hit because we used to rewrite all of the\nbuckets every time we switched to a coarser and coarser rounding\nconfiguration. Without some major surgery to how to delay sub-aggs\nwe'd end up rewriting the delay list zillions of time if there are many\nbuckets.\n\nThe multi-bucket version of the aggregator has a \"budget\" of \"wasted\"\nbuckets and only rewrites all of the buckets when we exceed that budget.\nNow that we don't rebucket every time we increase the rounding we can no\nlonger get an accurate count of the number of buckets! So instead the\naggregator uses an estimate of the number of buckets to trigger switching\nto a coarser rounding. This estimate is likely to be *terrible* when\nbuckets are far apart compared to the rounding. So it also uses the\ndifference between the first and last bucket to trigger switching to a\ncoarser rounding. Which covers for the shortcomings of the bucket\nestimation technique pretty well. It also causes the aggregator to emit\nfewer buckets in cases where they'd be reduced together on the\ncoordinating node. This is wonderful! But probably fairly rare.\n\nAll of that does buy us some speed improvements when the aggregator is\na child of multi-bucket aggregator:\nWithout metrics or time zone: 25% faster\nWith metrics: 15% faster\nWith time zone: 22% faster\n\nRelates to #56487\n","date":"2020-06-16 02:33:31","modifiedFileCount":"14","status":"M","submitter":"Nik Everett"},{"authorTime":"2020-08-07 05:14:20","codes":[{"authorDate":"2020-08-07 05:14:20","commitOrder":6,"curCode":"    public void testMatchAllDocs() throws IOException {\n        Map<String, Integer> expectedDocCount = new TreeMap<>();\n        expectedDocCount.put(\"2010-01-01T00:00:00.000Z\", 2);\n        expectedDocCount.put(\"2012-01-01T00:00:00.000Z\", 1);\n        expectedDocCount.put(\"2013-01-01T00:00:00.000Z\", 2);\n        expectedDocCount.put(\"2015-01-01T00:00:00.000Z\", 3);\n        expectedDocCount.put(\"2016-01-01T00:00:00.000Z\", 1);\n        expectedDocCount.put(\"2017-01-01T00:00:00.000Z\", 1);\n        expectedDocCount.put(\"2011-01-01T00:00:00.000Z\", 0);\n        expectedDocCount.put(\"2014-01-01T00:00:00.000Z\", 0);\n        testSearchCase(DEFAULT_QUERY, DATES_WITH_TIME,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n    }\n","date":"2020-08-07 05:14:20","endLine":124,"groupId":"104993","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testMatchAllDocs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/4c/b34acab2449fdaa8d6149b4effef11af92b7f1.src","preCode":"    public void testMatchAllDocs() throws IOException {\n        Map<String, Integer> expectedDocCount = new TreeMap<>();\n        expectedDocCount.put(\"2010-01-01T00:00:00.000Z\", 2);\n        expectedDocCount.put(\"2012-01-01T00:00:00.000Z\", 1);\n        expectedDocCount.put(\"2013-01-01T00:00:00.000Z\", 2);\n        expectedDocCount.put(\"2015-01-01T00:00:00.000Z\", 3);\n        expectedDocCount.put(\"2016-01-01T00:00:00.000Z\", 1);\n        expectedDocCount.put(\"2017-01-01T00:00:00.000Z\", 1);\n        testSearchCase(DEFAULT_QUERY, DATES_WITH_TIME,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n        expectedDocCount.put(\"2011-01-01T00:00:00.000Z\", 0);\n        expectedDocCount.put(\"2014-01-01T00:00:00.000Z\", 0);\n        testSearchAndReduceCase(DEFAULT_QUERY, DATES_WITH_TIME,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":110,"status":"M"},{"authorDate":"2020-08-07 05:14:20","commitOrder":6,"curCode":"    public void testIntervalHourWithTZ() throws IOException {\n        List<ZonedDateTime> datesForHourInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 35, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 10, 15, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 13, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 4, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 5, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 15, 59, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 48, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 59, 0, 0, ZoneOffset.UTC));\n        Map<String, Integer> expectedDocCount = new TreeMap<>();\n        expectedDocCount.put(\"2017-02-01T08:00:00.000-01:00\", 2);\n        expectedDocCount.put(\"2017-02-01T09:00:00.000-01:00\", 1);\n        expectedDocCount.put(\"2017-02-01T12:00:00.000-01:00\", 1);\n        expectedDocCount.put(\"2017-02-01T13:00:00.000-01:00\", 2);\n        expectedDocCount.put(\"2017-02-01T14:00:00.000-01:00\", 1);\n        expectedDocCount.put(\"2017-02-01T15:00:00.000-01:00\", 3);\n        expectedDocCount.put(\"2017-02-01T10:00:00.000-01:00\", 0);\n        expectedDocCount.put(\"2017-02-01T11:00:00.000-01:00\", 0);\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(ZoneOffset.ofHours(-1)),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n    }\n","date":"2020-08-07 05:14:20","endLine":578,"groupId":"104993","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testIntervalHourWithTZ","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-elasticsearch-10-0.7/blobInfo/CC_OUT/blobs/4c/b34acab2449fdaa8d6149b4effef11af92b7f1.src","preCode":"    public void testIntervalHourWithTZ() throws IOException {\n        List<ZonedDateTime> datesForHourInterval = Arrays.asList(\n            ZonedDateTime.of(2017, 2, 1, 9, 2, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 9, 35, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 10, 15, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 13, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 4, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 14, 5, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 15, 59, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 6, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 48, 0, 0, ZoneOffset.UTC),\n            ZonedDateTime.of(2017, 2, 1, 16, 59, 0, 0, ZoneOffset.UTC));\n        Map<String, Integer> expectedDocCount = new TreeMap<>();\n        expectedDocCount.put(\"2017-02-01T08:00:00.000-01:00\", 2);\n        expectedDocCount.put(\"2017-02-01T09:00:00.000-01:00\", 1);\n        expectedDocCount.put(\"2017-02-01T12:00:00.000-01:00\", 1);\n        expectedDocCount.put(\"2017-02-01T13:00:00.000-01:00\", 2);\n        expectedDocCount.put(\"2017-02-01T14:00:00.000-01:00\", 1);\n        expectedDocCount.put(\"2017-02-01T15:00:00.000-01:00\", 3);\n        testSearchCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(8).field(DATE_FIELD).timeZone(ZoneOffset.ofHours(-1)),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n        expectedDocCount.put(\"2017-02-01T10:00:00.000-01:00\", 0);\n        expectedDocCount.put(\"2017-02-01T11:00:00.000-01:00\", 0);\n        testSearchAndReduceCase(DEFAULT_QUERY, datesForHourInterval,\n            aggregation -> aggregation.setNumBuckets(10).field(DATE_FIELD).timeZone(ZoneOffset.ofHours(-1)),\n            result -> assertThat(bucketCountsAsMap(result), equalTo(expectedDocCount))\n        );\n    }\n","realPath":"server/src/test/java/org/elasticsearch/search/aggregations/bucket/histogram/AutoDateHistogramAggregatorTests.java","repoName":"elasticsearch","snippetEndLine":0,"snippetStartLine":0,"startLine":553,"status":"M"}],"commitId":"5e3ea6eb11c68bdcc9adda51715a6e1fea9186d6","commitMessage":"@@@Merge branch 'master' into feature/runtime_fields\n","date":"2020-08-07 05:14:20","modifiedFileCount":"73","status":"M","submitter":"Nik Everett"}]
