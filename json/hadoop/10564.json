[{"authorTime":"2018-01-17 22:14:11","codes":[{"authorDate":"2018-01-17 22:14:11","commitOrder":1,"curCode":"  private void testCommitterWithDuplicatedCommitInternal(int version) throws\n      Exception {\n    Job job = Job.getInstance();\n    FileOutputFormat.setOutputPath(job, outDir);\n    Configuration conf = job.getConfiguration();\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION,\n        version);\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\n    FileOutputCommitter committer = new FileOutputCommitter(outDir, tContext);\n\n    \r\n    committer.setupJob(jContext);\n    committer.setupTask(tContext);\n\n    \r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\n    writeOutput(theRecordWriter, tContext);\n\n    \r\n    committer.commitTask(tContext);\n    committer.commitJob(jContext);\n\n    \r\n    validateContent(outDir);\n\n    \r\n    try {\n      committer.commitJob(jContext);\n      if (version == 1) {\n        Assert.fail(\"Duplicate commit success: wrong behavior for version 1.\");\n      }\n    } catch (IOException e) {\n      if (version == 2) {\n        Assert.fail(\"Duplicate commit failed: wrong behavior for version 2.\");\n      }\n    }\n    FileUtil.fullyDelete(new File(outDir.toString()));\n  }\n","date":"2018-01-17 22:14:11","endLine":378,"groupId":"8667","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testCommitterWithDuplicatedCommitInternal","params":"(intversion)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hadoop-10-0.7/blobInfo/CC_OUT/blobs/cd/9d44b936d54073ed2d24fb50c40fdb2b151182.src","preCode":"  private void testCommitterWithDuplicatedCommitInternal(int version) throws\n      Exception {\n    Job job = Job.getInstance();\n    FileOutputFormat.setOutputPath(job, outDir);\n    Configuration conf = job.getConfiguration();\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION,\n        version);\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\n    FileOutputCommitter committer = new FileOutputCommitter(outDir, tContext);\n\n    \r\n    committer.setupJob(jContext);\n    committer.setupTask(tContext);\n\n    \r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\n    writeOutput(theRecordWriter, tContext);\n\n    \r\n    committer.commitTask(tContext);\n    committer.commitJob(jContext);\n\n    \r\n    validateContent(outDir);\n\n    \r\n    try {\n      committer.commitJob(jContext);\n      if (version == 1) {\n        Assert.fail(\"Duplicate commit success: wrong behavior for version 1.\");\n      }\n    } catch (IOException e) {\n      if (version == 2) {\n        Assert.fail(\"Duplicate commit failed: wrong behavior for version 2.\");\n      }\n    }\n    FileUtil.fullyDelete(new File(outDir.toString()));\n  }\n","realPath":"hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/output/TestFileOutputCommitter.java","repoName":"hadoop","snippetEndLine":0,"snippetStartLine":0,"startLine":338,"status":"B"},{"authorDate":"2018-01-17 22:14:11","commitOrder":1,"curCode":"  private void testMapFileOutputCommitterInternal(int version)\n      throws Exception {\n    Job job = Job.getInstance();\n    FileOutputFormat.setOutputPath(job, outDir);\n    Configuration conf = job.getConfiguration();\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION,\n        version);\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\n    FileOutputCommitter committer = new FileOutputCommitter(outDir, tContext);\n\n    \r\n    committer.setupJob(jContext);\n    committer.setupTask(tContext);\n\n    \r\n    MapFileOutputFormat theOutputFormat = new MapFileOutputFormat();\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\n    writeMapFileOutput(theRecordWriter, tContext);\n\n    \r\n    committer.commitTask(tContext);\n    committer.commitJob(jContext);\n\n    \r\n    \r\n    try {\n      MapFileOutputFormat.getReaders(outDir, conf);\n    } catch (Exception e) {\n      fail(\"Fail to read from MapFileOutputFormat: \" + e);\n      e.printStackTrace();\n    }\n\n    \r\n    validateMapFileOutputContent(FileSystem.get(job.getConfiguration()), outDir);\n    FileUtil.fullyDelete(new File(outDir.toString()));\n  }\n","date":"2018-01-17 22:14:11","endLine":539,"groupId":"8667","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testMapFileOutputCommitterInternal","params":"(intversion)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hadoop-10-0.7/blobInfo/CC_OUT/blobs/cd/9d44b936d54073ed2d24fb50c40fdb2b151182.src","preCode":"  private void testMapFileOutputCommitterInternal(int version)\n      throws Exception {\n    Job job = Job.getInstance();\n    FileOutputFormat.setOutputPath(job, outDir);\n    Configuration conf = job.getConfiguration();\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION,\n        version);\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\n    FileOutputCommitter committer = new FileOutputCommitter(outDir, tContext);\n\n    \r\n    committer.setupJob(jContext);\n    committer.setupTask(tContext);\n\n    \r\n    MapFileOutputFormat theOutputFormat = new MapFileOutputFormat();\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\n    writeMapFileOutput(theRecordWriter, tContext);\n\n    \r\n    committer.commitTask(tContext);\n    committer.commitJob(jContext);\n\n    \r\n    \r\n    try {\n      MapFileOutputFormat.getReaders(outDir, conf);\n    } catch (Exception e) {\n      fail(\"Fail to read from MapFileOutputFormat: \" + e);\n      e.printStackTrace();\n    }\n\n    \r\n    validateMapFileOutputContent(FileSystem.get(job.getConfiguration()), outDir);\n    FileUtil.fullyDelete(new File(outDir.toString()));\n  }\n","realPath":"hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/output/TestFileOutputCommitter.java","repoName":"hadoop","snippetEndLine":0,"snippetStartLine":0,"startLine":502,"status":"B"}],"commitId":"6e42d058292d9656e9ebc9a47be13280e3c919ea","commitMessage":"@@@MAPREDUCE-7029. FileOutputCommitter is slow on filesystems lacking recursive delete. Contributed by Karthik Palaniappan\n","date":"2018-01-17 22:14:11","modifiedFileCount":"2","status":"B","submitter":"Jason Lowe"},{"authorTime":"2018-06-15 00:38:20","codes":[{"authorDate":"2018-01-17 22:14:11","commitOrder":2,"curCode":"  private void testCommitterWithDuplicatedCommitInternal(int version) throws\n      Exception {\n    Job job = Job.getInstance();\n    FileOutputFormat.setOutputPath(job, outDir);\n    Configuration conf = job.getConfiguration();\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION,\n        version);\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\n    FileOutputCommitter committer = new FileOutputCommitter(outDir, tContext);\n\n    \r\n    committer.setupJob(jContext);\n    committer.setupTask(tContext);\n\n    \r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\n    writeOutput(theRecordWriter, tContext);\n\n    \r\n    committer.commitTask(tContext);\n    committer.commitJob(jContext);\n\n    \r\n    validateContent(outDir);\n\n    \r\n    try {\n      committer.commitJob(jContext);\n      if (version == 1) {\n        Assert.fail(\"Duplicate commit success: wrong behavior for version 1.\");\n      }\n    } catch (IOException e) {\n      if (version == 2) {\n        Assert.fail(\"Duplicate commit failed: wrong behavior for version 2.\");\n      }\n    }\n    FileUtil.fullyDelete(new File(outDir.toString()));\n  }\n","date":"2018-01-17 22:14:11","endLine":378,"groupId":"10564","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testCommitterWithDuplicatedCommitInternal","params":"(intversion)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hadoop-10-0.7/blobInfo/CC_OUT/blobs/cd/9d44b936d54073ed2d24fb50c40fdb2b151182.src","preCode":"  private void testCommitterWithDuplicatedCommitInternal(int version) throws\n      Exception {\n    Job job = Job.getInstance();\n    FileOutputFormat.setOutputPath(job, outDir);\n    Configuration conf = job.getConfiguration();\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION,\n        version);\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\n    FileOutputCommitter committer = new FileOutputCommitter(outDir, tContext);\n\n    \r\n    committer.setupJob(jContext);\n    committer.setupTask(tContext);\n\n    \r\n    TextOutputFormat theOutputFormat = new TextOutputFormat();\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\n    writeOutput(theRecordWriter, tContext);\n\n    \r\n    committer.commitTask(tContext);\n    committer.commitJob(jContext);\n\n    \r\n    validateContent(outDir);\n\n    \r\n    try {\n      committer.commitJob(jContext);\n      if (version == 1) {\n        Assert.fail(\"Duplicate commit success: wrong behavior for version 1.\");\n      }\n    } catch (IOException e) {\n      if (version == 2) {\n        Assert.fail(\"Duplicate commit failed: wrong behavior for version 2.\");\n      }\n    }\n    FileUtil.fullyDelete(new File(outDir.toString()));\n  }\n","realPath":"hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/output/TestFileOutputCommitter.java","repoName":"hadoop","snippetEndLine":0,"snippetStartLine":0,"startLine":338,"status":"N"},{"authorDate":"2018-06-15 00:38:20","commitOrder":2,"curCode":"  private void testMapFileOutputCommitterInternal(int version)\n      throws Exception {\n    Job job = Job.getInstance();\n    FileOutputFormat.setOutputPath(job, outDir);\n    Configuration conf = job.getConfiguration();\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION,\n        version);\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\n    FileOutputCommitter committer = new FileOutputCommitter(outDir, tContext);\n\n    \r\n    committer.setupJob(jContext);\n    committer.setupTask(tContext);\n\n    \r\n    MapFileOutputFormat theOutputFormat = new MapFileOutputFormat();\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\n    writeMapFileOutput(theRecordWriter, tContext);\n\n    \r\n    committer.commitTask(tContext);\n    committer.commitJob(jContext);\n\n    \r\n    \r\n    MapFile.Reader[] readers = {};\n    try {\n      readers = MapFileOutputFormat.getReaders(outDir, conf);\n      \r\n      validateMapFileOutputContent(FileSystem.get(job.getConfiguration()), outDir);\n    } finally {\n      IOUtils.cleanupWithLogger(null, readers);\n      FileUtil.fullyDelete(new File(outDir.toString()));\n    }\n  }\n","date":"2018-06-15 00:38:20","endLine":539,"groupId":"10564","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testMapFileOutputCommitterInternal","params":"(intversion)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hadoop-10-0.7/blobInfo/CC_OUT/blobs/fc/43dce1830d698a5f28512e913c3fb753408ef9.src","preCode":"  private void testMapFileOutputCommitterInternal(int version)\n      throws Exception {\n    Job job = Job.getInstance();\n    FileOutputFormat.setOutputPath(job, outDir);\n    Configuration conf = job.getConfiguration();\n    conf.set(MRJobConfig.TASK_ATTEMPT_ID, attempt);\n    conf.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION,\n        version);\n    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\n    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\n    FileOutputCommitter committer = new FileOutputCommitter(outDir, tContext);\n\n    \r\n    committer.setupJob(jContext);\n    committer.setupTask(tContext);\n\n    \r\n    MapFileOutputFormat theOutputFormat = new MapFileOutputFormat();\n    RecordWriter theRecordWriter = theOutputFormat.getRecordWriter(tContext);\n    writeMapFileOutput(theRecordWriter, tContext);\n\n    \r\n    committer.commitTask(tContext);\n    committer.commitJob(jContext);\n\n    \r\n    \r\n    try {\n      MapFileOutputFormat.getReaders(outDir, conf);\n    } catch (Exception e) {\n      fail(\"Fail to read from MapFileOutputFormat: \" + e);\n      e.printStackTrace();\n    }\n\n    \r\n    validateMapFileOutputContent(FileSystem.get(job.getConfiguration()), outDir);\n    FileUtil.fullyDelete(new File(outDir.toString()));\n  }\n","realPath":"hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/output/TestFileOutputCommitter.java","repoName":"hadoop","snippetEndLine":0,"snippetStartLine":0,"startLine":503,"status":"M"}],"commitId":"418cff4820ba73cdbfd09fc5879b8b3aa4e62d5f","commitMessage":"@@@Merge remote-tracking branch 'apache-commit/trunk' into HDDS-48\n","date":"2018-06-15 00:38:20","modifiedFileCount":"310","status":"M","submitter":"Arpit Agarwal"}]
