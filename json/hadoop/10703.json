[{"authorTime":"2018-05-02 05:19:53","codes":[{"authorDate":"2018-05-02 05:19:53","commitOrder":1,"curCode":"  protected List<FileStatus> listStatus(JobContext job\n                                        ) throws IOException {\n    Path[] dirs = getInputPaths(job);\n    if (dirs.length == 0) {\n      throw new IOException(\"No input paths specified in job\");\n    }\n    \n    \r\n    TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs, \n                                        job.getConfiguration());\n\n    \r\n    boolean recursive = getInputDirRecursive(job);\n\n    \r\n    \r\n    List<PathFilter> filters = new ArrayList<PathFilter>();\n    filters.add(hiddenFileFilter);\n    PathFilter jobFilter = getInputPathFilter(job);\n    if (jobFilter != null) {\n      filters.add(jobFilter);\n    }\n    PathFilter inputFilter = new MultiPathFilter(filters);\n    \n    List<FileStatus> result = null;\n\n    int numThreads = job.getConfiguration().getInt(LIST_STATUS_NUM_THREADS,\n        DEFAULT_LIST_STATUS_NUM_THREADS);\n    StopWatch sw = new StopWatch().start();\n    if (numThreads == 1) {\n      result = singleThreadedListStatus(job, dirs, inputFilter, recursive);\n    } else {\n      Iterable<FileStatus> locatedFiles = null;\n      try {\n        LocatedFileStatusFetcher locatedFileStatusFetcher = new LocatedFileStatusFetcher(\n            job.getConfiguration(), dirs, recursive, inputFilter, true);\n        locatedFiles = locatedFileStatusFetcher.getFileStatuses();\n      } catch (InterruptedException e) {\n        throw new IOException(\"Interrupted while getting file statuses\");\n      }\n      result = Lists.newArrayList(locatedFiles);\n    }\n    \n    sw.stop();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Time taken to get FileStatuses: \"\n          + sw.now(TimeUnit.MILLISECONDS));\n    }\n    LOG.info(\"Total input files to process : \" + result.size());\n    return result;\n  }\n","date":"2018-05-02 05:19:53","endLine":294,"groupId":"37486","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"listStatus","params":"(JobContextjob)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hadoop-10-0.7/blobInfo/CC_OUT/blobs/e2/d8e6fa7ce88b0900f472bebae0076edb038062.src","preCode":"  protected List<FileStatus> listStatus(JobContext job\n                                        ) throws IOException {\n    Path[] dirs = getInputPaths(job);\n    if (dirs.length == 0) {\n      throw new IOException(\"No input paths specified in job\");\n    }\n    \n    \r\n    TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs, \n                                        job.getConfiguration());\n\n    \r\n    boolean recursive = getInputDirRecursive(job);\n\n    \r\n    \r\n    List<PathFilter> filters = new ArrayList<PathFilter>();\n    filters.add(hiddenFileFilter);\n    PathFilter jobFilter = getInputPathFilter(job);\n    if (jobFilter != null) {\n      filters.add(jobFilter);\n    }\n    PathFilter inputFilter = new MultiPathFilter(filters);\n    \n    List<FileStatus> result = null;\n\n    int numThreads = job.getConfiguration().getInt(LIST_STATUS_NUM_THREADS,\n        DEFAULT_LIST_STATUS_NUM_THREADS);\n    StopWatch sw = new StopWatch().start();\n    if (numThreads == 1) {\n      result = singleThreadedListStatus(job, dirs, inputFilter, recursive);\n    } else {\n      Iterable<FileStatus> locatedFiles = null;\n      try {\n        LocatedFileStatusFetcher locatedFileStatusFetcher = new LocatedFileStatusFetcher(\n            job.getConfiguration(), dirs, recursive, inputFilter, true);\n        locatedFiles = locatedFileStatusFetcher.getFileStatuses();\n      } catch (InterruptedException e) {\n        throw new IOException(\"Interrupted while getting file statuses\");\n      }\n      result = Lists.newArrayList(locatedFiles);\n    }\n    \n    sw.stop();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Time taken to get FileStatuses: \"\n          + sw.now(TimeUnit.MILLISECONDS));\n    }\n    LOG.info(\"Total input files to process : \" + result.size());\n    return result;\n  }\n","realPath":"hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.java","repoName":"hadoop","snippetEndLine":0,"snippetStartLine":0,"startLine":244,"status":"B"},{"authorDate":"2018-05-02 05:19:53","commitOrder":1,"curCode":"  protected FileStatus[] listStatus(JobConf job) throws IOException {\n    Path[] dirs = getInputPaths(job);\n    if (dirs.length == 0) {\n      throw new IOException(\"No input paths specified in job\");\n    }\n\n    \r\n    TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs, job);\n    \n    \r\n    boolean recursive = job.getBoolean(INPUT_DIR_RECURSIVE, false);\n\n    \r\n    \r\n    List<PathFilter> filters = new ArrayList<PathFilter>();\n    filters.add(hiddenFileFilter);\n    PathFilter jobFilter = getInputPathFilter(job);\n    if (jobFilter != null) {\n      filters.add(jobFilter);\n    }\n    PathFilter inputFilter = new MultiPathFilter(filters);\n\n    FileStatus[] result;\n    int numThreads = job\n        .getInt(\n            org.apache.hadoop.mapreduce.lib.input.FileInputFormat.LIST_STATUS_NUM_THREADS,\n            org.apache.hadoop.mapreduce.lib.input.FileInputFormat.DEFAULT_LIST_STATUS_NUM_THREADS);\n    \n    StopWatch sw = new StopWatch().start();\n    if (numThreads == 1) {\n      List<FileStatus> locatedFiles = singleThreadedListStatus(job, dirs, inputFilter, recursive); \n      result = locatedFiles.toArray(new FileStatus[locatedFiles.size()]);\n    } else {\n      Iterable<FileStatus> locatedFiles = null;\n      try {\n        \n        LocatedFileStatusFetcher locatedFileStatusFetcher = new LocatedFileStatusFetcher(\n            job, dirs, recursive, inputFilter, false);\n        locatedFiles = locatedFileStatusFetcher.getFileStatuses();\n      } catch (InterruptedException e) {\n        throw new IOException(\"Interrupted while getting file statuses\");\n      }\n      result = Iterables.toArray(locatedFiles, FileStatus.class);\n    }\n\n    sw.stop();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Time taken to get FileStatuses: \"\n          + sw.now(TimeUnit.MILLISECONDS));\n    }\n    LOG.info(\"Total input files to process : \" + result.length);\n    return result;\n  }\n","date":"2018-05-02 05:19:53","endLine":261,"groupId":"4721","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"listStatus","params":"(JobConfjob)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hadoop-10-0.7/blobInfo/CC_OUT/blobs/fe/43991a0e7b83831c22173b0564575f6f66733d.src","preCode":"  protected FileStatus[] listStatus(JobConf job) throws IOException {\n    Path[] dirs = getInputPaths(job);\n    if (dirs.length == 0) {\n      throw new IOException(\"No input paths specified in job\");\n    }\n\n    \r\n    TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs, job);\n    \n    \r\n    boolean recursive = job.getBoolean(INPUT_DIR_RECURSIVE, false);\n\n    \r\n    \r\n    List<PathFilter> filters = new ArrayList<PathFilter>();\n    filters.add(hiddenFileFilter);\n    PathFilter jobFilter = getInputPathFilter(job);\n    if (jobFilter != null) {\n      filters.add(jobFilter);\n    }\n    PathFilter inputFilter = new MultiPathFilter(filters);\n\n    FileStatus[] result;\n    int numThreads = job\n        .getInt(\n            org.apache.hadoop.mapreduce.lib.input.FileInputFormat.LIST_STATUS_NUM_THREADS,\n            org.apache.hadoop.mapreduce.lib.input.FileInputFormat.DEFAULT_LIST_STATUS_NUM_THREADS);\n    \n    StopWatch sw = new StopWatch().start();\n    if (numThreads == 1) {\n      List<FileStatus> locatedFiles = singleThreadedListStatus(job, dirs, inputFilter, recursive); \n      result = locatedFiles.toArray(new FileStatus[locatedFiles.size()]);\n    } else {\n      Iterable<FileStatus> locatedFiles = null;\n      try {\n        \n        LocatedFileStatusFetcher locatedFileStatusFetcher = new LocatedFileStatusFetcher(\n            job, dirs, recursive, inputFilter, false);\n        locatedFiles = locatedFileStatusFetcher.getFileStatuses();\n      } catch (InterruptedException e) {\n        throw new IOException(\"Interrupted while getting file statuses\");\n      }\n      result = Iterables.toArray(locatedFiles, FileStatus.class);\n    }\n\n    sw.stop();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Time taken to get FileStatuses: \"\n          + sw.now(TimeUnit.MILLISECONDS));\n    }\n    LOG.info(\"Total input files to process : \" + result.length);\n    return result;\n  }\n","realPath":"hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java","repoName":"hadoop","snippetEndLine":0,"snippetStartLine":0,"startLine":209,"status":"B"}],"commitId":"68c6ec719da8e79ada31c8f3a82124f90b9a71fd","commitMessage":"@@@MAPREDUCE-7086. Add config to allow FileInputFormat to ignore directories when recursive=false. Contributed by Sergey Shelukhin\n","date":"2018-05-02 05:19:53","modifiedFileCount":"4","status":"B","submitter":"Jason Lowe"},{"authorTime":"2019-10-02 01:10:29","codes":[{"authorDate":"2019-10-02 01:10:29","commitOrder":2,"curCode":"  protected List<FileStatus> listStatus(JobContext job\n                                        ) throws IOException {\n    Path[] dirs = getInputPaths(job);\n    if (dirs.length == 0) {\n      throw new IOException(\"No input paths specified in job\");\n    }\n    \n    \r\n    TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs, \n                                        job.getConfiguration());\n\n    \r\n    boolean recursive = getInputDirRecursive(job);\n\n    \r\n    \r\n    List<PathFilter> filters = new ArrayList<PathFilter>();\n    filters.add(hiddenFileFilter);\n    PathFilter jobFilter = getInputPathFilter(job);\n    if (jobFilter != null) {\n      filters.add(jobFilter);\n    }\n    PathFilter inputFilter = new MultiPathFilter(filters);\n    \n    List<FileStatus> result = null;\n\n    int numThreads = job.getConfiguration().getInt(LIST_STATUS_NUM_THREADS,\n        DEFAULT_LIST_STATUS_NUM_THREADS);\n    StopWatch sw = new StopWatch().start();\n    if (numThreads == 1) {\n      result = singleThreadedListStatus(job, dirs, inputFilter, recursive);\n    } else {\n      Iterable<FileStatus> locatedFiles = null;\n      try {\n        LocatedFileStatusFetcher locatedFileStatusFetcher = new LocatedFileStatusFetcher(\n            job.getConfiguration(), dirs, recursive, inputFilter, true);\n        locatedFiles = locatedFileStatusFetcher.getFileStatuses();\n      } catch (InterruptedException e) {\n        throw (IOException)\n            new InterruptedIOException(\n                \"Interrupted while getting file statuses\")\n                .initCause(e);\n      }\n      result = Lists.newArrayList(locatedFiles);\n    }\n    \n    sw.stop();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Time taken to get FileStatuses: \"\n          + sw.now(TimeUnit.MILLISECONDS));\n    }\n    LOG.info(\"Total input files to process : \" + result.size());\n    return result;\n  }\n","date":"2019-10-02 01:11:05","endLine":302,"groupId":"10703","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"listStatus","params":"(JobContextjob)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hadoop-10-0.7/blobInfo/CC_OUT/blobs/22/efe1471f91f42f32e5aa29c7cc0101bf825065.src","preCode":"  protected List<FileStatus> listStatus(JobContext job\n                                        ) throws IOException {\n    Path[] dirs = getInputPaths(job);\n    if (dirs.length == 0) {\n      throw new IOException(\"No input paths specified in job\");\n    }\n    \n    \r\n    TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs, \n                                        job.getConfiguration());\n\n    \r\n    boolean recursive = getInputDirRecursive(job);\n\n    \r\n    \r\n    List<PathFilter> filters = new ArrayList<PathFilter>();\n    filters.add(hiddenFileFilter);\n    PathFilter jobFilter = getInputPathFilter(job);\n    if (jobFilter != null) {\n      filters.add(jobFilter);\n    }\n    PathFilter inputFilter = new MultiPathFilter(filters);\n    \n    List<FileStatus> result = null;\n\n    int numThreads = job.getConfiguration().getInt(LIST_STATUS_NUM_THREADS,\n        DEFAULT_LIST_STATUS_NUM_THREADS);\n    StopWatch sw = new StopWatch().start();\n    if (numThreads == 1) {\n      result = singleThreadedListStatus(job, dirs, inputFilter, recursive);\n    } else {\n      Iterable<FileStatus> locatedFiles = null;\n      try {\n        LocatedFileStatusFetcher locatedFileStatusFetcher = new LocatedFileStatusFetcher(\n            job.getConfiguration(), dirs, recursive, inputFilter, true);\n        locatedFiles = locatedFileStatusFetcher.getFileStatuses();\n      } catch (InterruptedException e) {\n        throw new IOException(\"Interrupted while getting file statuses\");\n      }\n      result = Lists.newArrayList(locatedFiles);\n    }\n    \n    sw.stop();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Time taken to get FileStatuses: \"\n          + sw.now(TimeUnit.MILLISECONDS));\n    }\n    LOG.info(\"Total input files to process : \" + result.size());\n    return result;\n  }\n","realPath":"hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/FileInputFormat.java","repoName":"hadoop","snippetEndLine":0,"snippetStartLine":0,"startLine":249,"status":"M"},{"authorDate":"2019-10-02 01:10:29","commitOrder":2,"curCode":"  protected FileStatus[] listStatus(JobConf job) throws IOException {\n    Path[] dirs = getInputPaths(job);\n    if (dirs.length == 0) {\n      throw new IOException(\"No input paths specified in job\");\n    }\n\n    \r\n    TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs, job);\n    \n    \r\n    boolean recursive = job.getBoolean(INPUT_DIR_RECURSIVE, false);\n\n    \r\n    \r\n    List<PathFilter> filters = new ArrayList<PathFilter>();\n    filters.add(hiddenFileFilter);\n    PathFilter jobFilter = getInputPathFilter(job);\n    if (jobFilter != null) {\n      filters.add(jobFilter);\n    }\n    PathFilter inputFilter = new MultiPathFilter(filters);\n\n    FileStatus[] result;\n    int numThreads = job\n        .getInt(\n            org.apache.hadoop.mapreduce.lib.input.FileInputFormat.LIST_STATUS_NUM_THREADS,\n            org.apache.hadoop.mapreduce.lib.input.FileInputFormat.DEFAULT_LIST_STATUS_NUM_THREADS);\n    \n    StopWatch sw = new StopWatch().start();\n    if (numThreads == 1) {\n      List<FileStatus> locatedFiles = singleThreadedListStatus(job, dirs, inputFilter, recursive); \n      result = locatedFiles.toArray(new FileStatus[locatedFiles.size()]);\n    } else {\n      Iterable<FileStatus> locatedFiles = null;\n      try {\n        \n        LocatedFileStatusFetcher locatedFileStatusFetcher = new LocatedFileStatusFetcher(\n            job, dirs, recursive, inputFilter, false);\n        locatedFiles = locatedFileStatusFetcher.getFileStatuses();\n      } catch (InterruptedException e) {\n        throw  (IOException)\n            new InterruptedIOException(\"Interrupted while getting file statuses\")\n                .initCause(e);\n      }\n      result = Iterables.toArray(locatedFiles, FileStatus.class);\n    }\n\n    sw.stop();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Time taken to get FileStatuses: \"\n          + sw.now(TimeUnit.MILLISECONDS));\n    }\n    LOG.info(\"Total input files to process : \" + result.length);\n    return result;\n  }\n","date":"2019-10-02 01:11:05","endLine":268,"groupId":"10703","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"listStatus","params":"(JobConfjob)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hadoop-10-0.7/blobInfo/CC_OUT/blobs/b3/e2b4ade80fcf607b0f7a794ab65305394ff86c.src","preCode":"  protected FileStatus[] listStatus(JobConf job) throws IOException {\n    Path[] dirs = getInputPaths(job);\n    if (dirs.length == 0) {\n      throw new IOException(\"No input paths specified in job\");\n    }\n\n    \r\n    TokenCache.obtainTokensForNamenodes(job.getCredentials(), dirs, job);\n    \n    \r\n    boolean recursive = job.getBoolean(INPUT_DIR_RECURSIVE, false);\n\n    \r\n    \r\n    List<PathFilter> filters = new ArrayList<PathFilter>();\n    filters.add(hiddenFileFilter);\n    PathFilter jobFilter = getInputPathFilter(job);\n    if (jobFilter != null) {\n      filters.add(jobFilter);\n    }\n    PathFilter inputFilter = new MultiPathFilter(filters);\n\n    FileStatus[] result;\n    int numThreads = job\n        .getInt(\n            org.apache.hadoop.mapreduce.lib.input.FileInputFormat.LIST_STATUS_NUM_THREADS,\n            org.apache.hadoop.mapreduce.lib.input.FileInputFormat.DEFAULT_LIST_STATUS_NUM_THREADS);\n    \n    StopWatch sw = new StopWatch().start();\n    if (numThreads == 1) {\n      List<FileStatus> locatedFiles = singleThreadedListStatus(job, dirs, inputFilter, recursive); \n      result = locatedFiles.toArray(new FileStatus[locatedFiles.size()]);\n    } else {\n      Iterable<FileStatus> locatedFiles = null;\n      try {\n        \n        LocatedFileStatusFetcher locatedFileStatusFetcher = new LocatedFileStatusFetcher(\n            job, dirs, recursive, inputFilter, false);\n        locatedFiles = locatedFileStatusFetcher.getFileStatuses();\n      } catch (InterruptedException e) {\n        throw new IOException(\"Interrupted while getting file statuses\");\n      }\n      result = Iterables.toArray(locatedFiles, FileStatus.class);\n    }\n\n    sw.stop();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Time taken to get FileStatuses: \"\n          + sw.now(TimeUnit.MILLISECONDS));\n    }\n    LOG.info(\"Total input files to process : \" + result.length);\n    return result;\n  }\n","realPath":"hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileInputFormat.java","repoName":"hadoop","snippetEndLine":0,"snippetStartLine":0,"startLine":214,"status":"M"}],"commitId":"1921e94292f0820985a0cfbf8922a2a1a67fe921","commitMessage":"@@@HADOOP-16458. LocatedFileStatusFetcher.getFileStatuses failing intermittently with S3\n\nContributed by Steve Loughran.\n\nIncludes\n-S3A glob scans don't bother trying to resolve symlinks\n-stack traces don't get lost in getFileStatuses() when exceptions are wrapped\n-debug level logging of what is up in Globber\n-Contains HADOOP-13373. Add S3A implementation of FSMainOperationsBaseTest.\n-ITestRestrictedReadAccess tests incomplete read access to files.\n\nThis adds a builder API for constructing globbers which other stores can use\nso that they too can skip symlink resolution when not needed.\n\nChange-Id: I23bcdb2783d6bd77cf168fdc165b1b4b334d91c7\n","date":"2019-10-02 01:11:05","modifiedFileCount":"10","status":"M","submitter":"Steve Loughran"}]
