[{"authorTime":"2020-03-28 09:36:10","codes":[{"authorDate":"2020-03-28 09:36:10","commitOrder":1,"curCode":"    public synchronized void close() {\n        final LinkedList<RuntimeException> suppressed = executeAll(\n            () -> cache.flush(name),\n            () -> cache.close(name),\n            wrapped()::close\n        );\n        if (!suppressed.isEmpty()) {\n            throwSuppressed(\"Caught an exception while closing caching window store for store \" + name(),\n                            suppressed);\n        }\n    }\n","date":"2020-03-28 09:36:10","endLine":311,"groupId":"6513","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"close","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/d2/bd02ee373c98e428d528eb78ad8c0570410c4b.src","preCode":"    public synchronized void close() {\n        final LinkedList<RuntimeException> suppressed = executeAll(\n            () -> cache.flush(name),\n            () -> cache.close(name),\n            wrapped()::close\n        );\n        if (!suppressed.isEmpty()) {\n            throwSuppressed(\"Caught an exception while closing caching window store for store \" + name(),\n                            suppressed);\n        }\n    }\n","realPath":"streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":301,"status":"B"},{"authorDate":"2020-03-28 09:36:10","commitOrder":1,"curCode":"    public void close() {\n        lock.writeLock().lock();\n        try {\n            final LinkedList<RuntimeException> suppressed = executeAll(\n                () -> cache.flush(cacheName),\n                () -> cache.close(cacheName),\n                wrapped()::close\n            );\n            if (!suppressed.isEmpty()) {\n                throwSuppressed(\"Caught an exception while closing caching key value store for store \" + name(),\n                                suppressed);\n            }\n        } finally {\n            lock.writeLock().unlock();\n        }\n    }\n","date":"2020-03-28 09:36:10","endLine":307,"groupId":"6513","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"close","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/14/f4e54e4817ed621f3e026008c11e1c7f020232.src","preCode":"    public void close() {\n        lock.writeLock().lock();\n        try {\n            final LinkedList<RuntimeException> suppressed = executeAll(\n                () -> cache.flush(cacheName),\n                () -> cache.close(cacheName),\n                wrapped()::close\n            );\n            if (!suppressed.isEmpty()) {\n                throwSuppressed(\"Caught an exception while closing caching key value store for store \" + name(),\n                                suppressed);\n            }\n        } finally {\n            lock.writeLock().unlock();\n        }\n    }\n","realPath":"streams/src/main/java/org/apache/kafka/streams/state/internals/CachingKeyValueStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":292,"status":"B"}],"commitId":"c595470713be1fd2daf93816a5dbf0e245a707a0","commitMessage":"@@@KAFKA-9770: Close underlying state store also when flush throws (#8368)\n\nWhen a caching state store is closed it calls its flush() method.\nIf flush() throws an exception the underlying state store is not closed.\n\nThis commit ensures that state stores underlying a wrapped state stores\nare closed even when preceding operations in the close method throw.\n\nCo-authored-by: John Roesler <vvcephei@apache.org>\nReviewers: John Roesler <vvcephei@apache.org>.  Guozhang Wang <wangguoz@gmail.com>.  Matthias J. Sax <matthias@confluent.io>","date":"2020-03-28 09:36:10","modifiedFileCount":"13","status":"B","submitter":"Bruno Cadonna"},{"authorTime":"2020-05-30 01:48:03","codes":[{"authorDate":"2020-05-30 01:48:03","commitOrder":2,"curCode":"    public synchronized void close() {\n        final LinkedList<RuntimeException> suppressed = executeAll(\n            () -> context.cache().flush(name),\n            () -> context.cache().close(name),\n            wrapped()::close\n        );\n        if (!suppressed.isEmpty()) {\n            throwSuppressed(\"Caught an exception while closing caching window store for store \" + name(),\n                            suppressed);\n        }\n    }\n","date":"2020-05-30 01:48:03","endLine":309,"groupId":"9022","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"close","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/78/e16a923fc020d35eb8159fbec3160e742edb81.src","preCode":"    public synchronized void close() {\n        final LinkedList<RuntimeException> suppressed = executeAll(\n            () -> cache.flush(name),\n            () -> cache.close(name),\n            wrapped()::close\n        );\n        if (!suppressed.isEmpty()) {\n            throwSuppressed(\"Caught an exception while closing caching window store for store \" + name(),\n                            suppressed);\n        }\n    }\n","realPath":"streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":299,"status":"M"},{"authorDate":"2020-05-30 01:48:03","commitOrder":2,"curCode":"    public void close() {\n        lock.writeLock().lock();\n        try {\n            final LinkedList<RuntimeException> suppressed = executeAll(\n                () -> context.cache().flush(cacheName),\n                () -> context.cache().close(cacheName),\n                wrapped()::close\n            );\n            if (!suppressed.isEmpty()) {\n                throwSuppressed(\"Caught an exception while closing caching key value store for store \" + name(),\n                                suppressed);\n            }\n        } finally {\n            lock.writeLock().unlock();\n        }\n    }\n","date":"2020-05-30 01:48:03","endLine":305,"groupId":"9022","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"close","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/35/02fadfa29fb5ab713760eb5a01b2ccdc88e5ee.src","preCode":"    public void close() {\n        lock.writeLock().lock();\n        try {\n            final LinkedList<RuntimeException> suppressed = executeAll(\n                () -> cache.flush(cacheName),\n                () -> cache.close(cacheName),\n                wrapped()::close\n            );\n            if (!suppressed.isEmpty()) {\n                throwSuppressed(\"Caught an exception while closing caching key value store for store \" + name(),\n                                suppressed);\n            }\n        } finally {\n            lock.writeLock().unlock();\n        }\n    }\n","realPath":"streams/src/main/java/org/apache/kafka/streams/state/internals/CachingKeyValueStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":290,"status":"M"}],"commitId":"9d52deca247d9e16cf530d655891b2bbe474ffae","commitMessage":"@@@KAFKA-9501: convert between active and standby without closing stores (#8248)\n\nThis PR has gone through several significant transitions of its own.  but here's the latest:\n\n* TaskManager just collects the tasks to transition and refers to the active/standby task creator to handle closing & recycling the old task and creating the new one. If we ever hit an exception during the close.  we bail and close all the remaining tasks as dirty.\n\n* The task creators tell the task to \"close but recycle state\". If this is successful.  it tells the recycled processor context and state manager that they should transition to the new type.\n\n* During \"close and recycle\" the task just does a normal clean close.  but instead of closing the state manager it informs it to recycle itself: maintain all of its store information (most importantly the current store offsets) but unregister the changelogs from the changelog reader\n\n* The new task will (re-)register its changelogs during initialization.  but skip re-registering any stores. It will still read the checkpoint file.  but only use the written offsets if the store offsets are not already initialized from pre-transition\n\n* To ensure we don't end up with manual compaction disabled for standbys.  we have to call the state restore listener's onRestoreEnd for any active restoring stores that are switching to standbys\n\nReviewers: John Roesler <vvcephei@apache.org>.  Guozhang Wang <wangguoz@gmail.com>","date":"2020-05-30 01:48:03","modifiedFileCount":"41","status":"M","submitter":"A. Sophie Blee-Goldman"},{"authorTime":"2020-05-30 01:48:03","codes":[{"authorDate":"2020-08-12 11:21:41","commitOrder":3,"curCode":"    public synchronized void close() {\n        final LinkedList<RuntimeException> suppressed = executeAll(\n            () -> context.cache().flush(cacheName),\n            () -> context.cache().close(cacheName),\n            wrapped()::close\n        );\n        if (!suppressed.isEmpty()) {\n            throwSuppressed(\"Caught an exception while closing caching window store for store \" + name(),\n                            suppressed);\n        }\n    }\n","date":"2020-08-12 11:21:41","endLine":319,"groupId":"102831","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"close","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/7d/f0a4b7d0975217df33a24a017193e1d69acf98.src","preCode":"    public synchronized void close() {\n        final LinkedList<RuntimeException> suppressed = executeAll(\n            () -> context.cache().flush(name),\n            () -> context.cache().close(name),\n            wrapped()::close\n        );\n        if (!suppressed.isEmpty()) {\n            throwSuppressed(\"Caught an exception while closing caching window store for store \" + name(),\n                            suppressed);\n        }\n    }\n","realPath":"streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":309,"status":"M"},{"authorDate":"2020-05-30 01:48:03","commitOrder":3,"curCode":"    public void close() {\n        lock.writeLock().lock();\n        try {\n            final LinkedList<RuntimeException> suppressed = executeAll(\n                () -> context.cache().flush(cacheName),\n                () -> context.cache().close(cacheName),\n                wrapped()::close\n            );\n            if (!suppressed.isEmpty()) {\n                throwSuppressed(\"Caught an exception while closing caching key value store for store \" + name(),\n                                suppressed);\n            }\n        } finally {\n            lock.writeLock().unlock();\n        }\n    }\n","date":"2020-05-30 01:48:03","endLine":305,"groupId":"102831","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"close","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/35/02fadfa29fb5ab713760eb5a01b2ccdc88e5ee.src","preCode":"    public void close() {\n        lock.writeLock().lock();\n        try {\n            final LinkedList<RuntimeException> suppressed = executeAll(\n                () -> context.cache().flush(cacheName),\n                () -> context.cache().close(cacheName),\n                wrapped()::close\n            );\n            if (!suppressed.isEmpty()) {\n                throwSuppressed(\"Caught an exception while closing caching key value store for store \" + name(),\n                                suppressed);\n            }\n        } finally {\n            lock.writeLock().unlock();\n        }\n    }\n","realPath":"streams/src/main/java/org/apache/kafka/streams/state/internals/CachingKeyValueStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":290,"status":"N"}],"commitId":"7915d5e5f826a71c11e1c9183c84702410f7209a","commitMessage":"@@@KAFKA-9450: Decouple flushing state from commiting (#8964)\n\nIn Kafka Streams the source-of-truth of a state store is in its changelog.  therefore when committing a state store we only need to make sure its changelog records are all flushed and committed.  but we do not actually need to make sure that the materialized state have to be flushed and persisted since they can always be restored from changelog when necessary.\n\nOn the other hand.  flushing a state store too frequently may have side effects.  e.g. rocksDB flushing would gets the memtable into an L0 sstable.  leaving many small L0 files to be compacted later.  which introduces larger overhead.\n\nTherefore this PR decouples flushing from committing.  such that we do not always flush the state store upon committing.  but only when sufficient data has been written since last time flushed. The checkpoint file would then also be overwritten only along with flushing the state store indicating its current known snapshot. This is okay since: a) if EOS is not enabled.  then it is fine if the local persisted state is actually ahead of the checkpoint.  b) if EOS is enabled.  then we would never write a checkpoint file until close.\n\nHere's a more detailed change list of this PR:\n\n1. Do not always flush state stores when calling pre-commit; move stateMgr.flush into post-commit to couple together with checkpointing.\n\n2. In post-commit.  we checkpoint when: a) The state store's snapshot has progressed much further compared to the previous checkpoint.  b) When the task is being closed.  in which case we enforce checkpointing.\n\n3. There are some tricky obstacles that I'd have to work around in a bit hacky way: for cache / suppression buffer.  we still need to flush them in pre-commit to make sure all records sent via producers.  while the underlying state store should not be flushed. I've decided to introduce a new API in CachingStateStore to be triggered in pre-commit.\n\nI've also made some minor changes piggy-backed in this PR:\n\n4. Do not delete checkpoint file upon loading it.  and as a result simplify the checkpointNeeded logic.  initializing the snapshotLastFlush to the loaded offsets.\n\n5. In closing.  also follow the commit -> suspend -> close ordering as in revocation / assignment.\n\n6. If enforceCheckpoint == true during RUNNING.  still calls maybeCheckpoint even with EOS since that is the case for suspending / closing.\n\nReviewers: John Roesler <john@confluent.io>.  A. Sophie Blee-Goldman <sophie@confluent.io>.  Matthias J. Sax <matthias@confluent.io>","date":"2020-08-12 11:21:41","modifiedFileCount":"24","status":"M","submitter":"Guozhang Wang"}]
