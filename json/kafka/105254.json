[{"authorTime":"2016-03-04 00:54:37","codes":[{"authorDate":"2016-11-30 07:30:40","commitOrder":4,"curCode":"    KafkaBasedLog<String, byte[]> setupAndCreateKafkaBasedLog(String topic, WorkerConfig config) {\n        Map<String, Object> producerProps = new HashMap<>();\n        producerProps.putAll(config.originals());\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>();\n        consumerProps.putAll(config.originals());\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        return createKafkaBasedLog(topic, producerProps, consumerProps, new ConsumeCallback());\n    }\n","date":"2016-11-30 07:31:14","endLine":420,"groupId":"14458","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"setupAndCreateKafkaBasedLog","params":"(Stringtopic@WorkerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/1a/46693e8874a27d01d961039d3cd48495c314a6.src","preCode":"    KafkaBasedLog<String, byte[]> setupAndCreateKafkaBasedLog(String topic, WorkerConfig config) {\n        Map<String, Object> producerProps = new HashMap<>();\n        producerProps.putAll(config.originals());\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>();\n        consumerProps.putAll(config.originals());\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        return createKafkaBasedLog(topic, producerProps, consumerProps, new ConsumeCallback());\n    }\n","realPath":"connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":407,"status":"B"},{"authorDate":"2016-03-04 00:54:37","commitOrder":4,"curCode":"    public void configure(WorkerConfig config) {\n        String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);\n        if (topic.equals(\"\"))\n            throw new ConfigException(\"Offset storage topic must be specified\");\n\n        data = new HashMap<>();\n\n        Map<String, Object> producerProps = new HashMap<>();\n        producerProps.putAll(config.originals());\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>();\n        consumerProps.putAll(config.originals());\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback);\n    }\n","date":"2016-03-04 00:54:37","endLine":81,"groupId":"14458","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"configure","params":"(WorkerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/e8/984fb894349cc18e0e18c46248c7b0dd5bf168.src","preCode":"    public void configure(WorkerConfig config) {\n        String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);\n        if (topic.equals(\"\"))\n            throw new ConfigException(\"Offset storage topic must be specified\");\n\n        data = new HashMap<>();\n\n        Map<String, Object> producerProps = new HashMap<>();\n        producerProps.putAll(config.originals());\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>();\n        consumerProps.putAll(config.originals());\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback);\n    }\n","realPath":"connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaOffsetBackingStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":62,"status":"NB"}],"commitId":"d98ca230a14b0aedae752fa97f5d55b3a0c49b9c","commitMessage":"@@@KAFKA-4397: Refactor Connect backing stores for thread safety\n\nAuthor: Konstantine Karantasis <konstantine@confluent.io>\n\nReviewers: Shikhar Bhushan <shikhar@confluent.io>.  Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #2123 from kkonstantine/KAFKA-4397-Refactor-connect-backing-stores-for-thread-safety\n","date":"2016-11-30 07:31:14","modifiedFileCount":"6","status":"M","submitter":"Konstantine Karantasis"},{"authorTime":"2017-05-19 07:02:29","codes":[{"authorDate":"2017-05-19 07:02:29","commitOrder":5,"curCode":"    KafkaBasedLog<String, byte[]> setupAndCreateKafkaBasedLog(String topic, final WorkerConfig config) {\n        Map<String, Object> producerProps = new HashMap<>();\n        producerProps.putAll(config.originals());\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>();\n        consumerProps.putAll(config.originals());\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(config.originals());\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic).\n                compacted().\n                partitions(1).\n                replicationFactor(config.getShort(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG)).\n                build();\n\n        return createKafkaBasedLog(topic, producerProps, consumerProps, new ConsumeCallback(), topicDescription, adminProps);\n    }\n","date":"2017-05-19 07:02:29","endLine":428,"groupId":"14458","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"setupAndCreateKafkaBasedLog","params":"(Stringtopic@finalWorkerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/dd/69c300b69b397f0dfaed063774973bfdfad840.src","preCode":"    KafkaBasedLog<String, byte[]> setupAndCreateKafkaBasedLog(String topic, WorkerConfig config) {\n        Map<String, Object> producerProps = new HashMap<>();\n        producerProps.putAll(config.originals());\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>();\n        consumerProps.putAll(config.originals());\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        return createKafkaBasedLog(topic, producerProps, consumerProps, new ConsumeCallback());\n    }\n","realPath":"connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":408,"status":"M"},{"authorDate":"2017-05-19 07:02:29","commitOrder":5,"curCode":"    public void configure(final WorkerConfig config) {\n        String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);\n        if (topic.equals(\"\"))\n            throw new ConfigException(\"Offset storage topic must be specified\");\n\n        data = new HashMap<>();\n\n        Map<String, Object> producerProps = new HashMap<>();\n        producerProps.putAll(config.originals());\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>();\n        consumerProps.putAll(config.originals());\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(config.originals());\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic).\n                compacted().\n                partitions(config.getInt(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG)).\n                replicationFactor(config.getShort(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG)).\n                build();\n\n        offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback, topicDescription, adminProps);\n    }\n","date":"2017-05-19 07:02:29","endLine":89,"groupId":"14458","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"configure","params":"(finalWorkerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/cb/4f0897887e2e2eff52924d46c6c9a84315223e.src","preCode":"    public void configure(WorkerConfig config) {\n        String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);\n        if (topic.equals(\"\"))\n            throw new ConfigException(\"Offset storage topic must be specified\");\n\n        data = new HashMap<>();\n\n        Map<String, Object> producerProps = new HashMap<>();\n        producerProps.putAll(config.originals());\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>();\n        consumerProps.putAll(config.originals());\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback);\n    }\n","realPath":"connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaOffsetBackingStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":63,"status":"M"}],"commitId":"56623efd73ec77e68cf35021d18d630b27062e82","commitMessage":"@@@KAFKA-4667: Connect uses AdminClient to create internal topics when needed (KIP-154)\n\nThe backing store for offsets.  status.  and configs now attempts to use the new AdminClient to look up the internal topics and create them if they don?t yet exist. If the necessary APIs are not available in the connected broker.  the stores fall back to the old behavior of relying upon auto-created topics. Kafka Connect requires a minimum of Apache Kafka 0.10.0.1-cp1.  and the AdminClient can work with all versions since 0.10.0.0.\n\nAll three of Connect?s internal topics are created as compacted topics.  and new distributed worker configuration properties control the replication factor for all three topics and the number of partitions for the offsets and status topics; the config topic requires a single partition and does not allow it to be set via configuration. All of these new configuration properties have sensible defaults.  meaning users can upgrade without having to change any of the existing configurations. In most situations.  existing Connect deployments will have already created the storage topics prior to upgrading.\n\nThe replication factor defaults to 3.  so anyone running Kafka clusters with fewer nodes than 3 will receive an error unless they explicitly set the replication factor for the three internal topics. This is actually desired behavior.  since it signals the users that they should be aware they are not using sufficient replication for production use.\n\nThe integration tests use a cluster with a single broker.  so they were changed to explicitly specify a replication factor of 1 and a single partition.\n\nThe `KafkaAdminClientTest` was refactored to extract a utility for setting up a `KafkaAdminClient` with a `MockClient` for unit tests.\n\nAuthor: Randall Hauch <rhauch@gmail.com>\n\nReviewers: Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #2984 from rhauch/kafka-4667\n","date":"2017-05-19 07:02:29","modifiedFileCount":"11","status":"M","submitter":"Randall Hauch"},{"authorTime":"2018-02-07 03:33:49","codes":[{"authorDate":"2018-02-07 03:33:49","commitOrder":6,"curCode":"    KafkaBasedLog<String, byte[]> setupAndCreateKafkaBasedLog(String topic, final WorkerConfig config) {\n        Map<String, Object> producerProps = new HashMap<>(config.originals());\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>(config.originals());\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(config.originals());\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic).\n                compacted().\n                partitions(1).\n                replicationFactor(config.getShort(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG)).\n                build();\n\n        return createKafkaBasedLog(topic, producerProps, consumerProps, new ConsumeCallback(), topicDescription, adminProps);\n    }\n","date":"2018-02-07 03:33:49","endLine":426,"groupId":"14458","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"setupAndCreateKafkaBasedLog","params":"(Stringtopic@finalWorkerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/5c/2c72c8d1b73d33006a3768c5cf95b3e41de059.src","preCode":"    KafkaBasedLog<String, byte[]> setupAndCreateKafkaBasedLog(String topic, final WorkerConfig config) {\n        Map<String, Object> producerProps = new HashMap<>();\n        producerProps.putAll(config.originals());\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>();\n        consumerProps.putAll(config.originals());\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(config.originals());\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic).\n                compacted().\n                partitions(1).\n                replicationFactor(config.getShort(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG)).\n                build();\n\n        return createKafkaBasedLog(topic, producerProps, consumerProps, new ConsumeCallback(), topicDescription, adminProps);\n    }\n","realPath":"connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":408,"status":"M"},{"authorDate":"2018-02-07 03:33:49","commitOrder":6,"curCode":"    public void configure(final WorkerConfig config) {\n        String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);\n        if (topic.equals(\"\"))\n            throw new ConfigException(\"Offset storage topic must be specified\");\n\n        data = new HashMap<>();\n\n        Map<String, Object> producerProps = new HashMap<>(config.originals());\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>(config.originals());\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(config.originals());\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic).\n                compacted().\n                partitions(config.getInt(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG)).\n                replicationFactor(config.getShort(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG)).\n                build();\n\n        offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback, topicDescription, adminProps);\n    }\n","date":"2018-02-07 03:33:49","endLine":87,"groupId":"14458","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"configure","params":"(finalWorkerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/2e/4e5232dfd91d4d238140eafb0bc165e6c39ca8.src","preCode":"    public void configure(final WorkerConfig config) {\n        String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);\n        if (topic.equals(\"\"))\n            throw new ConfigException(\"Offset storage topic must be specified\");\n\n        data = new HashMap<>();\n\n        Map<String, Object> producerProps = new HashMap<>();\n        producerProps.putAll(config.originals());\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>();\n        consumerProps.putAll(config.originals());\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(config.originals());\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic).\n                compacted().\n                partitions(config.getInt(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG)).\n                replicationFactor(config.getShort(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG)).\n                build();\n\n        offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback, topicDescription, adminProps);\n    }\n","realPath":"connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaOffsetBackingStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":63,"status":"M"}],"commitId":"c32860b22427ca32a9e83e4f741b85062bb22140","commitMessage":"@@@MINOR:  exchange redundant Collections.addAll with parameterized constructor (#4521)\n\n* Exchange manual copy to collection with Collections.addAll call\n* Exchange redundant Collections.addAll with parameterized constructor call\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>","date":"2018-02-07 03:33:49","modifiedFileCount":"10","status":"M","submitter":"Wladimir Schmidt"},{"authorTime":"2018-02-14 08:39:21","codes":[{"authorDate":"2018-02-14 08:39:21","commitOrder":7,"curCode":"    KafkaBasedLog<String, byte[]> setupAndCreateKafkaBasedLog(String topic, final WorkerConfig config) {\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic).\n                compacted().\n                partitions(1).\n                replicationFactor(config.getShort(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG)).\n                build();\n\n        return createKafkaBasedLog(topic, producerProps, consumerProps, new ConsumeCallback(), topicDescription, adminProps);\n    }\n","date":"2018-02-14 08:39:21","endLine":426,"groupId":"18389","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"setupAndCreateKafkaBasedLog","params":"(Stringtopic@finalWorkerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/b3/4e48390e168ad6d1942008536dc56ed2896d26.src","preCode":"    KafkaBasedLog<String, byte[]> setupAndCreateKafkaBasedLog(String topic, final WorkerConfig config) {\n        Map<String, Object> producerProps = new HashMap<>(config.originals());\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>(config.originals());\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(config.originals());\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic).\n                compacted().\n                partitions(1).\n                replicationFactor(config.getShort(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG)).\n                build();\n\n        return createKafkaBasedLog(topic, producerProps, consumerProps, new ConsumeCallback(), topicDescription, adminProps);\n    }\n","realPath":"connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":408,"status":"M"},{"authorDate":"2018-02-14 08:39:21","commitOrder":7,"curCode":"    public void configure(final WorkerConfig config) {\n        String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);\n        if (topic == null || topic.trim().length() == 0)\n            throw new ConfigException(\"Offset storage topic must be specified\");\n\n        data = new HashMap<>();\n\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic).\n                compacted().\n                partitions(config.getInt(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG)).\n                replicationFactor(config.getShort(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG)).\n                build();\n\n        offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback, topicDescription, adminProps);\n    }\n","date":"2018-02-14 08:39:21","endLine":88,"groupId":"18389","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"configure","params":"(finalWorkerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/f2/9f3c23d03ce8cf7b713dc1ca6c3abccf5005ab.src","preCode":"    public void configure(final WorkerConfig config) {\n        String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);\n        if (topic.equals(\"\"))\n            throw new ConfigException(\"Offset storage topic must be specified\");\n\n        data = new HashMap<>();\n\n        Map<String, Object> producerProps = new HashMap<>(config.originals());\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>(config.originals());\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(config.originals());\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic).\n                compacted().\n                partitions(config.getInt(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG)).\n                replicationFactor(config.getShort(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG)).\n                build();\n\n        offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback, topicDescription, adminProps);\n    }\n","realPath":"connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaOffsetBackingStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":63,"status":"M"}],"commitId":"2693e9be7412ec03173c8942e9ccfcc24cfbbce1","commitMessage":"@@@MINOR: Misc improvements on runtime / storage / metrics / config parts (#4525)\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>","date":"2018-02-14 08:39:21","modifiedFileCount":"7","status":"M","submitter":"Benedict Jin"},{"authorTime":"2018-08-02 02:04:17","codes":[{"authorDate":"2018-08-02 02:04:17","commitOrder":8,"curCode":"    KafkaBasedLog<String, byte[]> setupAndCreateKafkaBasedLog(String topic, final WorkerConfig config) {\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic).\n                compacted().\n                partitions(1).\n                replicationFactor(config.getShort(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG)).\n                build();\n\n        return createKafkaBasedLog(topic, producerProps, consumerProps, new ConsumeCallback(), topicDescription, adminProps);\n    }\n","date":"2018-08-02 02:04:17","endLine":431,"groupId":"18389","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"setupAndCreateKafkaBasedLog","params":"(Stringtopic@finalWorkerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/e7/ee632638d2ef84e72e356af29e1340d1570b3d.src","preCode":"    KafkaBasedLog<String, byte[]> setupAndCreateKafkaBasedLog(String topic, final WorkerConfig config) {\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic).\n                compacted().\n                partitions(1).\n                replicationFactor(config.getShort(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG)).\n                build();\n\n        return createKafkaBasedLog(topic, producerProps, consumerProps, new ConsumeCallback(), topicDescription, adminProps);\n    }\n","realPath":"connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":413,"status":"M"},{"authorDate":"2018-08-02 02:04:17","commitOrder":8,"curCode":"    public void configure(final WorkerConfig config) {\n        String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);\n        if (topic == null || topic.trim().length() == 0)\n            throw new ConfigException(\"Offset storage topic must be specified\");\n\n        data = new HashMap<>();\n\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic).\n                compacted().\n                partitions(config.getInt(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG)).\n                replicationFactor(config.getShort(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG)).\n                build();\n\n        offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback, topicDescription, adminProps);\n    }\n","date":"2018-08-02 02:04:17","endLine":88,"groupId":"18389","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"configure","params":"(finalWorkerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/19/5c498edb76fc766f6ed43a85870fa7c068d09d.src","preCode":"    public void configure(final WorkerConfig config) {\n        String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);\n        if (topic == null || topic.trim().length() == 0)\n            throw new ConfigException(\"Offset storage topic must be specified\");\n\n        data = new HashMap<>();\n\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic).\n                compacted().\n                partitions(config.getInt(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG)).\n                replicationFactor(config.getShort(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG)).\n                build();\n\n        offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback, topicDescription, adminProps);\n    }\n","realPath":"connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaOffsetBackingStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":63,"status":"M"}],"commitId":"c3e7c0bcb258061568294c0d96b62fea94ef8ee7","commitMessage":"@@@MINOR: Producers should set delivery timeout instead of retries  (#5425)\n\nUse delivery timeout instead of retries when possible and remove various TODOs associated with completion of KIP-91.\n\nReviewers: Ismael Juma <ismael@juma.me.uk>.  Guozhang Wang <wangguoz@gmail.com>","date":"2018-08-02 02:04:17","modifiedFileCount":"16","status":"M","submitter":"Jason Gustafson"},{"authorTime":"2020-05-23 22:00:32","codes":[{"authorDate":"2020-05-23 22:00:32","commitOrder":9,"curCode":"    KafkaBasedLog<String, byte[]> setupAndCreateKafkaBasedLog(String topic, final WorkerConfig config) {\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n\n        Map<String, Object> topicSettings = config instanceof DistributedConfig\n                                            ? ((DistributedConfig) config).configStorageTopicSettings()\n                                            : Collections.emptyMap();\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic)\n                .config(topicSettings) \r\n                .compacted()\n                .partitions(1)\n                .replicationFactor(config.getShort(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG))\n                .build();\n\n        return createKafkaBasedLog(topic, producerProps, consumerProps, new ConsumeCallback(), topicDescription, adminProps);\n    }\n","date":"2020-05-23 22:00:32","endLine":469,"groupId":"18389","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"setupAndCreateKafkaBasedLog","params":"(Stringtopic@finalWorkerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/3c/f2288450bb414b2c89e766b3dbed8b77b16f63.src","preCode":"    KafkaBasedLog<String, byte[]> setupAndCreateKafkaBasedLog(String topic, final WorkerConfig config) {\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic).\n                compacted().\n                partitions(1).\n                replicationFactor(config.getShort(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG)).\n                build();\n\n        return createKafkaBasedLog(topic, producerProps, consumerProps, new ConsumeCallback(), topicDescription, adminProps);\n    }\n","realPath":"connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":446,"status":"M"},{"authorDate":"2020-05-23 22:00:32","commitOrder":9,"curCode":"    public void configure(final WorkerConfig config) {\n        String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);\n        if (topic == null || topic.trim().length() == 0)\n            throw new ConfigException(\"Offset storage topic must be specified\");\n\n        data = new HashMap<>();\n\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n\n        Map<String, Object> topicSettings = config instanceof DistributedConfig\n                                            ? ((DistributedConfig) config).offsetStorageTopicSettings()\n                                            : Collections.emptyMap();\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic)\n                .config(topicSettings) \r\n                .compacted()\n                .partitions(config.getInt(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG))\n                .replicationFactor(config.getShort(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG))\n                .build();\n\n        offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback, topicDescription, adminProps);\n    }\n","date":"2020-05-23 22:00:32","endLine":94,"groupId":"18389","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"configure","params":"(finalWorkerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/61/00d072c4cb69927d16bf429267a2516c3c0e22.src","preCode":"    public void configure(final WorkerConfig config) {\n        String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);\n        if (topic == null || topic.trim().length() == 0)\n            throw new ConfigException(\"Offset storage topic must be specified\");\n\n        data = new HashMap<>();\n\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic).\n                compacted().\n                partitions(config.getInt(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG)).\n                replicationFactor(config.getShort(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG)).\n                build();\n\n        offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback, topicDescription, adminProps);\n    }\n","realPath":"connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaOffsetBackingStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":64,"status":"M"}],"commitId":"981ef5166d2c95cacb6cdc1d52ed0c866b473868","commitMessage":"@@@KAFKA-9931: Implement KIP-605 to expand support for Connect worker internal topic configurations (#8654)\n\nAdded support for -1 replication factor and partitions for distributed worker internal topics by expanding the allowed values for the internal topics? replication factor and partitions from positive values to also include -1 to signify that the broker defaults should be used.\n\nThe Kafka storage classes were already constructing a `NewTopic` object (always with a replication factor and partitions) and sending it to Kafka when required. This change will avoid setting the replication factor and/or number of partitions on this `NewTopic` if the worker configuration uses -1 for the corresponding configuration value.\n\nAlso added support for extra settings for internal topics on distributed config.  status.  and offset internal topics.\n\nQuite a few new tests were added to verify that the `TopicAdmin` utility class is correctly using the AdminClient.  and that the `DistributedConfig` validators for these configurations are correct. Also added integration tests for internal topic creation.  covering preexisting functionality plus the new functionality.\n\nAuthor: Randall Hauch <rhauch@gmail.com>\nReviewer: Konstantine Karantasis <konstantine@confluent.io>","date":"2020-05-23 22:00:32","modifiedFileCount":"12","status":"M","submitter":"Randall Hauch"},{"authorTime":"2020-05-28 09:18:36","codes":[{"authorDate":"2020-05-28 09:18:36","commitOrder":10,"curCode":"    KafkaBasedLog<String, byte[]> setupAndCreateKafkaBasedLog(String topic, final WorkerConfig config) {\n        String clusterId = ConnectUtils.lookupKafkaClusterId(config);\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);\n        ConnectUtils.addMetricsContextProperties(producerProps, config, clusterId);\n\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        ConnectUtils.addMetricsContextProperties(consumerProps, config, clusterId);\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n        ConnectUtils.addMetricsContextProperties(adminProps, config, clusterId);\n        Map<String, Object> topicSettings = config instanceof DistributedConfig\n                                            ? ((DistributedConfig) config).configStorageTopicSettings()\n                                            : Collections.emptyMap();\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic)\n                .config(topicSettings) \r\n                .compacted()\n                .partitions(1)\n                .replicationFactor(config.getShort(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG))\n                .build();\n\n        return createKafkaBasedLog(topic, producerProps, consumerProps, new ConsumeCallback(), topicDescription, adminProps);\n    }\n","date":"2020-05-28 09:18:36","endLine":474,"groupId":"21000","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"setupAndCreateKafkaBasedLog","params":"(Stringtopic@finalWorkerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/60/c2665c0457ed42397822d930e34435f97ab861.src","preCode":"    KafkaBasedLog<String, byte[]> setupAndCreateKafkaBasedLog(String topic, final WorkerConfig config) {\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n\n        Map<String, Object> topicSettings = config instanceof DistributedConfig\n                                            ? ((DistributedConfig) config).configStorageTopicSettings()\n                                            : Collections.emptyMap();\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic)\n                .config(topicSettings) \r\n                .compacted()\n                .partitions(1)\n                .replicationFactor(config.getShort(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG))\n                .build();\n\n        return createKafkaBasedLog(topic, producerProps, consumerProps, new ConsumeCallback(), topicDescription, adminProps);\n    }\n","realPath":"connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":447,"status":"M"},{"authorDate":"2020-05-28 09:18:36","commitOrder":10,"curCode":"    public void configure(final WorkerConfig config) {\n        String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);\n        if (topic == null || topic.trim().length() == 0)\n            throw new ConfigException(\"Offset storage topic must be specified\");\n\n        String clusterId = ConnectUtils.lookupKafkaClusterId(config);\n        data = new HashMap<>();\n\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);\n        ConnectUtils.addMetricsContextProperties(producerProps, config, clusterId);\n\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        ConnectUtils.addMetricsContextProperties(consumerProps, config, clusterId);\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n        ConnectUtils.addMetricsContextProperties(adminProps, config, clusterId);\n        Map<String, Object> topicSettings = config instanceof DistributedConfig\n                                            ? ((DistributedConfig) config).offsetStorageTopicSettings()\n                                            : Collections.emptyMap();\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic)\n                .config(topicSettings) \r\n                .compacted()\n                .partitions(config.getInt(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG))\n                .replicationFactor(config.getShort(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG))\n                .build();\n\n        offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback, topicDescription, adminProps);\n    }\n","date":"2020-05-28 09:18:36","endLine":98,"groupId":"21000","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"configure","params":"(finalWorkerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/1d/26d6504210c86bd01cf34d2c19bd817f07abe5.src","preCode":"    public void configure(final WorkerConfig config) {\n        String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);\n        if (topic == null || topic.trim().length() == 0)\n            throw new ConfigException(\"Offset storage topic must be specified\");\n\n        data = new HashMap<>();\n\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);\n\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n\n        Map<String, Object> topicSettings = config instanceof DistributedConfig\n                                            ? ((DistributedConfig) config).offsetStorageTopicSettings()\n                                            : Collections.emptyMap();\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic)\n                .config(topicSettings) \r\n                .compacted()\n                .partitions(config.getInt(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG))\n                .replicationFactor(config.getShort(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG))\n                .build();\n\n        offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback, topicDescription, adminProps);\n    }\n","realPath":"connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaOffsetBackingStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":65,"status":"M"}],"commitId":"9c833f665f349e5c292228f75188f5521282835d","commitMessage":"@@@KAFKA-9960: implement KIP-606 to add metadata context to MetricsReporter (#8691)\n\nImplemented KIP-606 to add metadata context to MetricsReporter.\n\nAuthor: Xiaodong Du <xdu@confluent.io>\nReviewers: David Arthur <mumrah@gmail.com>.  Randall Hauch <rhauch@gmail.com>.  Xavier L?aut? <xavier@confluent.io>.  Ryan Pridgeon <ryan.n.pridgeon@gmail.com>","date":"2020-05-28 09:18:36","modifiedFileCount":"35","status":"M","submitter":"xiaodongdu"},{"authorTime":"2021-02-10 01:09:41","codes":[{"authorDate":"2021-02-10 01:09:41","commitOrder":11,"curCode":"    KafkaBasedLog<String, byte[]> setupAndCreateKafkaBasedLog(String topic, final WorkerConfig config) {\n        String clusterId = ConnectUtils.lookupKafkaClusterId(config);\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);\n        ConnectUtils.addMetricsContextProperties(producerProps, config, clusterId);\n\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        ConnectUtils.addMetricsContextProperties(consumerProps, config, clusterId);\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n        ConnectUtils.addMetricsContextProperties(adminProps, config, clusterId);\n        Supplier<TopicAdmin> adminSupplier = topicAdminSupplier != null ? topicAdminSupplier : () -> new TopicAdmin(adminProps);\n        Map<String, Object> topicSettings = config instanceof DistributedConfig\n                                            ? ((DistributedConfig) config).configStorageTopicSettings()\n                                            : Collections.emptyMap();\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic)\n                .config(topicSettings) \r\n                .compacted()\n                .partitions(1)\n                .replicationFactor(config.getShort(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG))\n                .build();\n\n        return createKafkaBasedLog(topic, producerProps, consumerProps, new ConsumeCallback(), topicDescription, adminSupplier);\n    }\n","date":"2021-02-10 01:09:41","endLine":494,"groupId":"21000","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"setupAndCreateKafkaBasedLog","params":"(Stringtopic@finalWorkerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/d4/e6358e2ea99f4c311528e151b3320b0c7254cf.src","preCode":"    KafkaBasedLog<String, byte[]> setupAndCreateKafkaBasedLog(String topic, final WorkerConfig config) {\n        String clusterId = ConnectUtils.lookupKafkaClusterId(config);\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);\n        ConnectUtils.addMetricsContextProperties(producerProps, config, clusterId);\n\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        ConnectUtils.addMetricsContextProperties(consumerProps, config, clusterId);\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n        ConnectUtils.addMetricsContextProperties(adminProps, config, clusterId);\n        Map<String, Object> topicSettings = config instanceof DistributedConfig\n                                            ? ((DistributedConfig) config).configStorageTopicSettings()\n                                            : Collections.emptyMap();\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic)\n                .config(topicSettings) \r\n                .compacted()\n                .partitions(1)\n                .replicationFactor(config.getShort(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG))\n                .build();\n\n        return createKafkaBasedLog(topic, producerProps, consumerProps, new ConsumeCallback(), topicDescription, adminProps);\n    }\n","realPath":"connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":466,"status":"M"},{"authorDate":"2021-02-10 01:09:41","commitOrder":11,"curCode":"    public void configure(final WorkerConfig config) {\n        String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);\n        if (topic == null || topic.trim().length() == 0)\n            throw new ConfigException(\"Offset storage topic must be specified\");\n\n        String clusterId = ConnectUtils.lookupKafkaClusterId(config);\n        data = new HashMap<>();\n\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);\n        ConnectUtils.addMetricsContextProperties(producerProps, config, clusterId);\n\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        ConnectUtils.addMetricsContextProperties(consumerProps, config, clusterId);\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n        ConnectUtils.addMetricsContextProperties(adminProps, config, clusterId);\n        Supplier<TopicAdmin> adminSupplier = topicAdminSupplier != null ? topicAdminSupplier : () -> new TopicAdmin(adminProps);\n        Map<String, Object> topicSettings = config instanceof DistributedConfig\n                                            ? ((DistributedConfig) config).offsetStorageTopicSettings()\n                                            : Collections.emptyMap();\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic)\n                .config(topicSettings) \r\n                .compacted()\n                .partitions(config.getInt(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG))\n                .replicationFactor(config.getShort(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG))\n                .build();\n\n        offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback, topicDescription, adminSupplier);\n    }\n","date":"2021-02-10 01:09:41","endLine":113,"groupId":"21000","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"configure","params":"(finalWorkerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/26/b47f996b18a1d55e06d26b66807dbefc4c97aa.src","preCode":"    public void configure(final WorkerConfig config) {\n        String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);\n        if (topic == null || topic.trim().length() == 0)\n            throw new ConfigException(\"Offset storage topic must be specified\");\n\n        String clusterId = ConnectUtils.lookupKafkaClusterId(config);\n        data = new HashMap<>();\n\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);\n        ConnectUtils.addMetricsContextProperties(producerProps, config, clusterId);\n\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        ConnectUtils.addMetricsContextProperties(consumerProps, config, clusterId);\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n        ConnectUtils.addMetricsContextProperties(adminProps, config, clusterId);\n        Map<String, Object> topicSettings = config instanceof DistributedConfig\n                                            ? ((DistributedConfig) config).offsetStorageTopicSettings()\n                                            : Collections.emptyMap();\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic)\n                .config(topicSettings) \r\n                .compacted()\n                .partitions(config.getInt(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG))\n                .replicationFactor(config.getShort(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG))\n                .build();\n\n        offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback, topicDescription, adminProps);\n    }\n","realPath":"connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaOffsetBackingStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":79,"status":"M"}],"commitId":"982ea2f6a471c217c7400a725c9504d9d8348d02","commitMessage":"@@@KAFKA-10021: Changed Kafka backing stores to use shared admin client to get end offsets and create topics (#9780)\n\nThe existing `Kafka*BackingStore` classes used by Connect all use `KafkaBasedLog`.  which needs to frequently get the end offsets for the internal topic to know whether they are caught up. `KafkaBasedLog` uses its consumer to get the end offsets and to consume the records from the topic.\n\nHowever.  the Connect internal topics are often written very infrequently. This means that when the `KafkaBasedLog` used in the `Kafka*BackingStore` classes is already caught up and its last consumer poll is waiting for new records to appear.  the call to the consumer to fetch end offsets will block until the consumer returns after a new record is written (unlikely) or the consumer?s `fetch.max.wait.ms` setting (defaults to 500ms) ends and the consumer returns no more records. IOW.  the call to `KafkaBasedLog.readToEnd()` may block for some period of time even though it?s already caught up to the end.\n\nInstead.  we want the `KafkaBasedLog.readToEnd()` to always return quickly when the log is already caught up. The best way to do this is to have the `KafkaBackingStore` use the admin client (rather than the consumer) to fetch end offsets for the internal topic. The consumer and the admin API both use the same `ListOffset` broker API.  so the functionality is ultimately the same but we don't have to block for any ongoing consumer activity.\n\nEach Connect distributed runtime includes three instances of the `Kafka*BackingStore` classes.  which means we have three instances of `KafkaBasedLog`. We don't want three instances of the admin client.  and should have all three instances of the `KafkaBasedLog` share a single admin client instance. In fact.  each `Kafka*BackingStore` instance currently creates.  uses and closes an admin client instance when it checks and initializes that store's internal topic. If we change `Kafka*BackingStores` to share one admin client instance.  we can change that initialization logic to also reuse the supplied admin client instance.\n\nThe final challenge is that `KafkaBasedLog` has been used by projects outside of Apache Kafka. While `KafkaBasedLog` is definitely not in the public API for Connect.  we can make these changes in ways that are backward compatible: create new constructors and deprecate the old constructors. Connect can be changed to only use the new constructors.  and this will give time for any downstream users to make changes.\n\nThese changes are implemented as follows:\n1. Add a `KafkaBasedLog` constructor to accept in its parameters a supplier from which it can get an admin instance.  and deprecate the old constructor. We need a supplier rather than just passing an instance because `KafkaBasedLog` is instantiated before Connect starts up.  so we need to create the admin instance only when needed. At the same time.  we'll change the existing init function parameter from a no-arg function to accept an admin instance as an argument.  allowing that init function to reuse the shared admin instance used by the `KafkaBasedLog`. Note: if no admin supplier is provided (in deprecated constructor that is no longer used in AK).  the consumer is still used to get latest offsets.\n2. Add to the `Kafka*BackingStore` classes a new constructor with the same parameters but with an admin supplier.  and deprecate the old constructor. When the classes instantiate its `KafkaBasedLog` instance.  it would pass the admin supplier and pass an init function that takes an admin instance.\n3. Create a new `SharedTopicAdmin` that lazily creates the `TopicAdmin` (and underlying Admin client) when required.  and closes the admin objects when the `SharedTopicAdmin` is closed.\n4. Modify the existing `TopicAdmin` (used only in Connect) to encapsulate the logic of fetching end offsets using the admin client.  simplifying the logic in `KafkaBasedLog` mentioned in #1 above. Doing this also makes it easier to test that logic.\n5. Change `ConnectDistributed` to create a `SharedTopicAdmin` instance (that is `AutoCloseable`) before creating the `Kafka*BackingStore` instances.  passing the `SharedTopicAdmin` (which is an admin supplier) to all three `Kafka*BackingStore objects`.  and finally always closing the `SharedTopicAdmin` upon termination. (Shutdown of the worker occurs outside of the `ConnectDistributed` code.  so modify `DistributedHerder` to take in its constructor additional `AutoCloseable` objects that should be closed when the herder is closed.  and then modify `ConnectDistributed` to pass the `SharedTopicAdmin` as one of those `AutoCloseable` instances.)\n6. Change `MirrorMaker` similarly to `ConnectDistributed`.\n7. Change existing unit tests to no longer use deprecated constructors.\n8. Add unit tests for new functionality.\n\nAuthor: Randall Hauch <rhauch@gmail.com>\nReviewer: Konstantine Karantasis <konstantine@confluent.io>","date":"2021-02-10 01:09:41","modifiedFileCount":"12","status":"M","submitter":"Randall Hauch"},{"authorTime":"2021-02-20 01:49:56","codes":[{"authorDate":"2021-02-20 01:49:56","commitOrder":12,"curCode":"    KafkaBasedLog<String, byte[]> setupAndCreateKafkaBasedLog(String topic, final WorkerConfig config) {\n        String clusterId = ConnectUtils.lookupKafkaClusterId(config);\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);\n        ConnectUtils.addMetricsContextProperties(producerProps, config, clusterId);\n\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        ConnectUtils.addMetricsContextProperties(consumerProps, config, clusterId);\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n        ConnectUtils.addMetricsContextProperties(adminProps, config, clusterId);\n        Supplier<TopicAdmin> adminSupplier;\n        if (topicAdminSupplier != null) {\n            adminSupplier = topicAdminSupplier;\n        } else {\n            \r\n            ownTopicAdmin = new SharedTopicAdmin(adminProps);\n            adminSupplier = ownTopicAdmin;\n        }\n        Map<String, Object> topicSettings = config instanceof DistributedConfig\n                                            ? ((DistributedConfig) config).configStorageTopicSettings()\n                                            : Collections.emptyMap();\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic)\n                .config(topicSettings) \r\n                .compacted()\n                .partitions(1)\n                .replicationFactor(config.getShort(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG))\n                .build();\n\n        return createKafkaBasedLog(topic, producerProps, consumerProps, new ConsumeCallback(), topicDescription, adminSupplier);\n    }\n","date":"2021-02-20 01:49:56","endLine":509,"groupId":"105254","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"setupAndCreateKafkaBasedLog","params":"(Stringtopic@finalWorkerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/dc/fc28c5496b969af714e0910a4d3d1b92f66534.src","preCode":"    KafkaBasedLog<String, byte[]> setupAndCreateKafkaBasedLog(String topic, final WorkerConfig config) {\n        String clusterId = ConnectUtils.lookupKafkaClusterId(config);\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);\n        ConnectUtils.addMetricsContextProperties(producerProps, config, clusterId);\n\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        ConnectUtils.addMetricsContextProperties(consumerProps, config, clusterId);\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n        ConnectUtils.addMetricsContextProperties(adminProps, config, clusterId);\n        Supplier<TopicAdmin> adminSupplier = topicAdminSupplier != null ? topicAdminSupplier : () -> new TopicAdmin(adminProps);\n        Map<String, Object> topicSettings = config instanceof DistributedConfig\n                                            ? ((DistributedConfig) config).configStorageTopicSettings()\n                                            : Collections.emptyMap();\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic)\n                .config(topicSettings) \r\n                .compacted()\n                .partitions(1)\n                .replicationFactor(config.getShort(DistributedConfig.CONFIG_STORAGE_REPLICATION_FACTOR_CONFIG))\n                .build();\n\n        return createKafkaBasedLog(topic, producerProps, consumerProps, new ConsumeCallback(), topicDescription, adminSupplier);\n    }\n","realPath":"connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":474,"status":"M"},{"authorDate":"2021-02-20 01:49:56","commitOrder":12,"curCode":"    public void configure(final WorkerConfig config) {\n        String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);\n        if (topic == null || topic.trim().length() == 0)\n            throw new ConfigException(\"Offset storage topic must be specified\");\n\n        String clusterId = ConnectUtils.lookupKafkaClusterId(config);\n        data = new HashMap<>();\n\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);\n        ConnectUtils.addMetricsContextProperties(producerProps, config, clusterId);\n\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        ConnectUtils.addMetricsContextProperties(consumerProps, config, clusterId);\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n        ConnectUtils.addMetricsContextProperties(adminProps, config, clusterId);\n        Supplier<TopicAdmin> adminSupplier;\n        if (topicAdminSupplier != null) {\n            adminSupplier = topicAdminSupplier;\n        } else {\n            \r\n            ownTopicAdmin = new SharedTopicAdmin(adminProps);\n            adminSupplier = ownTopicAdmin;\n        }\n        Map<String, Object> topicSettings = config instanceof DistributedConfig\n                                            ? ((DistributedConfig) config).offsetStorageTopicSettings()\n                                            : Collections.emptyMap();\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic)\n                .config(topicSettings) \r\n                .compacted()\n                .partitions(config.getInt(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG))\n                .replicationFactor(config.getShort(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG))\n                .build();\n\n        offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback, topicDescription, adminSupplier);\n    }\n","date":"2021-02-20 01:49:56","endLine":122,"groupId":"105254","id":18,"instanceNumber":2,"isCurCommit":1,"methodName":"configure","params":"(finalWorkerConfigconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/31/3baf72c58c0616419832a394c20d0f42ea50de.src","preCode":"    public void configure(final WorkerConfig config) {\n        String topic = config.getString(DistributedConfig.OFFSET_STORAGE_TOPIC_CONFIG);\n        if (topic == null || topic.trim().length() == 0)\n            throw new ConfigException(\"Offset storage topic must be specified\");\n\n        String clusterId = ConnectUtils.lookupKafkaClusterId(config);\n        data = new HashMap<>();\n\n        Map<String, Object> originals = config.originals();\n        Map<String, Object> producerProps = new HashMap<>(originals);\n        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class.getName());\n        producerProps.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, Integer.MAX_VALUE);\n        ConnectUtils.addMetricsContextProperties(producerProps, config, clusterId);\n\n        Map<String, Object> consumerProps = new HashMap<>(originals);\n        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n        ConnectUtils.addMetricsContextProperties(consumerProps, config, clusterId);\n\n        Map<String, Object> adminProps = new HashMap<>(originals);\n        ConnectUtils.addMetricsContextProperties(adminProps, config, clusterId);\n        Supplier<TopicAdmin> adminSupplier = topicAdminSupplier != null ? topicAdminSupplier : () -> new TopicAdmin(adminProps);\n        Map<String, Object> topicSettings = config instanceof DistributedConfig\n                                            ? ((DistributedConfig) config).offsetStorageTopicSettings()\n                                            : Collections.emptyMap();\n        NewTopic topicDescription = TopicAdmin.defineTopic(topic)\n                .config(topicSettings) \r\n                .compacted()\n                .partitions(config.getInt(DistributedConfig.OFFSET_STORAGE_PARTITIONS_CONFIG))\n                .replicationFactor(config.getShort(DistributedConfig.OFFSET_STORAGE_REPLICATION_FACTOR_CONFIG))\n                .build();\n\n        offsetLog = createKafkaBasedLog(topic, producerProps, consumerProps, consumedCallback, topicDescription, adminSupplier);\n    }\n","realPath":"connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaOffsetBackingStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":81,"status":"M"}],"commitId":"c75a73862aa4bf752179e83c7fbeab771a2aef57","commitMessage":"@@@KAFKA-12340: Fix potential resource leak in Kafka*BackingStore (#10153)\n\nThese Kafka*BackingStore classes used in Connect have a recently-added deprecated constructor.  which is not used within AK. However.  this commit corrects a AdminClient resource leak if those deprecated constructors are used outside of AK. The fix simply ensures that the AdminClient created by the ?default? supplier is always closed when the Kafka*BackingStore is stopped.\n\nAuthor: Randall Hauch <rhauch@gmail.com>\nReviewers: Konstantine Karantasis <konstantine@confluent.io>.  Chia-Ping Tsai <chia7712@gmail.com>","date":"2021-02-20 01:49:56","modifiedFileCount":"3","status":"M","submitter":"Randall Hauch"}]
