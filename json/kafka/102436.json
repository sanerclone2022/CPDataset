[{"authorTime":"2021-02-24 12:41:02","codes":[{"authorDate":"2021-02-24 12:41:02","commitOrder":20,"curCode":"    public void shouldPunctuateActiveTask() {\n        final List<Long> punctuatedStreamTime = new ArrayList<>();\n        final List<Long> punctuatedWallClockTime = new ArrayList<>();\n        final org.apache.kafka.streams.processor.ProcessorSupplier<Object, Object> punctuateProcessor =\n            () -> new org.apache.kafka.streams.processor.AbstractProcessor<Object, Object>() {\n                @Override\n                public void init(final org.apache.kafka.streams.processor.ProcessorContext context) {\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.STREAM_TIME, punctuatedStreamTime::add);\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.WALL_CLOCK_TIME, punctuatedWallClockTime::add);\n                }\n\n                @Override\n                public void process(final Object key, final Object value) {}\n            };\n\n        internalStreamsBuilder.stream(Collections.singleton(topic1), consumed).process(punctuateProcessor);\n        internalStreamsBuilder.buildAndOptimizeTopology();\n\n        final StreamThread thread = createStreamThread(CLIENT_ID, config, false);\n\n        thread.setState(StreamThread.State.STARTING);\n        thread.rebalanceListener().onPartitionsRevoked(Collections.emptySet());\n        final List<TopicPartition> assignedPartitions = new ArrayList<>();\n\n        final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();\n\n        \r\n        assignedPartitions.add(t1p1);\n        activeTasks.put(task1, Collections.singleton(t1p1));\n\n        thread.taskManager().handleAssignment(activeTasks, emptyMap());\n\n        clientSupplier.consumer.assign(assignedPartitions);\n        clientSupplier.consumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));\n        thread.rebalanceListener().onPartitionsAssigned(assignedPartitions);\n\n        thread.runOnce();\n\n        assertEquals(0, punctuatedStreamTime.size());\n        assertEquals(0, punctuatedWallClockTime.size());\n\n        mockTime.sleep(100L);\n        for (long i = 0L; i < 10L; i++) {\n            clientSupplier.consumer.addRecord(new ConsumerRecord<>(\n                topic1,\n                1,\n                i,\n                i * 100L,\n                TimestampType.CREATE_TIME,\n                ConsumerRecord.NULL_CHECKSUM,\n                (\"K\" + i).getBytes().length,\n                (\"V\" + i).getBytes().length,\n                (\"K\" + i).getBytes(),\n                (\"V\" + i).getBytes()));\n        }\n\n        thread.runOnce();\n\n        assertEquals(1, punctuatedStreamTime.size());\n        assertEquals(1, punctuatedWallClockTime.size());\n\n        mockTime.sleep(100L);\n\n        thread.runOnce();\n\n        \r\n        assertEquals(1, punctuatedStreamTime.size());\n        assertEquals(2, punctuatedWallClockTime.size());\n    }\n","date":"2021-02-24 12:41:02","endLine":1873,"groupId":"21594","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"shouldPunctuateActiveTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/e2/b1549b905c054fe4a70f566a76f2173442c7da.src","preCode":"    public void shouldPunctuateActiveTask() {\n        final List<Long> punctuatedStreamTime = new ArrayList<>();\n        final List<Long> punctuatedWallClockTime = new ArrayList<>();\n        final org.apache.kafka.streams.processor.ProcessorSupplier<Object, Object> punctuateProcessor =\n            () -> new org.apache.kafka.streams.processor.AbstractProcessor<Object, Object>() {\n                @Override\n                public void init(final org.apache.kafka.streams.processor.ProcessorContext context) {\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.STREAM_TIME, punctuatedStreamTime::add);\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.WALL_CLOCK_TIME, punctuatedWallClockTime::add);\n                }\n\n                @Override\n                public void process(final Object key, final Object value) {}\n            };\n\n        internalStreamsBuilder.stream(Collections.singleton(topic1), consumed).process(punctuateProcessor);\n        internalStreamsBuilder.buildAndOptimizeTopology();\n\n        final StreamThread thread = createStreamThread(CLIENT_ID, config, false);\n\n        thread.setState(StreamThread.State.STARTING);\n        thread.rebalanceListener().onPartitionsRevoked(Collections.emptySet());\n        final List<TopicPartition> assignedPartitions = new ArrayList<>();\n\n        final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();\n\n        \r\n        assignedPartitions.add(t1p1);\n        activeTasks.put(task1, Collections.singleton(t1p1));\n\n        thread.taskManager().handleAssignment(activeTasks, emptyMap());\n\n        clientSupplier.consumer.assign(assignedPartitions);\n        clientSupplier.consumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));\n        thread.rebalanceListener().onPartitionsAssigned(assignedPartitions);\n\n        thread.runOnce();\n\n        assertEquals(0, punctuatedStreamTime.size());\n        assertEquals(0, punctuatedWallClockTime.size());\n\n        mockTime.sleep(100L);\n        for (long i = 0L; i < 10L; i++) {\n            clientSupplier.consumer.addRecord(new ConsumerRecord<>(\n                topic1,\n                1,\n                i,\n                i * 100L,\n                TimestampType.CREATE_TIME,\n                ConsumerRecord.NULL_CHECKSUM,\n                (\"K\" + i).getBytes().length,\n                (\"V\" + i).getBytes().length,\n                (\"K\" + i).getBytes(),\n                (\"V\" + i).getBytes()));\n        }\n\n        thread.runOnce();\n\n        assertEquals(1, punctuatedStreamTime.size());\n        assertEquals(1, punctuatedWallClockTime.size());\n\n        mockTime.sleep(100L);\n\n        thread.runOnce();\n\n        \r\n        assertEquals(1, punctuatedStreamTime.size());\n        assertEquals(2, punctuatedWallClockTime.size());\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1805,"status":"MB"},{"authorDate":"2021-02-24 12:41:02","commitOrder":20,"curCode":"    public void shouldPunctuateWithTimestampPreservedInProcessorContext() {\n        final org.apache.kafka.streams.kstream.TransformerSupplier<Object, Object, KeyValue<Object, Object>> punctuateProcessor =\n            () -> new org.apache.kafka.streams.kstream.Transformer<Object, Object, KeyValue<Object, Object>>() {\n                @Override\n                public void init(final org.apache.kafka.streams.processor.ProcessorContext context) {\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.WALL_CLOCK_TIME, timestamp -> context.forward(\"key\", \"value\"));\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.STREAM_TIME, timestamp -> context.forward(\"key\", \"value\"));\n                }\n\n                @Override\n                public KeyValue<Object, Object> transform(final Object key, final Object value) {\n                        return null;\n                    }\n\n                @Override\n                public void close() {}\n            };\n\n        final List<Long> peekedContextTime = new ArrayList<>();\n        final org.apache.kafka.streams.processor.ProcessorSupplier<Object, Object> peekProcessor =\n            () -> new org.apache.kafka.streams.processor.AbstractProcessor<Object, Object>() {\n                @Override\n                public void process(final Object key, final Object value) {\n                    peekedContextTime.add(context.timestamp());\n                }\n            };\n\n        internalStreamsBuilder.stream(Collections.singleton(topic1), consumed)\n            .transform(punctuateProcessor)\n            .process(peekProcessor);\n        internalStreamsBuilder.buildAndOptimizeTopology();\n\n        final long currTime = mockTime.milliseconds();\n        final StreamThread thread = createStreamThread(CLIENT_ID, config, false);\n\n        thread.setState(StreamThread.State.STARTING);\n        thread.rebalanceListener().onPartitionsRevoked(Collections.emptySet());\n        final List<TopicPartition> assignedPartitions = new ArrayList<>();\n\n        final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();\n\n        \r\n        assignedPartitions.add(t1p1);\n        activeTasks.put(task1, Collections.singleton(t1p1));\n\n        thread.taskManager().handleAssignment(activeTasks, emptyMap());\n\n        clientSupplier.consumer.assign(assignedPartitions);\n        clientSupplier.consumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));\n        thread.rebalanceListener().onPartitionsAssigned(assignedPartitions);\n\n        thread.runOnce();\n        assertEquals(0, peekedContextTime.size());\n\n        mockTime.sleep(100L);\n        thread.runOnce();\n\n        assertEquals(1, peekedContextTime.size());\n        assertEquals(currTime + 100L, peekedContextTime.get(0).longValue());\n\n        clientSupplier.consumer.addRecord(new ConsumerRecord<>(\n            topic1,\n            1,\n            0L,\n             100L,\n             TimestampType.CREATE_TIME,\n             ConsumerRecord.NULL_CHECKSUM,\n             \"K\".getBytes().length,\n             \"V\".getBytes().length,\n             \"K\".getBytes(),\n             \"V\".getBytes()));\n\n        thread.runOnce();\n\n        assertEquals(2, peekedContextTime.size());\n        assertEquals(0L, peekedContextTime.get(1).longValue());\n    }\n","date":"2021-02-24 12:41:02","endLine":1952,"groupId":"18819","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"shouldPunctuateWithTimestampPreservedInProcessorContext","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/e2/b1549b905c054fe4a70f566a76f2173442c7da.src","preCode":"    public void shouldPunctuateWithTimestampPreservedInProcessorContext() {\n        final org.apache.kafka.streams.kstream.TransformerSupplier<Object, Object, KeyValue<Object, Object>> punctuateProcessor =\n            () -> new org.apache.kafka.streams.kstream.Transformer<Object, Object, KeyValue<Object, Object>>() {\n                @Override\n                public void init(final org.apache.kafka.streams.processor.ProcessorContext context) {\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.WALL_CLOCK_TIME, timestamp -> context.forward(\"key\", \"value\"));\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.STREAM_TIME, timestamp -> context.forward(\"key\", \"value\"));\n                }\n\n                @Override\n                public KeyValue<Object, Object> transform(final Object key, final Object value) {\n                        return null;\n                    }\n\n                @Override\n                public void close() {}\n            };\n\n        final List<Long> peekedContextTime = new ArrayList<>();\n        final org.apache.kafka.streams.processor.ProcessorSupplier<Object, Object> peekProcessor =\n            () -> new org.apache.kafka.streams.processor.AbstractProcessor<Object, Object>() {\n                @Override\n                public void process(final Object key, final Object value) {\n                    peekedContextTime.add(context.timestamp());\n                }\n            };\n\n        internalStreamsBuilder.stream(Collections.singleton(topic1), consumed)\n            .transform(punctuateProcessor)\n            .process(peekProcessor);\n        internalStreamsBuilder.buildAndOptimizeTopology();\n\n        final long currTime = mockTime.milliseconds();\n        final StreamThread thread = createStreamThread(CLIENT_ID, config, false);\n\n        thread.setState(StreamThread.State.STARTING);\n        thread.rebalanceListener().onPartitionsRevoked(Collections.emptySet());\n        final List<TopicPartition> assignedPartitions = new ArrayList<>();\n\n        final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();\n\n        \r\n        assignedPartitions.add(t1p1);\n        activeTasks.put(task1, Collections.singleton(t1p1));\n\n        thread.taskManager().handleAssignment(activeTasks, emptyMap());\n\n        clientSupplier.consumer.assign(assignedPartitions);\n        clientSupplier.consumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));\n        thread.rebalanceListener().onPartitionsAssigned(assignedPartitions);\n\n        thread.runOnce();\n        assertEquals(0, peekedContextTime.size());\n\n        mockTime.sleep(100L);\n        thread.runOnce();\n\n        assertEquals(1, peekedContextTime.size());\n        assertEquals(currTime + 100L, peekedContextTime.get(0).longValue());\n\n        clientSupplier.consumer.addRecord(new ConsumerRecord<>(\n            topic1,\n            1,\n            0L,\n             100L,\n             TimestampType.CREATE_TIME,\n             ConsumerRecord.NULL_CHECKSUM,\n             \"K\".getBytes().length,\n             \"V\".getBytes().length,\n             \"K\".getBytes(),\n             \"V\".getBytes()));\n\n        thread.runOnce();\n\n        assertEquals(2, peekedContextTime.size());\n        assertEquals(0L, peekedContextTime.get(1).longValue());\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1876,"status":"B"}],"commitId":"f75efb96fae99a22eb54b5d0ef4e23b28fe8cd2d","commitMessage":"@@@KAFKA-12323: Set timestamp in record context when punctuate (#10170)\n\nWe need to preserve the timestamp when punctuating so that downstream operators would retain it via context.\n\nReviewers: Matthias J. Sax <matthias@confluent.io>","date":"2021-02-24 12:41:02","modifiedFileCount":"2","status":"M","submitter":"Guozhang Wang"},{"authorTime":"2021-03-02 04:11:11","codes":[{"authorDate":"2021-03-02 04:11:11","commitOrder":21,"curCode":"    public void shouldPunctuateActiveTask() {\n        final List<Long> punctuatedStreamTime = new ArrayList<>();\n        final List<Long> punctuatedWallClockTime = new ArrayList<>();\n        final org.apache.kafka.streams.processor.ProcessorSupplier<Object, Object> punctuateProcessor =\n            () -> new org.apache.kafka.streams.processor.AbstractProcessor<Object, Object>() {\n                @Override\n                public void init(final org.apache.kafka.streams.processor.ProcessorContext context) {\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.STREAM_TIME, punctuatedStreamTime::add);\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.WALL_CLOCK_TIME, punctuatedWallClockTime::add);\n                }\n\n                @Override\n                public void process(final Object key, final Object value) {}\n            };\n\n        internalStreamsBuilder.stream(Collections.singleton(topic1), consumed).process(punctuateProcessor);\n        internalStreamsBuilder.buildAndOptimizeTopology();\n\n        final StreamThread thread = createStreamThread(CLIENT_ID, config, false);\n\n        thread.setState(StreamThread.State.STARTING);\n        thread.rebalanceListener().onPartitionsRevoked(Collections.emptySet());\n        final List<TopicPartition> assignedPartitions = new ArrayList<>();\n\n        final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();\n\n        \r\n        assignedPartitions.add(t1p1);\n        activeTasks.put(task1, Collections.singleton(t1p1));\n\n        thread.taskManager().handleAssignment(activeTasks, emptyMap());\n\n        clientSupplier.consumer.assign(assignedPartitions);\n        clientSupplier.consumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));\n        thread.rebalanceListener().onPartitionsAssigned(assignedPartitions);\n\n        thread.runOnce();\n\n        assertEquals(0, punctuatedStreamTime.size());\n        assertEquals(0, punctuatedWallClockTime.size());\n\n        mockTime.sleep(100L);\n        clientSupplier.consumer.addRecord(new ConsumerRecord<>(\n            topic1,\n            1,\n            100L,\n            100L,\n            TimestampType.CREATE_TIME,\n            ConsumerRecord.NULL_CHECKSUM,\n            \"K\".getBytes().length,\n            \"V\".getBytes().length,\n            \"K\".getBytes(),\n            \"V\".getBytes()));\n\n        thread.runOnce();\n\n        assertEquals(1, punctuatedStreamTime.size());\n        assertEquals(1, punctuatedWallClockTime.size());\n\n        mockTime.sleep(100L);\n\n        thread.runOnce();\n\n        \r\n        assertEquals(1, punctuatedStreamTime.size());\n        assertEquals(2, punctuatedWallClockTime.size());\n    }\n","date":"2021-03-02 04:11:11","endLine":1871,"groupId":"21594","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"shouldPunctuateActiveTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/30/43423edd12c7b65b24f580c63efe75f1335b3e.src","preCode":"    public void shouldPunctuateActiveTask() {\n        final List<Long> punctuatedStreamTime = new ArrayList<>();\n        final List<Long> punctuatedWallClockTime = new ArrayList<>();\n        final org.apache.kafka.streams.processor.ProcessorSupplier<Object, Object> punctuateProcessor =\n            () -> new org.apache.kafka.streams.processor.AbstractProcessor<Object, Object>() {\n                @Override\n                public void init(final org.apache.kafka.streams.processor.ProcessorContext context) {\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.STREAM_TIME, punctuatedStreamTime::add);\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.WALL_CLOCK_TIME, punctuatedWallClockTime::add);\n                }\n\n                @Override\n                public void process(final Object key, final Object value) {}\n            };\n\n        internalStreamsBuilder.stream(Collections.singleton(topic1), consumed).process(punctuateProcessor);\n        internalStreamsBuilder.buildAndOptimizeTopology();\n\n        final StreamThread thread = createStreamThread(CLIENT_ID, config, false);\n\n        thread.setState(StreamThread.State.STARTING);\n        thread.rebalanceListener().onPartitionsRevoked(Collections.emptySet());\n        final List<TopicPartition> assignedPartitions = new ArrayList<>();\n\n        final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();\n\n        \r\n        assignedPartitions.add(t1p1);\n        activeTasks.put(task1, Collections.singleton(t1p1));\n\n        thread.taskManager().handleAssignment(activeTasks, emptyMap());\n\n        clientSupplier.consumer.assign(assignedPartitions);\n        clientSupplier.consumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));\n        thread.rebalanceListener().onPartitionsAssigned(assignedPartitions);\n\n        thread.runOnce();\n\n        assertEquals(0, punctuatedStreamTime.size());\n        assertEquals(0, punctuatedWallClockTime.size());\n\n        mockTime.sleep(100L);\n        for (long i = 0L; i < 10L; i++) {\n            clientSupplier.consumer.addRecord(new ConsumerRecord<>(\n                topic1,\n                1,\n                i,\n                i * 100L,\n                TimestampType.CREATE_TIME,\n                ConsumerRecord.NULL_CHECKSUM,\n                (\"K\" + i).getBytes().length,\n                (\"V\" + i).getBytes().length,\n                (\"K\" + i).getBytes(),\n                (\"V\" + i).getBytes()));\n        }\n\n        thread.runOnce();\n\n        assertEquals(1, punctuatedStreamTime.size());\n        assertEquals(1, punctuatedWallClockTime.size());\n\n        mockTime.sleep(100L);\n\n        thread.runOnce();\n\n        \r\n        assertEquals(1, punctuatedStreamTime.size());\n        assertEquals(2, punctuatedWallClockTime.size());\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1805,"status":"M"},{"authorDate":"2021-03-02 04:11:11","commitOrder":21,"curCode":"    public void shouldPunctuateWithTimestampPreservedInProcessorContext() {\n        final org.apache.kafka.streams.kstream.TransformerSupplier<Object, Object, KeyValue<Object, Object>> punctuateProcessor =\n            () -> new org.apache.kafka.streams.kstream.Transformer<Object, Object, KeyValue<Object, Object>>() {\n                @Override\n                public void init(final org.apache.kafka.streams.processor.ProcessorContext context) {\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.WALL_CLOCK_TIME, timestamp -> context.forward(\"key\", \"value\"));\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.STREAM_TIME, timestamp -> context.forward(\"key\", \"value\"));\n                }\n\n                @Override\n                public KeyValue<Object, Object> transform(final Object key, final Object value) {\n                        return null;\n                    }\n\n                @Override\n                public void close() {}\n            };\n\n        final List<Long> peekedContextTime = new ArrayList<>();\n        final org.apache.kafka.streams.processor.ProcessorSupplier<Object, Object> peekProcessor =\n            () -> new org.apache.kafka.streams.processor.AbstractProcessor<Object, Object>() {\n                @Override\n                public void process(final Object key, final Object value) {\n                    peekedContextTime.add(context.timestamp());\n                }\n            };\n\n        internalStreamsBuilder.stream(Collections.singleton(topic1), consumed)\n            .transform(punctuateProcessor)\n            .process(peekProcessor);\n        internalStreamsBuilder.buildAndOptimizeTopology();\n\n        final long currTime = mockTime.milliseconds();\n        final StreamThread thread = createStreamThread(CLIENT_ID, config, false);\n\n        thread.setState(StreamThread.State.STARTING);\n        thread.rebalanceListener().onPartitionsRevoked(Collections.emptySet());\n        final List<TopicPartition> assignedPartitions = new ArrayList<>();\n\n        final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();\n\n        \r\n        assignedPartitions.add(t1p1);\n        activeTasks.put(task1, Collections.singleton(t1p1));\n\n        thread.taskManager().handleAssignment(activeTasks, emptyMap());\n\n        clientSupplier.consumer.assign(assignedPartitions);\n        clientSupplier.consumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));\n        thread.rebalanceListener().onPartitionsAssigned(assignedPartitions);\n\n        thread.runOnce();\n        assertEquals(0, peekedContextTime.size());\n\n        mockTime.sleep(100L);\n        thread.runOnce();\n\n        assertEquals(1, peekedContextTime.size());\n        assertEquals(currTime + 100L, peekedContextTime.get(0).longValue());\n\n        clientSupplier.consumer.addRecord(new ConsumerRecord<>(\n            topic1,\n            1,\n            110L,\n            110L,\n            TimestampType.CREATE_TIME,\n            ConsumerRecord.NULL_CHECKSUM,\n            \"K\".getBytes().length,\n            \"V\".getBytes().length,\n            \"K\".getBytes(),\n            \"V\".getBytes()));\n\n        thread.runOnce();\n\n        assertEquals(2, peekedContextTime.size());\n        assertEquals(110L, peekedContextTime.get(1).longValue());\n    }\n","date":"2021-03-02 04:11:11","endLine":1950,"groupId":"18819","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"shouldPunctuateWithTimestampPreservedInProcessorContext","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/30/43423edd12c7b65b24f580c63efe75f1335b3e.src","preCode":"    public void shouldPunctuateWithTimestampPreservedInProcessorContext() {\n        final org.apache.kafka.streams.kstream.TransformerSupplier<Object, Object, KeyValue<Object, Object>> punctuateProcessor =\n            () -> new org.apache.kafka.streams.kstream.Transformer<Object, Object, KeyValue<Object, Object>>() {\n                @Override\n                public void init(final org.apache.kafka.streams.processor.ProcessorContext context) {\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.WALL_CLOCK_TIME, timestamp -> context.forward(\"key\", \"value\"));\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.STREAM_TIME, timestamp -> context.forward(\"key\", \"value\"));\n                }\n\n                @Override\n                public KeyValue<Object, Object> transform(final Object key, final Object value) {\n                        return null;\n                    }\n\n                @Override\n                public void close() {}\n            };\n\n        final List<Long> peekedContextTime = new ArrayList<>();\n        final org.apache.kafka.streams.processor.ProcessorSupplier<Object, Object> peekProcessor =\n            () -> new org.apache.kafka.streams.processor.AbstractProcessor<Object, Object>() {\n                @Override\n                public void process(final Object key, final Object value) {\n                    peekedContextTime.add(context.timestamp());\n                }\n            };\n\n        internalStreamsBuilder.stream(Collections.singleton(topic1), consumed)\n            .transform(punctuateProcessor)\n            .process(peekProcessor);\n        internalStreamsBuilder.buildAndOptimizeTopology();\n\n        final long currTime = mockTime.milliseconds();\n        final StreamThread thread = createStreamThread(CLIENT_ID, config, false);\n\n        thread.setState(StreamThread.State.STARTING);\n        thread.rebalanceListener().onPartitionsRevoked(Collections.emptySet());\n        final List<TopicPartition> assignedPartitions = new ArrayList<>();\n\n        final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();\n\n        \r\n        assignedPartitions.add(t1p1);\n        activeTasks.put(task1, Collections.singleton(t1p1));\n\n        thread.taskManager().handleAssignment(activeTasks, emptyMap());\n\n        clientSupplier.consumer.assign(assignedPartitions);\n        clientSupplier.consumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));\n        thread.rebalanceListener().onPartitionsAssigned(assignedPartitions);\n\n        thread.runOnce();\n        assertEquals(0, peekedContextTime.size());\n\n        mockTime.sleep(100L);\n        thread.runOnce();\n\n        assertEquals(1, peekedContextTime.size());\n        assertEquals(currTime + 100L, peekedContextTime.get(0).longValue());\n\n        clientSupplier.consumer.addRecord(new ConsumerRecord<>(\n            topic1,\n            1,\n            0L,\n             100L,\n             TimestampType.CREATE_TIME,\n             ConsumerRecord.NULL_CHECKSUM,\n             \"K\".getBytes().length,\n             \"V\".getBytes().length,\n             \"K\".getBytes(),\n             \"V\".getBytes()));\n\n        thread.runOnce();\n\n        assertEquals(2, peekedContextTime.size());\n        assertEquals(0L, peekedContextTime.get(1).longValue());\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1874,"status":"M"}],"commitId":"22dbf89886974c27c705bba653f9cfe36f70904c","commitMessage":"@@@KAFKA-12323 Follow-up: Refactor the unit test a bit (#10205)\n\nReviewers: Matthias J. Sax <matthias@confluent.io>","date":"2021-03-02 04:11:11","modifiedFileCount":"1","status":"M","submitter":"Guozhang Wang"},{"authorTime":"2021-04-15 05:38:37","codes":[{"authorDate":"2021-04-15 05:38:37","commitOrder":22,"curCode":"    public void shouldPunctuateActiveTask() {\n        final List<Long> punctuatedStreamTime = new ArrayList<>();\n        final List<Long> punctuatedWallClockTime = new ArrayList<>();\n        final org.apache.kafka.streams.processor.ProcessorSupplier<Object, Object> punctuateProcessor =\n            () -> new org.apache.kafka.streams.processor.AbstractProcessor<Object, Object>() {\n                @Override\n                public void init(final org.apache.kafka.streams.processor.ProcessorContext context) {\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.STREAM_TIME, punctuatedStreamTime::add);\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.WALL_CLOCK_TIME, punctuatedWallClockTime::add);\n                }\n\n                @Override\n                public void process(final Object key, final Object value) {}\n            };\n\n        internalStreamsBuilder.stream(Collections.singleton(topic1), consumed).process(punctuateProcessor);\n        internalStreamsBuilder.buildAndOptimizeTopology();\n\n        final StreamThread thread = createStreamThread(CLIENT_ID, config, false);\n\n        thread.setState(StreamThread.State.STARTING);\n        thread.rebalanceListener().onPartitionsRevoked(Collections.emptySet());\n        final List<TopicPartition> assignedPartitions = new ArrayList<>();\n\n        final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();\n\n        \r\n        assignedPartitions.add(t1p1);\n        activeTasks.put(task1, Collections.singleton(t1p1));\n\n        thread.taskManager().handleAssignment(activeTasks, emptyMap());\n\n        clientSupplier.consumer.assign(assignedPartitions);\n        clientSupplier.consumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));\n        thread.rebalanceListener().onPartitionsAssigned(assignedPartitions);\n\n        thread.runOnce();\n\n        assertEquals(0, punctuatedStreamTime.size());\n        assertEquals(0, punctuatedWallClockTime.size());\n\n        mockTime.sleep(100L);\n        clientSupplier.consumer.addRecord(new ConsumerRecord<>(\n            topic1,\n            1,\n            100L,\n            100L,\n            TimestampType.CREATE_TIME,\n            \"K\".getBytes().length,\n            \"V\".getBytes().length,\n            \"K\".getBytes(),\n            \"V\".getBytes(),\n            new RecordHeaders(),\n            Optional.empty()));\n\n        thread.runOnce();\n\n        assertEquals(1, punctuatedStreamTime.size());\n        assertEquals(1, punctuatedWallClockTime.size());\n\n        mockTime.sleep(100L);\n\n        thread.runOnce();\n\n        \r\n        assertEquals(1, punctuatedStreamTime.size());\n        assertEquals(2, punctuatedWallClockTime.size());\n    }\n","date":"2021-04-15 05:38:37","endLine":1876,"groupId":"102436","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"shouldPunctuateActiveTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/80/fb2c90b996aa0a11616f31c1270fb71aecf97c.src","preCode":"    public void shouldPunctuateActiveTask() {\n        final List<Long> punctuatedStreamTime = new ArrayList<>();\n        final List<Long> punctuatedWallClockTime = new ArrayList<>();\n        final org.apache.kafka.streams.processor.ProcessorSupplier<Object, Object> punctuateProcessor =\n            () -> new org.apache.kafka.streams.processor.AbstractProcessor<Object, Object>() {\n                @Override\n                public void init(final org.apache.kafka.streams.processor.ProcessorContext context) {\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.STREAM_TIME, punctuatedStreamTime::add);\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.WALL_CLOCK_TIME, punctuatedWallClockTime::add);\n                }\n\n                @Override\n                public void process(final Object key, final Object value) {}\n            };\n\n        internalStreamsBuilder.stream(Collections.singleton(topic1), consumed).process(punctuateProcessor);\n        internalStreamsBuilder.buildAndOptimizeTopology();\n\n        final StreamThread thread = createStreamThread(CLIENT_ID, config, false);\n\n        thread.setState(StreamThread.State.STARTING);\n        thread.rebalanceListener().onPartitionsRevoked(Collections.emptySet());\n        final List<TopicPartition> assignedPartitions = new ArrayList<>();\n\n        final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();\n\n        \r\n        assignedPartitions.add(t1p1);\n        activeTasks.put(task1, Collections.singleton(t1p1));\n\n        thread.taskManager().handleAssignment(activeTasks, emptyMap());\n\n        clientSupplier.consumer.assign(assignedPartitions);\n        clientSupplier.consumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));\n        thread.rebalanceListener().onPartitionsAssigned(assignedPartitions);\n\n        thread.runOnce();\n\n        assertEquals(0, punctuatedStreamTime.size());\n        assertEquals(0, punctuatedWallClockTime.size());\n\n        mockTime.sleep(100L);\n        clientSupplier.consumer.addRecord(new ConsumerRecord<>(\n            topic1,\n            1,\n            100L,\n            100L,\n            TimestampType.CREATE_TIME,\n            ConsumerRecord.NULL_CHECKSUM,\n            \"K\".getBytes().length,\n            \"V\".getBytes().length,\n            \"K\".getBytes(),\n            \"V\".getBytes()));\n\n        thread.runOnce();\n\n        assertEquals(1, punctuatedStreamTime.size());\n        assertEquals(1, punctuatedWallClockTime.size());\n\n        mockTime.sleep(100L);\n\n        thread.runOnce();\n\n        \r\n        assertEquals(1, punctuatedStreamTime.size());\n        assertEquals(2, punctuatedWallClockTime.size());\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1809,"status":"M"},{"authorDate":"2021-04-15 05:38:37","commitOrder":22,"curCode":"    public void shouldPunctuateWithTimestampPreservedInProcessorContext() {\n        final org.apache.kafka.streams.kstream.TransformerSupplier<Object, Object, KeyValue<Object, Object>> punctuateProcessor =\n            () -> new org.apache.kafka.streams.kstream.Transformer<Object, Object, KeyValue<Object, Object>>() {\n                @Override\n                public void init(final org.apache.kafka.streams.processor.ProcessorContext context) {\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.WALL_CLOCK_TIME, timestamp -> context.forward(\"key\", \"value\"));\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.STREAM_TIME, timestamp -> context.forward(\"key\", \"value\"));\n                }\n\n                @Override\n                public KeyValue<Object, Object> transform(final Object key, final Object value) {\n                        return null;\n                    }\n\n                @Override\n                public void close() {}\n            };\n\n        final List<Long> peekedContextTime = new ArrayList<>();\n        final org.apache.kafka.streams.processor.ProcessorSupplier<Object, Object> peekProcessor =\n            () -> new org.apache.kafka.streams.processor.AbstractProcessor<Object, Object>() {\n                @Override\n                public void process(final Object key, final Object value) {\n                    peekedContextTime.add(context.timestamp());\n                }\n            };\n\n        internalStreamsBuilder.stream(Collections.singleton(topic1), consumed)\n            .transform(punctuateProcessor)\n            .process(peekProcessor);\n        internalStreamsBuilder.buildAndOptimizeTopology();\n\n        final long currTime = mockTime.milliseconds();\n        final StreamThread thread = createStreamThread(CLIENT_ID, config, false);\n\n        thread.setState(StreamThread.State.STARTING);\n        thread.rebalanceListener().onPartitionsRevoked(Collections.emptySet());\n        final List<TopicPartition> assignedPartitions = new ArrayList<>();\n\n        final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();\n\n        \r\n        assignedPartitions.add(t1p1);\n        activeTasks.put(task1, Collections.singleton(t1p1));\n\n        thread.taskManager().handleAssignment(activeTasks, emptyMap());\n\n        clientSupplier.consumer.assign(assignedPartitions);\n        clientSupplier.consumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));\n        thread.rebalanceListener().onPartitionsAssigned(assignedPartitions);\n\n        thread.runOnce();\n        assertEquals(0, peekedContextTime.size());\n\n        mockTime.sleep(100L);\n        thread.runOnce();\n\n        assertEquals(1, peekedContextTime.size());\n        assertEquals(currTime + 100L, peekedContextTime.get(0).longValue());\n\n        clientSupplier.consumer.addRecord(new ConsumerRecord<>(\n            topic1,\n            1,\n            110L,\n            110L,\n            TimestampType.CREATE_TIME,\n            \"K\".getBytes().length,\n            \"V\".getBytes().length,\n            \"K\".getBytes(),\n            \"V\".getBytes(),\n            new RecordHeaders(),\n            Optional.empty()));\n\n        thread.runOnce();\n\n        assertEquals(2, peekedContextTime.size());\n        assertEquals(110L, peekedContextTime.get(1).longValue());\n    }\n","date":"2021-04-15 05:38:37","endLine":1956,"groupId":"102436","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"shouldPunctuateWithTimestampPreservedInProcessorContext","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/80/fb2c90b996aa0a11616f31c1270fb71aecf97c.src","preCode":"    public void shouldPunctuateWithTimestampPreservedInProcessorContext() {\n        final org.apache.kafka.streams.kstream.TransformerSupplier<Object, Object, KeyValue<Object, Object>> punctuateProcessor =\n            () -> new org.apache.kafka.streams.kstream.Transformer<Object, Object, KeyValue<Object, Object>>() {\n                @Override\n                public void init(final org.apache.kafka.streams.processor.ProcessorContext context) {\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.WALL_CLOCK_TIME, timestamp -> context.forward(\"key\", \"value\"));\n                    context.schedule(Duration.ofMillis(100L), PunctuationType.STREAM_TIME, timestamp -> context.forward(\"key\", \"value\"));\n                }\n\n                @Override\n                public KeyValue<Object, Object> transform(final Object key, final Object value) {\n                        return null;\n                    }\n\n                @Override\n                public void close() {}\n            };\n\n        final List<Long> peekedContextTime = new ArrayList<>();\n        final org.apache.kafka.streams.processor.ProcessorSupplier<Object, Object> peekProcessor =\n            () -> new org.apache.kafka.streams.processor.AbstractProcessor<Object, Object>() {\n                @Override\n                public void process(final Object key, final Object value) {\n                    peekedContextTime.add(context.timestamp());\n                }\n            };\n\n        internalStreamsBuilder.stream(Collections.singleton(topic1), consumed)\n            .transform(punctuateProcessor)\n            .process(peekProcessor);\n        internalStreamsBuilder.buildAndOptimizeTopology();\n\n        final long currTime = mockTime.milliseconds();\n        final StreamThread thread = createStreamThread(CLIENT_ID, config, false);\n\n        thread.setState(StreamThread.State.STARTING);\n        thread.rebalanceListener().onPartitionsRevoked(Collections.emptySet());\n        final List<TopicPartition> assignedPartitions = new ArrayList<>();\n\n        final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();\n\n        \r\n        assignedPartitions.add(t1p1);\n        activeTasks.put(task1, Collections.singleton(t1p1));\n\n        thread.taskManager().handleAssignment(activeTasks, emptyMap());\n\n        clientSupplier.consumer.assign(assignedPartitions);\n        clientSupplier.consumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));\n        thread.rebalanceListener().onPartitionsAssigned(assignedPartitions);\n\n        thread.runOnce();\n        assertEquals(0, peekedContextTime.size());\n\n        mockTime.sleep(100L);\n        thread.runOnce();\n\n        assertEquals(1, peekedContextTime.size());\n        assertEquals(currTime + 100L, peekedContextTime.get(0).longValue());\n\n        clientSupplier.consumer.addRecord(new ConsumerRecord<>(\n            topic1,\n            1,\n            110L,\n            110L,\n            TimestampType.CREATE_TIME,\n            ConsumerRecord.NULL_CHECKSUM,\n            \"K\".getBytes().length,\n            \"V\".getBytes().length,\n            \"K\".getBytes(),\n            \"V\".getBytes()));\n\n        thread.runOnce();\n\n        assertEquals(2, peekedContextTime.size());\n        assertEquals(110L, peekedContextTime.get(1).longValue());\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1879,"status":"M"}],"commitId":"89933f21f204abf75336464d3ac24a4fdd254628","commitMessage":"@@@KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0 (#10470)\n\nThe methods have been deprecated since 0.11 without replacement since\nmessage format 2 moved the checksum to the record batch (instead of the\nrecord).\n\nUnfortunately.  we did not deprecate the constructors that take a checksum\n(even though we intended to) so we cannot remove them. I have deprecated\nthem for removal in 4.0 and added a single non deprecated constructor to\n`ConsumerRecord` and `RecordMetadata` that take all remaining parameters.\n`ConsumerRecord` could do with one additional convenience constructor.  but\nthat requires a KIP and hence should be done separately.\n\nAlso:\n* Removed `ChecksumMessageFormatter`.  which is technically not public\nAPI.  but may have been used with the console consumer.\n* Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors\nto use the non deprecated ones.\n* Added tests for deprecated `ConsumerRecord/`RecordMetadata`\nconstructors.\n\nReviewers: Chia-Ping Tsai <chia7712@gmail.com>.  David Jacot <djacot@confluent.io>","date":"2021-04-15 05:38:37","modifiedFileCount":"47","status":"M","submitter":"Ismael Juma"}]
