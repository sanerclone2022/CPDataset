[{"authorTime":"2017-06-01 05:11:47","codes":[{"authorDate":"2017-06-01 05:11:47","commitOrder":1,"curCode":"    public void testBatchIterationV2() throws IOException {\n        if (magic != MAGIC_VALUE_V2)\n            return;\n\n        try (FileRecords fileRecords = FileRecords.open(tempFile())) {\n            long producerId = 83843L;\n            short producerEpoch = 15;\n            int baseSequence = 234;\n            int partitionLeaderEpoch = 9832;\n\n            SimpleRecord[] firstBatchRecords = new SimpleRecord[]{\n                new SimpleRecord(3241324L, \"a\".getBytes(), \"1\".getBytes()),\n                new SimpleRecord(234280L, \"b\".getBytes(), \"2\".getBytes())\n\n            };\n            SimpleRecord[] secondBatchRecords = new SimpleRecord[]{\n                new SimpleRecord(238423489L, \"c\".getBytes(), \"3\".getBytes()),\n                new SimpleRecord(897839L, null, \"4\".getBytes()),\n                new SimpleRecord(8234020L, \"e\".getBytes(), null)\n            };\n\n            fileRecords.append(MemoryRecords.withIdempotentRecords(magic, 15L, compression, producerId,\n                    producerEpoch, baseSequence, partitionLeaderEpoch, firstBatchRecords));\n            fileRecords.append(MemoryRecords.withTransactionalRecords(magic, 27L, compression, producerId,\n                    producerEpoch, baseSequence + firstBatchRecords.length, partitionLeaderEpoch, secondBatchRecords));\n            fileRecords.flush();\n\n            FileLogInputStream logInputStream = new FileLogInputStream(fileRecords.channel(), 0,\n                    fileRecords.sizeInBytes());\n\n            FileChannelRecordBatch firstBatch = logInputStream.nextBatch();\n            assertProducerData(firstBatch, producerId, producerEpoch, baseSequence, false, firstBatchRecords);\n            assertGenericRecordBatchData(firstBatch, 15L, 3241324L, firstBatchRecords);\n            assertEquals(partitionLeaderEpoch, firstBatch.partitionLeaderEpoch());\n\n            FileChannelRecordBatch secondBatch = logInputStream.nextBatch();\n            assertProducerData(secondBatch, producerId, producerEpoch, baseSequence + firstBatchRecords.length,\n                    true, secondBatchRecords);\n            assertGenericRecordBatchData(secondBatch, 27L, 238423489L, secondBatchRecords);\n            assertEquals(partitionLeaderEpoch, secondBatch.partitionLeaderEpoch());\n\n            assertNull(logInputStream.nextBatch());\n        }\n    }\n","date":"2017-06-01 05:11:47","endLine":188,"groupId":"2721","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testBatchIterationV2","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/d5/de4bd9b5208599d5adec014aead2f51fa053f5.src","preCode":"    public void testBatchIterationV2() throws IOException {\n        if (magic != MAGIC_VALUE_V2)\n            return;\n\n        try (FileRecords fileRecords = FileRecords.open(tempFile())) {\n            long producerId = 83843L;\n            short producerEpoch = 15;\n            int baseSequence = 234;\n            int partitionLeaderEpoch = 9832;\n\n            SimpleRecord[] firstBatchRecords = new SimpleRecord[]{\n                new SimpleRecord(3241324L, \"a\".getBytes(), \"1\".getBytes()),\n                new SimpleRecord(234280L, \"b\".getBytes(), \"2\".getBytes())\n\n            };\n            SimpleRecord[] secondBatchRecords = new SimpleRecord[]{\n                new SimpleRecord(238423489L, \"c\".getBytes(), \"3\".getBytes()),\n                new SimpleRecord(897839L, null, \"4\".getBytes()),\n                new SimpleRecord(8234020L, \"e\".getBytes(), null)\n            };\n\n            fileRecords.append(MemoryRecords.withIdempotentRecords(magic, 15L, compression, producerId,\n                    producerEpoch, baseSequence, partitionLeaderEpoch, firstBatchRecords));\n            fileRecords.append(MemoryRecords.withTransactionalRecords(magic, 27L, compression, producerId,\n                    producerEpoch, baseSequence + firstBatchRecords.length, partitionLeaderEpoch, secondBatchRecords));\n            fileRecords.flush();\n\n            FileLogInputStream logInputStream = new FileLogInputStream(fileRecords.channel(), 0,\n                    fileRecords.sizeInBytes());\n\n            FileChannelRecordBatch firstBatch = logInputStream.nextBatch();\n            assertProducerData(firstBatch, producerId, producerEpoch, baseSequence, false, firstBatchRecords);\n            assertGenericRecordBatchData(firstBatch, 15L, 3241324L, firstBatchRecords);\n            assertEquals(partitionLeaderEpoch, firstBatch.partitionLeaderEpoch());\n\n            FileChannelRecordBatch secondBatch = logInputStream.nextBatch();\n            assertProducerData(secondBatch, producerId, producerEpoch, baseSequence + firstBatchRecords.length,\n                    true, secondBatchRecords);\n            assertGenericRecordBatchData(secondBatch, 27L, 238423489L, secondBatchRecords);\n            assertEquals(partitionLeaderEpoch, secondBatch.partitionLeaderEpoch());\n\n            assertNull(logInputStream.nextBatch());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/FileLogInputStreamTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":145,"status":"B"},{"authorDate":"2017-06-01 05:11:47","commitOrder":1,"curCode":"    public void testBatchIterationIncompleteBatch() throws IOException {\n        try (FileRecords fileRecords = FileRecords.open(tempFile())) {\n            SimpleRecord firstBatchRecord = new SimpleRecord(100L, \"foo\".getBytes());\n            SimpleRecord secondBatchRecord = new SimpleRecord(200L, \"bar\".getBytes());\n\n            fileRecords.append(MemoryRecords.withRecords(magic, 0L, compression, CREATE_TIME, firstBatchRecord));\n            fileRecords.append(MemoryRecords.withRecords(magic, 1L, compression, CREATE_TIME, secondBatchRecord));\n            fileRecords.flush();\n            fileRecords.truncateTo(fileRecords.sizeInBytes() - 13);\n\n            FileLogInputStream logInputStream = new FileLogInputStream(fileRecords.channel(), 0,\n                    fileRecords.sizeInBytes());\n\n            FileChannelRecordBatch firstBatch = logInputStream.nextBatch();\n            assertNoProducerData(firstBatch);\n            assertGenericRecordBatchData(firstBatch, 0L, 100L, firstBatchRecord);\n\n            assertNull(logInputStream.nextBatch());\n        }\n    }\n","date":"2017-06-01 05:11:47","endLine":210,"groupId":"1553","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testBatchIterationIncompleteBatch","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/d5/de4bd9b5208599d5adec014aead2f51fa053f5.src","preCode":"    public void testBatchIterationIncompleteBatch() throws IOException {\n        try (FileRecords fileRecords = FileRecords.open(tempFile())) {\n            SimpleRecord firstBatchRecord = new SimpleRecord(100L, \"foo\".getBytes());\n            SimpleRecord secondBatchRecord = new SimpleRecord(200L, \"bar\".getBytes());\n\n            fileRecords.append(MemoryRecords.withRecords(magic, 0L, compression, CREATE_TIME, firstBatchRecord));\n            fileRecords.append(MemoryRecords.withRecords(magic, 1L, compression, CREATE_TIME, secondBatchRecord));\n            fileRecords.flush();\n            fileRecords.truncateTo(fileRecords.sizeInBytes() - 13);\n\n            FileLogInputStream logInputStream = new FileLogInputStream(fileRecords.channel(), 0,\n                    fileRecords.sizeInBytes());\n\n            FileChannelRecordBatch firstBatch = logInputStream.nextBatch();\n            assertNoProducerData(firstBatch);\n            assertGenericRecordBatchData(firstBatch, 0L, 100L, firstBatchRecord);\n\n            assertNull(logInputStream.nextBatch());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/FileLogInputStreamTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":191,"status":"B"}],"commitId":"81f0c1e8f2ba2d86f061361b5ee33bb8e6f640c5","commitMessage":"@@@KAFKA-5093; Avoid loading full batch data when possible when iterating FileRecords\n\nAuthor: Jason Gustafson <jason@confluent.io>\n\nReviewers: Ismael Juma <ismael@juma.me.uk>\n\nCloses #3160 from hachikuji/KAFKA-5093\n","date":"2017-06-01 05:11:47","modifiedFileCount":"11","status":"B","submitter":"Jason Gustafson"},{"authorTime":"2018-04-30 23:59:04","codes":[{"authorDate":"2018-04-30 23:59:04","commitOrder":2,"curCode":"    public void testBatchIterationV2() throws IOException {\n        if (magic != MAGIC_VALUE_V2)\n            return;\n\n        try (FileRecords fileRecords = FileRecords.open(tempFile())) {\n            long producerId = 83843L;\n            short producerEpoch = 15;\n            int baseSequence = 234;\n            int partitionLeaderEpoch = 9832;\n\n            SimpleRecord[] firstBatchRecords = new SimpleRecord[]{\n                new SimpleRecord(3241324L, \"a\".getBytes(), \"1\".getBytes()),\n                new SimpleRecord(234280L, \"b\".getBytes(), \"2\".getBytes())\n\n            };\n            SimpleRecord[] secondBatchRecords = new SimpleRecord[]{\n                new SimpleRecord(238423489L, \"c\".getBytes(), \"3\".getBytes()),\n                new SimpleRecord(897839L, null, \"4\".getBytes()),\n                new SimpleRecord(8234020L, \"e\".getBytes(), null)\n            };\n\n            fileRecords.append(MemoryRecords.withIdempotentRecords(magic, 15L, compression, producerId,\n                    producerEpoch, baseSequence, partitionLeaderEpoch, firstBatchRecords));\n            fileRecords.append(MemoryRecords.withTransactionalRecords(magic, 27L, compression, producerId,\n                    producerEpoch, baseSequence + firstBatchRecords.length, partitionLeaderEpoch, secondBatchRecords));\n            fileRecords.flush();\n\n            FileLogInputStream logInputStream = new FileLogInputStream(fileRecords, 0, fileRecords.sizeInBytes());\n\n            FileChannelRecordBatch firstBatch = logInputStream.nextBatch();\n            assertProducerData(firstBatch, producerId, producerEpoch, baseSequence, false, firstBatchRecords);\n            assertGenericRecordBatchData(firstBatch, 15L, 3241324L, firstBatchRecords);\n            assertEquals(partitionLeaderEpoch, firstBatch.partitionLeaderEpoch());\n\n            FileChannelRecordBatch secondBatch = logInputStream.nextBatch();\n            assertProducerData(secondBatch, producerId, producerEpoch, baseSequence + firstBatchRecords.length,\n                    true, secondBatchRecords);\n            assertGenericRecordBatchData(secondBatch, 27L, 238423489L, secondBatchRecords);\n            assertEquals(partitionLeaderEpoch, secondBatch.partitionLeaderEpoch());\n\n            assertNull(logInputStream.nextBatch());\n        }\n    }\n","date":"2018-04-30 23:59:04","endLine":184,"groupId":"2721","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testBatchIterationV2","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/95/b2a0c89c67a35a894c3aa03542893be695a563.src","preCode":"    public void testBatchIterationV2() throws IOException {\n        if (magic != MAGIC_VALUE_V2)\n            return;\n\n        try (FileRecords fileRecords = FileRecords.open(tempFile())) {\n            long producerId = 83843L;\n            short producerEpoch = 15;\n            int baseSequence = 234;\n            int partitionLeaderEpoch = 9832;\n\n            SimpleRecord[] firstBatchRecords = new SimpleRecord[]{\n                new SimpleRecord(3241324L, \"a\".getBytes(), \"1\".getBytes()),\n                new SimpleRecord(234280L, \"b\".getBytes(), \"2\".getBytes())\n\n            };\n            SimpleRecord[] secondBatchRecords = new SimpleRecord[]{\n                new SimpleRecord(238423489L, \"c\".getBytes(), \"3\".getBytes()),\n                new SimpleRecord(897839L, null, \"4\".getBytes()),\n                new SimpleRecord(8234020L, \"e\".getBytes(), null)\n            };\n\n            fileRecords.append(MemoryRecords.withIdempotentRecords(magic, 15L, compression, producerId,\n                    producerEpoch, baseSequence, partitionLeaderEpoch, firstBatchRecords));\n            fileRecords.append(MemoryRecords.withTransactionalRecords(magic, 27L, compression, producerId,\n                    producerEpoch, baseSequence + firstBatchRecords.length, partitionLeaderEpoch, secondBatchRecords));\n            fileRecords.flush();\n\n            FileLogInputStream logInputStream = new FileLogInputStream(fileRecords.channel(), 0,\n                    fileRecords.sizeInBytes());\n\n            FileChannelRecordBatch firstBatch = logInputStream.nextBatch();\n            assertProducerData(firstBatch, producerId, producerEpoch, baseSequence, false, firstBatchRecords);\n            assertGenericRecordBatchData(firstBatch, 15L, 3241324L, firstBatchRecords);\n            assertEquals(partitionLeaderEpoch, firstBatch.partitionLeaderEpoch());\n\n            FileChannelRecordBatch secondBatch = logInputStream.nextBatch();\n            assertProducerData(secondBatch, producerId, producerEpoch, baseSequence + firstBatchRecords.length,\n                    true, secondBatchRecords);\n            assertGenericRecordBatchData(secondBatch, 27L, 238423489L, secondBatchRecords);\n            assertEquals(partitionLeaderEpoch, secondBatch.partitionLeaderEpoch());\n\n            assertNull(logInputStream.nextBatch());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/FileLogInputStreamTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":142,"status":"M"},{"authorDate":"2018-04-30 23:59:04","commitOrder":2,"curCode":"    public void testBatchIterationIncompleteBatch() throws IOException {\n        try (FileRecords fileRecords = FileRecords.open(tempFile())) {\n            SimpleRecord firstBatchRecord = new SimpleRecord(100L, \"foo\".getBytes());\n            SimpleRecord secondBatchRecord = new SimpleRecord(200L, \"bar\".getBytes());\n\n            fileRecords.append(MemoryRecords.withRecords(magic, 0L, compression, CREATE_TIME, firstBatchRecord));\n            fileRecords.append(MemoryRecords.withRecords(magic, 1L, compression, CREATE_TIME, secondBatchRecord));\n            fileRecords.flush();\n            fileRecords.truncateTo(fileRecords.sizeInBytes() - 13);\n\n            FileLogInputStream logInputStream = new FileLogInputStream(fileRecords, 0, fileRecords.sizeInBytes());\n\n            FileChannelRecordBatch firstBatch = logInputStream.nextBatch();\n            assertNoProducerData(firstBatch);\n            assertGenericRecordBatchData(firstBatch, 0L, 100L, firstBatchRecord);\n\n            assertNull(logInputStream.nextBatch());\n        }\n    }\n","date":"2018-04-30 23:59:04","endLine":205,"groupId":"1553","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testBatchIterationIncompleteBatch","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/95/b2a0c89c67a35a894c3aa03542893be695a563.src","preCode":"    public void testBatchIterationIncompleteBatch() throws IOException {\n        try (FileRecords fileRecords = FileRecords.open(tempFile())) {\n            SimpleRecord firstBatchRecord = new SimpleRecord(100L, \"foo\".getBytes());\n            SimpleRecord secondBatchRecord = new SimpleRecord(200L, \"bar\".getBytes());\n\n            fileRecords.append(MemoryRecords.withRecords(magic, 0L, compression, CREATE_TIME, firstBatchRecord));\n            fileRecords.append(MemoryRecords.withRecords(magic, 1L, compression, CREATE_TIME, secondBatchRecord));\n            fileRecords.flush();\n            fileRecords.truncateTo(fileRecords.sizeInBytes() - 13);\n\n            FileLogInputStream logInputStream = new FileLogInputStream(fileRecords.channel(), 0,\n                    fileRecords.sizeInBytes());\n\n            FileChannelRecordBatch firstBatch = logInputStream.nextBatch();\n            assertNoProducerData(firstBatch);\n            assertGenericRecordBatchData(firstBatch, 0L, 100L, firstBatchRecord);\n\n            assertNull(logInputStream.nextBatch());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/FileLogInputStreamTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":187,"status":"M"}],"commitId":"f467c9c2438a8b182083879927ff171a6a2c6f2f","commitMessage":"@@@MINOR: Ensure exception messages include partition/segment info when possible (#4907)\n\nReviewers: Anna Povzner <anna@confluent.io>.  Ismael Juma <ismael@juma.me.uk>","date":"2018-04-30 23:59:04","modifiedFileCount":"4","status":"M","submitter":"Jason Gustafson"},{"authorTime":"2018-10-10 08:13:33","codes":[{"authorDate":"2018-04-30 23:59:04","commitOrder":3,"curCode":"    public void testBatchIterationV2() throws IOException {\n        if (magic != MAGIC_VALUE_V2)\n            return;\n\n        try (FileRecords fileRecords = FileRecords.open(tempFile())) {\n            long producerId = 83843L;\n            short producerEpoch = 15;\n            int baseSequence = 234;\n            int partitionLeaderEpoch = 9832;\n\n            SimpleRecord[] firstBatchRecords = new SimpleRecord[]{\n                new SimpleRecord(3241324L, \"a\".getBytes(), \"1\".getBytes()),\n                new SimpleRecord(234280L, \"b\".getBytes(), \"2\".getBytes())\n\n            };\n            SimpleRecord[] secondBatchRecords = new SimpleRecord[]{\n                new SimpleRecord(238423489L, \"c\".getBytes(), \"3\".getBytes()),\n                new SimpleRecord(897839L, null, \"4\".getBytes()),\n                new SimpleRecord(8234020L, \"e\".getBytes(), null)\n            };\n\n            fileRecords.append(MemoryRecords.withIdempotentRecords(magic, 15L, compression, producerId,\n                    producerEpoch, baseSequence, partitionLeaderEpoch, firstBatchRecords));\n            fileRecords.append(MemoryRecords.withTransactionalRecords(magic, 27L, compression, producerId,\n                    producerEpoch, baseSequence + firstBatchRecords.length, partitionLeaderEpoch, secondBatchRecords));\n            fileRecords.flush();\n\n            FileLogInputStream logInputStream = new FileLogInputStream(fileRecords, 0, fileRecords.sizeInBytes());\n\n            FileChannelRecordBatch firstBatch = logInputStream.nextBatch();\n            assertProducerData(firstBatch, producerId, producerEpoch, baseSequence, false, firstBatchRecords);\n            assertGenericRecordBatchData(firstBatch, 15L, 3241324L, firstBatchRecords);\n            assertEquals(partitionLeaderEpoch, firstBatch.partitionLeaderEpoch());\n\n            FileChannelRecordBatch secondBatch = logInputStream.nextBatch();\n            assertProducerData(secondBatch, producerId, producerEpoch, baseSequence + firstBatchRecords.length,\n                    true, secondBatchRecords);\n            assertGenericRecordBatchData(secondBatch, 27L, 238423489L, secondBatchRecords);\n            assertEquals(partitionLeaderEpoch, secondBatch.partitionLeaderEpoch());\n\n            assertNull(logInputStream.nextBatch());\n        }\n    }\n","date":"2018-04-30 23:59:04","endLine":184,"groupId":"2721","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testBatchIterationV2","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/95/b2a0c89c67a35a894c3aa03542893be695a563.src","preCode":"    public void testBatchIterationV2() throws IOException {\n        if (magic != MAGIC_VALUE_V2)\n            return;\n\n        try (FileRecords fileRecords = FileRecords.open(tempFile())) {\n            long producerId = 83843L;\n            short producerEpoch = 15;\n            int baseSequence = 234;\n            int partitionLeaderEpoch = 9832;\n\n            SimpleRecord[] firstBatchRecords = new SimpleRecord[]{\n                new SimpleRecord(3241324L, \"a\".getBytes(), \"1\".getBytes()),\n                new SimpleRecord(234280L, \"b\".getBytes(), \"2\".getBytes())\n\n            };\n            SimpleRecord[] secondBatchRecords = new SimpleRecord[]{\n                new SimpleRecord(238423489L, \"c\".getBytes(), \"3\".getBytes()),\n                new SimpleRecord(897839L, null, \"4\".getBytes()),\n                new SimpleRecord(8234020L, \"e\".getBytes(), null)\n            };\n\n            fileRecords.append(MemoryRecords.withIdempotentRecords(magic, 15L, compression, producerId,\n                    producerEpoch, baseSequence, partitionLeaderEpoch, firstBatchRecords));\n            fileRecords.append(MemoryRecords.withTransactionalRecords(magic, 27L, compression, producerId,\n                    producerEpoch, baseSequence + firstBatchRecords.length, partitionLeaderEpoch, secondBatchRecords));\n            fileRecords.flush();\n\n            FileLogInputStream logInputStream = new FileLogInputStream(fileRecords, 0, fileRecords.sizeInBytes());\n\n            FileChannelRecordBatch firstBatch = logInputStream.nextBatch();\n            assertProducerData(firstBatch, producerId, producerEpoch, baseSequence, false, firstBatchRecords);\n            assertGenericRecordBatchData(firstBatch, 15L, 3241324L, firstBatchRecords);\n            assertEquals(partitionLeaderEpoch, firstBatch.partitionLeaderEpoch());\n\n            FileChannelRecordBatch secondBatch = logInputStream.nextBatch();\n            assertProducerData(secondBatch, producerId, producerEpoch, baseSequence + firstBatchRecords.length,\n                    true, secondBatchRecords);\n            assertGenericRecordBatchData(secondBatch, 27L, 238423489L, secondBatchRecords);\n            assertEquals(partitionLeaderEpoch, secondBatch.partitionLeaderEpoch());\n\n            assertNull(logInputStream.nextBatch());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/FileLogInputStreamTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":142,"status":"N"},{"authorDate":"2018-10-10 08:13:33","commitOrder":3,"curCode":"    public void testBatchIterationIncompleteBatch() throws IOException {\n        if (compression == CompressionType.ZSTD && magic < MAGIC_VALUE_V2)\n            return;\n\n        try (FileRecords fileRecords = FileRecords.open(tempFile())) {\n            SimpleRecord firstBatchRecord = new SimpleRecord(100L, \"foo\".getBytes());\n            SimpleRecord secondBatchRecord = new SimpleRecord(200L, \"bar\".getBytes());\n\n            fileRecords.append(MemoryRecords.withRecords(magic, 0L, compression, CREATE_TIME, firstBatchRecord));\n            fileRecords.append(MemoryRecords.withRecords(magic, 1L, compression, CREATE_TIME, secondBatchRecord));\n            fileRecords.flush();\n            fileRecords.truncateTo(fileRecords.sizeInBytes() - 13);\n\n            FileLogInputStream logInputStream = new FileLogInputStream(fileRecords, 0, fileRecords.sizeInBytes());\n\n            FileChannelRecordBatch firstBatch = logInputStream.nextBatch();\n            assertNoProducerData(firstBatch);\n            assertGenericRecordBatchData(firstBatch, 0L, 100L, firstBatchRecord);\n\n            assertNull(logInputStream.nextBatch());\n        }\n    }\n","date":"2018-10-10 08:13:33","endLine":217,"groupId":"1553","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testBatchIterationIncompleteBatch","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/78/3a5b531ef27d5f5845f8f48960f9bb38980e54.src","preCode":"    public void testBatchIterationIncompleteBatch() throws IOException {\n        try (FileRecords fileRecords = FileRecords.open(tempFile())) {\n            SimpleRecord firstBatchRecord = new SimpleRecord(100L, \"foo\".getBytes());\n            SimpleRecord secondBatchRecord = new SimpleRecord(200L, \"bar\".getBytes());\n\n            fileRecords.append(MemoryRecords.withRecords(magic, 0L, compression, CREATE_TIME, firstBatchRecord));\n            fileRecords.append(MemoryRecords.withRecords(magic, 1L, compression, CREATE_TIME, secondBatchRecord));\n            fileRecords.flush();\n            fileRecords.truncateTo(fileRecords.sizeInBytes() - 13);\n\n            FileLogInputStream logInputStream = new FileLogInputStream(fileRecords, 0, fileRecords.sizeInBytes());\n\n            FileChannelRecordBatch firstBatch = logInputStream.nextBatch();\n            assertNoProducerData(firstBatch);\n            assertGenericRecordBatchData(firstBatch, 0L, 100L, firstBatchRecord);\n\n            assertNull(logInputStream.nextBatch());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/FileLogInputStreamTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":196,"status":"M"}],"commitId":"741cb761c5239297029a446518c332f6c4ed08f6","commitMessage":"@@@KAFKA-4514; Add Codec for ZStandard Compression (#2267)\n\nThis patch adds support for zstandard compression to Kafka as documented in KIP-110: https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression. \n\nReviewers: Ivan Babrou <ibobrik@gmail.com>.  Ismael Juma <ismael@juma.me.uk>.  Jason Gustafson <jason@confluent.io>","date":"2018-10-10 08:13:33","modifiedFileCount":"19","status":"M","submitter":"Lee Dongjin"},{"authorTime":"2021-01-14 08:17:45","codes":[{"authorDate":"2021-01-14 08:17:45","commitOrder":4,"curCode":"    public void testBatchIterationV2(Args args) throws IOException {\n        CompressionType compression = args.compression;\n        byte magic = args.magic;\n        if (magic != MAGIC_VALUE_V2)\n            return;\n\n        try (FileRecords fileRecords = FileRecords.open(tempFile())) {\n            long producerId = 83843L;\n            short producerEpoch = 15;\n            int baseSequence = 234;\n            int partitionLeaderEpoch = 9832;\n\n            SimpleRecord[] firstBatchRecords = new SimpleRecord[]{\n                new SimpleRecord(3241324L, \"a\".getBytes(), \"1\".getBytes()),\n                new SimpleRecord(234280L, \"b\".getBytes(), \"2\".getBytes())\n            };\n\n            SimpleRecord[] secondBatchRecords = new SimpleRecord[]{\n                new SimpleRecord(238423489L, \"c\".getBytes(), \"3\".getBytes()),\n                new SimpleRecord(897839L, null, \"4\".getBytes()),\n                new SimpleRecord(8234020L, \"e\".getBytes(), null)\n            };\n\n            fileRecords.append(MemoryRecords.withIdempotentRecords(magic, 15L, compression, producerId,\n                    producerEpoch, baseSequence, partitionLeaderEpoch, firstBatchRecords));\n            fileRecords.append(MemoryRecords.withTransactionalRecords(magic, 27L, compression, producerId,\n                    producerEpoch, baseSequence + firstBatchRecords.length, partitionLeaderEpoch, secondBatchRecords));\n            fileRecords.flush();\n\n            FileLogInputStream logInputStream = new FileLogInputStream(fileRecords, 0, fileRecords.sizeInBytes());\n\n            FileChannelRecordBatch firstBatch = logInputStream.nextBatch();\n            assertProducerData(firstBatch, producerId, producerEpoch, baseSequence, false, firstBatchRecords);\n            assertGenericRecordBatchData(args, firstBatch, 15L, 3241324L, firstBatchRecords);\n            assertEquals(partitionLeaderEpoch, firstBatch.partitionLeaderEpoch());\n\n            FileChannelRecordBatch secondBatch = logInputStream.nextBatch();\n            assertProducerData(secondBatch, producerId, producerEpoch, baseSequence + firstBatchRecords.length,\n                    true, secondBatchRecords);\n            assertGenericRecordBatchData(args, secondBatch, 27L, 238423489L, secondBatchRecords);\n            assertEquals(partitionLeaderEpoch, secondBatch.partitionLeaderEpoch());\n\n            assertNull(logInputStream.nextBatch());\n        }\n    }\n","date":"2021-01-14 08:17:45","endLine":227,"groupId":"103345","id":7,"instanceNumber":1,"isCurCommit":1,"methodName":"testBatchIterationV2","params":"(Argsargs)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/8e/204a942a8602cd37163fc17a08abf74149611b.src","preCode":"    public void testBatchIterationV2() throws IOException {\n        if (magic != MAGIC_VALUE_V2)\n            return;\n\n        try (FileRecords fileRecords = FileRecords.open(tempFile())) {\n            long producerId = 83843L;\n            short producerEpoch = 15;\n            int baseSequence = 234;\n            int partitionLeaderEpoch = 9832;\n\n            SimpleRecord[] firstBatchRecords = new SimpleRecord[]{\n                new SimpleRecord(3241324L, \"a\".getBytes(), \"1\".getBytes()),\n                new SimpleRecord(234280L, \"b\".getBytes(), \"2\".getBytes())\n            };\n\n            SimpleRecord[] secondBatchRecords = new SimpleRecord[]{\n                new SimpleRecord(238423489L, \"c\".getBytes(), \"3\".getBytes()),\n                new SimpleRecord(897839L, null, \"4\".getBytes()),\n                new SimpleRecord(8234020L, \"e\".getBytes(), null)\n            };\n\n            fileRecords.append(MemoryRecords.withIdempotentRecords(magic, 15L, compression, producerId,\n                    producerEpoch, baseSequence, partitionLeaderEpoch, firstBatchRecords));\n            fileRecords.append(MemoryRecords.withTransactionalRecords(magic, 27L, compression, producerId,\n                    producerEpoch, baseSequence + firstBatchRecords.length, partitionLeaderEpoch, secondBatchRecords));\n            fileRecords.flush();\n\n            FileLogInputStream logInputStream = new FileLogInputStream(fileRecords, 0, fileRecords.sizeInBytes());\n\n            FileChannelRecordBatch firstBatch = logInputStream.nextBatch();\n            assertProducerData(firstBatch, producerId, producerEpoch, baseSequence, false, firstBatchRecords);\n            assertGenericRecordBatchData(firstBatch, 15L, 3241324L, firstBatchRecords);\n            assertEquals(partitionLeaderEpoch, firstBatch.partitionLeaderEpoch());\n\n            FileChannelRecordBatch secondBatch = logInputStream.nextBatch();\n            assertProducerData(secondBatch, producerId, producerEpoch, baseSequence + firstBatchRecords.length,\n                    true, secondBatchRecords);\n            assertGenericRecordBatchData(secondBatch, 27L, 238423489L, secondBatchRecords);\n            assertEquals(partitionLeaderEpoch, secondBatch.partitionLeaderEpoch());\n\n            assertNull(logInputStream.nextBatch());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/FileLogInputStreamTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":183,"status":"M"},{"authorDate":"2021-01-14 08:17:45","commitOrder":4,"curCode":"    public void testBatchIterationIncompleteBatch(Args args) throws IOException {\n        CompressionType compression = args.compression;\n        byte magic = args.magic;\n        if (compression == CompressionType.ZSTD && magic < MAGIC_VALUE_V2)\n            return;\n\n        try (FileRecords fileRecords = FileRecords.open(tempFile())) {\n            SimpleRecord firstBatchRecord = new SimpleRecord(100L, \"foo\".getBytes());\n            SimpleRecord secondBatchRecord = new SimpleRecord(200L, \"bar\".getBytes());\n\n            fileRecords.append(MemoryRecords.withRecords(magic, 0L, compression, CREATE_TIME, firstBatchRecord));\n            fileRecords.append(MemoryRecords.withRecords(magic, 1L, compression, CREATE_TIME, secondBatchRecord));\n            fileRecords.flush();\n            fileRecords.truncateTo(fileRecords.sizeInBytes() - 13);\n\n            FileLogInputStream logInputStream = new FileLogInputStream(fileRecords, 0, fileRecords.sizeInBytes());\n\n            FileChannelRecordBatch firstBatch = logInputStream.nextBatch();\n            assertNoProducerData(firstBatch);\n            assertGenericRecordBatchData(args, firstBatch, 0L, 100L, firstBatchRecord);\n\n            assertNull(logInputStream.nextBatch());\n        }\n    }\n","date":"2021-01-14 08:17:45","endLine":254,"groupId":"103345","id":8,"instanceNumber":2,"isCurCommit":1,"methodName":"testBatchIterationIncompleteBatch","params":"(Argsargs)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/8e/204a942a8602cd37163fc17a08abf74149611b.src","preCode":"    public void testBatchIterationIncompleteBatch() throws IOException {\n        if (compression == CompressionType.ZSTD && magic < MAGIC_VALUE_V2)\n            return;\n\n        try (FileRecords fileRecords = FileRecords.open(tempFile())) {\n            SimpleRecord firstBatchRecord = new SimpleRecord(100L, \"foo\".getBytes());\n            SimpleRecord secondBatchRecord = new SimpleRecord(200L, \"bar\".getBytes());\n\n            fileRecords.append(MemoryRecords.withRecords(magic, 0L, compression, CREATE_TIME, firstBatchRecord));\n            fileRecords.append(MemoryRecords.withRecords(magic, 1L, compression, CREATE_TIME, secondBatchRecord));\n            fileRecords.flush();\n            fileRecords.truncateTo(fileRecords.sizeInBytes() - 13);\n\n            FileLogInputStream logInputStream = new FileLogInputStream(fileRecords, 0, fileRecords.sizeInBytes());\n\n            FileChannelRecordBatch firstBatch = logInputStream.nextBatch();\n            assertNoProducerData(firstBatch);\n            assertGenericRecordBatchData(firstBatch, 0L, 100L, firstBatchRecord);\n\n            assertNull(logInputStream.nextBatch());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/FileLogInputStreamTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":231,"status":"M"}],"commitId":"52b8aa0fdce1872b5b525b62dc3ac2241cfaa379","commitMessage":"@@@KAFKA-7340: Migrate clients module to JUnit 5 (#9874)\n\n* Use the packages/classes from JUnit 5\n* Move description in `assert` methods to last parameter\n* Convert parameterized tests so that they work with JUnit 5\n* Remove `hamcrest`.  it didn't seem to add much value\n* Fix `Utils.mkEntry` to have correct `equals` implementation\n* Add a missing `@Test` annotation in `SslSelectorTest` override\n* Adjust regex in `SaslAuthenticatorTest` due to small change in the\nassert failure string in JUnit 5\n\nReviewers: Chia-Ping Tsai <chia7712@gmail.com>","date":"2021-01-14 08:17:45","modifiedFileCount":"254","status":"M","submitter":"Ismael Juma"}]
