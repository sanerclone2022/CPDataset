[{"authorTime":"2016-12-14 02:26:25","codes":[{"authorDate":"2016-12-14 02:26:25","commitOrder":1,"curCode":"    public void buildUsingLogAppendTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, Record.MAGIC_VALUE_V1, compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, buffer.capacity());\n        builder.append(0L, 0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(1L, 0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(2L, 0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        Iterator<Record> iterator = records.records();\n        while (iterator.hasNext()) {\n            Record record = iterator.next();\n            assertEquals(TimestampType.LOG_APPEND_TIME, record.timestampType());\n            assertEquals(logAppendTime, record.timestamp());\n        }\n    }\n","date":"2016-12-14 02:26:25","endLine":130,"groupId":"22628","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"buildUsingLogAppendTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/40/fa212e215f27fb613cef27117ac83b5ff6d6e8.src","preCode":"    public void buildUsingLogAppendTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, Record.MAGIC_VALUE_V1, compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, buffer.capacity());\n        builder.append(0L, 0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(1L, 0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(2L, 0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        Iterator<Record> iterator = records.records();\n        while (iterator.hasNext()) {\n            Record record = iterator.next();\n            assertEquals(TimestampType.LOG_APPEND_TIME, record.timestampType());\n            assertEquals(logAppendTime, record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":107,"status":"B"},{"authorDate":"2016-12-14 02:26:25","commitOrder":1,"curCode":"    public void buildUsingCreateTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, Record.MAGIC_VALUE_V1, compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, buffer.capacity());\n        builder.append(0L, 0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(1L, 2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(2L, 1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(2L, info.maxTimestamp);\n\n        if (compressionType == CompressionType.NONE)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        Iterator<Record> iterator = records.records();\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        while (iterator.hasNext()) {\n            Record record = iterator.next();\n            assertEquals(TimestampType.CREATE_TIME, record.timestampType());\n            assertEquals(expectedTimestamps[i++], record.timestamp());\n        }\n    }\n","date":"2016-12-14 02:26:25","endLine":188,"groupId":"22628","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"buildUsingCreateTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/40/fa212e215f27fb613cef27117ac83b5ff6d6e8.src","preCode":"    public void buildUsingCreateTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, Record.MAGIC_VALUE_V1, compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, buffer.capacity());\n        builder.append(0L, 0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(1L, 2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(2L, 1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(2L, info.maxTimestamp);\n\n        if (compressionType == CompressionType.NONE)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        Iterator<Record> iterator = records.records();\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        while (iterator.hasNext()) {\n            Record record = iterator.next();\n            assertEquals(TimestampType.CREATE_TIME, record.timestampType());\n            assertEquals(expectedTimestamps[i++], record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":160,"status":"B"}],"commitId":"67f1e5b91bf073151ff57d5d656693e385726697","commitMessage":"@@@KAFKA-4390; Replace MessageSet usage with client-side alternatives\n\nAuthor: Jason Gustafson <jason@confluent.io>\n\nReviewers: Ismael Juma <ismael@juma.me.uk>.  Guozhang Wang <wangguoz@gmail.com>.  Jun Rao <junrao@gmail.com>\n\nCloses #2140 from hachikuji/KAFKA4390\n","date":"2016-12-14 02:26:25","modifiedFileCount":"25","status":"B","submitter":"Jason Gustafson"},{"authorTime":"2016-12-17 02:41:27","codes":[{"authorDate":"2016-12-17 02:41:27","commitOrder":2,"curCode":"    public void buildUsingLogAppendTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, Record.MAGIC_VALUE_V1, compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, buffer.capacity());\n        builder.append(0L, 0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(1L, 0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(2L, 0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        for (Record record : records.records()) {\n            assertEquals(TimestampType.LOG_APPEND_TIME, record.timestampType());\n            assertEquals(logAppendTime, record.timestamp());\n        }\n    }\n","date":"2016-12-17 02:41:27","endLine":127,"groupId":"22628","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"buildUsingLogAppendTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/a5/2976b7485de9943dde766a2a62907c4776e3b2.src","preCode":"    public void buildUsingLogAppendTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, Record.MAGIC_VALUE_V1, compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, buffer.capacity());\n        builder.append(0L, 0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(1L, 0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(2L, 0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        Iterator<Record> iterator = records.records();\n        while (iterator.hasNext()) {\n            Record record = iterator.next();\n            assertEquals(TimestampType.LOG_APPEND_TIME, record.timestampType());\n            assertEquals(logAppendTime, record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":106,"status":"M"},{"authorDate":"2016-12-17 02:41:27","commitOrder":2,"curCode":"    public void buildUsingCreateTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, Record.MAGIC_VALUE_V1, compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, buffer.capacity());\n        builder.append(0L, 0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(1L, 2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(2L, 1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(2L, info.maxTimestamp);\n\n        if (compressionType == CompressionType.NONE)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        for (Record record : records.records()) {\n            assertEquals(TimestampType.CREATE_TIME, record.timestampType());\n            assertEquals(expectedTimestamps[i++], record.timestamp());\n        }\n    }\n","date":"2016-12-17 02:41:27","endLine":181,"groupId":"22628","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"buildUsingCreateTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/a5/2976b7485de9943dde766a2a62907c4776e3b2.src","preCode":"    public void buildUsingCreateTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, Record.MAGIC_VALUE_V1, compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, buffer.capacity());\n        builder.append(0L, 0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(1L, 2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(2L, 1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(2L, info.maxTimestamp);\n\n        if (compressionType == CompressionType.NONE)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        Iterator<Record> iterator = records.records();\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        while (iterator.hasNext()) {\n            Record record = iterator.next();\n            assertEquals(TimestampType.CREATE_TIME, record.timestampType());\n            assertEquals(expectedTimestamps[i++], record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":155,"status":"M"}],"commitId":"b58b6a1bef0ecdc2107a415e222af099fcd9bce3","commitMessage":"@@@MINOR: Replace deepIterator/shallowIterator with deepEntries/shallowEntries\n\nThe latter return `Iterable` instead of `Iterator` so that enhanced foreach can be used\nin Java.\n\nAuthor: Ismael Juma <ismael@juma.me.uk>\n\nReviewers: Jason Gustafson <jason@confluent.io>\n\nCloses #2261 from ijuma/deepEntries-shallowEntries\n","date":"2016-12-17 02:41:27","modifiedFileCount":"11","status":"M","submitter":"Ismael Juma"},{"authorTime":"2016-12-21 08:07:10","codes":[{"authorDate":"2016-12-21 08:07:10","commitOrder":3,"curCode":"    public void buildUsingLogAppendTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, Record.MAGIC_VALUE_V1, compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        for (Record record : records.records()) {\n            assertEquals(TimestampType.LOG_APPEND_TIME, record.timestampType());\n            assertEquals(logAppendTime, record.timestamp());\n        }\n    }\n","date":"2016-12-21 08:07:10","endLine":125,"groupId":"14813","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"buildUsingLogAppendTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/03/4faf631dfd1a42d4aa6ec144e8cf9bf804e39b.src","preCode":"    public void buildUsingLogAppendTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, Record.MAGIC_VALUE_V1, compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, buffer.capacity());\n        builder.append(0L, 0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(1L, 0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(2L, 0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        for (Record record : records.records()) {\n            assertEquals(TimestampType.LOG_APPEND_TIME, record.timestampType());\n            assertEquals(logAppendTime, record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":104,"status":"M"},{"authorDate":"2016-12-21 08:07:10","commitOrder":3,"curCode":"    public void buildUsingCreateTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, Record.MAGIC_VALUE_V1, compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(2L, info.maxTimestamp);\n\n        if (compressionType == CompressionType.NONE)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        for (Record record : records.records()) {\n            assertEquals(TimestampType.CREATE_TIME, record.timestampType());\n            assertEquals(expectedTimestamps[i++], record.timestamp());\n        }\n    }\n","date":"2016-12-21 08:07:10","endLine":178,"groupId":"14813","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"buildUsingCreateTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/03/4faf631dfd1a42d4aa6ec144e8cf9bf804e39b.src","preCode":"    public void buildUsingCreateTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, Record.MAGIC_VALUE_V1, compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, buffer.capacity());\n        builder.append(0L, 0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(1L, 2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(2L, 1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(2L, info.maxTimestamp);\n\n        if (compressionType == CompressionType.NONE)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        for (Record record : records.records()) {\n            assertEquals(TimestampType.CREATE_TIME, record.timestampType());\n            assertEquals(expectedTimestamps[i++], record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":152,"status":"M"}],"commitId":"0f86dbe89da19ed1cc9142a5362cfa2fe3bc48ee","commitMessage":"@@@MINOR: Support auto-incrementing offsets in MemoryRecordsBuilder\n\nAuthor: Jason Gustafson <jason@confluent.io>\n\nReviewers: Ismael Juma <ismael@juma.me.uk>\n\nCloses #2282 from hachikuji/builder-autoincrement-offsets\n","date":"2016-12-21 08:07:10","modifiedFileCount":"8","status":"M","submitter":"Jason Gustafson"},{"authorTime":"2017-03-25 03:38:36","codes":[{"authorDate":"2017-03-25 03:38:36","commitOrder":4,"curCode":"    public void buildUsingLogAppendTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V1, compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH,\n                RecordBatch.NO_SEQUENCE, false, RecordBatch.UNKNOWN_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.LOG_APPEND_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(logAppendTime, record.timestamp());\n        }\n    }\n","date":"2017-03-25 03:38:43","endLine":230,"groupId":"592","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"buildUsingLogAppendTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/ef/48783b2dadb03529e02f87832852a4a519b253.src","preCode":"    public void buildUsingLogAppendTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, Record.MAGIC_VALUE_V1, compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        for (Record record : records.records()) {\n            assertEquals(TimestampType.LOG_APPEND_TIME, record.timestampType());\n            assertEquals(logAppendTime, record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":207,"status":"M"},{"authorDate":"2017-03-25 03:38:36","commitOrder":4,"curCode":"    public void buildUsingCreateTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V1, compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE,\n                false, RecordBatch.UNKNOWN_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(2L, info.maxTimestamp);\n\n        if (compressionType == CompressionType.NONE)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.CREATE_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(expectedTimestamps[i++], record.timestamp());\n        }\n    }\n","date":"2017-03-25 03:38:43","endLine":261,"groupId":"8417","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"buildUsingCreateTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/ef/48783b2dadb03529e02f87832852a4a519b253.src","preCode":"    public void buildUsingCreateTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, Record.MAGIC_VALUE_V1, compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(2L, info.maxTimestamp);\n\n        if (compressionType == CompressionType.NONE)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        for (Record record : records.records()) {\n            assertEquals(TimestampType.CREATE_TIME, record.timestampType());\n            assertEquals(expectedTimestamps[i++], record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":233,"status":"M"}],"commitId":"5bd06f1d542e6b588a1d402d059bc24690017d32","commitMessage":"@@@KAFKA-4816; Message format changes for idempotent/transactional producer (KIP-98)\n\nAuthor: Jason Gustafson <jason@confluent.io>\n\nReviewers: Jun Rao <junrao@gmail.com>.  Apurva Mehta <apurva@confluent.io>.  Guozhang Wang <wangguoz@gmail.com>.  Ismael Juma <ismael@juma.me.uk>\n\nCloses #2614 from hachikuji/exactly-once-message-format\n","date":"2017-03-25 03:38:43","modifiedFileCount":"55","status":"M","submitter":"Jason Gustafson"},{"authorTime":"2017-04-08 01:29:55","codes":[{"authorDate":"2017-04-08 01:29:55","commitOrder":5,"curCode":"    public void buildUsingLogAppendTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V1, compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH,\n                RecordBatch.NO_SEQUENCE, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        if (compressionType != CompressionType.NONE)\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(0L, info.shallowOffsetOfMaxTimestamp);\n\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.LOG_APPEND_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(logAppendTime, record.timestamp());\n        }\n    }\n","date":"2017-04-08 01:29:55","endLine":233,"groupId":"592","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"buildUsingLogAppendTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/3c/c8c20d199ad042526676244ff7f5b1a685211e.src","preCode":"    public void buildUsingLogAppendTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V1, compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH,\n                RecordBatch.NO_SEQUENCE, false, RecordBatch.UNKNOWN_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.LOG_APPEND_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(logAppendTime, record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":207,"status":"M"},{"authorDate":"2017-04-08 01:29:55","commitOrder":5,"curCode":"    public void buildUsingCreateTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V1, compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE,\n                false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(2L, info.maxTimestamp);\n\n        if (compressionType == CompressionType.NONE)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.CREATE_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(expectedTimestamps[i++], record.timestamp());\n        }\n    }\n","date":"2017-04-08 01:29:55","endLine":264,"groupId":"8417","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"buildUsingCreateTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/3c/c8c20d199ad042526676244ff7f5b1a685211e.src","preCode":"    public void buildUsingCreateTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V1, compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE,\n                false, RecordBatch.UNKNOWN_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(2L, info.maxTimestamp);\n\n        if (compressionType == CompressionType.NONE)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.CREATE_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(expectedTimestamps[i++], record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":236,"status":"M"}],"commitId":"5cf64f06a877a181d12a2ae2390516ba1a572135","commitMessage":"@@@MINOR: Log append validation improvements\n\n- Consistent validation across different code paths in LogValidator\n- Validate baseOffset for message format V2\n- Flesh out LogValidatorTest to check producerId.  baseSequence.  producerEpoch and partitionLeaderEpoch.\n\nAuthor: Ismael Juma <ismael@juma.me.uk>\n\nReviewers: Jun Rao <junrao@gmail.com>\n\nCloses #2802 from ijuma/validate-base-offset\n","date":"2017-04-08 01:29:55","modifiedFileCount":"7","status":"M","submitter":"Ismael Juma"},{"authorTime":"2017-05-07 02:49:35","codes":[{"authorDate":"2017-05-07 02:49:35","commitOrder":6,"curCode":"    public void buildUsingLogAppendTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V1, compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH,\n                RecordBatch.NO_SEQUENCE, false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        if (compressionType != CompressionType.NONE)\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(0L, info.shallowOffsetOfMaxTimestamp);\n\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.LOG_APPEND_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(logAppendTime, record.timestamp());\n        }\n    }\n","date":"2017-05-07 02:49:35","endLine":290,"groupId":"592","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"buildUsingLogAppendTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/04/67522d5cf75d920c3642179ade0a65b0ec9b71.src","preCode":"    public void buildUsingLogAppendTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V1, compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH,\n                RecordBatch.NO_SEQUENCE, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        if (compressionType != CompressionType.NONE)\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(0L, info.shallowOffsetOfMaxTimestamp);\n\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.LOG_APPEND_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(logAppendTime, record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":264,"status":"M"},{"authorDate":"2017-05-07 02:49:35","commitOrder":6,"curCode":"    public void buildUsingCreateTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V1, compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE,\n                false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(2L, info.maxTimestamp);\n\n        if (compressionType == CompressionType.NONE)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.CREATE_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(expectedTimestamps[i++], record.timestamp());\n        }\n    }\n","date":"2017-05-07 02:49:35","endLine":321,"groupId":"8417","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"buildUsingCreateTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/04/67522d5cf75d920c3642179ade0a65b0ec9b71.src","preCode":"    public void buildUsingCreateTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V1, compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE,\n                false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(2L, info.maxTimestamp);\n\n        if (compressionType == CompressionType.NONE)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.CREATE_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(expectedTimestamps[i++], record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":293,"status":"M"}],"commitId":"e71dce89c0da50f3eccc47d0fc050c92d5a99b88","commitMessage":"@@@KAFKA-5121; Implement transaction index for KIP-98\n\nAuthor: Jason Gustafson <jason@confluent.io>\n\nReviewers: Ismael Juma <ismael@juma.me.uk>.  Jun Rao <junrao@gmail.com>\n\nCloses #2910 from hachikuji/eos-txn-index\n","date":"2017-05-07 02:49:35","modifiedFileCount":"23","status":"M","submitter":"Jason Gustafson"},{"authorTime":"2018-10-10 08:13:33","codes":[{"authorDate":"2018-10-10 08:13:33","commitOrder":7,"curCode":"    public void buildUsingLogAppendTime() {\n        expectExceptionWithZStd(compressionType, RecordBatch.MAGIC_VALUE_V1);\n\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V1, compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH,\n                RecordBatch.NO_SEQUENCE, false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        if (compressionType != CompressionType.NONE)\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(0L, info.shallowOffsetOfMaxTimestamp);\n\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.LOG_APPEND_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(logAppendTime, record.timestamp());\n        }\n    }\n","date":"2018-10-10 08:13:33","endLine":335,"groupId":"592","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"buildUsingLogAppendTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/55/2b4e6c42b0320519617a3e867e37621754b5b8.src","preCode":"    public void buildUsingLogAppendTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V1, compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH,\n                RecordBatch.NO_SEQUENCE, false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        if (compressionType != CompressionType.NONE)\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(0L, info.shallowOffsetOfMaxTimestamp);\n\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.LOG_APPEND_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(logAppendTime, record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":307,"status":"M"},{"authorDate":"2018-10-10 08:13:33","commitOrder":7,"curCode":"    public void buildUsingCreateTime() {\n        expectExceptionWithZStd(compressionType, RecordBatch.MAGIC_VALUE_V1);\n\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V1, compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE,\n                false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(2L, info.maxTimestamp);\n\n        if (compressionType == CompressionType.NONE)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.CREATE_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(expectedTimestamps[i++], record.timestamp());\n        }\n    }\n","date":"2018-10-10 08:13:33","endLine":368,"groupId":"8417","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"buildUsingCreateTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/55/2b4e6c42b0320519617a3e867e37621754b5b8.src","preCode":"    public void buildUsingCreateTime() {\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V1, compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE,\n                false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(2L, info.maxTimestamp);\n\n        if (compressionType == CompressionType.NONE)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.CREATE_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(expectedTimestamps[i++], record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":338,"status":"M"}],"commitId":"741cb761c5239297029a446518c332f6c4ed08f6","commitMessage":"@@@KAFKA-4514; Add Codec for ZStandard Compression (#2267)\n\nThis patch adds support for zstandard compression to Kafka as documented in KIP-110: https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression. \n\nReviewers: Ivan Babrou <ibobrik@gmail.com>.  Ismael Juma <ismael@juma.me.uk>.  Jason Gustafson <jason@confluent.io>","date":"2018-10-10 08:13:33","modifiedFileCount":"19","status":"M","submitter":"Lee Dongjin"},{"authorTime":"2019-02-12 14:06:14","codes":[{"authorDate":"2019-02-12 14:06:14","commitOrder":8,"curCode":"    public void buildUsingLogAppendTime() {\n        byte magic = RecordBatch.MAGIC_VALUE_V1;\n        assumeAtLeastV2OrNotZstd(magic);\n\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, magic, compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH,\n                RecordBatch.NO_SEQUENCE, false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        if (compressionType != CompressionType.NONE)\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(0L, info.shallowOffsetOfMaxTimestamp);\n\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.LOG_APPEND_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(logAppendTime, record.timestamp());\n        }\n    }\n","date":"2019-02-12 14:06:14","endLine":344,"groupId":"592","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"buildUsingLogAppendTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/52/2915f064034a48d7b82076dddd6e5f3622c726.src","preCode":"    public void buildUsingLogAppendTime() {\n        expectExceptionWithZStd(compressionType, RecordBatch.MAGIC_VALUE_V1);\n\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V1, compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH,\n                RecordBatch.NO_SEQUENCE, false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        if (compressionType != CompressionType.NONE)\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(0L, info.shallowOffsetOfMaxTimestamp);\n\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.LOG_APPEND_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(logAppendTime, record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":315,"status":"M"},{"authorDate":"2019-02-12 14:06:14","commitOrder":8,"curCode":"    public void buildUsingCreateTime() {\n        byte magic = RecordBatch.MAGIC_VALUE_V1;\n        assumeAtLeastV2OrNotZstd(magic);\n\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, magic, compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE,\n                false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(2L, info.maxTimestamp);\n\n        if (compressionType == CompressionType.NONE)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.CREATE_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(expectedTimestamps[i++], record.timestamp());\n        }\n    }\n","date":"2019-02-12 14:06:14","endLine":378,"groupId":"8417","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"buildUsingCreateTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/52/2915f064034a48d7b82076dddd6e5f3622c726.src","preCode":"    public void buildUsingCreateTime() {\n        expectExceptionWithZStd(compressionType, RecordBatch.MAGIC_VALUE_V1);\n\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, RecordBatch.MAGIC_VALUE_V1, compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE,\n                false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(2L, info.maxTimestamp);\n\n        if (compressionType == CompressionType.NONE)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.CREATE_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(expectedTimestamps[i++], record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":347,"status":"M"}],"commitId":"c7f99bc2bd9af5eb6ca9e20a02d5806c52d434b3","commitMessage":"@@@MINOR: Update JUnit to 4.13 and annotate log cleaner integration test (#6248)\n\nJUnit 4.13 fixes the issue where `Category` and `Parameterized` annotations\ncould not be used together. It also deprecates `ExpectedException` and\n`assertThat`. Given this.  we:\n\n- Replace `ExpectedException` with the newly introduced `assertThrows`.\n- Replace `Assert.assertThat` with `MatcherAssert.assertThat`.\n- Annotate `AbstractLogCleanerIntegrationTest` with `IntegrationTest` category.\n\nReviewers: Ewen Cheslack-Postava <ewen@confluent.io>.  David Arthur <mumrah@gmail.com>","date":"2019-02-12 14:06:14","modifiedFileCount":"40","status":"M","submitter":"Ismael Juma"},{"authorTime":"2021-01-14 08:17:45","codes":[{"authorDate":"2021-01-14 08:17:45","commitOrder":9,"curCode":"    public void buildUsingLogAppendTime(Args args) {\n        byte magic = RecordBatch.MAGIC_VALUE_V1;\n        assumeAtLeastV2OrNotZstd(magic, args.compressionType);\n\n        ByteBuffer buffer = allocateBuffer(1024, args);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, magic, args.compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH,\n                RecordBatch.NO_SEQUENCE, false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        if (args.compressionType != CompressionType.NONE)\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(0L, info.shallowOffsetOfMaxTimestamp);\n\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.LOG_APPEND_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(logAppendTime, record.timestamp());\n        }\n    }\n","date":"2021-01-14 08:17:45","endLine":419,"groupId":"21206","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"buildUsingLogAppendTime","params":"(Argsargs)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/d6/a3801f2e2b011d1049c2879d9a284e26bce88b.src","preCode":"    public void buildUsingLogAppendTime() {\n        byte magic = RecordBatch.MAGIC_VALUE_V1;\n        assumeAtLeastV2OrNotZstd(magic);\n\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, magic, compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH,\n                RecordBatch.NO_SEQUENCE, false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        if (compressionType != CompressionType.NONE)\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(0L, info.shallowOffsetOfMaxTimestamp);\n\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.LOG_APPEND_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(logAppendTime, record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":391,"status":"M"},{"authorDate":"2021-01-14 08:17:45","commitOrder":9,"curCode":"    public void buildUsingCreateTime(Args args) {\n        byte magic = RecordBatch.MAGIC_VALUE_V1;\n        assumeAtLeastV2OrNotZstd(magic, args.compressionType);\n\n        ByteBuffer buffer = allocateBuffer(1024, args);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, magic, args.compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE,\n                false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(2L, info.maxTimestamp);\n\n        if (args.compressionType == CompressionType.NONE)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.CREATE_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(expectedTimestamps[i++], record.timestamp());\n        }\n    }\n","date":"2021-01-14 08:17:45","endLine":453,"groupId":"21206","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"buildUsingCreateTime","params":"(Argsargs)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/d6/a3801f2e2b011d1049c2879d9a284e26bce88b.src","preCode":"    public void buildUsingCreateTime() {\n        byte magic = RecordBatch.MAGIC_VALUE_V1;\n        assumeAtLeastV2OrNotZstd(magic);\n\n        ByteBuffer buffer = ByteBuffer.allocate(1024);\n        buffer.position(bufferOffset);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, magic, compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE,\n                false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(2L, info.maxTimestamp);\n\n        if (compressionType == CompressionType.NONE)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.CREATE_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(expectedTimestamps[i++], record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":423,"status":"M"}],"commitId":"52b8aa0fdce1872b5b525b62dc3ac2241cfaa379","commitMessage":"@@@KAFKA-7340: Migrate clients module to JUnit 5 (#9874)\n\n* Use the packages/classes from JUnit 5\n* Move description in `assert` methods to last parameter\n* Convert parameterized tests so that they work with JUnit 5\n* Remove `hamcrest`.  it didn't seem to add much value\n* Fix `Utils.mkEntry` to have correct `equals` implementation\n* Add a missing `@Test` annotation in `SslSelectorTest` override\n* Adjust regex in `SaslAuthenticatorTest` due to small change in the\nassert failure string in JUnit 5\n\nReviewers: Chia-Ping Tsai <chia7712@gmail.com>","date":"2021-01-14 08:17:45","modifiedFileCount":"254","status":"M","submitter":"Ismael Juma"},{"authorTime":"2021-02-18 11:15:56","codes":[{"authorDate":"2021-02-18 11:15:56","commitOrder":10,"curCode":"    public void buildUsingLogAppendTime(Args args) {\n        byte magic = args.magic;\n        ByteBuffer buffer = allocateBuffer(1024, args);\n        long logAppendTime = System.currentTimeMillis();\n\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, magic, args.compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH,\n                RecordBatch.NO_SEQUENCE, false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        if (args.compressionType == CompressionType.NONE && magic <= MAGIC_VALUE_V1)\n            assertEquals(0L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        for (RecordBatch batch : records.batches()) {\n            if (magic == MAGIC_VALUE_V0) {\n                assertEquals(TimestampType.NO_TIMESTAMP_TYPE, batch.timestampType());\n            } else {\n                assertEquals(TimestampType.LOG_APPEND_TIME, batch.timestampType());\n                for (Record record : batch)\n                    assertEquals(logAppendTime, record.timestamp());\n            }\n        }\n    }\n","date":"2021-02-18 11:15:56","endLine":391,"groupId":"103319","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"buildUsingLogAppendTime","params":"(Argsargs)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/13/eaa9d21cba1291906138fb0a625835796c4a33.src","preCode":"    public void buildUsingLogAppendTime(Args args) {\n        byte magic = RecordBatch.MAGIC_VALUE_V1;\n        assumeAtLeastV2OrNotZstd(magic, args.compressionType);\n\n        ByteBuffer buffer = allocateBuffer(1024, args);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, magic, args.compressionType,\n                TimestampType.LOG_APPEND_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH,\n                RecordBatch.NO_SEQUENCE, false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(0L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(0L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(logAppendTime, info.maxTimestamp);\n\n        if (args.compressionType != CompressionType.NONE)\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(0L, info.shallowOffsetOfMaxTimestamp);\n\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.LOG_APPEND_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(logAppendTime, record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":361,"status":"M"},{"authorDate":"2021-02-18 11:15:56","commitOrder":10,"curCode":"    public void buildUsingCreateTime(Args args) {\n        byte magic = args.magic;\n        ByteBuffer buffer = allocateBuffer(1024, args);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, magic, args.compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE,\n                false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        if (magic == MAGIC_VALUE_V0) {\n            assertEquals(-1, info.maxTimestamp);\n        } else {\n            assertEquals(2L, info.maxTimestamp);\n        }\n\n        if (args.compressionType == CompressionType.NONE && magic == MAGIC_VALUE_V1)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        for (RecordBatch batch : records.batches()) {\n            if (magic == MAGIC_VALUE_V0) {\n                assertEquals(TimestampType.NO_TIMESTAMP_TYPE, batch.timestampType());\n            } else {\n                assertEquals(TimestampType.CREATE_TIME, batch.timestampType());\n                for (Record record : batch)\n                    assertEquals(expectedTimestamps[i++], record.timestamp());\n            }\n        }\n    }\n","date":"2021-02-18 11:15:56","endLine":430,"groupId":"103319","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"buildUsingCreateTime","params":"(Argsargs)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/13/eaa9d21cba1291906138fb0a625835796c4a33.src","preCode":"    public void buildUsingCreateTime(Args args) {\n        byte magic = RecordBatch.MAGIC_VALUE_V1;\n        assumeAtLeastV2OrNotZstd(magic, args.compressionType);\n\n        ByteBuffer buffer = allocateBuffer(1024, args);\n\n        long logAppendTime = System.currentTimeMillis();\n        MemoryRecordsBuilder builder = new MemoryRecordsBuilder(buffer, magic, args.compressionType,\n                TimestampType.CREATE_TIME, 0L, logAppendTime, RecordBatch.NO_PRODUCER_ID, RecordBatch.NO_PRODUCER_EPOCH, RecordBatch.NO_SEQUENCE,\n                false, false, RecordBatch.NO_PARTITION_LEADER_EPOCH, buffer.capacity());\n        builder.append(0L, \"a\".getBytes(), \"1\".getBytes());\n        builder.append(2L, \"b\".getBytes(), \"2\".getBytes());\n        builder.append(1L, \"c\".getBytes(), \"3\".getBytes());\n        MemoryRecords records = builder.build();\n\n        MemoryRecordsBuilder.RecordsInfo info = builder.info();\n        assertEquals(2L, info.maxTimestamp);\n\n        if (args.compressionType == CompressionType.NONE)\n            assertEquals(1L, info.shallowOffsetOfMaxTimestamp);\n        else\n            assertEquals(2L, info.shallowOffsetOfMaxTimestamp);\n\n        int i = 0;\n        long[] expectedTimestamps = new long[] {0L, 2L, 1L};\n        for (RecordBatch batch : records.batches()) {\n            assertEquals(TimestampType.CREATE_TIME, batch.timestampType());\n            for (Record record : batch)\n                assertEquals(expectedTimestamps[i++], record.timestamp());\n        }\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/MemoryRecordsBuilderTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":394,"status":"M"}],"commitId":"46690113cd0066e57f914539978cfdf69ebbef63","commitMessage":"@@@KAFKA-10885 Refactor MemoryRecordsBuilderTest/MemoryRecordsTest to avoid a lot of? (#9906)\n\nReviewers: Ismael Juma <ismael@juma.me.uk>.  Chia-Ping Tsai <chia7712@gmail.com>","date":"2021-02-18 11:15:56","modifiedFileCount":"2","status":"M","submitter":"Geordie"}]
