[{"authorTime":"2020-12-08 06:06:25","codes":[{"authorDate":"2020-12-08 06:06:25","commitOrder":1,"curCode":"    public void testBatchWriteReadSnapshot() throws IOException {\n        Path tempDir = TestUtils.tempDirectory().toPath();\n        OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(10L, 3);\n        int bufferSize = 256;\n        int batchSize = 3;\n        int batches = 10;\n\n        try (FileRawSnapshotWriter snapshot = FileRawSnapshotWriter.create(tempDir, offsetAndEpoch)) {\n            for (int i = 0; i < batches; i++) {\n                ByteBuffer[] buffers = IntStream\n                    .range(0, batchSize)\n                    .mapToObj(ignore -> ByteBuffer.wrap(randomBytes(bufferSize))).toArray(ByteBuffer[]::new);\n\n                snapshot.append(buildRecords(buffers).buffer());\n            }\n\n            snapshot.freeze();\n        }\n\n        try (FileRawSnapshotReader snapshot = FileRawSnapshotReader.open(tempDir, offsetAndEpoch)) {\n            int countBatches = 0;\n            int countRecords = 0;\n            for (RecordBatch batch : snapshot) {\n                countBatches += 1;\n\n                Iterator<Record> records = batch.streamingIterator(new GrowableBufferSupplier());\n                while (records.hasNext()) {\n                    Record record = records.next();\n\n                    countRecords += 1;\n\n                    assertFalse(record.hasKey());\n                    assertTrue(record.hasValue());\n                    assertEquals(bufferSize, record.value().remaining());\n                }\n            }\n\n            assertEquals(batches, countBatches);\n            assertEquals(batches * batchSize, countRecords);\n        }\n    }\n","date":"2020-12-08 06:06:25","endLine":151,"groupId":"19990","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testBatchWriteReadSnapshot","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/e5/2943558f2cc81094838956385976e3cba28767.src","preCode":"    public void testBatchWriteReadSnapshot() throws IOException {\n        Path tempDir = TestUtils.tempDirectory().toPath();\n        OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(10L, 3);\n        int bufferSize = 256;\n        int batchSize = 3;\n        int batches = 10;\n\n        try (FileRawSnapshotWriter snapshot = FileRawSnapshotWriter.create(tempDir, offsetAndEpoch)) {\n            for (int i = 0; i < batches; i++) {\n                ByteBuffer[] buffers = IntStream\n                    .range(0, batchSize)\n                    .mapToObj(ignore -> ByteBuffer.wrap(randomBytes(bufferSize))).toArray(ByteBuffer[]::new);\n\n                snapshot.append(buildRecords(buffers).buffer());\n            }\n\n            snapshot.freeze();\n        }\n\n        try (FileRawSnapshotReader snapshot = FileRawSnapshotReader.open(tempDir, offsetAndEpoch)) {\n            int countBatches = 0;\n            int countRecords = 0;\n            for (RecordBatch batch : snapshot) {\n                countBatches += 1;\n\n                Iterator<Record> records = batch.streamingIterator(new GrowableBufferSupplier());\n                while (records.hasNext()) {\n                    Record record = records.next();\n\n                    countRecords += 1;\n\n                    assertFalse(record.hasKey());\n                    assertTrue(record.hasValue());\n                    assertEquals(bufferSize, record.value().remaining());\n                }\n            }\n\n            assertEquals(batches, countBatches);\n            assertEquals(batches * batchSize, countRecords);\n        }\n    }\n","realPath":"raft/src/test/java/org/apache/kafka/snapshot/FileRawSnapshotTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":111,"status":"B"},{"authorDate":"2020-12-08 06:06:25","commitOrder":1,"curCode":"    public void testBufferWriteReadSnapshot() throws IOException {\n        Path tempDir = TestUtils.tempDirectory().toPath();\n        OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(10L, 3);\n        int bufferSize = 256;\n        int batchSize = 3;\n        int batches = 10;\n        int expectedSize = 0;\n\n        try (FileRawSnapshotWriter snapshot = FileRawSnapshotWriter.create(tempDir, offsetAndEpoch)) {\n            for (int i = 0; i < batches; i++) {\n                ByteBuffer[] buffers = IntStream\n                    .range(0, batchSize)\n                    .mapToObj(ignore -> ByteBuffer.wrap(randomBytes(bufferSize))).toArray(ByteBuffer[]::new);\n\n                MemoryRecords records = buildRecords(buffers);\n                snapshot.append(records.buffer());\n                expectedSize += records.sizeInBytes();\n            }\n\n            assertEquals(expectedSize, snapshot.sizeInBytes());\n\n            snapshot.freeze();\n        }\n\n        \r\n        assertTrue(Files.exists(Snapshots.snapshotPath(tempDir, offsetAndEpoch)));\n        assertEquals(expectedSize, Files.size(Snapshots.snapshotPath(tempDir, offsetAndEpoch)));\n\n        try (FileRawSnapshotReader snapshot = FileRawSnapshotReader.open(tempDir, offsetAndEpoch)) {\n            int countBatches = 0;\n            int countRecords = 0;\n\n            for (RecordBatch batch : snapshot) {\n                countBatches += 1;\n\n                Iterator<Record> records = batch.streamingIterator(new GrowableBufferSupplier());\n                while (records.hasNext()) {\n                    Record record = records.next();\n\n                    countRecords += 1;\n\n                    assertFalse(record.hasKey());\n                    assertTrue(record.hasValue());\n                    assertEquals(bufferSize, record.value().remaining());\n                }\n            }\n\n            assertEquals(batches, countBatches);\n            assertEquals(batches * batchSize, countRecords);\n        }\n    }\n","date":"2020-12-08 06:06:25","endLine":204,"groupId":"19990","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testBufferWriteReadSnapshot","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/e5/2943558f2cc81094838956385976e3cba28767.src","preCode":"    public void testBufferWriteReadSnapshot() throws IOException {\n        Path tempDir = TestUtils.tempDirectory().toPath();\n        OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(10L, 3);\n        int bufferSize = 256;\n        int batchSize = 3;\n        int batches = 10;\n        int expectedSize = 0;\n\n        try (FileRawSnapshotWriter snapshot = FileRawSnapshotWriter.create(tempDir, offsetAndEpoch)) {\n            for (int i = 0; i < batches; i++) {\n                ByteBuffer[] buffers = IntStream\n                    .range(0, batchSize)\n                    .mapToObj(ignore -> ByteBuffer.wrap(randomBytes(bufferSize))).toArray(ByteBuffer[]::new);\n\n                MemoryRecords records = buildRecords(buffers);\n                snapshot.append(records.buffer());\n                expectedSize += records.sizeInBytes();\n            }\n\n            assertEquals(expectedSize, snapshot.sizeInBytes());\n\n            snapshot.freeze();\n        }\n\n        \r\n        assertTrue(Files.exists(Snapshots.snapshotPath(tempDir, offsetAndEpoch)));\n        assertEquals(expectedSize, Files.size(Snapshots.snapshotPath(tempDir, offsetAndEpoch)));\n\n        try (FileRawSnapshotReader snapshot = FileRawSnapshotReader.open(tempDir, offsetAndEpoch)) {\n            int countBatches = 0;\n            int countRecords = 0;\n\n            for (RecordBatch batch : snapshot) {\n                countBatches += 1;\n\n                Iterator<Record> records = batch.streamingIterator(new GrowableBufferSupplier());\n                while (records.hasNext()) {\n                    Record record = records.next();\n\n                    countRecords += 1;\n\n                    assertFalse(record.hasKey());\n                    assertTrue(record.hasValue());\n                    assertEquals(bufferSize, record.value().remaining());\n                }\n            }\n\n            assertEquals(batches, countBatches);\n            assertEquals(batches * batchSize, countRecords);\n        }\n    }\n","realPath":"raft/src/test/java/org/apache/kafka/snapshot/FileRawSnapshotTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":154,"status":"B"}],"commitId":"ab0807dd858887d934d2be520608d20bf765b609","commitMessage":"@@@KAFKA-10394: Add classes to read and write snapshot for KIP-630 (#9512)\n\nThis PR adds support for generating snapshot for KIP-630.\n\n1. Adds the interfaces `RawSnapshotWriter` and `RawSnapshotReader` and the implementations `FileRawSnapshotWriter` and `FileRawSnapshotReader` respectively. These interfaces and implementations are low level API for writing and reading snapshots. They are internal to the Raft implementation and are not exposed to the users of `RaftClient`. They operation at the `Record` level. These types are exposed to the `RaftClient` through the `ReplicatedLog` interface.\n\n2. Adds a buffered snapshot writer: `SnapshotWriter<T>`. This type is a higher-level type and it is exposed through the `RaftClient` interface. A future PR will add the related `SnapshotReader<T>`.  which will be used by the state machine to load a snapshot.\n\nReviewers: Jason Gustafson <jason@confluent.io>","date":"2020-12-08 06:06:25","modifiedFileCount":"7","status":"B","submitter":"Jos? Armando Garc?a Sancio"},{"authorTime":"2021-01-27 02:33:36","codes":[{"authorDate":"2021-01-27 02:33:36","commitOrder":2,"curCode":"    public void testBatchWriteReadSnapshot() throws IOException {\n        Path tempDir = TestUtils.tempDirectory().toPath();\n        OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(10L, 3);\n        int bufferSize = 256;\n        int batchSize = 3;\n        int batches = 10;\n\n        try (FileRawSnapshotWriter snapshot = FileRawSnapshotWriter.create(tempDir, offsetAndEpoch)) {\n            for (int i = 0; i < batches; i++) {\n                ByteBuffer[] buffers = IntStream\n                    .range(0, batchSize)\n                    .mapToObj(ignore -> ByteBuffer.wrap(randomBytes(bufferSize))).toArray(ByteBuffer[]::new);\n\n                snapshot.append(buildRecords(buffers));\n            }\n\n            snapshot.freeze();\n        }\n\n        try (FileRawSnapshotReader snapshot = FileRawSnapshotReader.open(tempDir, offsetAndEpoch)) {\n            int countBatches = 0;\n            int countRecords = 0;\n            for (RecordBatch batch : snapshot) {\n                countBatches += 1;\n\n                Iterator<Record> records = batch.streamingIterator(new GrowableBufferSupplier());\n                while (records.hasNext()) {\n                    Record record = records.next();\n\n                    countRecords += 1;\n\n                    assertFalse(record.hasKey());\n                    assertTrue(record.hasValue());\n                    assertEquals(bufferSize, record.value().remaining());\n                }\n            }\n\n            assertEquals(batches, countBatches);\n            assertEquals(batches * batchSize, countRecords);\n        }\n    }\n","date":"2021-01-27 02:33:36","endLine":196,"groupId":"19990","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testBatchWriteReadSnapshot","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/b7/e6eda12a1f4ca7d3d2f93b2acea6a8cd422e9a.src","preCode":"    public void testBatchWriteReadSnapshot() throws IOException {\n        Path tempDir = TestUtils.tempDirectory().toPath();\n        OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(10L, 3);\n        int bufferSize = 256;\n        int batchSize = 3;\n        int batches = 10;\n\n        try (FileRawSnapshotWriter snapshot = FileRawSnapshotWriter.create(tempDir, offsetAndEpoch)) {\n            for (int i = 0; i < batches; i++) {\n                ByteBuffer[] buffers = IntStream\n                    .range(0, batchSize)\n                    .mapToObj(ignore -> ByteBuffer.wrap(randomBytes(bufferSize))).toArray(ByteBuffer[]::new);\n\n                snapshot.append(buildRecords(buffers).buffer());\n            }\n\n            snapshot.freeze();\n        }\n\n        try (FileRawSnapshotReader snapshot = FileRawSnapshotReader.open(tempDir, offsetAndEpoch)) {\n            int countBatches = 0;\n            int countRecords = 0;\n            for (RecordBatch batch : snapshot) {\n                countBatches += 1;\n\n                Iterator<Record> records = batch.streamingIterator(new GrowableBufferSupplier());\n                while (records.hasNext()) {\n                    Record record = records.next();\n\n                    countRecords += 1;\n\n                    assertFalse(record.hasKey());\n                    assertTrue(record.hasValue());\n                    assertEquals(bufferSize, record.value().remaining());\n                }\n            }\n\n            assertEquals(batches, countBatches);\n            assertEquals(batches * batchSize, countRecords);\n        }\n    }\n","realPath":"raft/src/test/java/org/apache/kafka/snapshot/FileRawSnapshotTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":156,"status":"M"},{"authorDate":"2021-01-27 02:33:36","commitOrder":2,"curCode":"    public void testBufferWriteReadSnapshot() throws IOException {\n        Path tempDir = TestUtils.tempDirectory().toPath();\n        OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(10L, 3);\n        int bufferSize = 256;\n        int batchSize = 3;\n        int batches = 10;\n        int expectedSize = 0;\n\n        try (FileRawSnapshotWriter snapshot = FileRawSnapshotWriter.create(tempDir, offsetAndEpoch)) {\n            for (int i = 0; i < batches; i++) {\n                ByteBuffer[] buffers = IntStream\n                    .range(0, batchSize)\n                    .mapToObj(ignore -> ByteBuffer.wrap(randomBytes(bufferSize))).toArray(ByteBuffer[]::new);\n\n                UnalignedMemoryRecords records = buildRecords(buffers);\n                snapshot.append(records);\n                expectedSize += records.sizeInBytes();\n            }\n\n            assertEquals(expectedSize, snapshot.sizeInBytes());\n\n            snapshot.freeze();\n        }\n\n        \r\n        assertTrue(Files.exists(Snapshots.snapshotPath(tempDir, offsetAndEpoch)));\n        assertEquals(expectedSize, Files.size(Snapshots.snapshotPath(tempDir, offsetAndEpoch)));\n\n        try (FileRawSnapshotReader snapshot = FileRawSnapshotReader.open(tempDir, offsetAndEpoch)) {\n            int countBatches = 0;\n            int countRecords = 0;\n\n            for (RecordBatch batch : snapshot) {\n                countBatches += 1;\n\n                Iterator<Record> records = batch.streamingIterator(new GrowableBufferSupplier());\n                while (records.hasNext()) {\n                    Record record = records.next();\n\n                    countRecords += 1;\n\n                    assertFalse(record.hasKey());\n                    assertTrue(record.hasValue());\n                    assertEquals(bufferSize, record.value().remaining());\n                }\n            }\n\n            assertEquals(batches, countBatches);\n            assertEquals(batches * batchSize, countRecords);\n        }\n    }\n","date":"2021-01-27 02:33:36","endLine":249,"groupId":"19990","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testBufferWriteReadSnapshot","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/b7/e6eda12a1f4ca7d3d2f93b2acea6a8cd422e9a.src","preCode":"    public void testBufferWriteReadSnapshot() throws IOException {\n        Path tempDir = TestUtils.tempDirectory().toPath();\n        OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(10L, 3);\n        int bufferSize = 256;\n        int batchSize = 3;\n        int batches = 10;\n        int expectedSize = 0;\n\n        try (FileRawSnapshotWriter snapshot = FileRawSnapshotWriter.create(tempDir, offsetAndEpoch)) {\n            for (int i = 0; i < batches; i++) {\n                ByteBuffer[] buffers = IntStream\n                    .range(0, batchSize)\n                    .mapToObj(ignore -> ByteBuffer.wrap(randomBytes(bufferSize))).toArray(ByteBuffer[]::new);\n\n                MemoryRecords records = buildRecords(buffers);\n                snapshot.append(records.buffer());\n                expectedSize += records.sizeInBytes();\n            }\n\n            assertEquals(expectedSize, snapshot.sizeInBytes());\n\n            snapshot.freeze();\n        }\n\n        \r\n        assertTrue(Files.exists(Snapshots.snapshotPath(tempDir, offsetAndEpoch)));\n        assertEquals(expectedSize, Files.size(Snapshots.snapshotPath(tempDir, offsetAndEpoch)));\n\n        try (FileRawSnapshotReader snapshot = FileRawSnapshotReader.open(tempDir, offsetAndEpoch)) {\n            int countBatches = 0;\n            int countRecords = 0;\n\n            for (RecordBatch batch : snapshot) {\n                countBatches += 1;\n\n                Iterator<Record> records = batch.streamingIterator(new GrowableBufferSupplier());\n                while (records.hasNext()) {\n                    Record record = records.next();\n\n                    countRecords += 1;\n\n                    assertFalse(record.hasKey());\n                    assertTrue(record.hasValue());\n                    assertEquals(bufferSize, record.value().remaining());\n                }\n            }\n\n            assertEquals(batches, countBatches);\n            assertEquals(batches * batchSize, countRecords);\n        }\n    }\n","realPath":"raft/src/test/java/org/apache/kafka/snapshot/FileRawSnapshotTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":199,"status":"M"}],"commitId":"a26db2a1ec931fa43b350b37639e6116b7ccffc4","commitMessage":"@@@KAFKA-10694; Implement zero copy for FetchSnapshot (#9819)\n\nThis patch adds zero-copy support for the `FetchSnapshot` API. Unlike the normal `Fetch` API.  records are not assumed to be offset-aligned in `FetchSnapshot` responses. Hence this patch introduces a new `UnalignedRecords` type which allows us to use most of the existing logic to support zero-copy while preserving type safety in the snapshot APIs.\n\nReviewers: Jos? Armando Garc?a Sancio <jsancio@gmail.com>.  Jason Gustafson <jason@confluent.io>","date":"2021-01-27 02:33:36","modifiedFileCount":"20","status":"M","submitter":"dengziming"},{"authorTime":"2021-01-30 06:06:01","codes":[{"authorDate":"2021-01-30 06:06:01","commitOrder":3,"curCode":"    public void testBatchWriteReadSnapshot() throws IOException {\n        OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(10L, 3);\n        int bufferSize = 256;\n        int batchSize = 3;\n        int batches = 10;\n\n        try (FileRawSnapshotWriter snapshot = createSnapshotWriter(tempDir, offsetAndEpoch)) {\n            for (int i = 0; i < batches; i++) {\n                ByteBuffer[] buffers = IntStream\n                    .range(0, batchSize)\n                    .mapToObj(ignore -> ByteBuffer.wrap(randomBytes(bufferSize))).toArray(ByteBuffer[]::new);\n\n                snapshot.append(buildRecords(buffers));\n            }\n\n            snapshot.freeze();\n        }\n\n        try (FileRawSnapshotReader snapshot = FileRawSnapshotReader.open(tempDir, offsetAndEpoch)) {\n            int countBatches = 0;\n            int countRecords = 0;\n            for (RecordBatch batch : snapshot) {\n                countBatches += 1;\n\n                Iterator<Record> records = batch.streamingIterator(new GrowableBufferSupplier());\n                while (records.hasNext()) {\n                    Record record = records.next();\n\n                    countRecords += 1;\n\n                    assertFalse(record.hasKey());\n                    assertTrue(record.hasValue());\n                    assertEquals(bufferSize, record.value().remaining());\n                }\n            }\n\n            assertEquals(batches, countBatches);\n            assertEquals(batches * batchSize, countRecords);\n        }\n    }\n","date":"2021-01-30 06:06:01","endLine":208,"groupId":"19990","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testBatchWriteReadSnapshot","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/37/dac9feaee91566d86bb06a56f84ffd2072a71d.src","preCode":"    public void testBatchWriteReadSnapshot() throws IOException {\n        Path tempDir = TestUtils.tempDirectory().toPath();\n        OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(10L, 3);\n        int bufferSize = 256;\n        int batchSize = 3;\n        int batches = 10;\n\n        try (FileRawSnapshotWriter snapshot = FileRawSnapshotWriter.create(tempDir, offsetAndEpoch)) {\n            for (int i = 0; i < batches; i++) {\n                ByteBuffer[] buffers = IntStream\n                    .range(0, batchSize)\n                    .mapToObj(ignore -> ByteBuffer.wrap(randomBytes(bufferSize))).toArray(ByteBuffer[]::new);\n\n                snapshot.append(buildRecords(buffers));\n            }\n\n            snapshot.freeze();\n        }\n\n        try (FileRawSnapshotReader snapshot = FileRawSnapshotReader.open(tempDir, offsetAndEpoch)) {\n            int countBatches = 0;\n            int countRecords = 0;\n            for (RecordBatch batch : snapshot) {\n                countBatches += 1;\n\n                Iterator<Record> records = batch.streamingIterator(new GrowableBufferSupplier());\n                while (records.hasNext()) {\n                    Record record = records.next();\n\n                    countRecords += 1;\n\n                    assertFalse(record.hasKey());\n                    assertTrue(record.hasValue());\n                    assertEquals(bufferSize, record.value().remaining());\n                }\n            }\n\n            assertEquals(batches, countBatches);\n            assertEquals(batches * batchSize, countRecords);\n        }\n    }\n","realPath":"raft/src/test/java/org/apache/kafka/snapshot/FileRawSnapshotTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":169,"status":"M"},{"authorDate":"2021-01-30 06:06:01","commitOrder":3,"curCode":"    public void testBufferWriteReadSnapshot() throws IOException {\n        OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(10L, 3);\n        int bufferSize = 256;\n        int batchSize = 3;\n        int batches = 10;\n        int expectedSize = 0;\n\n        try (FileRawSnapshotWriter snapshot = createSnapshotWriter(tempDir, offsetAndEpoch)) {\n            for (int i = 0; i < batches; i++) {\n                ByteBuffer[] buffers = IntStream\n                    .range(0, batchSize)\n                    .mapToObj(ignore -> ByteBuffer.wrap(randomBytes(bufferSize))).toArray(ByteBuffer[]::new);\n\n                UnalignedMemoryRecords records = buildRecords(buffers);\n                snapshot.append(records);\n                expectedSize += records.sizeInBytes();\n            }\n\n            assertEquals(expectedSize, snapshot.sizeInBytes());\n\n            snapshot.freeze();\n        }\n\n        \r\n        assertTrue(Files.exists(Snapshots.snapshotPath(tempDir, offsetAndEpoch)));\n        assertEquals(expectedSize, Files.size(Snapshots.snapshotPath(tempDir, offsetAndEpoch)));\n\n        try (FileRawSnapshotReader snapshot = FileRawSnapshotReader.open(tempDir, offsetAndEpoch)) {\n            int countBatches = 0;\n            int countRecords = 0;\n\n            for (RecordBatch batch : snapshot) {\n                countBatches += 1;\n\n                Iterator<Record> records = batch.streamingIterator(new GrowableBufferSupplier());\n                while (records.hasNext()) {\n                    Record record = records.next();\n\n                    countRecords += 1;\n\n                    assertFalse(record.hasKey());\n                    assertTrue(record.hasValue());\n                    assertEquals(bufferSize, record.value().remaining());\n                }\n            }\n\n            assertEquals(batches, countBatches);\n            assertEquals(batches * batchSize, countRecords);\n        }\n    }\n","date":"2021-01-30 06:06:01","endLine":260,"groupId":"19990","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testBufferWriteReadSnapshot","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/37/dac9feaee91566d86bb06a56f84ffd2072a71d.src","preCode":"    public void testBufferWriteReadSnapshot() throws IOException {\n        Path tempDir = TestUtils.tempDirectory().toPath();\n        OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(10L, 3);\n        int bufferSize = 256;\n        int batchSize = 3;\n        int batches = 10;\n        int expectedSize = 0;\n\n        try (FileRawSnapshotWriter snapshot = FileRawSnapshotWriter.create(tempDir, offsetAndEpoch)) {\n            for (int i = 0; i < batches; i++) {\n                ByteBuffer[] buffers = IntStream\n                    .range(0, batchSize)\n                    .mapToObj(ignore -> ByteBuffer.wrap(randomBytes(bufferSize))).toArray(ByteBuffer[]::new);\n\n                UnalignedMemoryRecords records = buildRecords(buffers);\n                snapshot.append(records);\n                expectedSize += records.sizeInBytes();\n            }\n\n            assertEquals(expectedSize, snapshot.sizeInBytes());\n\n            snapshot.freeze();\n        }\n\n        \r\n        assertTrue(Files.exists(Snapshots.snapshotPath(tempDir, offsetAndEpoch)));\n        assertEquals(expectedSize, Files.size(Snapshots.snapshotPath(tempDir, offsetAndEpoch)));\n\n        try (FileRawSnapshotReader snapshot = FileRawSnapshotReader.open(tempDir, offsetAndEpoch)) {\n            int countBatches = 0;\n            int countRecords = 0;\n\n            for (RecordBatch batch : snapshot) {\n                countBatches += 1;\n\n                Iterator<Record> records = batch.streamingIterator(new GrowableBufferSupplier());\n                while (records.hasNext()) {\n                    Record record = records.next();\n\n                    countRecords += 1;\n\n                    assertFalse(record.hasKey());\n                    assertTrue(record.hasValue());\n                    assertEquals(bufferSize, record.value().remaining());\n                }\n            }\n\n            assertEquals(batches, countBatches);\n            assertEquals(batches * batchSize, countRecords);\n        }\n    }\n","realPath":"raft/src/test/java/org/apache/kafka/snapshot/FileRawSnapshotTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":211,"status":"M"}],"commitId":"5b3351e10b339f5ef43fa4ec9e23544acad298c3","commitMessage":"@@@KAFKA-10761; Kafka Raft update log start offset (#9816)\n\nAdds support for nonzero log start offsets.\n\nChanges to `Log`:\n1. Add a new \"reason\" for increasing the log start offset. This is used by `KafkaMetadataLog` when a snapshot is generated.\n2. `LogAppendInfo` should return if it was rolled because of an records append. A log is rolled when a new segment is created. This is used by `KafkaMetadataLog` to in some cases delete the created segment based on the log start offset.\n\nChanges to `KafkaMetadataLog`:\n1. Update both append functions to delete old segments based on the log start offset whenever the log is rolled.\n2. Update `lastFetchedEpoch` to return the epoch of the latest snapshot whenever the log is empty.\n3. Add a function that empties the log whenever the latest snapshot is greater than the replicated log. This is used when first loading the `KafkaMetadataLog` and whenever the `KafkaRaftClient` downloads a snapshot from the leader.\n\nChanges to `KafkaRaftClient`:\n1. Improve `validateFetchOffsetAndEpoch` so that it can handle fetch offset and last fetched epoch that are smaller than the log start offset. This is in addition to the existing code that check for a diverging log. This is used by the raft client to determine if the Fetch response should include a diverging epoch or a snapshot id. \n2. When a follower finishes fetching a snapshot from the leader fully truncate the local log.\n3. When polling the current state the raft client should check if the state machine has generated a new snapshot and update the log start offset accordingly.\n\nReviewers: Jason Gustafson <jason@confluent.io>","date":"2021-01-30 06:06:01","modifiedFileCount":"11","status":"M","submitter":"Jos? Armando Garc?a Sancio"},{"authorTime":"2021-05-02 01:05:45","codes":[{"authorDate":"2021-05-02 01:05:45","commitOrder":4,"curCode":"    public void testBatchWriteReadSnapshot() throws IOException {\n        OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(10L, 3);\n        int bufferSize = 256;\n        int batchSize = 3;\n        int numberOfBatches = 10;\n\n        try (FileRawSnapshotWriter snapshot = createSnapshotWriter(tempDir, offsetAndEpoch)) {\n            for (int i = 0; i < numberOfBatches; i++) {\n                ByteBuffer[] buffers = IntStream\n                    .range(0, batchSize)\n                    .mapToObj(ignore -> ByteBuffer.wrap(randomBytes(bufferSize))).toArray(ByteBuffer[]::new);\n\n                snapshot.append(buildRecords(buffers));\n            }\n\n            snapshot.freeze();\n        }\n\n        try (FileRawSnapshotReader snapshot = FileRawSnapshotReader.open(tempDir, offsetAndEpoch)) {\n            int countBatches = 0;\n            int countRecords = 0;\n\n            Iterator<RecordBatch> batches = Utils.covariantCast(snapshot.records().batchIterator());\n            while (batches.hasNext()) {\n                RecordBatch batch = batches.next();\n                countBatches += 1;\n\n                Iterator<Record> records = batch.streamingIterator(new GrowableBufferSupplier());\n                while (records.hasNext()) {\n                    Record record = records.next();\n\n                    countRecords += 1;\n\n                    assertFalse(record.hasKey());\n                    assertTrue(record.hasValue());\n                    assertEquals(bufferSize, record.value().remaining());\n                }\n            }\n\n            assertEquals(numberOfBatches, countBatches);\n            assertEquals(numberOfBatches * batchSize, countRecords);\n        }\n    }\n","date":"2021-05-02 01:05:45","endLine":214,"groupId":"104368","id":7,"instanceNumber":1,"isCurCommit":1,"methodName":"testBatchWriteReadSnapshot","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/ef/38d465c54400a05edf8714044b2b81ac821de8.src","preCode":"    public void testBatchWriteReadSnapshot() throws IOException {\n        OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(10L, 3);\n        int bufferSize = 256;\n        int batchSize = 3;\n        int batches = 10;\n\n        try (FileRawSnapshotWriter snapshot = createSnapshotWriter(tempDir, offsetAndEpoch)) {\n            for (int i = 0; i < batches; i++) {\n                ByteBuffer[] buffers = IntStream\n                    .range(0, batchSize)\n                    .mapToObj(ignore -> ByteBuffer.wrap(randomBytes(bufferSize))).toArray(ByteBuffer[]::new);\n\n                snapshot.append(buildRecords(buffers));\n            }\n\n            snapshot.freeze();\n        }\n\n        try (FileRawSnapshotReader snapshot = FileRawSnapshotReader.open(tempDir, offsetAndEpoch)) {\n            int countBatches = 0;\n            int countRecords = 0;\n            for (RecordBatch batch : snapshot) {\n                countBatches += 1;\n\n                Iterator<Record> records = batch.streamingIterator(new GrowableBufferSupplier());\n                while (records.hasNext()) {\n                    Record record = records.next();\n\n                    countRecords += 1;\n\n                    assertFalse(record.hasKey());\n                    assertTrue(record.hasValue());\n                    assertEquals(bufferSize, record.value().remaining());\n                }\n            }\n\n            assertEquals(batches, countBatches);\n            assertEquals(batches * batchSize, countRecords);\n        }\n    }\n","realPath":"raft/src/test/java/org/apache/kafka/snapshot/FileRawSnapshotTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":172,"status":"M"},{"authorDate":"2021-05-02 01:05:45","commitOrder":4,"curCode":"    public void testBufferWriteReadSnapshot() throws IOException {\n        OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(10L, 3);\n        int bufferSize = 256;\n        int batchSize = 3;\n        int numberOfBatches = 10;\n        int expectedSize = 0;\n\n        try (FileRawSnapshotWriter snapshot = createSnapshotWriter(tempDir, offsetAndEpoch)) {\n            for (int i = 0; i < numberOfBatches; i++) {\n                ByteBuffer[] buffers = IntStream\n                    .range(0, batchSize)\n                    .mapToObj(ignore -> ByteBuffer.wrap(randomBytes(bufferSize))).toArray(ByteBuffer[]::new);\n\n                UnalignedMemoryRecords records = buildRecords(buffers);\n                snapshot.append(records);\n                expectedSize += records.sizeInBytes();\n            }\n\n            assertEquals(expectedSize, snapshot.sizeInBytes());\n\n            snapshot.freeze();\n        }\n\n        \r\n        assertTrue(Files.exists(Snapshots.snapshotPath(tempDir, offsetAndEpoch)));\n        assertEquals(expectedSize, Files.size(Snapshots.snapshotPath(tempDir, offsetAndEpoch)));\n\n        try (FileRawSnapshotReader snapshot = FileRawSnapshotReader.open(tempDir, offsetAndEpoch)) {\n            int countBatches = 0;\n            int countRecords = 0;\n\n            Iterator<RecordBatch> batches = Utils.covariantCast(snapshot.records().batchIterator());\n            while (batches.hasNext()) {\n                RecordBatch batch = batches.next();\n                countBatches += 1;\n\n                Iterator<Record> records = batch.streamingIterator(new GrowableBufferSupplier());\n                while (records.hasNext()) {\n                    Record record = records.next();\n\n                    countRecords += 1;\n\n                    assertFalse(record.hasKey());\n                    assertTrue(record.hasValue());\n                    assertEquals(bufferSize, record.value().remaining());\n                }\n            }\n\n            assertEquals(numberOfBatches, countBatches);\n            assertEquals(numberOfBatches * batchSize, countRecords);\n        }\n    }\n","date":"2021-05-02 01:05:45","endLine":268,"groupId":"104368","id":8,"instanceNumber":2,"isCurCommit":1,"methodName":"testBufferWriteReadSnapshot","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/ef/38d465c54400a05edf8714044b2b81ac821de8.src","preCode":"    public void testBufferWriteReadSnapshot() throws IOException {\n        OffsetAndEpoch offsetAndEpoch = new OffsetAndEpoch(10L, 3);\n        int bufferSize = 256;\n        int batchSize = 3;\n        int batches = 10;\n        int expectedSize = 0;\n\n        try (FileRawSnapshotWriter snapshot = createSnapshotWriter(tempDir, offsetAndEpoch)) {\n            for (int i = 0; i < batches; i++) {\n                ByteBuffer[] buffers = IntStream\n                    .range(0, batchSize)\n                    .mapToObj(ignore -> ByteBuffer.wrap(randomBytes(bufferSize))).toArray(ByteBuffer[]::new);\n\n                UnalignedMemoryRecords records = buildRecords(buffers);\n                snapshot.append(records);\n                expectedSize += records.sizeInBytes();\n            }\n\n            assertEquals(expectedSize, snapshot.sizeInBytes());\n\n            snapshot.freeze();\n        }\n\n        \r\n        assertTrue(Files.exists(Snapshots.snapshotPath(tempDir, offsetAndEpoch)));\n        assertEquals(expectedSize, Files.size(Snapshots.snapshotPath(tempDir, offsetAndEpoch)));\n\n        try (FileRawSnapshotReader snapshot = FileRawSnapshotReader.open(tempDir, offsetAndEpoch)) {\n            int countBatches = 0;\n            int countRecords = 0;\n\n            for (RecordBatch batch : snapshot) {\n                countBatches += 1;\n\n                Iterator<Record> records = batch.streamingIterator(new GrowableBufferSupplier());\n                while (records.hasNext()) {\n                    Record record = records.next();\n\n                    countRecords += 1;\n\n                    assertFalse(record.hasKey());\n                    assertTrue(record.hasValue());\n                    assertEquals(bufferSize, record.value().remaining());\n                }\n            }\n\n            assertEquals(batches, countBatches);\n            assertEquals(batches * batchSize, countRecords);\n        }\n    }\n","realPath":"raft/src/test/java/org/apache/kafka/snapshot/FileRawSnapshotTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":217,"status":"M"}],"commitId":"6203bf8b94c7c340671c1729f4a8e4fcc302605e","commitMessage":"@@@KAFKA-12154; Raft Snapshot Loading API (#10085)\n\nImplement Raft Snapshot loading API.\n\n1. Adds a new method `handleSnapshot` to `raft.Listener` which is called whenever the `RaftClient` determines that the `Listener` needs to load a new snapshot before reading the log. This happens when the `Listener`'s next offset is less than the log start offset also known as the earliest snapshot.\n\n2.  Adds a new type `SnapshotReader<T>` which provides a `Iterator<Batch<T>>` interface and de-serializes records in the `RawSnapshotReader` into `T`s\n\n3.  Adds a new type `RecordsIterator<T>` that implements an `Iterator<Batch<T>>` by scanning a `Records` object and deserializes the batches and records into `Batch<T>`. This type is used by both `SnapshotReader<T>` and `RecordsBatchReader<T>` internally to implement the `Iterator` interface that they expose. \n\n4. Changes the `MockLog` implementation to read one or two batches at a time. The previous implementation always read from the given offset to the high-watermark. This made it impossible to test interesting snapshot loading scenarios.\n\n5. Removed `throws IOException` from some methods. Some of types were inconsistently throwing `IOException` in some cases and throwing `RuntimeException(....  new IOException(...))` in others. This PR improves the consistent by wrapping `IOException` in `RuntimeException` in a few more places and replacing `Closeable` with `AutoCloseable`.\n\n6. Updated the Kafka Raft simulation test to take into account snapshot. `ReplicatedCounter` was updated to generate snapshot after 10 records get committed. This means that the `ConsistentCommittedData` validation was extended to take snapshots into account. Also added a new invariant to ensure that the log start offset is consistently set with the earliest snapshot.\n\nReviewers: dengziming <swzmdeng@163.com>.  David Arthur <mumrah@gmail.com>.  Jason Gustafson <jason@confluent.io>","date":"2021-05-02 01:05:45","modifiedFileCount":"25","status":"M","submitter":"Jos? Armando Garc?a Sancio"}]
