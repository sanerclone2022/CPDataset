[{"authorTime":"2017-03-25 03:38:36","codes":[{"authorDate":"2017-03-25 03:38:36","commitOrder":1,"curCode":"    public void testCompressedIterationWithNullValue() throws Exception {\n        ByteBuffer buffer = ByteBuffer.allocate(128);\n        DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));\n        AbstractLegacyRecordBatch.writeHeader(out, 0L, LegacyRecord.RECORD_OVERHEAD_V1);\n        LegacyRecord.write(out, RecordBatch.MAGIC_VALUE_V1, 1L, (byte[]) null, null,\n                CompressionType.GZIP, TimestampType.CREATE_TIME);\n\n        buffer.flip();\n\n        MemoryRecords records = MemoryRecords.readableRecords(buffer);\n        for (Record record : records.records())\n            fail(\"Iteration should have caused invalid record error\");\n    }\n","date":"2017-03-25 03:38:43","endLine":45,"groupId":"9740","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompressedIterationWithNullValue","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/dd/718bf355014b72f7c5dd2b7618a4941b26ecf3.src","preCode":"    public void testCompressedIterationWithNullValue() throws Exception {\n        ByteBuffer buffer = ByteBuffer.allocate(128);\n        DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));\n        AbstractLegacyRecordBatch.writeHeader(out, 0L, LegacyRecord.RECORD_OVERHEAD_V1);\n        LegacyRecord.write(out, RecordBatch.MAGIC_VALUE_V1, 1L, (byte[]) null, null,\n                CompressionType.GZIP, TimestampType.CREATE_TIME);\n\n        buffer.flip();\n\n        MemoryRecords records = MemoryRecords.readableRecords(buffer);\n        for (Record record : records.records())\n            fail(\"Iteration should have caused invalid record error\");\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/SimpleLegacyRecordTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":33,"status":"B"},{"authorDate":"2017-03-25 03:38:36","commitOrder":1,"curCode":"    public void testCompressedIterationWithEmptyRecords() throws Exception {\n        ByteBuffer emptyCompressedValue = ByteBuffer.allocate(64);\n        OutputStream gzipOutput = CompressionType.GZIP.wrapForOutput(new ByteBufferOutputStream(emptyCompressedValue),\n                RecordBatch.MAGIC_VALUE_V1, 64);\n        gzipOutput.close();\n        emptyCompressedValue.flip();\n\n        ByteBuffer buffer = ByteBuffer.allocate(128);\n        DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));\n        AbstractLegacyRecordBatch.writeHeader(out, 0L, LegacyRecord.RECORD_OVERHEAD_V1 + emptyCompressedValue.remaining());\n        LegacyRecord.write(out, RecordBatch.MAGIC_VALUE_V1, 1L, null, Utils.toArray(emptyCompressedValue),\n                CompressionType.GZIP, TimestampType.CREATE_TIME);\n\n        buffer.flip();\n\n        MemoryRecords records = MemoryRecords.readableRecords(buffer);\n        for (Record record : records.records())\n            fail(\"Iteration should have caused invalid record error\");\n    }\n","date":"2017-03-25 03:38:43","endLine":66,"groupId":"12185","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testCompressedIterationWithEmptyRecords","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/dd/718bf355014b72f7c5dd2b7618a4941b26ecf3.src","preCode":"    public void testCompressedIterationWithEmptyRecords() throws Exception {\n        ByteBuffer emptyCompressedValue = ByteBuffer.allocate(64);\n        OutputStream gzipOutput = CompressionType.GZIP.wrapForOutput(new ByteBufferOutputStream(emptyCompressedValue),\n                RecordBatch.MAGIC_VALUE_V1, 64);\n        gzipOutput.close();\n        emptyCompressedValue.flip();\n\n        ByteBuffer buffer = ByteBuffer.allocate(128);\n        DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));\n        AbstractLegacyRecordBatch.writeHeader(out, 0L, LegacyRecord.RECORD_OVERHEAD_V1 + emptyCompressedValue.remaining());\n        LegacyRecord.write(out, RecordBatch.MAGIC_VALUE_V1, 1L, null, Utils.toArray(emptyCompressedValue),\n                CompressionType.GZIP, TimestampType.CREATE_TIME);\n\n        buffer.flip();\n\n        MemoryRecords records = MemoryRecords.readableRecords(buffer);\n        for (Record record : records.records())\n            fail(\"Iteration should have caused invalid record error\");\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/SimpleLegacyRecordTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":48,"status":"B"}],"commitId":"5bd06f1d542e6b588a1d402d059bc24690017d32","commitMessage":"@@@KAFKA-4816; Message format changes for idempotent/transactional producer (KIP-98)\n\nAuthor: Jason Gustafson <jason@confluent.io>\n\nReviewers: Jun Rao <junrao@gmail.com>.  Apurva Mehta <apurva@confluent.io>.  Guozhang Wang <wangguoz@gmail.com>.  Ismael Juma <ismael@juma.me.uk>\n\nCloses #2614 from hachikuji/exactly-once-message-format\n","date":"2017-03-25 03:38:43","modifiedFileCount":"55","status":"B","submitter":"Jason Gustafson"},{"authorTime":"2017-03-30 02:10:43","codes":[{"authorDate":"2017-03-30 02:10:43","commitOrder":2,"curCode":"    public void testCompressedIterationWithNullValue() throws Exception {\n        ByteBuffer buffer = ByteBuffer.allocate(128);\n        DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));\n        AbstractLegacyRecordBatch.writeHeader(out, 0L, LegacyRecord.RECORD_OVERHEAD_V1);\n        LegacyRecord.write(out, RecordBatch.MAGIC_VALUE_V1, 1L, (byte[]) null, null,\n                CompressionType.GZIP, TimestampType.CREATE_TIME);\n\n        buffer.flip();\n\n        MemoryRecords records = MemoryRecords.readableRecords(buffer);\n        if (records.records().iterator().hasNext())\n            fail(\"Iteration should have caused invalid record error\");\n    }\n","date":"2017-03-30 02:10:43","endLine":45,"groupId":"9740","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompressedIterationWithNullValue","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/b4/09af635187db8cd7677329b7304d29b9fdfd28.src","preCode":"    public void testCompressedIterationWithNullValue() throws Exception {\n        ByteBuffer buffer = ByteBuffer.allocate(128);\n        DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));\n        AbstractLegacyRecordBatch.writeHeader(out, 0L, LegacyRecord.RECORD_OVERHEAD_V1);\n        LegacyRecord.write(out, RecordBatch.MAGIC_VALUE_V1, 1L, (byte[]) null, null,\n                CompressionType.GZIP, TimestampType.CREATE_TIME);\n\n        buffer.flip();\n\n        MemoryRecords records = MemoryRecords.readableRecords(buffer);\n        for (Record record : records.records())\n            fail(\"Iteration should have caused invalid record error\");\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/SimpleLegacyRecordTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":33,"status":"M"},{"authorDate":"2017-03-30 02:10:43","commitOrder":2,"curCode":"    public void testCompressedIterationWithEmptyRecords() throws Exception {\n        ByteBuffer emptyCompressedValue = ByteBuffer.allocate(64);\n        OutputStream gzipOutput = CompressionType.GZIP.wrapForOutput(new ByteBufferOutputStream(emptyCompressedValue),\n                RecordBatch.MAGIC_VALUE_V1, 64);\n        gzipOutput.close();\n        emptyCompressedValue.flip();\n\n        ByteBuffer buffer = ByteBuffer.allocate(128);\n        DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));\n        AbstractLegacyRecordBatch.writeHeader(out, 0L, LegacyRecord.RECORD_OVERHEAD_V1 + emptyCompressedValue.remaining());\n        LegacyRecord.write(out, RecordBatch.MAGIC_VALUE_V1, 1L, null, Utils.toArray(emptyCompressedValue),\n                CompressionType.GZIP, TimestampType.CREATE_TIME);\n\n        buffer.flip();\n\n        MemoryRecords records = MemoryRecords.readableRecords(buffer);\n        if (records.records().iterator().hasNext())\n            fail(\"Iteration should have caused invalid record error\");\n    }\n","date":"2017-03-30 02:10:43","endLine":66,"groupId":"12185","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testCompressedIterationWithEmptyRecords","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/b4/09af635187db8cd7677329b7304d29b9fdfd28.src","preCode":"    public void testCompressedIterationWithEmptyRecords() throws Exception {\n        ByteBuffer emptyCompressedValue = ByteBuffer.allocate(64);\n        OutputStream gzipOutput = CompressionType.GZIP.wrapForOutput(new ByteBufferOutputStream(emptyCompressedValue),\n                RecordBatch.MAGIC_VALUE_V1, 64);\n        gzipOutput.close();\n        emptyCompressedValue.flip();\n\n        ByteBuffer buffer = ByteBuffer.allocate(128);\n        DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));\n        AbstractLegacyRecordBatch.writeHeader(out, 0L, LegacyRecord.RECORD_OVERHEAD_V1 + emptyCompressedValue.remaining());\n        LegacyRecord.write(out, RecordBatch.MAGIC_VALUE_V1, 1L, null, Utils.toArray(emptyCompressedValue),\n                CompressionType.GZIP, TimestampType.CREATE_TIME);\n\n        buffer.flip();\n\n        MemoryRecords records = MemoryRecords.readableRecords(buffer);\n        for (Record record : records.records())\n            fail(\"Iteration should have caused invalid record error\");\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/SimpleLegacyRecordTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":48,"status":"M"}],"commitId":"6feaa8a581daaa76ff7c85e4a0b9c9aa4284fe99","commitMessage":"@@@KAFKA-1449; Use CRC32C for checksum of V2 message format\n\nI manually tested that Crc32CTest and AbstractChecksums pass with JDK 9. I also verified that `Java9ChecksumFactory` is used in that case.\n\nAuthor: Ismael Juma <ismael@juma.me.uk>\n\nReviewers: Jason Gustafson <jason@confluent.io>\n\nCloses #2739 from ijuma/kafka-1449-crc32c\n","date":"2017-03-30 02:10:43","modifiedFileCount":"8","status":"M","submitter":"Ismael Juma"},{"authorTime":"2017-06-03 04:20:02","codes":[{"authorDate":"2017-03-30 02:10:43","commitOrder":3,"curCode":"    public void testCompressedIterationWithNullValue() throws Exception {\n        ByteBuffer buffer = ByteBuffer.allocate(128);\n        DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));\n        AbstractLegacyRecordBatch.writeHeader(out, 0L, LegacyRecord.RECORD_OVERHEAD_V1);\n        LegacyRecord.write(out, RecordBatch.MAGIC_VALUE_V1, 1L, (byte[]) null, null,\n                CompressionType.GZIP, TimestampType.CREATE_TIME);\n\n        buffer.flip();\n\n        MemoryRecords records = MemoryRecords.readableRecords(buffer);\n        if (records.records().iterator().hasNext())\n            fail(\"Iteration should have caused invalid record error\");\n    }\n","date":"2017-03-30 02:10:43","endLine":45,"groupId":"9740","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompressedIterationWithNullValue","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/b4/09af635187db8cd7677329b7304d29b9fdfd28.src","preCode":"    public void testCompressedIterationWithNullValue() throws Exception {\n        ByteBuffer buffer = ByteBuffer.allocate(128);\n        DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));\n        AbstractLegacyRecordBatch.writeHeader(out, 0L, LegacyRecord.RECORD_OVERHEAD_V1);\n        LegacyRecord.write(out, RecordBatch.MAGIC_VALUE_V1, 1L, (byte[]) null, null,\n                CompressionType.GZIP, TimestampType.CREATE_TIME);\n\n        buffer.flip();\n\n        MemoryRecords records = MemoryRecords.readableRecords(buffer);\n        if (records.records().iterator().hasNext())\n            fail(\"Iteration should have caused invalid record error\");\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/SimpleLegacyRecordTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":33,"status":"N"},{"authorDate":"2017-06-03 04:20:02","commitOrder":3,"curCode":"    public void testCompressedIterationWithEmptyRecords() throws Exception {\n        ByteBuffer emptyCompressedValue = ByteBuffer.allocate(64);\n        OutputStream gzipOutput = CompressionType.GZIP.wrapForOutput(new ByteBufferOutputStream(emptyCompressedValue),\n                RecordBatch.MAGIC_VALUE_V1);\n        gzipOutput.close();\n        emptyCompressedValue.flip();\n\n        ByteBuffer buffer = ByteBuffer.allocate(128);\n        DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));\n        AbstractLegacyRecordBatch.writeHeader(out, 0L, LegacyRecord.RECORD_OVERHEAD_V1 + emptyCompressedValue.remaining());\n        LegacyRecord.write(out, RecordBatch.MAGIC_VALUE_V1, 1L, null, Utils.toArray(emptyCompressedValue),\n                CompressionType.GZIP, TimestampType.CREATE_TIME);\n\n        buffer.flip();\n\n        MemoryRecords records = MemoryRecords.readableRecords(buffer);\n        if (records.records().iterator().hasNext())\n            fail(\"Iteration should have caused invalid record error\");\n    }\n","date":"2017-06-03 04:20:02","endLine":66,"groupId":"12185","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testCompressedIterationWithEmptyRecords","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/5f/578a873d0dc338dd777a9541cfbf5f09110aae.src","preCode":"    public void testCompressedIterationWithEmptyRecords() throws Exception {\n        ByteBuffer emptyCompressedValue = ByteBuffer.allocate(64);\n        OutputStream gzipOutput = CompressionType.GZIP.wrapForOutput(new ByteBufferOutputStream(emptyCompressedValue),\n                RecordBatch.MAGIC_VALUE_V1, 64);\n        gzipOutput.close();\n        emptyCompressedValue.flip();\n\n        ByteBuffer buffer = ByteBuffer.allocate(128);\n        DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));\n        AbstractLegacyRecordBatch.writeHeader(out, 0L, LegacyRecord.RECORD_OVERHEAD_V1 + emptyCompressedValue.remaining());\n        LegacyRecord.write(out, RecordBatch.MAGIC_VALUE_V1, 1L, null, Utils.toArray(emptyCompressedValue),\n                CompressionType.GZIP, TimestampType.CREATE_TIME);\n\n        buffer.flip();\n\n        MemoryRecords records = MemoryRecords.readableRecords(buffer);\n        if (records.records().iterator().hasNext())\n            fail(\"Iteration should have caused invalid record error\");\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/SimpleLegacyRecordTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":48,"status":"M"}],"commitId":"4959444afc927026f48f5c7d9babed7b9f1bea50","commitMessage":"@@@KAFKA-5236; Increase the block/buffer size when compressing with Snappy or Gzip\n\nWe had originally increased Snappy?s block size as part of KAFKA-3704. However. \nwe had some issues with excessive memory usage in the producer and we reverted\nit in 7c6ee8d5e.\n\nAfter more investigation.  we fixed the underlying reason why memory usage seemed\nto grow much more than expected via KAFKA-3747 (included in 0.10.0.1).\n\nIn 0.10.2.  we changed the broker to use the same classes as the producer and the\nbroker?s block size for Snappy was changed from 32 KB to 1KB. As reported in\nKAFKA-5236.  the on disk size is.  in some cases.  50% larger when the data is compressed\nwith 1 KB instead of 32 KB as the block size.\n\nAs discussed in KAFKA-3704.  it may be worth making this configurable and/or allocate\nthe compression buffers from the producer pool. However.  for 0.11.0.0.  I think the\nsimplest thing to do is to default to 32 KB for Snappy (the default if no block size\nis provided).\n\nI also increased the Gzip buffer size. 1 KB is too small and the default is smaller\nstill (512 bytes). 8 KB (which is the default buffer size for BufferedOutputStream)\nseemed like a reasonable default.\n\nAuthor: Ismael Juma <ismael@juma.me.uk>\n\nReviewers: Jason Gustafson <jason@confluent.io>\n\nCloses #3205 from ijuma/kafka-5236-snappy-block-size\n","date":"2017-06-03 04:20:02","modifiedFileCount":"4","status":"M","submitter":"Ismael Juma"},{"authorTime":"2021-01-10 20:20:13","codes":[{"authorDate":"2021-01-10 20:20:13","commitOrder":4,"curCode":"    public void testCompressedIterationWithNullValue() throws Exception {\n        ByteBuffer buffer = ByteBuffer.allocate(128);\n        DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));\n        AbstractLegacyRecordBatch.writeHeader(out, 0L, LegacyRecord.RECORD_OVERHEAD_V1);\n        LegacyRecord.write(out, RecordBatch.MAGIC_VALUE_V1, 1L, (byte[]) null, null,\n                CompressionType.GZIP, TimestampType.CREATE_TIME);\n\n        buffer.flip();\n        MemoryRecords records = MemoryRecords.readableRecords(buffer);\n        assertThrows(InvalidRecordException.class, () -> records.records().iterator().hasNext());\n    }\n","date":"2021-01-10 20:20:13","endLine":45,"groupId":"103355","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompressedIterationWithNullValue","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/ec/5f2ee8eaf1700b5ede56ff40a2e1947b25c1f1.src","preCode":"    public void testCompressedIterationWithNullValue() throws Exception {\n        ByteBuffer buffer = ByteBuffer.allocate(128);\n        DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));\n        AbstractLegacyRecordBatch.writeHeader(out, 0L, LegacyRecord.RECORD_OVERHEAD_V1);\n        LegacyRecord.write(out, RecordBatch.MAGIC_VALUE_V1, 1L, (byte[]) null, null,\n                CompressionType.GZIP, TimestampType.CREATE_TIME);\n\n        buffer.flip();\n\n        MemoryRecords records = MemoryRecords.readableRecords(buffer);\n        if (records.records().iterator().hasNext())\n            fail(\"Iteration should have caused invalid record error\");\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/SimpleLegacyRecordTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":35,"status":"M"},{"authorDate":"2021-01-10 20:20:13","commitOrder":4,"curCode":"    public void testCompressedIterationWithEmptyRecords() throws Exception {\n        ByteBuffer emptyCompressedValue = ByteBuffer.allocate(64);\n        OutputStream gzipOutput = CompressionType.GZIP.wrapForOutput(new ByteBufferOutputStream(emptyCompressedValue),\n                RecordBatch.MAGIC_VALUE_V1);\n        gzipOutput.close();\n        emptyCompressedValue.flip();\n\n        ByteBuffer buffer = ByteBuffer.allocate(128);\n        DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));\n        AbstractLegacyRecordBatch.writeHeader(out, 0L, LegacyRecord.RECORD_OVERHEAD_V1 + emptyCompressedValue.remaining());\n        LegacyRecord.write(out, RecordBatch.MAGIC_VALUE_V1, 1L, null, Utils.toArray(emptyCompressedValue),\n                CompressionType.GZIP, TimestampType.CREATE_TIME);\n\n        buffer.flip();\n\n        MemoryRecords records = MemoryRecords.readableRecords(buffer);\n        assertThrows(InvalidRecordException.class, () -> records.records().iterator().hasNext());\n    }\n","date":"2021-01-10 20:20:13","endLine":65,"groupId":"103355","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testCompressedIterationWithEmptyRecords","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/ec/5f2ee8eaf1700b5ede56ff40a2e1947b25c1f1.src","preCode":"    public void testCompressedIterationWithEmptyRecords() throws Exception {\n        ByteBuffer emptyCompressedValue = ByteBuffer.allocate(64);\n        OutputStream gzipOutput = CompressionType.GZIP.wrapForOutput(new ByteBufferOutputStream(emptyCompressedValue),\n                RecordBatch.MAGIC_VALUE_V1);\n        gzipOutput.close();\n        emptyCompressedValue.flip();\n\n        ByteBuffer buffer = ByteBuffer.allocate(128);\n        DataOutputStream out = new DataOutputStream(new ByteBufferOutputStream(buffer));\n        AbstractLegacyRecordBatch.writeHeader(out, 0L, LegacyRecord.RECORD_OVERHEAD_V1 + emptyCompressedValue.remaining());\n        LegacyRecord.write(out, RecordBatch.MAGIC_VALUE_V1, 1L, null, Utils.toArray(emptyCompressedValue),\n                CompressionType.GZIP, TimestampType.CREATE_TIME);\n\n        buffer.flip();\n\n        MemoryRecords records = MemoryRecords.readableRecords(buffer);\n        if (records.records().iterator().hasNext())\n            fail(\"Iteration should have caused invalid record error\");\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/common/record/SimpleLegacyRecordTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":48,"status":"M"}],"commitId":"913a019d6c9b03eb44706deb7fb164f79c1f601f","commitMessage":"@@@MINOR: replace test \"expected\" parameter by assertThrows (#9520)\n\nThis PR includes following changes.\n\n1. @Test(expected = Exception.class) is replaced by assertThrows\n2. remove reference to org.scalatest.Assertions\n3. change the magic code from 1 to 2 for testAppendAtInvalidOffset to test ZSTD\n4. rename testMaybeAddPartitionToTransactionXXXX to testNotReadyForSendXXX\n5. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from TransactionsTest#testTimeout\n\nReviewers: Ismael Juma <ismael@confluent.io>","date":"2021-01-10 20:20:13","modifiedFileCount":"166","status":"M","submitter":"Chia-Ping Tsai"}]
