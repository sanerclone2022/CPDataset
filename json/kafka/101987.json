[{"authorTime":"2017-12-05 02:21:42","codes":[{"authorDate":"2017-12-05 02:21:42","commitOrder":1,"curCode":"    public void shouldSendPurgeData() {\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        final Map<TopicPartition, RecordsToDelete> recordsToDelete = Collections.singletonMap(t1p1, RecordsToDelete.beforeOffset(5L));\n        final DeleteRecordsResult deleteRecordsResult = new DeleteRecordsResult(Collections.singletonMap(t1p1, (KafkaFuture<DeletedRecords>) futureDeletedRecords));\n\n        futureDeletedRecords.complete(null);\n\n        EasyMock.expect(active.recordsToDelete()).andReturn(Collections.singletonMap(t1p1, 5L)).times(2);\n        EasyMock.expect(adminClient.deleteRecords(recordsToDelete)).andReturn(deleteRecordsResult).times(2);\n        replay();\n\n        taskManager.maybePurgeCommitedRecords();\n        taskManager.maybePurgeCommitedRecords();\n        verify(active, adminClient);\n    }\n","date":"2017-12-05 02:21:42","endLine":570,"groupId":"13089","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"shouldSendPurgeData","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/37/a683cb923b529bf5088ad387f983e8ff81672b.src","preCode":"    public void shouldSendPurgeData() {\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        final Map<TopicPartition, RecordsToDelete> recordsToDelete = Collections.singletonMap(t1p1, RecordsToDelete.beforeOffset(5L));\n        final DeleteRecordsResult deleteRecordsResult = new DeleteRecordsResult(Collections.singletonMap(t1p1, (KafkaFuture<DeletedRecords>) futureDeletedRecords));\n\n        futureDeletedRecords.complete(null);\n\n        EasyMock.expect(active.recordsToDelete()).andReturn(Collections.singletonMap(t1p1, 5L)).times(2);\n        EasyMock.expect(adminClient.deleteRecords(recordsToDelete)).andReturn(deleteRecordsResult).times(2);\n        replay();\n\n        taskManager.maybePurgeCommitedRecords();\n        taskManager.maybePurgeCommitedRecords();\n        verify(active, adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":556,"status":"B"},{"authorDate":"2017-12-05 02:21:42","commitOrder":1,"curCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        final KafkaFuture<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        final Map<TopicPartition, RecordsToDelete> recordsToDelete = Collections.singletonMap(t1p1, RecordsToDelete.beforeOffset(5L));\n        final DeleteRecordsResult deleteRecordsResult = new DeleteRecordsResult(Collections.singletonMap(t1p1, futureDeletedRecords));\n\n        EasyMock.expect(active.recordsToDelete()).andReturn(Collections.singletonMap(t1p1, 5L)).once();\n        EasyMock.expect(adminClient.deleteRecords(recordsToDelete)).andReturn(deleteRecordsResult).once();\n        replay();\n\n        taskManager.maybePurgeCommitedRecords();\n        \r\n        taskManager.maybePurgeCommitedRecords();\n        verify(active, adminClient);\n    }\n","date":"2017-12-05 02:21:42","endLine":586,"groupId":"14491","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"shouldNotSendPurgeDataIfPreviousNotDone","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/37/a683cb923b529bf5088ad387f983e8ff81672b.src","preCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        final KafkaFuture<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        final Map<TopicPartition, RecordsToDelete> recordsToDelete = Collections.singletonMap(t1p1, RecordsToDelete.beforeOffset(5L));\n        final DeleteRecordsResult deleteRecordsResult = new DeleteRecordsResult(Collections.singletonMap(t1p1, futureDeletedRecords));\n\n        EasyMock.expect(active.recordsToDelete()).andReturn(Collections.singletonMap(t1p1, 5L)).once();\n        EasyMock.expect(adminClient.deleteRecords(recordsToDelete)).andReturn(deleteRecordsResult).once();\n        replay();\n\n        taskManager.maybePurgeCommitedRecords();\n        \r\n        taskManager.maybePurgeCommitedRecords();\n        verify(active, adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":573,"status":"B"}],"commitId":"4b8a29f12a142d02be0b64eac71975b6a129d04a","commitMessage":"@@@KAFKA-6150: KIP-204 part III; Purge repartition topics with the admin client\n\n1. Add the repartition topics information into ProcessorTopology: personally I do not like leaking this information into the topology but it seems not other simple way around.\n2. StreamTask: added one more function to expose the consumed offsets from repartition topics only.\n3. TaskManager: use the AdminClient to send the gathered offsets to delete only if a) previous call has completed and client intentionally ignore-and-log any errors.  or b) no requests have ever called before.\n\nNOTE that this code depends on the assumption that purge is only called right after the commit has succeeded.  hence we presume all consumed offsets are committed.\n\n4. MINOR: Added a few more constructor for ProcessorTopology for cleaner unit tests.\n5. MINOR: Extracted MockStateStore out of the deprecated class.\n6. MINOR: Made a pass over some unit test classes for clean ups.\n\nAuthor: Guozhang Wang <wangguoz@gmail.com>\n\nReviewers: Matthias J. Sax <matthias@confluent.io>.  Damian Guy <damian.guy@gmail.com>\n\nCloses #4270 from guozhangwang/K6150-purge-repartition-topics\n","date":"2017-12-05 02:21:42","modifiedFileCount":"20","status":"B","submitter":"Guozhang Wang"},{"authorTime":"2019-05-30 23:33:37","codes":[{"authorDate":"2019-05-30 23:33:37","commitOrder":2,"curCode":"    public void shouldSendPurgeData() {\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        final Map<TopicPartition, RecordsToDelete> recordsToDelete = Collections.singletonMap(t1p1, RecordsToDelete.beforeOffset(5L));\n        final DeleteRecordsResult deleteRecordsResult = new DeleteRecordsResult(Collections.singletonMap(t1p1, (KafkaFuture<DeletedRecords>) futureDeletedRecords));\n\n        futureDeletedRecords.complete(null);\n\n        expect(active.recordsToDelete()).andReturn(Collections.singletonMap(t1p1, 5L)).times(2);\n        expect(adminClient.deleteRecords(recordsToDelete)).andReturn(deleteRecordsResult).times(2);\n        replay();\n\n        taskManager.maybePurgeCommitedRecords();\n        taskManager.maybePurgeCommitedRecords();\n        verify(active, adminClient);\n    }\n","date":"2019-05-30 23:33:37","endLine":562,"groupId":"13089","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"shouldSendPurgeData","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/fc/f275b104d0229e3118ce80c4728a8096ac326e.src","preCode":"    public void shouldSendPurgeData() {\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        final Map<TopicPartition, RecordsToDelete> recordsToDelete = Collections.singletonMap(t1p1, RecordsToDelete.beforeOffset(5L));\n        final DeleteRecordsResult deleteRecordsResult = new DeleteRecordsResult(Collections.singletonMap(t1p1, (KafkaFuture<DeletedRecords>) futureDeletedRecords));\n\n        futureDeletedRecords.complete(null);\n\n        EasyMock.expect(active.recordsToDelete()).andReturn(Collections.singletonMap(t1p1, 5L)).times(2);\n        EasyMock.expect(adminClient.deleteRecords(recordsToDelete)).andReturn(deleteRecordsResult).times(2);\n        replay();\n\n        taskManager.maybePurgeCommitedRecords();\n        taskManager.maybePurgeCommitedRecords();\n        verify(active, adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":548,"status":"M"},{"authorDate":"2019-05-30 23:33:37","commitOrder":2,"curCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        final KafkaFuture<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        final Map<TopicPartition, RecordsToDelete> recordsToDelete = Collections.singletonMap(t1p1, RecordsToDelete.beforeOffset(5L));\n        final DeleteRecordsResult deleteRecordsResult = new DeleteRecordsResult(Collections.singletonMap(t1p1, futureDeletedRecords));\n\n        expect(active.recordsToDelete()).andReturn(Collections.singletonMap(t1p1, 5L)).once();\n        expect(adminClient.deleteRecords(recordsToDelete)).andReturn(deleteRecordsResult).once();\n        replay();\n\n        taskManager.maybePurgeCommitedRecords();\n        \r\n        taskManager.maybePurgeCommitedRecords();\n        verify(active, adminClient);\n    }\n","date":"2019-05-30 23:33:37","endLine":578,"groupId":"14491","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"shouldNotSendPurgeDataIfPreviousNotDone","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/fc/f275b104d0229e3118ce80c4728a8096ac326e.src","preCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        final KafkaFuture<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        final Map<TopicPartition, RecordsToDelete> recordsToDelete = Collections.singletonMap(t1p1, RecordsToDelete.beforeOffset(5L));\n        final DeleteRecordsResult deleteRecordsResult = new DeleteRecordsResult(Collections.singletonMap(t1p1, futureDeletedRecords));\n\n        EasyMock.expect(active.recordsToDelete()).andReturn(Collections.singletonMap(t1p1, 5L)).once();\n        EasyMock.expect(adminClient.deleteRecords(recordsToDelete)).andReturn(deleteRecordsResult).once();\n        replay();\n\n        taskManager.maybePurgeCommitedRecords();\n        \r\n        taskManager.maybePurgeCommitedRecords();\n        verify(active, adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":565,"status":"M"}],"commitId":"9f1ce60a9c0eae99388504349870018f0dd55afa","commitMessage":"@@@KAFKA-8187: Add wait time for other thread in the same jvm to free the locks (#6818)\n\nFix KAFKA-8187: State store record loss across multiple reassignments when using standby tasks.\nDo not let the thread to transit to RUNNING until all tasks (including standby tasks) are ready.\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>.   Bill Bejeck <bbejeck@gmail.com>","date":"2019-05-30 23:33:37","modifiedFileCount":"2","status":"M","submitter":"Lifei Chen"},{"authorTime":"2020-02-05 13:06:39","codes":[{"authorDate":"2020-02-05 13:06:39","commitOrder":3,"curCode":"    public void shouldSendPurgeData() {\n        resetToStrict(adminClient);\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(17L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andReturn(singletonList(task00)).anyTimes();\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.checkForCompletedRestoration(), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","date":"2020-02-05 13:06:39","endLine":531,"groupId":"6581","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"shouldSendPurgeData","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/db/c237a56ec342266470947bb7ea7915d24e10b6.src","preCode":"    public void shouldSendPurgeData() {\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        final Map<TopicPartition, RecordsToDelete> recordsToDelete = Collections.singletonMap(t1p1, RecordsToDelete.beforeOffset(5L));\n        final DeleteRecordsResult deleteRecordsResult = new DeleteRecordsResult(Collections.singletonMap(t1p1, (KafkaFuture<DeletedRecords>) futureDeletedRecords));\n\n        futureDeletedRecords.complete(null);\n\n        expect(active.recordsToDelete()).andReturn(Collections.singletonMap(t1p1, 5L)).times(2);\n        expect(adminClient.deleteRecords(recordsToDelete)).andReturn(deleteRecordsResult).times(2);\n        replay();\n\n        taskManager.maybePurgeCommitedRecords();\n        taskManager.maybePurgeCommitedRecords();\n        verify(active, adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":497,"status":"M"},{"authorDate":"2020-02-05 13:06:39","commitOrder":3,"curCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        resetToStrict(adminClient);\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, futureDeletedRecords)));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andReturn(singletonList(task00)).anyTimes();\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.checkForCompletedRestoration(), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        \r\n        \r\n        \r\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","date":"2020-02-05 13:06:39","endLine":570,"groupId":"6581","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"shouldNotSendPurgeDataIfPreviousNotDone","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/db/c237a56ec342266470947bb7ea7915d24e10b6.src","preCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        final KafkaFuture<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        final Map<TopicPartition, RecordsToDelete> recordsToDelete = Collections.singletonMap(t1p1, RecordsToDelete.beforeOffset(5L));\n        final DeleteRecordsResult deleteRecordsResult = new DeleteRecordsResult(Collections.singletonMap(t1p1, futureDeletedRecords));\n\n        expect(active.recordsToDelete()).andReturn(Collections.singletonMap(t1p1, 5L)).once();\n        expect(adminClient.deleteRecords(recordsToDelete)).andReturn(deleteRecordsResult).once();\n        replay();\n\n        taskManager.maybePurgeCommitedRecords();\n        \r\n        taskManager.maybePurgeCommitedRecords();\n        verify(active, adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":534,"status":"M"}],"commitId":"4090f9a2b0a95e4da127e4786007542276d97520","commitMessage":"@@@KAFKA-9113: Clean up task management and state management (#7997)\n\nThis PR is collaborated by Guozhang Wang and John Roesler. It is a significant tech debt cleanup on task management and state management.  and is broken down by several sub-tasks listed below:\n\nExtract embedded clients (producer and consumer) into RecordCollector from StreamTask.\nguozhangwang#2\nguozhangwang#5\n\nConsolidate the standby updating and active restoring logic into ChangelogReader and extract out of StreamThread.\nguozhangwang#3\nguozhangwang#4\n\nIntroduce Task state life cycle (created.  restoring.  running.  suspended.  closing).  and refactor the task operations based on the current state.\nguozhangwang#6\nguozhangwang#7\n\nConsolidate AssignedTasks into TaskManager and simplify the logic of changelog management and task management (since they are already moved in step 2) and 3)).\nguozhangwang#8\nguozhangwang#9\n\nAlso simplified the StreamThread logic a bit as the embedded clients / changelog restoration logic has been moved into step 1) and 2).\nguozhangwang#10\n\nReviewers: A. Sophie Blee-Goldman <sophie@confluent.io>.  Bruno Cadonna <bruno@confluent.io>.  Boyang Chen <boyang@confluent.io>","date":"2020-02-05 13:06:39","modifiedFileCount":"76","status":"M","submitter":"Guozhang Wang"},{"authorTime":"2020-02-21 05:24:38","codes":[{"authorDate":"2020-02-21 05:24:38","commitOrder":4,"curCode":"    public void shouldSendPurgeData() {\n        resetToStrict(adminClient);\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(17L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andReturn(singletonList(task00)).anyTimes();\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","date":"2020-02-21 05:24:38","endLine":531,"groupId":"6581","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"shouldSendPurgeData","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/df/d29c63fd3106b31582b07b7bfc3fd3a9e2a6ee.src","preCode":"    public void shouldSendPurgeData() {\n        resetToStrict(adminClient);\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(17L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andReturn(singletonList(task00)).anyTimes();\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.checkForCompletedRestoration(), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":497,"status":"M"},{"authorDate":"2020-02-21 05:24:38","commitOrder":4,"curCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        resetToStrict(adminClient);\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, futureDeletedRecords)));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andReturn(singletonList(task00)).anyTimes();\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        \r\n        \r\n        \r\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","date":"2020-02-21 05:24:38","endLine":570,"groupId":"6581","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"shouldNotSendPurgeDataIfPreviousNotDone","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/df/d29c63fd3106b31582b07b7bfc3fd3a9e2a6ee.src","preCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        resetToStrict(adminClient);\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, futureDeletedRecords)));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andReturn(singletonList(task00)).anyTimes();\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.checkForCompletedRestoration(), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        \r\n        \r\n        \r\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":534,"status":"M"}],"commitId":"0d16c26e3c0affadfada1ce91faa617dc065dbcd","commitMessage":"@@@HOTFIX: don't try to remove uninitialized changelogs from assignment & don't prematurely mark task closed (#8140)\n\nThis fixes two issues which together caused the soak to crash/some test to fail occasionally.\n\nWhat happened was: In the main StreamThread loop we initialized a new task in TaskManager#checkForCompletedRestoration which includes registering.  but not initializing.  its changelogs. We then complete the loop and call poll.  which resulted in a rebalance that revoked the newly-initialized task. In TaskManager#handleAssignment we then closed the task cleanly and go to remove the changelogs from the StoreChangelogReader only to get an IllegalStateException because the changelog partitions were not in the restore consumer's assignment (due to being uninitialized).\n\nThis by itself should^ be a recoverable error.  as we catch exceptions here and retry closing the task as unclean. Of course the task actually was successfully closed (clean) so we now get an unexpected exception Illegal state CLOSED while closing active task\n\nThe fix(es) I'd propose are:\n\n1. Keep the restore consumer's assignment in sync with the registered changelogs.  ie the set ChangelogReader#changelogs but pause them until they are initialized edit: since the consumer does still perform some actions (gg fetches) on paused partitions.  we should avoid adding uninitialized changelogs to the restore consumer's assignment. Instead.  we should just skip them when removing.\n2. Move the StoreChangelogReader#remove call to before the task.closeClean so that the task is only marked as closed if everything was successful. We should do so regardless.  as we should (attempt to) remove the changelogs even if the clean close failed and we must do unclean.\n\nReviewers: John Roesler <john@confluent.io>.  Guozhang Wang <wangguoz@gmail.com>","date":"2020-02-21 05:24:38","modifiedFileCount":"7","status":"M","submitter":"A. Sophie Blee-Goldman"},{"authorTime":"2020-03-14 11:56:59","codes":[{"authorDate":"2020-03-14 11:56:59","commitOrder":5,"curCode":"    public void shouldSendPurgeData() {\n        resetToStrict(adminClient);\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(17L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andReturn(singletonList(task00)).anyTimes();\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","date":"2020-03-14 11:56:59","endLine":1162,"groupId":"6581","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"shouldSendPurgeData","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/62/043e391cd183f1929bbcf3fbb49169d473a9e2.src","preCode":"    public void shouldSendPurgeData() {\n        resetToStrict(adminClient);\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(17L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andReturn(singletonList(task00)).anyTimes();\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1128,"status":"M"},{"authorDate":"2020-03-14 11:56:59","commitOrder":5,"curCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        resetToStrict(adminClient);\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, futureDeletedRecords)));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andReturn(singletonList(task00)).anyTimes();\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        \r\n        \r\n        \r\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","date":"2020-03-14 11:56:59","endLine":1201,"groupId":"6581","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"shouldNotSendPurgeDataIfPreviousNotDone","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/62/043e391cd183f1929bbcf3fbb49169d473a9e2.src","preCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        resetToStrict(adminClient);\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, futureDeletedRecords)));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andReturn(singletonList(task00)).anyTimes();\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        \r\n        \r\n        \r\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1165,"status":"M"}],"commitId":"542853d99b9e0d660a9cf9317be8a3f8fce4c765","commitMessage":"@@@KAFKA-6145: Pt 2. Include offset sums in subscription (#8246)\n\nKIP-441 Pt. 2: Compute sum of offsets across all stores/changelogs in a task and include them in the subscription.\n\nPreviously each thread would just encode every task on disk.  but we now need to read the changelog file which is unsafe to do without a lock on the task directory. So.  each thread now encodes only its assigned active and standby tasks.  and ignores any already-locked tasks.\n\nIn some cases there may be unowned and unlocked tasks on disk that were reassigned to another instance and haven't been cleaned up yet by the background thread. Each StreamThread makes a weak effort to lock any such task directories it finds.  and if successful is then responsible for computing and reporting that task's offset sum (based on reading the checkpoint file)\n\nThis PR therefore also addresses two orthogonal issues:\n\n1. Prevent background cleaner thread from deleting unowned stores during a rebalance\n2. Deduplicate standby tasks in subscription: each thread used to include every (non-active) task found on disk in its \"standby task\" set.  which meant every active.  standby.  and unowned task was encoded by every thread.\n\nReviewers: Bruno Cadonna <bruno@confluent.io>.  John Roesler <vvcephei@apache.org>","date":"2020-03-14 11:56:59","modifiedFileCount":"10","status":"M","submitter":"A. Sophie Blee-Goldman"},{"authorTime":"2020-10-20 02:07:56","codes":[{"authorDate":"2020-10-20 02:07:56","commitOrder":6,"curCode":"    public void shouldSendPurgeData() {\n        resetToStrict(adminClient);\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(17L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andReturn(singletonList(task00)).anyTimes();\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(time.milliseconds()), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","date":"2020-10-20 02:07:56","endLine":1959,"groupId":"6581","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"shouldSendPurgeData","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/61/5e148d7adfac58b89f87a980544d1d0dd371d2.src","preCode":"    public void shouldSendPurgeData() {\n        resetToStrict(adminClient);\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(17L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andReturn(singletonList(task00)).anyTimes();\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1925,"status":"M"},{"authorDate":"2020-10-20 02:07:56","commitOrder":6,"curCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        resetToStrict(adminClient);\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, futureDeletedRecords)));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andReturn(singletonList(task00)).anyTimes();\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(time.milliseconds()), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        \r\n        \r\n        \r\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","date":"2020-10-20 02:07:56","endLine":1998,"groupId":"6581","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"shouldNotSendPurgeDataIfPreviousNotDone","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/61/5e148d7adfac58b89f87a980544d1d0dd371d2.src","preCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        resetToStrict(adminClient);\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, futureDeletedRecords)));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andReturn(singletonList(task00)).anyTimes();\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        \r\n        \r\n        \r\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1962,"status":"M"}],"commitId":"aef6cd6e9995b42db2cefa7d715321d0edee5628","commitMessage":"@@@KAFKA-9274: Add timeout handling for state restore and StandbyTasks (#9368)\n\n* Part of KIP-572\n* If a TimeoutException happens during restore of active tasks.  or updating standby tasks.  we need to trigger task.timeout.ms timeout.\n\nReviewers: John Roesler <john@confluent.io>","date":"2020-10-20 02:07:56","modifiedFileCount":"15","status":"M","submitter":"Matthias J. Sax"},{"authorTime":"2021-01-22 00:52:34","codes":[{"authorDate":"2021-01-22 00:52:34","commitOrder":7,"curCode":"    public void shouldSendPurgeData() {\n        resetToStrict(adminClient);\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(17L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andStubReturn(singletonList(task00));\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(time.milliseconds()), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","date":"2021-01-22 00:52:34","endLine":2021,"groupId":"6581","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"shouldSendPurgeData","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/1b/a1151510f36176da74cab4e2af690ba566a41a.src","preCode":"    public void shouldSendPurgeData() {\n        resetToStrict(adminClient);\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(17L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andReturn(singletonList(task00)).anyTimes();\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(time.milliseconds()), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1987,"status":"M"},{"authorDate":"2021-01-22 00:52:34","commitOrder":7,"curCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        resetToStrict(adminClient);\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, futureDeletedRecords)));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andStubReturn(singletonList(task00));\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(time.milliseconds()), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        \r\n        \r\n        \r\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","date":"2021-01-22 00:52:34","endLine":2060,"groupId":"6581","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"shouldNotSendPurgeDataIfPreviousNotDone","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/1b/a1151510f36176da74cab4e2af690ba566a41a.src","preCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        resetToStrict(adminClient);\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, futureDeletedRecords)));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andReturn(singletonList(task00)).anyTimes();\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(time.milliseconds()), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        \r\n        \r\n        \r\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":2024,"status":"M"}],"commitId":"92e72f7bf96841d7991f1d71f440c2da06dd89cf","commitMessage":"@@@KAFKA-12185: fix ConcurrentModificationException in newly added Tasks container class (#9940)\n\nReviewers: Guozhang Wang <guozhand@confluent.io>.  A. Sophie Blee-Goldman <sophie@confluent.io>","date":"2021-01-22 00:52:34","modifiedFileCount":"2","status":"M","submitter":"Matthias J. Sax"},{"authorTime":"2021-02-06 09:25:50","codes":[{"authorDate":"2021-02-06 09:25:50","commitOrder":8,"curCode":"    public void shouldSendPurgeData() {\n        resetToStrict(adminClient);\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(17L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andStubReturn(singletonList(task00));\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(time.milliseconds(), null), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","date":"2021-02-06 09:25:50","endLine":1999,"groupId":"6581","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"shouldSendPurgeData","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/9a/3a8c92c30322fe86e22b4a1174900049e5d642.src","preCode":"    public void shouldSendPurgeData() {\n        resetToStrict(adminClient);\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(17L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andStubReturn(singletonList(task00));\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(time.milliseconds()), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1965,"status":"M"},{"authorDate":"2021-02-06 09:25:50","commitOrder":8,"curCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        resetToStrict(adminClient);\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, futureDeletedRecords)));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andStubReturn(singletonList(task00));\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(time.milliseconds(), null), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        \r\n        \r\n        \r\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","date":"2021-02-06 09:25:50","endLine":2038,"groupId":"6581","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"shouldNotSendPurgeDataIfPreviousNotDone","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/9a/3a8c92c30322fe86e22b4a1174900049e5d642.src","preCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        resetToStrict(adminClient);\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, futureDeletedRecords)));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andStubReturn(singletonList(task00));\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(time.milliseconds()), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        \r\n        \r\n        \r\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":2002,"status":"M"}],"commitId":"0bc394cc1d19f1e41dd6646e9ac0e09b91fb1398","commitMessage":"@@@KAFKA-9274: handle TimeoutException on task reset (#10000)\n\nPart of KIP-572: We move the offset reset for the internal \"main consumer\" when we revive a corrupted task.  from the \"task cleanup\" code path.  to the \"task init\" code path. For this case.  we have already logic in place to handle TimeoutException that might be thrown by consumer#committed() method call.\n\nReviewers: A. Sophie Blee-Goldman <sophie@confluent.io>\n","date":"2021-02-06 09:25:50","modifiedFileCount":"10","status":"M","submitter":"Matthias J. Sax"},{"authorTime":"2021-02-07 05:04:30","codes":[{"authorDate":"2021-02-07 05:04:30","commitOrder":9,"curCode":"    public void shouldSendPurgeData() {\n        resetToStrict(adminClient);\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(17L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andStubReturn(singletonList(task00));\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(time.milliseconds()), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","date":"2021-02-07 05:04:30","endLine":2023,"groupId":"6581","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"shouldSendPurgeData","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/36/224e0cd5c002edd02d0ef2ee8e1586fa832b3d.src","preCode":"    public void shouldSendPurgeData() {\n        resetToStrict(adminClient);\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(17L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andStubReturn(singletonList(task00));\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(time.milliseconds(), null), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1989,"status":"M"},{"authorDate":"2021-02-07 05:04:30","commitOrder":9,"curCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        resetToStrict(adminClient);\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, futureDeletedRecords)));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andStubReturn(singletonList(task00));\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(time.milliseconds()), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        \r\n        \r\n        \r\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","date":"2021-02-07 05:04:30","endLine":2062,"groupId":"6581","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"shouldNotSendPurgeDataIfPreviousNotDone","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/36/224e0cd5c002edd02d0ef2ee8e1586fa832b3d.src","preCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        resetToStrict(adminClient);\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, futureDeletedRecords)));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andStubReturn(singletonList(task00));\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(time.milliseconds(), null), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        \r\n        \r\n        \r\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":2026,"status":"M"}],"commitId":"d2cb2dc45d536ae124e3da25d6d5a4e932a23a27","commitMessage":"@@@KAFKA-9751: Forward CreateTopicsRequest for FindCoordinator/Metadata when topic creation is needed (#9579)\n\nConsolidate auto topic creation logic to either forward a CreateTopicRequest or handling the creation directly as AutoTopicCreationManager.  when handling FindCoordinator/Metadata request.\n\nCo-authored-by: Jason Gustafson <jason@confluent.io>\n\nReviewers: Jason Gustafson <jason@confluent.io>","date":"2021-02-07 05:04:30","modifiedFileCount":"15","status":"M","submitter":"Boyang Chen"},{"authorTime":"2021-03-23 04:39:29","codes":[{"authorDate":"2021-03-23 04:39:29","commitOrder":10,"curCode":"    public void shouldSendPurgeData() {\n        resetToStrict(adminClient);\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(17L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andStubReturn(singletonList(task00));\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(time.milliseconds(), null), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","date":"2021-03-23 04:39:29","endLine":2018,"groupId":"101987","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"shouldSendPurgeData","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/49/ee261715837dd2363bd8ef051c1d1588778d92.src","preCode":"    public void shouldSendPurgeData() {\n        resetToStrict(adminClient);\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(17L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, completedFuture())));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andStubReturn(singletonList(task00));\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(time.milliseconds()), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1984,"status":"M"},{"authorDate":"2021-03-23 04:39:29","commitOrder":10,"curCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        resetToStrict(adminClient);\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, futureDeletedRecords)));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andStubReturn(singletonList(task00));\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(time.milliseconds(), null), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        \r\n        \r\n        \r\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","date":"2021-03-23 04:39:29","endLine":2057,"groupId":"101987","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"shouldNotSendPurgeDataIfPreviousNotDone","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/49/ee261715837dd2363bd8ef051c1d1588778d92.src","preCode":"    public void shouldNotSendPurgeDataIfPreviousNotDone() {\n        resetToStrict(adminClient);\n        final KafkaFutureImpl<DeletedRecords> futureDeletedRecords = new KafkaFutureImpl<>();\n        expect(adminClient.deleteRecords(singletonMap(t1p1, RecordsToDelete.beforeOffset(5L))))\n            .andReturn(new DeleteRecordsResult(singletonMap(t1p1, futureDeletedRecords)));\n        replay(adminClient);\n\n        final Map<TopicPartition, Long> purgableOffsets = new HashMap<>();\n        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true) {\n            @Override\n            public Map<TopicPartition, Long> purgeableOffsets() {\n                return purgableOffsets;\n            }\n        };\n\n        expectRestoreToBeCompleted(consumer, changeLogReader);\n        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n            .andStubReturn(singletonList(task00));\n\n        replay(activeTaskCreator, consumer, changeLogReader);\n\n        taskManager.handleAssignment(taskId00Assignment, emptyMap());\n        assertThat(taskManager.tryToCompleteRestoration(time.milliseconds()), is(true));\n\n        assertThat(task00.state(), is(Task.State.RUNNING));\n\n        purgableOffsets.put(t1p1, 5L);\n        taskManager.maybePurgeCommittedRecords();\n\n        \r\n        \r\n        \r\n        purgableOffsets.put(t1p1, 17L);\n        taskManager.maybePurgeCommittedRecords();\n\n        verify(adminClient);\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":2021,"status":"M"}],"commitId":"80f373d34f7716a54fa9ec1e37a27c65cbbae0f2","commitMessage":"@@@(Cherry-pick) KAFKA-9274: handle TimeoutException on task reset (#10000) (#10372)\n\nThis PR was removed by accident in trunk and 2.8.  bringing it back.\n\nCo-authored-by: Matthias J. Sax <matthias@confluent.io>\nReviewers: Matthias J. Sax <matthias@confluent.io>","date":"2021-03-23 04:39:29","modifiedFileCount":"10","status":"M","submitter":"Boyang Chen"}]
