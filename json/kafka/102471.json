[{"authorTime":"2019-10-11 07:23:18","codes":[{"authorDate":"2019-10-11 07:23:18","commitOrder":1,"curCode":"    private void runTest(final String optimizationConfig, final int expectedNumberRepartitionTopics) {\n\n        final StreamsBuilder builder = new StreamsBuilder();\n\n        final KStream<String, String> sourceStream =\n            builder.stream(INPUT_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceStream\"));\n\n        final KStream<String, String> mappedStream = sourceStream\n            .map((k, v) -> KeyValue.pair(k.toUpperCase(Locale.getDefault()), v), Named.as(\"source-map\"));\n\n        mappedStream\n            .filter((k, v) -> k.equals(\"B\"), Named.as(\"process-filter\"))\n            .mapValues(v -> v.toUpperCase(Locale.getDefault()), Named.as(\"process-mapValues\"))\n            .process(() -> new SimpleProcessor(processorValueCollector), Named.as(\"process\"));\n\n        final KStream<String, Long> countStream = mappedStream\n            .groupByKey(Grouped.as(\"count-groupByKey\"))\n            .count(Named.as(\"count\"), Materialized.<String, Long>as(Stores.inMemoryKeyValueStore(\"count-store\"))\n                                                                .withKeySerde(Serdes.String())\n                                                                .withValueSerde(Serdes.Long()))\n            .toStream(Named.as(\"count-toStream\"));\n\n        countStream.to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()).withName(\"count-to\"));\n\n        mappedStream\n            .groupByKey(Grouped.as(\"aggregate-groupByKey\"))\n            .aggregate(initializer,\n                       aggregator,\n                       Named.as(\"aggregate\"),\n                       Materialized.<String, Integer>as(Stores.inMemoryKeyValueStore(\"aggregate-store\"))\n                                                    .withKeySerde(Serdes.String())\n                                                    .withValueSerde(Serdes.Integer()))\n            .toStream(Named.as(\"aggregate-toStream\"))\n            .to(AGGREGATION_TOPIC, Produced.with(Serdes.String(), Serdes.Integer()).withName(\"reduce-to\"));\n\n        \r\n        mappedStream\n            .filter((k, v) -> true, Named.as(\"reduce-filter\"))\n            .peek((k, v) -> System.out.println(k + \":\" + v), Named.as(\"reduce-peek\"))\n            .groupByKey(Grouped.as(\"reduce-groupByKey\"))\n            .reduce(reducer,\n                    Named.as(\"reducer\"),\n                    Materialized.as(Stores.inMemoryKeyValueStore(\"reduce-store\")))\n            .toStream(Named.as(\"reduce-toStream\"))\n            .to(REDUCE_TOPIC, Produced.with(Serdes.String(), Serdes.String()));\n\n        mappedStream\n            .filter((k, v) -> k.equals(\"A\"), Named.as(\"join-filter\"))\n            .join(countStream, (v1, v2) -> v1 + \":\" + v2.toString(),\n                  JoinWindows.of(ofMillis(5000)),\n                  StreamJoined.<String, String, Long>with(Stores.inMemoryWindowStore(\"join-store\", ofDays(1), ofMillis(10000), true),\n                                                          Stores.inMemoryWindowStore(\"other-join-store\",  ofDays(1), ofMillis(10000), true))\n                                                    .withName(\"join\")\n                                                    .withKeySerde(Serdes.String())\n                                                    .withValueSerde(Serdes.String())\n                                                    .withOtherValueSerde(Serdes.Long()))\n            .to(JOINED_TOPIC, Produced.as(\"join-to\"));\n\n        streamsConfiguration.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION, optimizationConfig);\n        final Topology topology = builder.build(streamsConfiguration);\n\n        topologyTestDriver = new TopologyTestDriver(topology, streamsConfiguration);\n\n        final TestInputTopic<String, String> inputTopicA = topologyTestDriver.createInputTopic(INPUT_TOPIC, stringSerializer, stringSerializer);\n        final TestOutputTopic<String, Long> countOutputTopic = topologyTestDriver.createOutputTopic(COUNT_TOPIC, stringDeserializer, new LongDeserializer());\n        final TestOutputTopic<String, Integer> aggregationOutputTopic = topologyTestDriver.createOutputTopic(AGGREGATION_TOPIC, stringDeserializer, new IntegerDeserializer());\n        final TestOutputTopic<String, String> reduceOutputTopic = topologyTestDriver.createOutputTopic(REDUCE_TOPIC, stringDeserializer, stringDeserializer);\n        final TestOutputTopic<String, String> joinedOutputTopic = topologyTestDriver.createOutputTopic(JOINED_TOPIC, stringDeserializer, stringDeserializer);\n\n        inputTopicA.pipeKeyValueList(getKeyValues());\n\n        \r\n        final String topologyString = topology.describe().toString();\n        if (optimizationConfig.equals(StreamsConfig.OPTIMIZE)) {\n            assertEquals(EXPECTED_OPTIMIZED_TOPOLOGY, topologyString);\n        } else {\n            assertEquals(EXPECTED_UNOPTIMIZED_TOPOLOGY, topologyString);\n        }\n\n        \r\n        assertEquals(expectedNumberRepartitionTopics, getCountOfRepartitionTopicsFound(topologyString));\n\n        \r\n        assertThat(3, equalTo(processorValueCollector.size()));\n        assertThat(processorValueCollector, equalTo(expectedCollectedProcessorValues));\n\n        \r\n        assertThat(countOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedCountKeyValues)));\n        assertThat(aggregationOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedAggKeyValues)));\n        assertThat(reduceOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedReduceKeyValues)));\n        assertThat(joinedOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedJoinKeyValues)));\n    }\n","date":"2019-10-11 07:23:18","endLine":236,"groupId":"1432","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"runTest","params":"(finalStringoptimizationConfig@finalintexpectedNumberRepartitionTopics)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/84/25ad7001ae12f2449eb24b512428170f34853d.src","preCode":"    private void runTest(final String optimizationConfig, final int expectedNumberRepartitionTopics) {\n\n        final StreamsBuilder builder = new StreamsBuilder();\n\n        final KStream<String, String> sourceStream =\n            builder.stream(INPUT_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceStream\"));\n\n        final KStream<String, String> mappedStream = sourceStream\n            .map((k, v) -> KeyValue.pair(k.toUpperCase(Locale.getDefault()), v), Named.as(\"source-map\"));\n\n        mappedStream\n            .filter((k, v) -> k.equals(\"B\"), Named.as(\"process-filter\"))\n            .mapValues(v -> v.toUpperCase(Locale.getDefault()), Named.as(\"process-mapValues\"))\n            .process(() -> new SimpleProcessor(processorValueCollector), Named.as(\"process\"));\n\n        final KStream<String, Long> countStream = mappedStream\n            .groupByKey(Grouped.as(\"count-groupByKey\"))\n            .count(Named.as(\"count\"), Materialized.<String, Long>as(Stores.inMemoryKeyValueStore(\"count-store\"))\n                                                                .withKeySerde(Serdes.String())\n                                                                .withValueSerde(Serdes.Long()))\n            .toStream(Named.as(\"count-toStream\"));\n\n        countStream.to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()).withName(\"count-to\"));\n\n        mappedStream\n            .groupByKey(Grouped.as(\"aggregate-groupByKey\"))\n            .aggregate(initializer,\n                       aggregator,\n                       Named.as(\"aggregate\"),\n                       Materialized.<String, Integer>as(Stores.inMemoryKeyValueStore(\"aggregate-store\"))\n                                                    .withKeySerde(Serdes.String())\n                                                    .withValueSerde(Serdes.Integer()))\n            .toStream(Named.as(\"aggregate-toStream\"))\n            .to(AGGREGATION_TOPIC, Produced.with(Serdes.String(), Serdes.Integer()).withName(\"reduce-to\"));\n\n        \r\n        mappedStream\n            .filter((k, v) -> true, Named.as(\"reduce-filter\"))\n            .peek((k, v) -> System.out.println(k + \":\" + v), Named.as(\"reduce-peek\"))\n            .groupByKey(Grouped.as(\"reduce-groupByKey\"))\n            .reduce(reducer,\n                    Named.as(\"reducer\"),\n                    Materialized.as(Stores.inMemoryKeyValueStore(\"reduce-store\")))\n            .toStream(Named.as(\"reduce-toStream\"))\n            .to(REDUCE_TOPIC, Produced.with(Serdes.String(), Serdes.String()));\n\n        mappedStream\n            .filter((k, v) -> k.equals(\"A\"), Named.as(\"join-filter\"))\n            .join(countStream, (v1, v2) -> v1 + \":\" + v2.toString(),\n                  JoinWindows.of(ofMillis(5000)),\n                  StreamJoined.<String, String, Long>with(Stores.inMemoryWindowStore(\"join-store\", ofDays(1), ofMillis(10000), true),\n                                                          Stores.inMemoryWindowStore(\"other-join-store\",  ofDays(1), ofMillis(10000), true))\n                                                    .withName(\"join\")\n                                                    .withKeySerde(Serdes.String())\n                                                    .withValueSerde(Serdes.String())\n                                                    .withOtherValueSerde(Serdes.Long()))\n            .to(JOINED_TOPIC, Produced.as(\"join-to\"));\n\n        streamsConfiguration.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION, optimizationConfig);\n        final Topology topology = builder.build(streamsConfiguration);\n\n        topologyTestDriver = new TopologyTestDriver(topology, streamsConfiguration);\n\n        final TestInputTopic<String, String> inputTopicA = topologyTestDriver.createInputTopic(INPUT_TOPIC, stringSerializer, stringSerializer);\n        final TestOutputTopic<String, Long> countOutputTopic = topologyTestDriver.createOutputTopic(COUNT_TOPIC, stringDeserializer, new LongDeserializer());\n        final TestOutputTopic<String, Integer> aggregationOutputTopic = topologyTestDriver.createOutputTopic(AGGREGATION_TOPIC, stringDeserializer, new IntegerDeserializer());\n        final TestOutputTopic<String, String> reduceOutputTopic = topologyTestDriver.createOutputTopic(REDUCE_TOPIC, stringDeserializer, stringDeserializer);\n        final TestOutputTopic<String, String> joinedOutputTopic = topologyTestDriver.createOutputTopic(JOINED_TOPIC, stringDeserializer, stringDeserializer);\n\n        inputTopicA.pipeKeyValueList(getKeyValues());\n\n        \r\n        final String topologyString = topology.describe().toString();\n        if (optimizationConfig.equals(StreamsConfig.OPTIMIZE)) {\n            assertEquals(EXPECTED_OPTIMIZED_TOPOLOGY, topologyString);\n        } else {\n            assertEquals(EXPECTED_UNOPTIMIZED_TOPOLOGY, topologyString);\n        }\n\n        \r\n        assertEquals(expectedNumberRepartitionTopics, getCountOfRepartitionTopicsFound(topologyString));\n\n        \r\n        assertThat(3, equalTo(processorValueCollector.size()));\n        assertThat(processorValueCollector, equalTo(expectedCollectedProcessorValues));\n\n        \r\n        assertThat(countOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedCountKeyValues)));\n        assertThat(aggregationOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedAggKeyValues)));\n        assertThat(reduceOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedReduceKeyValues)));\n        assertThat(joinedOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedJoinKeyValues)));\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionOptimizingTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":145,"status":"B"},{"authorDate":"2019-10-11 07:23:18","commitOrder":1,"curCode":"    private void runTest(final String optimizationConfig, final int expectedNumberRepartitionTopics) {\n\n        streamsConfiguration.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION, optimizationConfig);\n\n        final StreamsBuilder builder = new StreamsBuilder();\n\n        final KStream<String, String> sourceAStream =\n            builder.stream(INPUT_A_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceAStream\"));\n\n        final KStream<String, String> sourceBStream =\n            builder.stream(INPUT_B_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceBStream\"));\n\n        final KStream<String, String> mappedAStream =\n            sourceAStream.map((k, v) -> KeyValue.pair(v.split(\":\")[0], v), Named.as(\"mappedAStream\"));\n        final KStream<String, String> mappedBStream =\n            sourceBStream.map((k, v) -> KeyValue.pair(v.split(\":\")[0], v), Named.as(\"mappedBStream\"));\n\n        final KStream<String, String> mergedStream = mappedAStream.merge(mappedBStream, Named.as(\"mergedStream\"));\n\n        mergedStream\n            .groupByKey(Grouped.as(\"long-groupByKey\"))\n            .count(Named.as(\"long-count\"), Materialized.as(Stores.inMemoryKeyValueStore(\"long-store\")))\n            .toStream(Named.as(\"long-toStream\"))\n            .to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()).withName(\"long-to\"));\n\n        mergedStream\n            .groupByKey(Grouped.as(\"string-groupByKey\"))\n            .count(Named.as(\"string-count\"), Materialized.as(Stores.inMemoryKeyValueStore(\"string-store\")))\n            .toStream(Named.as(\"string-toStream\"))\n            .mapValues(v -> v.toString(), Named.as(\"string-mapValues\"))\n            .to(STRING_COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.String()).withName(\"string-to\"));\n\n        final Topology topology = builder.build(streamsConfiguration);\n\n        topologyTestDriver = new TopologyTestDriver(topology, streamsConfiguration);\n\n        final TestInputTopic<String, String> inputTopicA = topologyTestDriver.createInputTopic(INPUT_A_TOPIC, stringSerializer, stringSerializer);\n        final TestInputTopic<String, String> inputTopicB = topologyTestDriver.createInputTopic(INPUT_B_TOPIC, stringSerializer, stringSerializer);\n\n        final TestOutputTopic<String, Long> countOutputTopic = topologyTestDriver.createOutputTopic(COUNT_TOPIC, stringDeserializer, new LongDeserializer());\n        final TestOutputTopic<String, String> stringCountOutputTopic = topologyTestDriver.createOutputTopic(STRING_COUNT_TOPIC, stringDeserializer, stringDeserializer);\n\n        inputTopicA.pipeKeyValueList(getKeyValues());\n        inputTopicB.pipeKeyValueList(getKeyValues());\n\n        final String topologyString = topology.describe().toString();\n\n        \r\n        if (optimizationConfig.equals(StreamsConfig.OPTIMIZE)) {\n            assertEquals(EXPECTED_OPTIMIZED_TOPOLOGY, topologyString);\n        } else {\n            assertEquals(EXPECTED_UNOPTIMIZED_TOPOLOGY, topologyString);\n        }\n\n        \r\n        assertEquals(expectedNumberRepartitionTopics, getCountOfRepartitionTopicsFound(topologyString));\n\n        \r\n        assertThat(countOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedCountKeyValues)));\n        assertThat(stringCountOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedStringCountKeyValues)));\n    }\n","date":"2019-10-11 07:23:18","endLine":181,"groupId":"8383","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"runTest","params":"(finalStringoptimizationConfig@finalintexpectedNumberRepartitionTopics)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/0d/081f71c5632485fc46bfa780b9b2679dbb7674.src","preCode":"    private void runTest(final String optimizationConfig, final int expectedNumberRepartitionTopics) {\n\n        streamsConfiguration.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION, optimizationConfig);\n\n        final StreamsBuilder builder = new StreamsBuilder();\n\n        final KStream<String, String> sourceAStream =\n            builder.stream(INPUT_A_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceAStream\"));\n\n        final KStream<String, String> sourceBStream =\n            builder.stream(INPUT_B_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceBStream\"));\n\n        final KStream<String, String> mappedAStream =\n            sourceAStream.map((k, v) -> KeyValue.pair(v.split(\":\")[0], v), Named.as(\"mappedAStream\"));\n        final KStream<String, String> mappedBStream =\n            sourceBStream.map((k, v) -> KeyValue.pair(v.split(\":\")[0], v), Named.as(\"mappedBStream\"));\n\n        final KStream<String, String> mergedStream = mappedAStream.merge(mappedBStream, Named.as(\"mergedStream\"));\n\n        mergedStream\n            .groupByKey(Grouped.as(\"long-groupByKey\"))\n            .count(Named.as(\"long-count\"), Materialized.as(Stores.inMemoryKeyValueStore(\"long-store\")))\n            .toStream(Named.as(\"long-toStream\"))\n            .to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()).withName(\"long-to\"));\n\n        mergedStream\n            .groupByKey(Grouped.as(\"string-groupByKey\"))\n            .count(Named.as(\"string-count\"), Materialized.as(Stores.inMemoryKeyValueStore(\"string-store\")))\n            .toStream(Named.as(\"string-toStream\"))\n            .mapValues(v -> v.toString(), Named.as(\"string-mapValues\"))\n            .to(STRING_COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.String()).withName(\"string-to\"));\n\n        final Topology topology = builder.build(streamsConfiguration);\n\n        topologyTestDriver = new TopologyTestDriver(topology, streamsConfiguration);\n\n        final TestInputTopic<String, String> inputTopicA = topologyTestDriver.createInputTopic(INPUT_A_TOPIC, stringSerializer, stringSerializer);\n        final TestInputTopic<String, String> inputTopicB = topologyTestDriver.createInputTopic(INPUT_B_TOPIC, stringSerializer, stringSerializer);\n\n        final TestOutputTopic<String, Long> countOutputTopic = topologyTestDriver.createOutputTopic(COUNT_TOPIC, stringDeserializer, new LongDeserializer());\n        final TestOutputTopic<String, String> stringCountOutputTopic = topologyTestDriver.createOutputTopic(STRING_COUNT_TOPIC, stringDeserializer, stringDeserializer);\n\n        inputTopicA.pipeKeyValueList(getKeyValues());\n        inputTopicB.pipeKeyValueList(getKeyValues());\n\n        final String topologyString = topology.describe().toString();\n\n        \r\n        if (optimizationConfig.equals(StreamsConfig.OPTIMIZE)) {\n            assertEquals(EXPECTED_OPTIMIZED_TOPOLOGY, topologyString);\n        } else {\n            assertEquals(EXPECTED_UNOPTIMIZED_TOPOLOGY, topologyString);\n        }\n\n        \r\n        assertEquals(expectedNumberRepartitionTopics, getCountOfRepartitionTopicsFound(topologyString));\n\n        \r\n        assertThat(countOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedCountKeyValues)));\n        assertThat(stringCountOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedStringCountKeyValues)));\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionWithMergeOptimizingTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":121,"status":"B"}],"commitId":"cc6525a74631d44d093ef75ecedb47f422004f13","commitMessage":"@@@KAFKA-8743: Flaky Test Repartition{WithMerge}OptimizingIntegrationTest (#7472)\n\nAll four flavors of the repartition/optimization tests have been reported as flaky and failed in one place or another:\n* RepartitionOptimizingIntegrationTest.shouldSendCorrectRecords_OPTIMIZED\n* RepartitionOptimizingIntegrationTest.shouldSendCorrectRecords_NO_OPTIMIZATION\n* RepartitionWithMergeOptimizingIntegrationTest.shouldSendCorrectRecords_OPTIMIZED\n* RepartitionWithMergeOptimizingIntegrationTest.shouldSendCorrectRecords_NO_OPTIMIZATION\n\nThey're pretty similar so it makes sense to knock them all out at once. This PR does three things:\n\n* Switch to in-memory stores wherever possible\n* Name all operators and update the Topology accordingly (not really a flaky test fix.  but had to update the topology names anyway because of the IM stores so figured might as well)\n* Port to TopologyTestDriver -- this is the \"real\" fix.  should make a big difference as these repartition tests required multiple roundtrips with the Kafka cluster (while using only the default timeout)\n\nReviewers: Bill Bejeck <bill@confluent.io>.  Guozhang Wang <wangguoz@gmail.com>","date":"2019-10-11 07:23:18","modifiedFileCount":"1","status":"B","submitter":"A. Sophie Blee-Goldman"},{"authorTime":"2020-06-20 08:41:06","codes":[{"authorDate":"2020-06-20 08:41:06","commitOrder":2,"curCode":"    private void runTest(final String optimizationConfig, final int expectedNumberRepartitionTopics) {\n\n        final StreamsBuilder builder = new StreamsBuilder();\n\n        final KStream<String, String> sourceStream =\n            builder.stream(INPUT_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceStream\"));\n\n        final KStream<String, String> mappedStream = sourceStream\n            .map((k, v) -> KeyValue.pair(k.toUpperCase(Locale.getDefault()), v), Named.as(\"source-map\"));\n\n        mappedStream\n            .filter((k, v) -> k.equals(\"B\"), Named.as(\"process-filter\"))\n            .mapValues(v -> v.toUpperCase(Locale.getDefault()), Named.as(\"process-mapValues\"))\n            .process(() -> new SimpleProcessor(processorValueCollector), Named.as(\"process\"));\n\n        final KStream<String, Long> countStream = mappedStream\n            .groupByKey(Grouped.as(\"count-groupByKey\"))\n            .count(Named.as(\"count\"), Materialized.<String, Long>as(Stores.inMemoryKeyValueStore(\"count-store\"))\n                                                                .withKeySerde(Serdes.String())\n                                                                .withValueSerde(Serdes.Long()))\n            .toStream(Named.as(\"count-toStream\"));\n\n        countStream.to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()).withName(\"count-to\"));\n\n        mappedStream\n            .groupByKey(Grouped.as(\"aggregate-groupByKey\"))\n            .aggregate(initializer,\n                       aggregator,\n                       Named.as(\"aggregate\"),\n                       Materialized.<String, Integer>as(Stores.inMemoryKeyValueStore(\"aggregate-store\"))\n                                                    .withKeySerde(Serdes.String())\n                                                    .withValueSerde(Serdes.Integer()))\n            .toStream(Named.as(\"aggregate-toStream\"))\n            .to(AGGREGATION_TOPIC, Produced.with(Serdes.String(), Serdes.Integer()).withName(\"reduce-to\"));\n\n        \r\n        mappedStream\n            .filter((k, v) -> true, Named.as(\"reduce-filter\"))\n            .peek((k, v) -> System.out.println(k + \":\" + v), Named.as(\"reduce-peek\"))\n            .groupByKey(Grouped.as(\"reduce-groupByKey\"))\n            .reduce(reducer,\n                    Named.as(\"reducer\"),\n                    Materialized.as(Stores.inMemoryKeyValueStore(\"reduce-store\")))\n            .toStream(Named.as(\"reduce-toStream\"))\n            .to(REDUCE_TOPIC, Produced.with(Serdes.String(), Serdes.String()));\n\n        mappedStream\n            .filter((k, v) -> k.equals(\"A\"), Named.as(\"join-filter\"))\n            .join(countStream, (v1, v2) -> v1 + \":\" + v2.toString(),\n                  JoinWindows.of(ofMillis(5000)),\n                  StreamJoined.<String, String, Long>with(Stores.inMemoryWindowStore(\"join-store\", ofDays(1), ofMillis(10000), true),\n                                                          Stores.inMemoryWindowStore(\"other-join-store\",  ofDays(1), ofMillis(10000), true))\n                                                    .withName(\"join\")\n                                                    .withKeySerde(Serdes.String())\n                                                    .withValueSerde(Serdes.String())\n                                                    .withOtherValueSerde(Serdes.Long()))\n            .to(JOINED_TOPIC, Produced.as(\"join-to\"));\n\n        streamsConfiguration.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION_CONFIG, optimizationConfig);\n        final Topology topology = builder.build(streamsConfiguration);\n\n        topologyTestDriver = new TopologyTestDriver(topology, streamsConfiguration);\n\n        final TestInputTopic<String, String> inputTopicA = topologyTestDriver.createInputTopic(INPUT_TOPIC, stringSerializer, stringSerializer);\n        final TestOutputTopic<String, Long> countOutputTopic = topologyTestDriver.createOutputTopic(COUNT_TOPIC, stringDeserializer, new LongDeserializer());\n        final TestOutputTopic<String, Integer> aggregationOutputTopic = topologyTestDriver.createOutputTopic(AGGREGATION_TOPIC, stringDeserializer, new IntegerDeserializer());\n        final TestOutputTopic<String, String> reduceOutputTopic = topologyTestDriver.createOutputTopic(REDUCE_TOPIC, stringDeserializer, stringDeserializer);\n        final TestOutputTopic<String, String> joinedOutputTopic = topologyTestDriver.createOutputTopic(JOINED_TOPIC, stringDeserializer, stringDeserializer);\n\n        inputTopicA.pipeKeyValueList(getKeyValues());\n\n        \r\n        final String topologyString = topology.describe().toString();\n        if (optimizationConfig.equals(StreamsConfig.OPTIMIZE)) {\n            assertEquals(EXPECTED_OPTIMIZED_TOPOLOGY, topologyString);\n        } else {\n            assertEquals(EXPECTED_UNOPTIMIZED_TOPOLOGY, topologyString);\n        }\n\n        \r\n        assertEquals(expectedNumberRepartitionTopics, getCountOfRepartitionTopicsFound(topologyString));\n\n        \r\n        assertThat(3, equalTo(processorValueCollector.size()));\n        assertThat(processorValueCollector, equalTo(expectedCollectedProcessorValues));\n\n        \r\n        assertThat(countOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedCountKeyValues)));\n        assertThat(aggregationOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedAggKeyValues)));\n        assertThat(reduceOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedReduceKeyValues)));\n        assertThat(joinedOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedJoinKeyValues)));\n    }\n","date":"2020-06-20 08:41:06","endLine":236,"groupId":"7297","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"runTest","params":"(finalStringoptimizationConfig@finalintexpectedNumberRepartitionTopics)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/68/1119829f4ba46c001165102f07b5d8e4d21e80.src","preCode":"    private void runTest(final String optimizationConfig, final int expectedNumberRepartitionTopics) {\n\n        final StreamsBuilder builder = new StreamsBuilder();\n\n        final KStream<String, String> sourceStream =\n            builder.stream(INPUT_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceStream\"));\n\n        final KStream<String, String> mappedStream = sourceStream\n            .map((k, v) -> KeyValue.pair(k.toUpperCase(Locale.getDefault()), v), Named.as(\"source-map\"));\n\n        mappedStream\n            .filter((k, v) -> k.equals(\"B\"), Named.as(\"process-filter\"))\n            .mapValues(v -> v.toUpperCase(Locale.getDefault()), Named.as(\"process-mapValues\"))\n            .process(() -> new SimpleProcessor(processorValueCollector), Named.as(\"process\"));\n\n        final KStream<String, Long> countStream = mappedStream\n            .groupByKey(Grouped.as(\"count-groupByKey\"))\n            .count(Named.as(\"count\"), Materialized.<String, Long>as(Stores.inMemoryKeyValueStore(\"count-store\"))\n                                                                .withKeySerde(Serdes.String())\n                                                                .withValueSerde(Serdes.Long()))\n            .toStream(Named.as(\"count-toStream\"));\n\n        countStream.to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()).withName(\"count-to\"));\n\n        mappedStream\n            .groupByKey(Grouped.as(\"aggregate-groupByKey\"))\n            .aggregate(initializer,\n                       aggregator,\n                       Named.as(\"aggregate\"),\n                       Materialized.<String, Integer>as(Stores.inMemoryKeyValueStore(\"aggregate-store\"))\n                                                    .withKeySerde(Serdes.String())\n                                                    .withValueSerde(Serdes.Integer()))\n            .toStream(Named.as(\"aggregate-toStream\"))\n            .to(AGGREGATION_TOPIC, Produced.with(Serdes.String(), Serdes.Integer()).withName(\"reduce-to\"));\n\n        \r\n        mappedStream\n            .filter((k, v) -> true, Named.as(\"reduce-filter\"))\n            .peek((k, v) -> System.out.println(k + \":\" + v), Named.as(\"reduce-peek\"))\n            .groupByKey(Grouped.as(\"reduce-groupByKey\"))\n            .reduce(reducer,\n                    Named.as(\"reducer\"),\n                    Materialized.as(Stores.inMemoryKeyValueStore(\"reduce-store\")))\n            .toStream(Named.as(\"reduce-toStream\"))\n            .to(REDUCE_TOPIC, Produced.with(Serdes.String(), Serdes.String()));\n\n        mappedStream\n            .filter((k, v) -> k.equals(\"A\"), Named.as(\"join-filter\"))\n            .join(countStream, (v1, v2) -> v1 + \":\" + v2.toString(),\n                  JoinWindows.of(ofMillis(5000)),\n                  StreamJoined.<String, String, Long>with(Stores.inMemoryWindowStore(\"join-store\", ofDays(1), ofMillis(10000), true),\n                                                          Stores.inMemoryWindowStore(\"other-join-store\",  ofDays(1), ofMillis(10000), true))\n                                                    .withName(\"join\")\n                                                    .withKeySerde(Serdes.String())\n                                                    .withValueSerde(Serdes.String())\n                                                    .withOtherValueSerde(Serdes.Long()))\n            .to(JOINED_TOPIC, Produced.as(\"join-to\"));\n\n        streamsConfiguration.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION, optimizationConfig);\n        final Topology topology = builder.build(streamsConfiguration);\n\n        topologyTestDriver = new TopologyTestDriver(topology, streamsConfiguration);\n\n        final TestInputTopic<String, String> inputTopicA = topologyTestDriver.createInputTopic(INPUT_TOPIC, stringSerializer, stringSerializer);\n        final TestOutputTopic<String, Long> countOutputTopic = topologyTestDriver.createOutputTopic(COUNT_TOPIC, stringDeserializer, new LongDeserializer());\n        final TestOutputTopic<String, Integer> aggregationOutputTopic = topologyTestDriver.createOutputTopic(AGGREGATION_TOPIC, stringDeserializer, new IntegerDeserializer());\n        final TestOutputTopic<String, String> reduceOutputTopic = topologyTestDriver.createOutputTopic(REDUCE_TOPIC, stringDeserializer, stringDeserializer);\n        final TestOutputTopic<String, String> joinedOutputTopic = topologyTestDriver.createOutputTopic(JOINED_TOPIC, stringDeserializer, stringDeserializer);\n\n        inputTopicA.pipeKeyValueList(getKeyValues());\n\n        \r\n        final String topologyString = topology.describe().toString();\n        if (optimizationConfig.equals(StreamsConfig.OPTIMIZE)) {\n            assertEquals(EXPECTED_OPTIMIZED_TOPOLOGY, topologyString);\n        } else {\n            assertEquals(EXPECTED_UNOPTIMIZED_TOPOLOGY, topologyString);\n        }\n\n        \r\n        assertEquals(expectedNumberRepartitionTopics, getCountOfRepartitionTopicsFound(topologyString));\n\n        \r\n        assertThat(3, equalTo(processorValueCollector.size()));\n        assertThat(processorValueCollector, equalTo(expectedCollectedProcessorValues));\n\n        \r\n        assertThat(countOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedCountKeyValues)));\n        assertThat(aggregationOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedAggKeyValues)));\n        assertThat(reduceOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedReduceKeyValues)));\n        assertThat(joinedOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedJoinKeyValues)));\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionOptimizingTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":145,"status":"M"},{"authorDate":"2020-06-20 08:41:06","commitOrder":2,"curCode":"    private void runTest(final String optimizationConfig, final int expectedNumberRepartitionTopics) {\n\n        streamsConfiguration.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION_CONFIG, optimizationConfig);\n\n        final StreamsBuilder builder = new StreamsBuilder();\n\n        final KStream<String, String> sourceAStream =\n            builder.stream(INPUT_A_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceAStream\"));\n\n        final KStream<String, String> sourceBStream =\n            builder.stream(INPUT_B_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceBStream\"));\n\n        final KStream<String, String> mappedAStream =\n            sourceAStream.map((k, v) -> KeyValue.pair(v.split(\":\")[0], v), Named.as(\"mappedAStream\"));\n        final KStream<String, String> mappedBStream =\n            sourceBStream.map((k, v) -> KeyValue.pair(v.split(\":\")[0], v), Named.as(\"mappedBStream\"));\n\n        final KStream<String, String> mergedStream = mappedAStream.merge(mappedBStream, Named.as(\"mergedStream\"));\n\n        mergedStream\n            .groupByKey(Grouped.as(\"long-groupByKey\"))\n            .count(Named.as(\"long-count\"), Materialized.as(Stores.inMemoryKeyValueStore(\"long-store\")))\n            .toStream(Named.as(\"long-toStream\"))\n            .to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()).withName(\"long-to\"));\n\n        mergedStream\n            .groupByKey(Grouped.as(\"string-groupByKey\"))\n            .count(Named.as(\"string-count\"), Materialized.as(Stores.inMemoryKeyValueStore(\"string-store\")))\n            .toStream(Named.as(\"string-toStream\"))\n            .mapValues(v -> v.toString(), Named.as(\"string-mapValues\"))\n            .to(STRING_COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.String()).withName(\"string-to\"));\n\n        final Topology topology = builder.build(streamsConfiguration);\n\n        topologyTestDriver = new TopologyTestDriver(topology, streamsConfiguration);\n\n        final TestInputTopic<String, String> inputTopicA = topologyTestDriver.createInputTopic(INPUT_A_TOPIC, stringSerializer, stringSerializer);\n        final TestInputTopic<String, String> inputTopicB = topologyTestDriver.createInputTopic(INPUT_B_TOPIC, stringSerializer, stringSerializer);\n\n        final TestOutputTopic<String, Long> countOutputTopic = topologyTestDriver.createOutputTopic(COUNT_TOPIC, stringDeserializer, new LongDeserializer());\n        final TestOutputTopic<String, String> stringCountOutputTopic = topologyTestDriver.createOutputTopic(STRING_COUNT_TOPIC, stringDeserializer, stringDeserializer);\n\n        inputTopicA.pipeKeyValueList(getKeyValues());\n        inputTopicB.pipeKeyValueList(getKeyValues());\n\n        final String topologyString = topology.describe().toString();\n\n        \r\n        if (optimizationConfig.equals(StreamsConfig.OPTIMIZE)) {\n            assertEquals(EXPECTED_OPTIMIZED_TOPOLOGY, topologyString);\n        } else {\n            assertEquals(EXPECTED_UNOPTIMIZED_TOPOLOGY, topologyString);\n        }\n\n        \r\n        assertEquals(expectedNumberRepartitionTopics, getCountOfRepartitionTopicsFound(topologyString));\n\n        \r\n        assertThat(countOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedCountKeyValues)));\n        assertThat(stringCountOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedStringCountKeyValues)));\n    }\n","date":"2020-06-20 08:41:06","endLine":181,"groupId":"8383","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"runTest","params":"(finalStringoptimizationConfig@finalintexpectedNumberRepartitionTopics)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/b5/0ef8ecee0cbe4dd0eca682cab444bfb7964cf4.src","preCode":"    private void runTest(final String optimizationConfig, final int expectedNumberRepartitionTopics) {\n\n        streamsConfiguration.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION, optimizationConfig);\n\n        final StreamsBuilder builder = new StreamsBuilder();\n\n        final KStream<String, String> sourceAStream =\n            builder.stream(INPUT_A_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceAStream\"));\n\n        final KStream<String, String> sourceBStream =\n            builder.stream(INPUT_B_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceBStream\"));\n\n        final KStream<String, String> mappedAStream =\n            sourceAStream.map((k, v) -> KeyValue.pair(v.split(\":\")[0], v), Named.as(\"mappedAStream\"));\n        final KStream<String, String> mappedBStream =\n            sourceBStream.map((k, v) -> KeyValue.pair(v.split(\":\")[0], v), Named.as(\"mappedBStream\"));\n\n        final KStream<String, String> mergedStream = mappedAStream.merge(mappedBStream, Named.as(\"mergedStream\"));\n\n        mergedStream\n            .groupByKey(Grouped.as(\"long-groupByKey\"))\n            .count(Named.as(\"long-count\"), Materialized.as(Stores.inMemoryKeyValueStore(\"long-store\")))\n            .toStream(Named.as(\"long-toStream\"))\n            .to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()).withName(\"long-to\"));\n\n        mergedStream\n            .groupByKey(Grouped.as(\"string-groupByKey\"))\n            .count(Named.as(\"string-count\"), Materialized.as(Stores.inMemoryKeyValueStore(\"string-store\")))\n            .toStream(Named.as(\"string-toStream\"))\n            .mapValues(v -> v.toString(), Named.as(\"string-mapValues\"))\n            .to(STRING_COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.String()).withName(\"string-to\"));\n\n        final Topology topology = builder.build(streamsConfiguration);\n\n        topologyTestDriver = new TopologyTestDriver(topology, streamsConfiguration);\n\n        final TestInputTopic<String, String> inputTopicA = topologyTestDriver.createInputTopic(INPUT_A_TOPIC, stringSerializer, stringSerializer);\n        final TestInputTopic<String, String> inputTopicB = topologyTestDriver.createInputTopic(INPUT_B_TOPIC, stringSerializer, stringSerializer);\n\n        final TestOutputTopic<String, Long> countOutputTopic = topologyTestDriver.createOutputTopic(COUNT_TOPIC, stringDeserializer, new LongDeserializer());\n        final TestOutputTopic<String, String> stringCountOutputTopic = topologyTestDriver.createOutputTopic(STRING_COUNT_TOPIC, stringDeserializer, stringDeserializer);\n\n        inputTopicA.pipeKeyValueList(getKeyValues());\n        inputTopicB.pipeKeyValueList(getKeyValues());\n\n        final String topologyString = topology.describe().toString();\n\n        \r\n        if (optimizationConfig.equals(StreamsConfig.OPTIMIZE)) {\n            assertEquals(EXPECTED_OPTIMIZED_TOPOLOGY, topologyString);\n        } else {\n            assertEquals(EXPECTED_UNOPTIMIZED_TOPOLOGY, topologyString);\n        }\n\n        \r\n        assertEquals(expectedNumberRepartitionTopics, getCountOfRepartitionTopicsFound(topologyString));\n\n        \r\n        assertThat(countOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedCountKeyValues)));\n        assertThat(stringCountOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedStringCountKeyValues)));\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionWithMergeOptimizingTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":121,"status":"M"}],"commitId":"712cc5d073da7595dab836c95372c13fc61047ee","commitMessage":"@@@KAFKA-10168: fix StreamsConfig parameter name variable (#8865)\n\nImplements KIP-626.\n\nReviewers: Boyang Chen <boyang@confluent.io>.  John Roesler <john@confluent.io>","date":"2020-06-20 08:41:06","modifiedFileCount":"20","status":"M","submitter":"Matthias J. Sax"},{"authorTime":"2020-06-20 08:41:06","codes":[{"authorDate":"2021-03-29 03:33:40","commitOrder":3,"curCode":"    private void runTest(final String optimizationConfig, final int expectedNumberRepartitionTopics) {\n\n        final StreamsBuilder builder = new StreamsBuilder();\n\n        final KStream<String, String> sourceStream =\n            builder.stream(INPUT_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceStream\"));\n\n        final KStream<String, String> mappedStream = sourceStream\n            .map((k, v) -> KeyValue.pair(k.toUpperCase(Locale.getDefault()), v), Named.as(\"source-map\"));\n\n        mappedStream\n            .filter((k, v) -> k.equals(\"B\"), Named.as(\"process-filter\"))\n            .mapValues(v -> v.toUpperCase(Locale.getDefault()), Named.as(\"process-mapValues\"))\n            .process(() -> new SimpleProcessor(processorValueCollector), Named.as(\"process\"));\n\n        final KStream<String, Long> countStream = mappedStream\n            .groupByKey(Grouped.as(\"count-groupByKey\"))\n            .count(Named.as(\"count\"), Materialized.<String, Long>as(Stores.inMemoryKeyValueStore(\"count-store\"))\n                                                                .withKeySerde(Serdes.String())\n                                                                .withValueSerde(Serdes.Long()))\n            .toStream(Named.as(\"count-toStream\"));\n\n        countStream.to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()).withName(\"count-to\"));\n\n        mappedStream\n            .groupByKey(Grouped.as(\"aggregate-groupByKey\"))\n            .aggregate(initializer,\n                       aggregator,\n                       Named.as(\"aggregate\"),\n                       Materialized.<String, Integer>as(Stores.inMemoryKeyValueStore(\"aggregate-store\"))\n                                                    .withKeySerde(Serdes.String())\n                                                    .withValueSerde(Serdes.Integer()))\n            .toStream(Named.as(\"aggregate-toStream\"))\n            .to(AGGREGATION_TOPIC, Produced.with(Serdes.String(), Serdes.Integer()).withName(\"reduce-to\"));\n\n        \r\n        mappedStream\n            .filter((k, v) -> true, Named.as(\"reduce-filter\"))\n            .peek((k, v) -> System.out.println(k + \":\" + v), Named.as(\"reduce-peek\"))\n            .groupByKey(Grouped.as(\"reduce-groupByKey\"))\n            .reduce(reducer,\n                    Named.as(\"reducer\"),\n                    Materialized.as(Stores.inMemoryKeyValueStore(\"reduce-store\")))\n            .toStream(Named.as(\"reduce-toStream\"))\n            .to(REDUCE_TOPIC, Produced.with(Serdes.String(), Serdes.String()));\n\n        mappedStream\n            .filter((k, v) -> k.equals(\"A\"), Named.as(\"join-filter\"))\n            .join(countStream, (v1, v2) -> v1 + \":\" + v2.toString(),\n                  JoinWindows.of(ofMillis(5000)),\n                  StreamJoined.<String, String, Long>with(Stores.inMemoryWindowStore(\"join-store\", ofDays(1).plus(ofMillis(10000)), ofMillis(10000), true),\n                                                          Stores.inMemoryWindowStore(\"other-join-store\", ofDays(1).plus(ofMillis(10000)), ofMillis(10000), true))\n                                                    .withName(\"join\")\n                                                    .withKeySerde(Serdes.String())\n                                                    .withValueSerde(Serdes.String())\n                                                    .withOtherValueSerde(Serdes.Long()))\n            .to(JOINED_TOPIC, Produced.as(\"join-to\"));\n\n        streamsConfiguration.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION_CONFIG, optimizationConfig);\n        final Topology topology = builder.build(streamsConfiguration);\n\n        topologyTestDriver = new TopologyTestDriver(topology, streamsConfiguration);\n\n        final TestInputTopic<String, String> inputTopicA = topologyTestDriver.createInputTopic(INPUT_TOPIC, stringSerializer, stringSerializer);\n        final TestOutputTopic<String, Long> countOutputTopic = topologyTestDriver.createOutputTopic(COUNT_TOPIC, stringDeserializer, new LongDeserializer());\n        final TestOutputTopic<String, Integer> aggregationOutputTopic = topologyTestDriver.createOutputTopic(AGGREGATION_TOPIC, stringDeserializer, new IntegerDeserializer());\n        final TestOutputTopic<String, String> reduceOutputTopic = topologyTestDriver.createOutputTopic(REDUCE_TOPIC, stringDeserializer, stringDeserializer);\n        final TestOutputTopic<String, String> joinedOutputTopic = topologyTestDriver.createOutputTopic(JOINED_TOPIC, stringDeserializer, stringDeserializer);\n\n        inputTopicA.pipeKeyValueList(getKeyValues());\n\n        \r\n        final String topologyString = topology.describe().toString();\n        if (optimizationConfig.equals(StreamsConfig.OPTIMIZE)) {\n            assertEquals(EXPECTED_OPTIMIZED_TOPOLOGY, topologyString);\n        } else {\n            assertEquals(EXPECTED_UNOPTIMIZED_TOPOLOGY, topologyString);\n        }\n\n        \r\n        assertEquals(expectedNumberRepartitionTopics, getCountOfRepartitionTopicsFound(topologyString));\n\n        \r\n        assertThat(3, equalTo(processorValueCollector.size()));\n        assertThat(processorValueCollector, equalTo(expectedCollectedProcessorValues));\n\n        \r\n        assertThat(countOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedCountKeyValues)));\n        assertThat(aggregationOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedAggKeyValues)));\n        assertThat(reduceOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedReduceKeyValues)));\n        assertThat(joinedOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedJoinKeyValues)));\n    }\n","date":"2021-03-29 03:33:40","endLine":229,"groupId":"9332","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"runTest","params":"(finalStringoptimizationConfig@finalintexpectedNumberRepartitionTopics)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/d8/2f6c9d1a855ac128bc0b8abd04f45a06583ab6.src","preCode":"    private void runTest(final String optimizationConfig, final int expectedNumberRepartitionTopics) {\n\n        final StreamsBuilder builder = new StreamsBuilder();\n\n        final KStream<String, String> sourceStream =\n            builder.stream(INPUT_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceStream\"));\n\n        final KStream<String, String> mappedStream = sourceStream\n            .map((k, v) -> KeyValue.pair(k.toUpperCase(Locale.getDefault()), v), Named.as(\"source-map\"));\n\n        mappedStream\n            .filter((k, v) -> k.equals(\"B\"), Named.as(\"process-filter\"))\n            .mapValues(v -> v.toUpperCase(Locale.getDefault()), Named.as(\"process-mapValues\"))\n            .process(() -> new SimpleProcessor(processorValueCollector), Named.as(\"process\"));\n\n        final KStream<String, Long> countStream = mappedStream\n            .groupByKey(Grouped.as(\"count-groupByKey\"))\n            .count(Named.as(\"count\"), Materialized.<String, Long>as(Stores.inMemoryKeyValueStore(\"count-store\"))\n                                                                .withKeySerde(Serdes.String())\n                                                                .withValueSerde(Serdes.Long()))\n            .toStream(Named.as(\"count-toStream\"));\n\n        countStream.to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()).withName(\"count-to\"));\n\n        mappedStream\n            .groupByKey(Grouped.as(\"aggregate-groupByKey\"))\n            .aggregate(initializer,\n                       aggregator,\n                       Named.as(\"aggregate\"),\n                       Materialized.<String, Integer>as(Stores.inMemoryKeyValueStore(\"aggregate-store\"))\n                                                    .withKeySerde(Serdes.String())\n                                                    .withValueSerde(Serdes.Integer()))\n            .toStream(Named.as(\"aggregate-toStream\"))\n            .to(AGGREGATION_TOPIC, Produced.with(Serdes.String(), Serdes.Integer()).withName(\"reduce-to\"));\n\n        \r\n        mappedStream\n            .filter((k, v) -> true, Named.as(\"reduce-filter\"))\n            .peek((k, v) -> System.out.println(k + \":\" + v), Named.as(\"reduce-peek\"))\n            .groupByKey(Grouped.as(\"reduce-groupByKey\"))\n            .reduce(reducer,\n                    Named.as(\"reducer\"),\n                    Materialized.as(Stores.inMemoryKeyValueStore(\"reduce-store\")))\n            .toStream(Named.as(\"reduce-toStream\"))\n            .to(REDUCE_TOPIC, Produced.with(Serdes.String(), Serdes.String()));\n\n        mappedStream\n            .filter((k, v) -> k.equals(\"A\"), Named.as(\"join-filter\"))\n            .join(countStream, (v1, v2) -> v1 + \":\" + v2.toString(),\n                  JoinWindows.of(ofMillis(5000)),\n                  StreamJoined.<String, String, Long>with(Stores.inMemoryWindowStore(\"join-store\", ofDays(1), ofMillis(10000), true),\n                                                          Stores.inMemoryWindowStore(\"other-join-store\",  ofDays(1), ofMillis(10000), true))\n                                                    .withName(\"join\")\n                                                    .withKeySerde(Serdes.String())\n                                                    .withValueSerde(Serdes.String())\n                                                    .withOtherValueSerde(Serdes.Long()))\n            .to(JOINED_TOPIC, Produced.as(\"join-to\"));\n\n        streamsConfiguration.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION_CONFIG, optimizationConfig);\n        final Topology topology = builder.build(streamsConfiguration);\n\n        topologyTestDriver = new TopologyTestDriver(topology, streamsConfiguration);\n\n        final TestInputTopic<String, String> inputTopicA = topologyTestDriver.createInputTopic(INPUT_TOPIC, stringSerializer, stringSerializer);\n        final TestOutputTopic<String, Long> countOutputTopic = topologyTestDriver.createOutputTopic(COUNT_TOPIC, stringDeserializer, new LongDeserializer());\n        final TestOutputTopic<String, Integer> aggregationOutputTopic = topologyTestDriver.createOutputTopic(AGGREGATION_TOPIC, stringDeserializer, new IntegerDeserializer());\n        final TestOutputTopic<String, String> reduceOutputTopic = topologyTestDriver.createOutputTopic(REDUCE_TOPIC, stringDeserializer, stringDeserializer);\n        final TestOutputTopic<String, String> joinedOutputTopic = topologyTestDriver.createOutputTopic(JOINED_TOPIC, stringDeserializer, stringDeserializer);\n\n        inputTopicA.pipeKeyValueList(getKeyValues());\n\n        \r\n        final String topologyString = topology.describe().toString();\n        if (optimizationConfig.equals(StreamsConfig.OPTIMIZE)) {\n            assertEquals(EXPECTED_OPTIMIZED_TOPOLOGY, topologyString);\n        } else {\n            assertEquals(EXPECTED_UNOPTIMIZED_TOPOLOGY, topologyString);\n        }\n\n        \r\n        assertEquals(expectedNumberRepartitionTopics, getCountOfRepartitionTopicsFound(topologyString));\n\n        \r\n        assertThat(3, equalTo(processorValueCollector.size()));\n        assertThat(processorValueCollector, equalTo(expectedCollectedProcessorValues));\n\n        \r\n        assertThat(countOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedCountKeyValues)));\n        assertThat(aggregationOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedAggKeyValues)));\n        assertThat(reduceOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedReduceKeyValues)));\n        assertThat(joinedOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedJoinKeyValues)));\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionOptimizingTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":138,"status":"M"},{"authorDate":"2020-06-20 08:41:06","commitOrder":3,"curCode":"    private void runTest(final String optimizationConfig, final int expectedNumberRepartitionTopics) {\n\n        streamsConfiguration.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION_CONFIG, optimizationConfig);\n\n        final StreamsBuilder builder = new StreamsBuilder();\n\n        final KStream<String, String> sourceAStream =\n            builder.stream(INPUT_A_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceAStream\"));\n\n        final KStream<String, String> sourceBStream =\n            builder.stream(INPUT_B_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceBStream\"));\n\n        final KStream<String, String> mappedAStream =\n            sourceAStream.map((k, v) -> KeyValue.pair(v.split(\":\")[0], v), Named.as(\"mappedAStream\"));\n        final KStream<String, String> mappedBStream =\n            sourceBStream.map((k, v) -> KeyValue.pair(v.split(\":\")[0], v), Named.as(\"mappedBStream\"));\n\n        final KStream<String, String> mergedStream = mappedAStream.merge(mappedBStream, Named.as(\"mergedStream\"));\n\n        mergedStream\n            .groupByKey(Grouped.as(\"long-groupByKey\"))\n            .count(Named.as(\"long-count\"), Materialized.as(Stores.inMemoryKeyValueStore(\"long-store\")))\n            .toStream(Named.as(\"long-toStream\"))\n            .to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()).withName(\"long-to\"));\n\n        mergedStream\n            .groupByKey(Grouped.as(\"string-groupByKey\"))\n            .count(Named.as(\"string-count\"), Materialized.as(Stores.inMemoryKeyValueStore(\"string-store\")))\n            .toStream(Named.as(\"string-toStream\"))\n            .mapValues(v -> v.toString(), Named.as(\"string-mapValues\"))\n            .to(STRING_COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.String()).withName(\"string-to\"));\n\n        final Topology topology = builder.build(streamsConfiguration);\n\n        topologyTestDriver = new TopologyTestDriver(topology, streamsConfiguration);\n\n        final TestInputTopic<String, String> inputTopicA = topologyTestDriver.createInputTopic(INPUT_A_TOPIC, stringSerializer, stringSerializer);\n        final TestInputTopic<String, String> inputTopicB = topologyTestDriver.createInputTopic(INPUT_B_TOPIC, stringSerializer, stringSerializer);\n\n        final TestOutputTopic<String, Long> countOutputTopic = topologyTestDriver.createOutputTopic(COUNT_TOPIC, stringDeserializer, new LongDeserializer());\n        final TestOutputTopic<String, String> stringCountOutputTopic = topologyTestDriver.createOutputTopic(STRING_COUNT_TOPIC, stringDeserializer, stringDeserializer);\n\n        inputTopicA.pipeKeyValueList(getKeyValues());\n        inputTopicB.pipeKeyValueList(getKeyValues());\n\n        final String topologyString = topology.describe().toString();\n\n        \r\n        if (optimizationConfig.equals(StreamsConfig.OPTIMIZE)) {\n            assertEquals(EXPECTED_OPTIMIZED_TOPOLOGY, topologyString);\n        } else {\n            assertEquals(EXPECTED_UNOPTIMIZED_TOPOLOGY, topologyString);\n        }\n\n        \r\n        assertEquals(expectedNumberRepartitionTopics, getCountOfRepartitionTopicsFound(topologyString));\n\n        \r\n        assertThat(countOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedCountKeyValues)));\n        assertThat(stringCountOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedStringCountKeyValues)));\n    }\n","date":"2020-06-20 08:41:06","endLine":181,"groupId":"8383","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"runTest","params":"(finalStringoptimizationConfig@finalintexpectedNumberRepartitionTopics)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/b5/0ef8ecee0cbe4dd0eca682cab444bfb7964cf4.src","preCode":"    private void runTest(final String optimizationConfig, final int expectedNumberRepartitionTopics) {\n\n        streamsConfiguration.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION_CONFIG, optimizationConfig);\n\n        final StreamsBuilder builder = new StreamsBuilder();\n\n        final KStream<String, String> sourceAStream =\n            builder.stream(INPUT_A_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceAStream\"));\n\n        final KStream<String, String> sourceBStream =\n            builder.stream(INPUT_B_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceBStream\"));\n\n        final KStream<String, String> mappedAStream =\n            sourceAStream.map((k, v) -> KeyValue.pair(v.split(\":\")[0], v), Named.as(\"mappedAStream\"));\n        final KStream<String, String> mappedBStream =\n            sourceBStream.map((k, v) -> KeyValue.pair(v.split(\":\")[0], v), Named.as(\"mappedBStream\"));\n\n        final KStream<String, String> mergedStream = mappedAStream.merge(mappedBStream, Named.as(\"mergedStream\"));\n\n        mergedStream\n            .groupByKey(Grouped.as(\"long-groupByKey\"))\n            .count(Named.as(\"long-count\"), Materialized.as(Stores.inMemoryKeyValueStore(\"long-store\")))\n            .toStream(Named.as(\"long-toStream\"))\n            .to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()).withName(\"long-to\"));\n\n        mergedStream\n            .groupByKey(Grouped.as(\"string-groupByKey\"))\n            .count(Named.as(\"string-count\"), Materialized.as(Stores.inMemoryKeyValueStore(\"string-store\")))\n            .toStream(Named.as(\"string-toStream\"))\n            .mapValues(v -> v.toString(), Named.as(\"string-mapValues\"))\n            .to(STRING_COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.String()).withName(\"string-to\"));\n\n        final Topology topology = builder.build(streamsConfiguration);\n\n        topologyTestDriver = new TopologyTestDriver(topology, streamsConfiguration);\n\n        final TestInputTopic<String, String> inputTopicA = topologyTestDriver.createInputTopic(INPUT_A_TOPIC, stringSerializer, stringSerializer);\n        final TestInputTopic<String, String> inputTopicB = topologyTestDriver.createInputTopic(INPUT_B_TOPIC, stringSerializer, stringSerializer);\n\n        final TestOutputTopic<String, Long> countOutputTopic = topologyTestDriver.createOutputTopic(COUNT_TOPIC, stringDeserializer, new LongDeserializer());\n        final TestOutputTopic<String, String> stringCountOutputTopic = topologyTestDriver.createOutputTopic(STRING_COUNT_TOPIC, stringDeserializer, stringDeserializer);\n\n        inputTopicA.pipeKeyValueList(getKeyValues());\n        inputTopicB.pipeKeyValueList(getKeyValues());\n\n        final String topologyString = topology.describe().toString();\n\n        \r\n        if (optimizationConfig.equals(StreamsConfig.OPTIMIZE)) {\n            assertEquals(EXPECTED_OPTIMIZED_TOPOLOGY, topologyString);\n        } else {\n            assertEquals(EXPECTED_UNOPTIMIZED_TOPOLOGY, topologyString);\n        }\n\n        \r\n        assertEquals(expectedNumberRepartitionTopics, getCountOfRepartitionTopicsFound(topologyString));\n\n        \r\n        assertThat(countOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedCountKeyValues)));\n        assertThat(stringCountOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedStringCountKeyValues)));\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionWithMergeOptimizingTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":121,"status":"N"}],"commitId":"d5fd491bf7e92e4b07a0a371dfa766161ac301d2","commitMessage":"@@@KAFKA-7106: remove deprecated Windows APIs (#10378)\n\n1. Remove all deprecated APIs in KIP-328.\n2. Remove deprecated APIs in Windows in KIP-358.\n\nReviewers: John Roesler <vvcephei@apache.org>","date":"2021-03-29 03:33:40","modifiedFileCount":"16","status":"M","submitter":"Guozhang Wang"},{"authorTime":"2020-06-20 08:41:06","codes":[{"authorDate":"2021-07-17 02:22:26","commitOrder":4,"curCode":"    private void runTest(final String optimizationConfig, final int expectedNumberRepartitionTopics) {\n\n        final StreamsBuilder builder = new StreamsBuilder();\n\n        final KStream<String, String> sourceStream =\n            builder.stream(INPUT_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceStream\"));\n\n        final KStream<String, String> mappedStream = sourceStream\n            .map((k, v) -> KeyValue.pair(k.toUpperCase(Locale.getDefault()), v), Named.as(\"source-map\"));\n\n        mappedStream\n            .filter((k, v) -> k.equals(\"B\"), Named.as(\"process-filter\"))\n            .mapValues(v -> v.toUpperCase(Locale.getDefault()), Named.as(\"process-mapValues\"))\n            .process(() -> new SimpleProcessor(processorValueCollector), Named.as(\"process\"));\n\n        final KStream<String, Long> countStream = mappedStream\n            .groupByKey(Grouped.as(\"count-groupByKey\"))\n            .count(Named.as(\"count\"), Materialized.<String, Long>as(Stores.inMemoryKeyValueStore(\"count-store\"))\n                                                                .withKeySerde(Serdes.String())\n                                                                .withValueSerde(Serdes.Long()))\n            .toStream(Named.as(\"count-toStream\"));\n\n        countStream.to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()).withName(\"count-to\"));\n\n        mappedStream\n            .groupByKey(Grouped.as(\"aggregate-groupByKey\"))\n            .aggregate(initializer,\n                       aggregator,\n                       Named.as(\"aggregate\"),\n                       Materialized.<String, Integer>as(Stores.inMemoryKeyValueStore(\"aggregate-store\"))\n                                                    .withKeySerde(Serdes.String())\n                                                    .withValueSerde(Serdes.Integer()))\n            .toStream(Named.as(\"aggregate-toStream\"))\n            .to(AGGREGATION_TOPIC, Produced.with(Serdes.String(), Serdes.Integer()).withName(\"reduce-to\"));\n\n        \r\n        mappedStream\n            .filter((k, v) -> true, Named.as(\"reduce-filter\"))\n            .peek((k, v) -> System.out.println(k + \":\" + v), Named.as(\"reduce-peek\"))\n            .groupByKey(Grouped.as(\"reduce-groupByKey\"))\n            .reduce(reducer,\n                    Named.as(\"reducer\"),\n                    Materialized.as(Stores.inMemoryKeyValueStore(\"reduce-store\")))\n            .toStream(Named.as(\"reduce-toStream\"))\n            .to(REDUCE_TOPIC, Produced.with(Serdes.String(), Serdes.String()));\n\n        mappedStream\n            .filter((k, v) -> k.equals(\"A\"), Named.as(\"join-filter\"))\n            .join(countStream, (v1, v2) -> v1 + \":\" + v2.toString(),\n                  JoinWindows.of(ofMillis(5000)),\n                  StreamJoined.<String, String, Long>with(Stores.inMemoryWindowStore(\"join-store\", ofDays(1), ofMillis(10000), true),\n                                       Stores.inMemoryWindowStore(\"other-join-store\", ofDays(1), ofMillis(10000), true))\n                          .withName(\"join\")\n                          .withKeySerde(Serdes.String())\n                          .withValueSerde(Serdes.String())\n                          .withOtherValueSerde(Serdes.Long()))\n            .to(JOINED_TOPIC, Produced.as(\"join-to\"));\n\n        streamsConfiguration.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION_CONFIG, optimizationConfig);\n        final Topology topology = builder.build(streamsConfiguration);\n\n        topologyTestDriver = new TopologyTestDriver(topology, streamsConfiguration);\n\n        final TestInputTopic<String, String> inputTopicA = topologyTestDriver.createInputTopic(INPUT_TOPIC, stringSerializer, stringSerializer);\n        final TestOutputTopic<String, Long> countOutputTopic = topologyTestDriver.createOutputTopic(COUNT_TOPIC, stringDeserializer, new LongDeserializer());\n        final TestOutputTopic<String, Integer> aggregationOutputTopic = topologyTestDriver.createOutputTopic(AGGREGATION_TOPIC, stringDeserializer, new IntegerDeserializer());\n        final TestOutputTopic<String, String> reduceOutputTopic = topologyTestDriver.createOutputTopic(REDUCE_TOPIC, stringDeserializer, stringDeserializer);\n        final TestOutputTopic<String, String> joinedOutputTopic = topologyTestDriver.createOutputTopic(JOINED_TOPIC, stringDeserializer, stringDeserializer);\n\n        inputTopicA.pipeKeyValueList(getKeyValues());\n\n        \r\n        final String topologyString = topology.describe().toString();\n        if (optimizationConfig.equals(StreamsConfig.OPTIMIZE)) {\n            assertEquals(EXPECTED_OPTIMIZED_TOPOLOGY, topologyString);\n        } else {\n            assertEquals(EXPECTED_UNOPTIMIZED_TOPOLOGY, topologyString);\n        }\n\n        \r\n        assertEquals(expectedNumberRepartitionTopics, getCountOfRepartitionTopicsFound(topologyString));\n\n        \r\n        assertThat(3, equalTo(processorValueCollector.size()));\n        assertThat(processorValueCollector, equalTo(expectedCollectedProcessorValues));\n\n        \r\n        assertThat(countOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedCountKeyValues)));\n        assertThat(aggregationOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedAggKeyValues)));\n        assertThat(reduceOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedReduceKeyValues)));\n        assertThat(joinedOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedJoinKeyValues)));\n    }\n","date":"2021-07-17 02:22:26","endLine":230,"groupId":"102471","id":7,"instanceNumber":1,"isCurCommit":1,"methodName":"runTest","params":"(finalStringoptimizationConfig@finalintexpectedNumberRepartitionTopics)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/eb/817cc76a1e18e179b16b5965e10497764a5b2b.src","preCode":"    private void runTest(final String optimizationConfig, final int expectedNumberRepartitionTopics) {\n\n        final StreamsBuilder builder = new StreamsBuilder();\n\n        final KStream<String, String> sourceStream =\n            builder.stream(INPUT_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceStream\"));\n\n        final KStream<String, String> mappedStream = sourceStream\n            .map((k, v) -> KeyValue.pair(k.toUpperCase(Locale.getDefault()), v), Named.as(\"source-map\"));\n\n        mappedStream\n            .filter((k, v) -> k.equals(\"B\"), Named.as(\"process-filter\"))\n            .mapValues(v -> v.toUpperCase(Locale.getDefault()), Named.as(\"process-mapValues\"))\n            .process(() -> new SimpleProcessor(processorValueCollector), Named.as(\"process\"));\n\n        final KStream<String, Long> countStream = mappedStream\n            .groupByKey(Grouped.as(\"count-groupByKey\"))\n            .count(Named.as(\"count\"), Materialized.<String, Long>as(Stores.inMemoryKeyValueStore(\"count-store\"))\n                                                                .withKeySerde(Serdes.String())\n                                                                .withValueSerde(Serdes.Long()))\n            .toStream(Named.as(\"count-toStream\"));\n\n        countStream.to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()).withName(\"count-to\"));\n\n        mappedStream\n            .groupByKey(Grouped.as(\"aggregate-groupByKey\"))\n            .aggregate(initializer,\n                       aggregator,\n                       Named.as(\"aggregate\"),\n                       Materialized.<String, Integer>as(Stores.inMemoryKeyValueStore(\"aggregate-store\"))\n                                                    .withKeySerde(Serdes.String())\n                                                    .withValueSerde(Serdes.Integer()))\n            .toStream(Named.as(\"aggregate-toStream\"))\n            .to(AGGREGATION_TOPIC, Produced.with(Serdes.String(), Serdes.Integer()).withName(\"reduce-to\"));\n\n        \r\n        mappedStream\n            .filter((k, v) -> true, Named.as(\"reduce-filter\"))\n            .peek((k, v) -> System.out.println(k + \":\" + v), Named.as(\"reduce-peek\"))\n            .groupByKey(Grouped.as(\"reduce-groupByKey\"))\n            .reduce(reducer,\n                    Named.as(\"reducer\"),\n                    Materialized.as(Stores.inMemoryKeyValueStore(\"reduce-store\")))\n            .toStream(Named.as(\"reduce-toStream\"))\n            .to(REDUCE_TOPIC, Produced.with(Serdes.String(), Serdes.String()));\n\n        mappedStream\n            .filter((k, v) -> k.equals(\"A\"), Named.as(\"join-filter\"))\n            .join(countStream, (v1, v2) -> v1 + \":\" + v2.toString(),\n                  JoinWindows.of(ofMillis(5000)),\n                  StreamJoined.<String, String, Long>with(Stores.inMemoryWindowStore(\"join-store\", ofDays(1).plus(ofMillis(10000)), ofMillis(10000), true),\n                                                          Stores.inMemoryWindowStore(\"other-join-store\", ofDays(1).plus(ofMillis(10000)), ofMillis(10000), true))\n                                                    .withName(\"join\")\n                                                    .withKeySerde(Serdes.String())\n                                                    .withValueSerde(Serdes.String())\n                                                    .withOtherValueSerde(Serdes.Long()))\n            .to(JOINED_TOPIC, Produced.as(\"join-to\"));\n\n        streamsConfiguration.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION_CONFIG, optimizationConfig);\n        final Topology topology = builder.build(streamsConfiguration);\n\n        topologyTestDriver = new TopologyTestDriver(topology, streamsConfiguration);\n\n        final TestInputTopic<String, String> inputTopicA = topologyTestDriver.createInputTopic(INPUT_TOPIC, stringSerializer, stringSerializer);\n        final TestOutputTopic<String, Long> countOutputTopic = topologyTestDriver.createOutputTopic(COUNT_TOPIC, stringDeserializer, new LongDeserializer());\n        final TestOutputTopic<String, Integer> aggregationOutputTopic = topologyTestDriver.createOutputTopic(AGGREGATION_TOPIC, stringDeserializer, new IntegerDeserializer());\n        final TestOutputTopic<String, String> reduceOutputTopic = topologyTestDriver.createOutputTopic(REDUCE_TOPIC, stringDeserializer, stringDeserializer);\n        final TestOutputTopic<String, String> joinedOutputTopic = topologyTestDriver.createOutputTopic(JOINED_TOPIC, stringDeserializer, stringDeserializer);\n\n        inputTopicA.pipeKeyValueList(getKeyValues());\n\n        \r\n        final String topologyString = topology.describe().toString();\n        if (optimizationConfig.equals(StreamsConfig.OPTIMIZE)) {\n            assertEquals(EXPECTED_OPTIMIZED_TOPOLOGY, topologyString);\n        } else {\n            assertEquals(EXPECTED_UNOPTIMIZED_TOPOLOGY, topologyString);\n        }\n\n        \r\n        assertEquals(expectedNumberRepartitionTopics, getCountOfRepartitionTopicsFound(topologyString));\n\n        \r\n        assertThat(3, equalTo(processorValueCollector.size()));\n        assertThat(processorValueCollector, equalTo(expectedCollectedProcessorValues));\n\n        \r\n        assertThat(countOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedCountKeyValues)));\n        assertThat(aggregationOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedAggKeyValues)));\n        assertThat(reduceOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedReduceKeyValues)));\n        assertThat(joinedOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedJoinKeyValues)));\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionOptimizingTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":139,"status":"M"},{"authorDate":"2020-06-20 08:41:06","commitOrder":4,"curCode":"    private void runTest(final String optimizationConfig, final int expectedNumberRepartitionTopics) {\n\n        streamsConfiguration.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION_CONFIG, optimizationConfig);\n\n        final StreamsBuilder builder = new StreamsBuilder();\n\n        final KStream<String, String> sourceAStream =\n            builder.stream(INPUT_A_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceAStream\"));\n\n        final KStream<String, String> sourceBStream =\n            builder.stream(INPUT_B_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceBStream\"));\n\n        final KStream<String, String> mappedAStream =\n            sourceAStream.map((k, v) -> KeyValue.pair(v.split(\":\")[0], v), Named.as(\"mappedAStream\"));\n        final KStream<String, String> mappedBStream =\n            sourceBStream.map((k, v) -> KeyValue.pair(v.split(\":\")[0], v), Named.as(\"mappedBStream\"));\n\n        final KStream<String, String> mergedStream = mappedAStream.merge(mappedBStream, Named.as(\"mergedStream\"));\n\n        mergedStream\n            .groupByKey(Grouped.as(\"long-groupByKey\"))\n            .count(Named.as(\"long-count\"), Materialized.as(Stores.inMemoryKeyValueStore(\"long-store\")))\n            .toStream(Named.as(\"long-toStream\"))\n            .to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()).withName(\"long-to\"));\n\n        mergedStream\n            .groupByKey(Grouped.as(\"string-groupByKey\"))\n            .count(Named.as(\"string-count\"), Materialized.as(Stores.inMemoryKeyValueStore(\"string-store\")))\n            .toStream(Named.as(\"string-toStream\"))\n            .mapValues(v -> v.toString(), Named.as(\"string-mapValues\"))\n            .to(STRING_COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.String()).withName(\"string-to\"));\n\n        final Topology topology = builder.build(streamsConfiguration);\n\n        topologyTestDriver = new TopologyTestDriver(topology, streamsConfiguration);\n\n        final TestInputTopic<String, String> inputTopicA = topologyTestDriver.createInputTopic(INPUT_A_TOPIC, stringSerializer, stringSerializer);\n        final TestInputTopic<String, String> inputTopicB = topologyTestDriver.createInputTopic(INPUT_B_TOPIC, stringSerializer, stringSerializer);\n\n        final TestOutputTopic<String, Long> countOutputTopic = topologyTestDriver.createOutputTopic(COUNT_TOPIC, stringDeserializer, new LongDeserializer());\n        final TestOutputTopic<String, String> stringCountOutputTopic = topologyTestDriver.createOutputTopic(STRING_COUNT_TOPIC, stringDeserializer, stringDeserializer);\n\n        inputTopicA.pipeKeyValueList(getKeyValues());\n        inputTopicB.pipeKeyValueList(getKeyValues());\n\n        final String topologyString = topology.describe().toString();\n\n        \r\n        if (optimizationConfig.equals(StreamsConfig.OPTIMIZE)) {\n            assertEquals(EXPECTED_OPTIMIZED_TOPOLOGY, topologyString);\n        } else {\n            assertEquals(EXPECTED_UNOPTIMIZED_TOPOLOGY, topologyString);\n        }\n\n        \r\n        assertEquals(expectedNumberRepartitionTopics, getCountOfRepartitionTopicsFound(topologyString));\n\n        \r\n        assertThat(countOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedCountKeyValues)));\n        assertThat(stringCountOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedStringCountKeyValues)));\n    }\n","date":"2020-06-20 08:41:06","endLine":181,"groupId":"102471","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"runTest","params":"(finalStringoptimizationConfig@finalintexpectedNumberRepartitionTopics)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/b5/0ef8ecee0cbe4dd0eca682cab444bfb7964cf4.src","preCode":"    private void runTest(final String optimizationConfig, final int expectedNumberRepartitionTopics) {\n\n        streamsConfiguration.setProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION_CONFIG, optimizationConfig);\n\n        final StreamsBuilder builder = new StreamsBuilder();\n\n        final KStream<String, String> sourceAStream =\n            builder.stream(INPUT_A_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceAStream\"));\n\n        final KStream<String, String> sourceBStream =\n            builder.stream(INPUT_B_TOPIC, Consumed.with(Serdes.String(), Serdes.String()).withName(\"sourceBStream\"));\n\n        final KStream<String, String> mappedAStream =\n            sourceAStream.map((k, v) -> KeyValue.pair(v.split(\":\")[0], v), Named.as(\"mappedAStream\"));\n        final KStream<String, String> mappedBStream =\n            sourceBStream.map((k, v) -> KeyValue.pair(v.split(\":\")[0], v), Named.as(\"mappedBStream\"));\n\n        final KStream<String, String> mergedStream = mappedAStream.merge(mappedBStream, Named.as(\"mergedStream\"));\n\n        mergedStream\n            .groupByKey(Grouped.as(\"long-groupByKey\"))\n            .count(Named.as(\"long-count\"), Materialized.as(Stores.inMemoryKeyValueStore(\"long-store\")))\n            .toStream(Named.as(\"long-toStream\"))\n            .to(COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.Long()).withName(\"long-to\"));\n\n        mergedStream\n            .groupByKey(Grouped.as(\"string-groupByKey\"))\n            .count(Named.as(\"string-count\"), Materialized.as(Stores.inMemoryKeyValueStore(\"string-store\")))\n            .toStream(Named.as(\"string-toStream\"))\n            .mapValues(v -> v.toString(), Named.as(\"string-mapValues\"))\n            .to(STRING_COUNT_TOPIC, Produced.with(Serdes.String(), Serdes.String()).withName(\"string-to\"));\n\n        final Topology topology = builder.build(streamsConfiguration);\n\n        topologyTestDriver = new TopologyTestDriver(topology, streamsConfiguration);\n\n        final TestInputTopic<String, String> inputTopicA = topologyTestDriver.createInputTopic(INPUT_A_TOPIC, stringSerializer, stringSerializer);\n        final TestInputTopic<String, String> inputTopicB = topologyTestDriver.createInputTopic(INPUT_B_TOPIC, stringSerializer, stringSerializer);\n\n        final TestOutputTopic<String, Long> countOutputTopic = topologyTestDriver.createOutputTopic(COUNT_TOPIC, stringDeserializer, new LongDeserializer());\n        final TestOutputTopic<String, String> stringCountOutputTopic = topologyTestDriver.createOutputTopic(STRING_COUNT_TOPIC, stringDeserializer, stringDeserializer);\n\n        inputTopicA.pipeKeyValueList(getKeyValues());\n        inputTopicB.pipeKeyValueList(getKeyValues());\n\n        final String topologyString = topology.describe().toString();\n\n        \r\n        if (optimizationConfig.equals(StreamsConfig.OPTIMIZE)) {\n            assertEquals(EXPECTED_OPTIMIZED_TOPOLOGY, topologyString);\n        } else {\n            assertEquals(EXPECTED_UNOPTIMIZED_TOPOLOGY, topologyString);\n        }\n\n        \r\n        assertEquals(expectedNumberRepartitionTopics, getCountOfRepartitionTopicsFound(topologyString));\n\n        \r\n        assertThat(countOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedCountKeyValues)));\n        assertThat(stringCountOutputTopic.readKeyValuesToMap(), equalTo(keyValueListToMap(expectedStringCountKeyValues)));\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionWithMergeOptimizingTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":121,"status":"N"}],"commitId":"13b2df733a963963b0275266eed2b5e156cd56a0","commitMessage":"@@@MINOR: Default GRACE with Old API should set as 24H minus window-size / inactivity-gap (#10953)\n\nIn 2.8 and before.  we computed the default grace period with Math.max(maintainDurationMs - sizeMs.  0); in method gracePeriodMs() in TimeWindows.  SessionWindows.  and JoinWindows. That means that the default grace period has never been 24 hours but 24 hours - window size. Since gracePeriodMs() is used to compute the retention time of the changelog topic for the corresponding window state store and the segments for the window state store it is important to keep the same computation for the deprecated methods. Otherwise.  Streams app that run with 2.8 and before might not be compatible with Streams 3.0 because the retention time of the changelog topics created with older Streams apps will be smaller than the assumed retention time for Streams apps in 3.0. For example.  with a window size of 10 hours.  an old Streams app would have created a changelog topic with retention time 10 hours (window size) + 14 hours (default grace period.  24 hours - 10 hours). A 3.0 Streams app would assume a retention time of 10 hours (window size) + 24 hours (deprecated default grace period as currently specified on trunk). In the presence of failures.  where a state store needs to recreated.  records might get lost.  because before the failure the state store of a 3.0 Streams app contained 10 hours + 24 hours of records whereas the changelog topic that was created with the old Streams app would only contain 10 hours + 14 hours of records.\n\nAll this happened due to us always stating that the default grace period was 24 hours although it was not completely correct and a connected and unfortunate misunderstanding when we removed deprecated windows APIs (#10378).\n\nCo-authors: Bruno Cadonna <cadonna@apache.org>\nReviewers: Luke Chen <showuon@gmail.com>.  Matthias J. Sax <mjsax@apache.org>.  Bruno Cadonna <cadonna@apache.org>","date":"2021-07-17 02:22:26","modifiedFileCount":"7","status":"M","submitter":"Guozhang Wang"}]
