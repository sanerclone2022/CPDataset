[{"authorTime":"2017-03-31 05:39:28","codes":[{"authorDate":"2017-03-31 05:39:28","commitOrder":1,"curCode":"    public void testFetchResponseMetricsPartialResponse() {\n        subscriptions.assignFromUser(singleton(tp1));\n        subscriptions.seek(tp1, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricName(\"fetch-size-avg\", metricGroup));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricName(\"records-per-request-avg\", metricGroup));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(2, recordsCountAverage.value(), EPSILON);\n    }\n","date":"2017-03-31 05:39:28","endLine":918,"groupId":"15906","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testFetchResponseMetricsPartialResponse","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/09/2f54944b3bf2597737d9ee6fb066189123621d.src","preCode":"    public void testFetchResponseMetricsPartialResponse() {\n        subscriptions.assignFromUser(singleton(tp1));\n        subscriptions.seek(tp1, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricName(\"fetch-size-avg\", metricGroup));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricName(\"records-per-request-avg\", metricGroup));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(2, recordsCountAverage.value(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":895,"status":"B"},{"authorDate":"2017-03-31 05:39:28","commitOrder":1,"curCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        subscriptions.assignFromUser(Utils.mkSet(tp1, tp2));\n        subscriptions.seek(tp1, 0);\n        subscriptions.seek(tp2, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricName(\"fetch-size-avg\", metricGroup));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricName(\"records-per-request-avg\", metricGroup));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp2, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponse.PartitionData> partitions = new HashMap<>();\n        partitions.put(tp1, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));\n        partitions.put(tp2, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null,\n                MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(new FetchResponse(new LinkedHashMap<>(partitions), 0));\n        consumerClient.poll(0);\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(3, recordsCountAverage.value(), EPSILON);\n    }\n","date":"2017-03-31 05:39:28","endLine":993,"groupId":"5970","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testFetchResponseMetricsWithOnePartitionAtTheWrongOffset","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/09/2f54944b3bf2597737d9ee6fb066189123621d.src","preCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        subscriptions.assignFromUser(Utils.mkSet(tp1, tp2));\n        subscriptions.seek(tp1, 0);\n        subscriptions.seek(tp2, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricName(\"fetch-size-avg\", metricGroup));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricName(\"records-per-request-avg\", metricGroup));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp2, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponse.PartitionData> partitions = new HashMap<>();\n        partitions.put(tp1, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));\n        partitions.put(tp2, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null,\n                MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(new FetchResponse(new LinkedHashMap<>(partitions), 0));\n        consumerClient.poll(0);\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(3, recordsCountAverage.value(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":956,"status":"B"}],"commitId":"dd71e4a8d830c9de40b5ec3f987f60a1d2f26b39","commitMessage":"@@@MINOR: Ensure streaming iterator is closed by Fetcher\n\nAuthor: Jason Gustafson <jason@confluent.io>\nAuthor: Ismael Juma <github@juma.me.uk>\n\nReviewers: Ismael Juma <ismael@juma.me.uk>\n\nCloses #2762 from hachikuji/ensure-decompression-stream-closed\n","date":"2017-03-31 05:39:28","modifiedFileCount":"12","status":"B","submitter":"Jason Gustafson"},{"authorTime":"2017-05-27 06:34:20","codes":[{"authorDate":"2017-05-27 06:34:20","commitOrder":2,"curCode":"    public void testFetchResponseMetricsPartialResponse() {\n        subscriptions.assignFromUser(singleton(tp1));\n        subscriptions.seek(tp1, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(2, recordsCountAverage.value(), EPSILON);\n    }\n","date":"2017-05-27 06:34:20","endLine":1169,"groupId":"15906","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testFetchResponseMetricsPartialResponse","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/72/0079cdfc034823ee5c7972a0ea6892ec656c36.src","preCode":"    public void testFetchResponseMetricsPartialResponse() {\n        subscriptions.assignFromUser(singleton(tp1));\n        subscriptions.seek(tp1, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricName(\"fetch-size-avg\", metricGroup));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricName(\"records-per-request-avg\", metricGroup));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(2, recordsCountAverage.value(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1146,"status":"M"},{"authorDate":"2017-05-27 06:34:20","commitOrder":2,"curCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        subscriptions.assignFromUser(Utils.mkSet(tp1, tp2));\n        subscriptions.seek(tp1, 0);\n        subscriptions.seek(tp2, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp2, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponse.PartitionData> partitions = new HashMap<>();\n        partitions.put(tp1, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));\n        partitions.put(tp2, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null,\n                MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(new FetchResponse(new LinkedHashMap<>(partitions), 0));\n        consumerClient.poll(0);\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(3, recordsCountAverage.value(), EPSILON);\n    }\n","date":"2017-05-27 06:34:20","endLine":1244,"groupId":"5970","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testFetchResponseMetricsWithOnePartitionAtTheWrongOffset","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/72/0079cdfc034823ee5c7972a0ea6892ec656c36.src","preCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        subscriptions.assignFromUser(Utils.mkSet(tp1, tp2));\n        subscriptions.seek(tp1, 0);\n        subscriptions.seek(tp2, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricName(\"fetch-size-avg\", metricGroup));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricName(\"records-per-request-avg\", metricGroup));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp2, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponse.PartitionData> partitions = new HashMap<>();\n        partitions.put(tp1, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));\n        partitions.put(tp2, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null,\n                MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(new FetchResponse(new LinkedHashMap<>(partitions), 0));\n        consumerClient.poll(0);\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(3, recordsCountAverage.value(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1207,"status":"M"}],"commitId":"0bc4f75eedf14af6d0b2e3e9be62a460b1049d0b","commitMessage":"@@@KAFKA-5191: Autogenerate Consumer Fetcher metrics\n\nAutogenerate docs for the Consumer Fetcher's metrics. This is a smaller subset of the original PR https://github.com/apache/kafka/pull/1202.\n\nCC ijuma benstopford hachikuji\n\nAuthor: James Cheng <jylcheng@yahoo.com>\n\nReviewers: Ismael Juma <ismael@juma.me.uk>.  Guozhang Wang <wangguoz@gmail.com>\n\nCloses #2993 from wushujames/fetcher_metrics_docs\n","date":"2017-05-27 06:34:20","modifiedFileCount":"7","status":"M","submitter":"James Cheng"},{"authorTime":"2017-05-27 06:34:20","codes":[{"authorDate":"2017-06-07 23:14:09","commitOrder":3,"curCode":"    public void testFetchResponseMetricsPartialResponse() {\n        subscriptions.assignFromUser(singleton(tp1));\n        subscriptions.seek(tp1, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(tp1, records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(2, recordsCountAverage.value(), EPSILON);\n    }\n","date":"2017-06-07 23:36:57","endLine":1259,"groupId":"15906","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testFetchResponseMetricsPartialResponse","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/ca/d17bced5c9ddb754d16670dd8e3698ef961dee.src","preCode":"    public void testFetchResponseMetricsPartialResponse() {\n        subscriptions.assignFromUser(singleton(tp1));\n        subscriptions.seek(tp1, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(2, recordsCountAverage.value(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1236,"status":"M"},{"authorDate":"2017-05-27 06:34:20","commitOrder":3,"curCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        subscriptions.assignFromUser(Utils.mkSet(tp1, tp2));\n        subscriptions.seek(tp1, 0);\n        subscriptions.seek(tp2, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp2, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponse.PartitionData> partitions = new HashMap<>();\n        partitions.put(tp1, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));\n        partitions.put(tp2, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null,\n                MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(new FetchResponse(new LinkedHashMap<>(partitions), 0));\n        consumerClient.poll(0);\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(3, recordsCountAverage.value(), EPSILON);\n    }\n","date":"2017-05-27 06:34:20","endLine":1244,"groupId":"5970","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testFetchResponseMetricsWithOnePartitionAtTheWrongOffset","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/72/0079cdfc034823ee5c7972a0ea6892ec656c36.src","preCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        subscriptions.assignFromUser(Utils.mkSet(tp1, tp2));\n        subscriptions.seek(tp1, 0);\n        subscriptions.seek(tp2, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp2, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponse.PartitionData> partitions = new HashMap<>();\n        partitions.put(tp1, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));\n        partitions.put(tp2, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null,\n                MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(new FetchResponse(new LinkedHashMap<>(partitions), 0));\n        consumerClient.poll(0);\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(3, recordsCountAverage.value(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1207,"status":"N"}],"commitId":"dcbdce31ba525771016be5be4abc4a2067e0890b","commitMessage":"@@@KAFKA-5378; Return LSO in FetchResponse plus some metrics\n\nAuthor: Jason Gustafson <jason@confluent.io>\n\nReviewers: Jun Rao <junrao@gmail.com>.  Ismael Juma <ismael@juma.me.uk>\n\nCloses #3248 from hachikuji/KAFKA-5378\n","date":"2017-06-07 23:36:57","modifiedFileCount":"7","status":"M","submitter":"Jason Gustafson"},{"authorTime":"2017-07-21 08:31:24","codes":[{"authorDate":"2017-07-21 08:31:24","commitOrder":4,"curCode":"    public void testFetchResponseMetricsPartialResponse() {\n        subscriptions.assignFromUser(singleton(tp0));\n        subscriptions.seek(tp0, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(tp0, records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(2, recordsCountAverage.value(), EPSILON);\n    }\n","date":"2017-07-21 08:38:30","endLine":1260,"groupId":"15906","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testFetchResponseMetricsPartialResponse","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/c0/edcfd909b3299ef450a60742a598cc9aea1fbb.src","preCode":"    public void testFetchResponseMetricsPartialResponse() {\n        subscriptions.assignFromUser(singleton(tp1));\n        subscriptions.seek(tp1, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(tp1, records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(2, recordsCountAverage.value(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1237,"status":"M"},{"authorDate":"2017-07-21 08:31:24","commitOrder":4,"curCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        subscriptions.assignFromUser(Utils.mkSet(tp0, tp1));\n        subscriptions.seek(tp0, 0);\n        subscriptions.seek(tp1, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp1, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponse.PartitionData> partitions = new HashMap<>();\n        partitions.put(tp0, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));\n        partitions.put(tp1, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null,\n                MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(new FetchResponse(new LinkedHashMap<>(partitions), 0));\n        consumerClient.poll(0);\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(3, recordsCountAverage.value(), EPSILON);\n    }\n","date":"2017-07-21 08:38:30","endLine":1335,"groupId":"19985","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testFetchResponseMetricsWithOnePartitionAtTheWrongOffset","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/c0/edcfd909b3299ef450a60742a598cc9aea1fbb.src","preCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        subscriptions.assignFromUser(Utils.mkSet(tp1, tp2));\n        subscriptions.seek(tp1, 0);\n        subscriptions.seek(tp2, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp2, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponse.PartitionData> partitions = new HashMap<>();\n        partitions.put(tp1, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));\n        partitions.put(tp2, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null,\n                MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(new FetchResponse(new LinkedHashMap<>(partitions), 0));\n        consumerClient.poll(0);\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(3, recordsCountAverage.value(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1298,"status":"M"}],"commitId":"5bb53e034e4f8a06550dd06377fae7b3c2137ce2","commitMessage":"@@@KAFKA-5534; KafkaConsumer `offsetForTimes` result should include partitions with no offset\n\nFor topics that support timestamp search.  if no offset is found for a partition.  the partition should still be included in the result with a `null` offset value. This `KafkaConsumer` method currently excludes such partitions from the result.\n\nAuthor: Vahid Hashemian <vahidhashemian@us.ibm.com>\n\nReviewers: Ismael Juma <ismael@juma.me.uk>.  Jason Gustafson <jason@confluent.io>\n\nCloses #3460 from vahidhashemian/KAFKA-5534\n","date":"2017-07-21 08:38:30","modifiedFileCount":"2","status":"M","submitter":"Vahid Hashemian"},{"authorTime":"2018-02-06 02:09:17","codes":[{"authorDate":"2017-07-21 08:31:24","commitOrder":5,"curCode":"    public void testFetchResponseMetricsPartialResponse() {\n        subscriptions.assignFromUser(singleton(tp0));\n        subscriptions.seek(tp0, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(tp0, records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(2, recordsCountAverage.value(), EPSILON);\n    }\n","date":"2017-07-21 08:38:30","endLine":1260,"groupId":"15906","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testFetchResponseMetricsPartialResponse","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/c0/edcfd909b3299ef450a60742a598cc9aea1fbb.src","preCode":"    public void testFetchResponseMetricsPartialResponse() {\n        subscriptions.assignFromUser(singleton(tp0));\n        subscriptions.seek(tp0, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(tp0, records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(2, recordsCountAverage.value(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1237,"status":"N"},{"authorDate":"2018-02-06 02:09:17","commitOrder":5,"curCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        subscriptions.assignFromUser(Utils.mkSet(tp0, tp1));\n        subscriptions.seek(tp0, 0);\n        subscriptions.seek(tp1, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp1, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponse.PartitionData> partitions = new HashMap<>();\n        partitions.put(tp0, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));\n        partitions.put(tp1, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null,\n                MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(new FetchResponse(Errors.NONE, new LinkedHashMap<>(partitions),\n            0, INVALID_SESSION_ID));\n        consumerClient.poll(0);\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(3, recordsCountAverage.value(), EPSILON);\n    }\n","date":"2018-02-06 02:09:17","endLine":1386,"groupId":"19985","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testFetchResponseMetricsWithOnePartitionAtTheWrongOffset","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/a0/205e7f19ec21137a0a3ba0db83bf7bd317c538.src","preCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        subscriptions.assignFromUser(Utils.mkSet(tp0, tp1));\n        subscriptions.seek(tp0, 0);\n        subscriptions.seek(tp1, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp1, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponse.PartitionData> partitions = new HashMap<>();\n        partitions.put(tp0, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));\n        partitions.put(tp1, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null,\n                MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(new FetchResponse(new LinkedHashMap<>(partitions), 0));\n        consumerClient.poll(0);\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(3, recordsCountAverage.value(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1348,"status":"M"}],"commitId":"7fe1c2b3d3a78ea3ffb9e269563653626861fbd2","commitMessage":"@@@KAFKA-6254; Incremental fetch requests\n\nAuthor: Colin P. Mccabe <cmccabe@confluent.io>\n\nReviewers: Jason Gustafson <jason@confluent.io>.  Ismael Juma <ismael@juma.me.uk>.  Jun Rao <junrao@gmail.com>\n\nCloses #4418 from cmccabe/KAFKA-6254\n","date":"2018-02-06 02:09:17","modifiedFileCount":"8","status":"M","submitter":"Colin P. Mccabe"},{"authorTime":"2018-08-04 08:25:07","codes":[{"authorDate":"2017-07-21 08:31:24","commitOrder":6,"curCode":"    public void testFetchResponseMetricsPartialResponse() {\n        subscriptions.assignFromUser(singleton(tp0));\n        subscriptions.seek(tp0, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(tp0, records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(2, recordsCountAverage.value(), EPSILON);\n    }\n","date":"2017-07-21 08:38:30","endLine":1260,"groupId":"15906","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testFetchResponseMetricsPartialResponse","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/c0/edcfd909b3299ef450a60742a598cc9aea1fbb.src","preCode":"    public void testFetchResponseMetricsPartialResponse() {\n        subscriptions.assignFromUser(singleton(tp0));\n        subscriptions.seek(tp0, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(tp0, records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(2, recordsCountAverage.value(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1237,"status":"N"},{"authorDate":"2018-08-04 08:25:07","commitOrder":6,"curCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        subscriptions.assignFromUser(Utils.mkSet(tp0, tp1));\n        subscriptions.seek(tp0, 0);\n        subscriptions.seek(tp1, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp1, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> partitions = new HashMap<>();\n        partitions.put(tp0, new FetchResponse.PartitionData<>(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));\n        partitions.put(tp1, new FetchResponse.PartitionData<>(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null,\n                MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(new FetchResponse<>(Errors.NONE, new LinkedHashMap<>(partitions),\n                0, INVALID_SESSION_ID));\n        consumerClient.poll(time.timer(0));\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(3, recordsCountAverage.value(), EPSILON);\n    }\n","date":"2018-08-04 08:25:07","endLine":1756,"groupId":"11308","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testFetchResponseMetricsWithOnePartitionAtTheWrongOffset","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/1a/82faa0a3dd9620f0fe91d4802b7330247fca35.src","preCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        subscriptions.assignFromUser(Utils.mkSet(tp0, tp1));\n        subscriptions.seek(tp0, 0);\n        subscriptions.seek(tp1, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp1, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponse.PartitionData> partitions = new HashMap<>();\n        partitions.put(tp0, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));\n        partitions.put(tp1, new FetchResponse.PartitionData(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null,\n                MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(new FetchResponse(Errors.NONE, new LinkedHashMap<>(partitions),\n                0, INVALID_SESSION_ID));\n        consumerClient.poll(0);\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(3, recordsCountAverage.value(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1718,"status":"M"}],"commitId":"fc5f6b0e46ff81302b3e445fed0cdf454c942792","commitMessage":"@@@MINOR: Add Timer to simplify timeout bookkeeping and use it in the consumer (#5087)\n\nWe currently do a lot of bookkeeping for timeouts which is both error-prone and distracting. This patch adds a new `Timer` class to simplify this logic and control unnecessary calls to system time. In particular.  this helps with nested timeout operations. The consumer has been updated to use the new class.\n\nReviewers: Ismael Juma <ismael@juma.me.uk>.  Guozhang Wang <wangguoz@gmail.com>","date":"2018-08-04 08:25:07","modifiedFileCount":"17","status":"M","submitter":"Jason Gustafson"},{"authorTime":"2018-09-13 02:05:19","codes":[{"authorDate":"2018-09-13 02:05:19","commitOrder":7,"curCode":"    public void testFetchResponseMetricsPartialResponse() {\n        subscriptions.assignFromUser(singleton(tp0));\n        subscriptions.seek(tp0, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(tp0, records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, (Double) fetchSizeAverage.metricValue(), EPSILON);\n        assertEquals(2, (Double) recordsCountAverage.metricValue(), EPSILON);\n    }\n","date":"2018-09-13 02:05:19","endLine":1779,"groupId":"15906","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testFetchResponseMetricsPartialResponse","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/48/d61b9e0724bc4ed612c29434b75cfb16dd46f6.src","preCode":"    public void testFetchResponseMetricsPartialResponse() {\n        subscriptions.assignFromUser(singleton(tp0));\n        subscriptions.seek(tp0, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(tp0, records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(2, recordsCountAverage.value(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1756,"status":"M"},{"authorDate":"2018-09-13 02:05:19","commitOrder":7,"curCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        subscriptions.assignFromUser(Utils.mkSet(tp0, tp1));\n        subscriptions.seek(tp0, 0);\n        subscriptions.seek(tp1, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp1, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> partitions = new HashMap<>();\n        partitions.put(tp0, new FetchResponse.PartitionData<>(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));\n        partitions.put(tp1, new FetchResponse.PartitionData<>(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null,\n                MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(new FetchResponse<>(Errors.NONE, new LinkedHashMap<>(partitions),\n                0, INVALID_SESSION_ID));\n        consumerClient.poll(time.timer(0));\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, (Double) fetchSizeAverage.metricValue(), EPSILON);\n        assertEquals(3, (Double) recordsCountAverage.metricValue(), EPSILON);\n    }\n","date":"2018-09-13 02:05:19","endLine":1856,"groupId":"11308","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testFetchResponseMetricsWithOnePartitionAtTheWrongOffset","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/48/d61b9e0724bc4ed612c29434b75cfb16dd46f6.src","preCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        subscriptions.assignFromUser(Utils.mkSet(tp0, tp1));\n        subscriptions.seek(tp0, 0);\n        subscriptions.seek(tp1, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp1, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> partitions = new HashMap<>();\n        partitions.put(tp0, new FetchResponse.PartitionData<>(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));\n        partitions.put(tp1, new FetchResponse.PartitionData<>(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null,\n                MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(new FetchResponse<>(Errors.NONE, new LinkedHashMap<>(partitions),\n                0, INVALID_SESSION_ID));\n        consumerClient.poll(time.timer(0));\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);\n        assertEquals(3, recordsCountAverage.value(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1818,"status":"M"}],"commitId":"c121f4eb82da654acbdd133a556cfe1f9197a46a","commitMessage":"@@@MINOR: Remove deprecated Metric.value() method usage (#5626)\n\nReviewers: Viktor Somogyi <viktorsomogyi@gmail.com>.  John Roesler <john@confluent.io>.  Rajini Sivaram <rajinisivaram@googlemail.com>\n","date":"2018-09-13 02:05:19","modifiedFileCount":"7","status":"M","submitter":"Manikumar Reddy O"},{"authorTime":"2019-03-08 08:29:19","codes":[{"authorDate":"2019-03-08 08:29:19","commitOrder":8,"curCode":"    public void testFetchResponseMetricsPartialResponse() {\n        buildFetcher();\n\n        assignFromUser(singleton(tp0));\n        subscriptions.seek(tp0, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(tp0, records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, (Double) fetchSizeAverage.metricValue(), EPSILON);\n        assertEquals(2, (Double) recordsCountAverage.metricValue(), EPSILON);\n    }\n","date":"2019-03-08 08:29:19","endLine":1989,"groupId":"15906","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testFetchResponseMetricsPartialResponse","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/3f/e7ca05c0c67d0b72786aad7f9bd09a11025ac3.src","preCode":"    public void testFetchResponseMetricsPartialResponse() {\n        subscriptions.assignFromUser(singleton(tp0));\n        subscriptions.seek(tp0, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(tp0, records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, (Double) fetchSizeAverage.metricValue(), EPSILON);\n        assertEquals(2, (Double) recordsCountAverage.metricValue(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1964,"status":"M"},{"authorDate":"2019-03-08 08:29:19","commitOrder":8,"curCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        buildFetcher();\n\n        assignFromUser(Utils.mkSet(tp0, tp1));\n        subscriptions.seek(tp0, 0);\n        subscriptions.seek(tp1, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp1, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> partitions = new HashMap<>();\n        partitions.put(tp0, new FetchResponse.PartitionData<>(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));\n        partitions.put(tp1, new FetchResponse.PartitionData<>(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null,\n                MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(new FetchResponse<>(Errors.NONE, new LinkedHashMap<>(partitions),\n                0, INVALID_SESSION_ID));\n        consumerClient.poll(time.timer(0));\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, (Double) fetchSizeAverage.metricValue(), EPSILON);\n        assertEquals(3, (Double) recordsCountAverage.metricValue(), EPSILON);\n    }\n","date":"2019-03-08 08:29:19","endLine":2069,"groupId":"11308","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testFetchResponseMetricsWithOnePartitionAtTheWrongOffset","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/3f/e7ca05c0c67d0b72786aad7f9bd09a11025ac3.src","preCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        subscriptions.assignFromUser(Utils.mkSet(tp0, tp1));\n        subscriptions.seek(tp0, 0);\n        subscriptions.seek(tp1, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp1, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> partitions = new HashMap<>();\n        partitions.put(tp0, new FetchResponse.PartitionData<>(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));\n        partitions.put(tp1, new FetchResponse.PartitionData<>(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null,\n                MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(new FetchResponse<>(Errors.NONE, new LinkedHashMap<>(partitions),\n                0, INVALID_SESSION_ID));\n        consumerClient.poll(time.timer(0));\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, (Double) fetchSizeAverage.metricValue(), EPSILON);\n        assertEquals(3, (Double) recordsCountAverage.metricValue(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":2029,"status":"M"}],"commitId":"460e46c3bb76a361d0706b263c03696005e12566","commitMessage":"@@@KAFKA-7831; Do not modify subscription state from background thread (#6221)\n\nMetadata may be updated from the background thread.  so we need to protect access to SubscriptionState. This patch restructures the metadata handling so that we only check pattern subscriptions in the foreground. Additionally.  it improves the following:\n\n1. SubscriptionState is now the source of truth for the topics that will be fetched. We had a lot of messy logic previously to try and keep the the topic set in Metadata consistent with the subscription.  so this simplifies the logic.\n2. The metadata needs for the producer and consumer are quite different.  so it made sense to separate the custom logic into separate extensions of Metadata. For example.  only the producer requires topic expiration.\n3. We've always had an edge case in which a metadata change with an inflight request may cause us to effectively miss an expected update. This patch implements a separate version inside Metadata which is bumped when the needed topics changes.\n4. This patch removes the MetadataListener.  which was the cause of https://issues.apache.org/jira/browse/KAFKA-7764. \n\nReviewers: David Arthur <mumrah@gmail.com>.  Rajini Sivaram <rajinisivaram@googlemail.com>","date":"2019-03-08 08:29:19","modifiedFileCount":"30","status":"M","submitter":"Jason Gustafson"},{"authorTime":"2021-03-04 18:06:50","codes":[{"authorDate":"2019-03-08 08:29:19","commitOrder":9,"curCode":"    public void testFetchResponseMetricsPartialResponse() {\n        buildFetcher();\n\n        assignFromUser(singleton(tp0));\n        subscriptions.seek(tp0, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(tp0, records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, (Double) fetchSizeAverage.metricValue(), EPSILON);\n        assertEquals(2, (Double) recordsCountAverage.metricValue(), EPSILON);\n    }\n","date":"2019-03-08 08:29:19","endLine":1989,"groupId":"15906","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"testFetchResponseMetricsPartialResponse","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/3f/e7ca05c0c67d0b72786aad7f9bd09a11025ac3.src","preCode":"    public void testFetchResponseMetricsPartialResponse() {\n        buildFetcher();\n\n        assignFromUser(singleton(tp0));\n        subscriptions.seek(tp0, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(tp0, records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, (Double) fetchSizeAverage.metricValue(), EPSILON);\n        assertEquals(2, (Double) recordsCountAverage.metricValue(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1964,"status":"N"},{"authorDate":"2021-03-04 18:06:50","commitOrder":9,"curCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        buildFetcher();\n\n        assignFromUser(Utils.mkSet(tp0, tp1));\n        subscriptions.seek(tp0, 0);\n        subscriptions.seek(tp1, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp1, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponseData.PartitionData> partitions = new HashMap<>();\n        partitions.put(tp0, new FetchResponseData.PartitionData()\n                .setPartitionIndex(tp0.partition())\n                .setHighWatermark(100)\n                .setLogStartOffset(0)\n                .setRecords(records));\n        partitions.put(tp1, new FetchResponseData.PartitionData()\n                .setPartitionIndex(tp1.partition())\n                .setHighWatermark(100)\n                .setLogStartOffset(0)\n                .setRecords(MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, new LinkedHashMap<>(partitions)));\n        consumerClient.poll(time.timer(0));\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, (Double) fetchSizeAverage.metricValue(), EPSILON);\n        assertEquals(3, (Double) recordsCountAverage.metricValue(), EPSILON);\n    }\n","date":"2021-03-04 18:06:50","endLine":2432,"groupId":"15908","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testFetchResponseMetricsWithOnePartitionAtTheWrongOffset","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/98/5d95947c23675530c9db6a31c6f3fbd2d6cecf.src","preCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        buildFetcher();\n\n        assignFromUser(Utils.mkSet(tp0, tp1));\n        subscriptions.seek(tp0, 0);\n        subscriptions.seek(tp1, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp1, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponse.PartitionData<MemoryRecords>> partitions = new HashMap<>();\n        partitions.put(tp0, new FetchResponse.PartitionData<>(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));\n        partitions.put(tp1, new FetchResponse.PartitionData<>(Errors.NONE, 100,\n                FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null,\n                MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(new FetchResponse<>(Errors.NONE, new LinkedHashMap<>(partitions),\n                0, INVALID_SESSION_ID));\n        consumerClient.poll(time.timer(0));\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, (Double) fetchSizeAverage.metricValue(), EPSILON);\n        assertEquals(3, (Double) recordsCountAverage.metricValue(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":2388,"status":"M"}],"commitId":"8205051e90e3ea16165f8dc1f5c81af744bb1b9a","commitMessage":"@@@MINOR: remove FetchResponse.AbortedTransaction and redundant construc? (#9758)\n\n1. rename INVALID_HIGHWATERMARK to INVALID_HIGH_WATERMARK\n2. replace FetchResponse.AbortedTransaction by FetchResponseData.AbortedTransaction\n3. remove redundant constructors from FetchResponse.PartitionData\n4. rename recordSet to records\n5. add helpers \"recordsOrFail\" and \"recordsSize\" to FetchResponse to process record casting\n\nReviewers: Ismael Juma <ismael@juma.me.uk>","date":"2021-03-04 18:06:50","modifiedFileCount":"15","status":"M","submitter":"Chia-Ping Tsai"},{"authorTime":"2021-07-08 07:02:37","codes":[{"authorDate":"2019-03-08 08:29:19","commitOrder":10,"curCode":"    public void testFetchResponseMetricsPartialResponse() {\n        buildFetcher();\n\n        assignFromUser(singleton(tp0));\n        subscriptions.seek(tp0, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(tp0, records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, (Double) fetchSizeAverage.metricValue(), EPSILON);\n        assertEquals(2, (Double) recordsCountAverage.metricValue(), EPSILON);\n    }\n","date":"2019-03-08 08:29:19","endLine":1989,"groupId":"103946","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"testFetchResponseMetricsPartialResponse","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/3f/e7ca05c0c67d0b72786aad7f9bd09a11025ac3.src","preCode":"    public void testFetchResponseMetricsPartialResponse() {\n        buildFetcher();\n\n        assignFromUser(singleton(tp0));\n        subscriptions.seek(tp0, 1);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        int expectedBytes = 0;\n        for (Record record : records.records()) {\n            if (record.offset() >= 1)\n                expectedBytes += record.sizeInBytes();\n        }\n\n        fetchRecords(tp0, records, Errors.NONE, 100L, 0);\n        assertEquals(expectedBytes, (Double) fetchSizeAverage.metricValue(), EPSILON);\n        assertEquals(2, (Double) recordsCountAverage.metricValue(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":1964,"status":"N"},{"authorDate":"2021-07-08 07:02:37","commitOrder":10,"curCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        buildFetcher();\n\n        assignFromUser(Utils.mkSet(tp0, tp1));\n        subscriptions.seek(tp0, 0);\n        subscriptions.seek(tp1, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp1, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponseData.PartitionData> partitions = new HashMap<>();\n        partitions.put(tp0, new FetchResponseData.PartitionData()\n                .setPartitionIndex(tp0.partition())\n                .setHighWatermark(100)\n                .setLogStartOffset(0)\n                .setRecords(records));\n        partitions.put(tp1, new FetchResponseData.PartitionData()\n                .setPartitionIndex(tp1.partition())\n                .setHighWatermark(100)\n                .setLogStartOffset(0)\n                .setRecords(MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, new LinkedHashMap<>(partitions), topicIds));\n        consumerClient.poll(time.timer(0));\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, (Double) fetchSizeAverage.metricValue(), EPSILON);\n        assertEquals(3, (Double) recordsCountAverage.metricValue(), EPSILON);\n    }\n","date":"2021-07-08 07:02:37","endLine":2518,"groupId":"103946","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"testFetchResponseMetricsWithOnePartitionAtTheWrongOffset","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/ec/a8d4d0de616ad523f86ea20b0b0f6e6fc16cd5.src","preCode":"    public void testFetchResponseMetricsWithOnePartitionAtTheWrongOffset() {\n        buildFetcher();\n\n        assignFromUser(Utils.mkSet(tp0, tp1));\n        subscriptions.seek(tp0, 0);\n        subscriptions.seek(tp1, 0);\n\n        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();\n        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));\n        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));\n\n        \r\n        assertEquals(1, fetcher.sendFetches());\n        subscriptions.seek(tp1, 5);\n\n        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,\n                TimestampType.CREATE_TIME, 0L);\n        for (int v = 0; v < 3; v++)\n            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, \"key\".getBytes(), (\"value-\" + v).getBytes());\n        MemoryRecords records = builder.build();\n\n        Map<TopicPartition, FetchResponseData.PartitionData> partitions = new HashMap<>();\n        partitions.put(tp0, new FetchResponseData.PartitionData()\n                .setPartitionIndex(tp0.partition())\n                .setHighWatermark(100)\n                .setLogStartOffset(0)\n                .setRecords(records));\n        partitions.put(tp1, new FetchResponseData.PartitionData()\n                .setPartitionIndex(tp1.partition())\n                .setHighWatermark(100)\n                .setLogStartOffset(0)\n                .setRecords(MemoryRecords.withRecords(CompressionType.NONE, new SimpleRecord(\"val\".getBytes()))));\n\n        client.prepareResponse(FetchResponse.of(Errors.NONE, 0, INVALID_SESSION_ID, new LinkedHashMap<>(partitions)));\n        consumerClient.poll(time.timer(0));\n        fetcher.fetchedRecords();\n\n        \r\n        int expectedBytes = 0;\n        for (Record record : records.records())\n            expectedBytes += record.sizeInBytes();\n\n        assertEquals(expectedBytes, (Double) fetchSizeAverage.metricValue(), EPSILON);\n        assertEquals(3, (Double) recordsCountAverage.metricValue(), EPSILON);\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":2474,"status":"M"}],"commitId":"2b8aff58b575c199ee8372e5689420c9d77357a5","commitMessage":"@@@KAFKA-10580: Add topic ID support to Fetch request (#9944)\n\nUpdated FetchRequest and FetchResponse to use topic IDs rather than topic names.\nSome of the complicated code is found in FetchSession and FetchSessionHandler.\nWe need to be able to store topic IDs and maintain a cache on the broker for IDs that may not have been resolved. On incremental fetch requests.  we will try to resolve them or remove them if in toForget.\n\nReviewers: Rajini Sivaram <rajinisivaram@googlemail.com>.  Chia-Ping Tsai <chia7712@gmail.com>.  Jun Rao <junrao@gmail.com>","date":"2021-07-08 07:02:37","modifiedFileCount":"23","status":"M","submitter":"Justine Olshan"}]
