[{"authorTime":"2019-10-31 00:24:59","codes":[{"authorDate":"2019-08-27 07:28:22","commitOrder":2,"curCode":"    private void produceRecordsForTwoSegments(final Duration segmentInterval) throws Exception {\n        final MockTime mockTime = new MockTime(Math.max(segmentInterval.toMillis(), 60_000L));\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"A\")),\n            TestUtils.producerConfig(\n                CLUSTER.bootstrapServers(),\n                IntegerSerializer.class,\n                StringSerializer.class,\n                new Properties()),\n            mockTime.milliseconds()\n        );\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"B\")),\n            TestUtils.producerConfig(\n                CLUSTER.bootstrapServers(),\n                IntegerSerializer.class,\n                StringSerializer.class,\n                new Properties()),\n            mockTime.milliseconds()\n        );\n    }\n","date":"2019-08-27 07:28:22","endLine":250,"groupId":"12499","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"produceRecordsForTwoSegments","params":"(finalDurationsegmentInterval)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/01/04d024e986c1053bd9473adad4cdcd6384de73.src","preCode":"    private void produceRecordsForTwoSegments(final Duration segmentInterval) throws Exception {\n        final MockTime mockTime = new MockTime(Math.max(segmentInterval.toMillis(), 60_000L));\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"A\")),\n            TestUtils.producerConfig(\n                CLUSTER.bootstrapServers(),\n                IntegerSerializer.class,\n                StringSerializer.class,\n                new Properties()),\n            mockTime.milliseconds()\n        );\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"B\")),\n            TestUtils.producerConfig(\n                CLUSTER.bootstrapServers(),\n                IntegerSerializer.class,\n                StringSerializer.class,\n                new Properties()),\n            mockTime.milliseconds()\n        );\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":228,"status":"NB"},{"authorDate":"2019-10-31 00:24:59","commitOrder":2,"curCode":"    private void produceRecordsForClosingWindow(final Duration windowSize) throws Exception {\n        final MockTime mockTime = new MockTime(windowSize.toMillis() + 1);\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"A\")),\n            TestUtils.producerConfig(\n                CLUSTER.bootstrapServers(),\n                IntegerSerializer.class,\n                StringSerializer.class,\n                new Properties()),\n            mockTime.milliseconds()\n        );\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"B\")),\n            TestUtils.producerConfig(\n                CLUSTER.bootstrapServers(),\n                IntegerSerializer.class,\n                StringSerializer.class,\n                new Properties()),\n            mockTime.milliseconds()\n        );\n    }\n","date":"2019-10-31 00:24:59","endLine":319,"groupId":"12499","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"produceRecordsForClosingWindow","params":"(finalDurationwindowSize)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/39/12a50d686b47a755c4b0a0ceb7f3a4ee5e5fa9.src","preCode":"    private void produceRecordsForClosingWindow(final Duration windowSize) throws Exception {\n        final MockTime mockTime = new MockTime(windowSize.toMillis() + 1);\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"A\")),\n            TestUtils.producerConfig(\n                CLUSTER.bootstrapServers(),\n                IntegerSerializer.class,\n                StringSerializer.class,\n                new Properties()),\n            mockTime.milliseconds()\n        );\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"B\")),\n            TestUtils.producerConfig(\n                CLUSTER.bootstrapServers(),\n                IntegerSerializer.class,\n                StringSerializer.class,\n                new Properties()),\n            mockTime.milliseconds()\n        );\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":297,"status":"B"}],"commitId":"fc0f82372e1e456cbd43490b9eba957c4a0d3eb5","commitMessage":"@@@KAFKA-8980: Refactor state-store-level streams metrics (#7584)\n\nRefactors metrics according to KIP-444\nIntroduces StateStoreMetrics as a central provider for state store metrics\nAdds metric scope (a.k.a. store type) to the in-memory suppression buffer\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>.   Bill Bejeck <bbejeck@gmail.com>","date":"2019-10-31 00:24:59","modifiedFileCount":"26","status":"M","submitter":"Bruno Cadonna"},{"authorTime":"2020-04-21 02:00:58","codes":[{"authorDate":"2020-04-21 02:00:58","commitOrder":3,"curCode":"    private void produceRecordsForTwoSegments(final Duration segmentInterval) throws Exception {\n        final MockTime mockTime = new MockTime(Math.max(segmentInterval.toMillis(), 60_000L));\n        final Properties props = TestUtils.producerConfig(\n            CLUSTER.bootstrapServers(),\n            IntegerSerializer.class,\n            StringSerializer.class,\n            new Properties());\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"A\")),\n            props,\n            mockTime.milliseconds()\n        );\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"B\")),\n            props,\n            mockTime.milliseconds()\n        );\n    }\n","date":"2020-04-21 02:00:58","endLine":283,"groupId":"511","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"produceRecordsForTwoSegments","params":"(finalDurationsegmentInterval)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/f8/389fc4cd27242a006919b7699317e9b735bcf2.src","preCode":"    private void produceRecordsForTwoSegments(final Duration segmentInterval) throws Exception {\n        final MockTime mockTime = new MockTime(Math.max(segmentInterval.toMillis(), 60_000L));\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"A\")),\n            TestUtils.producerConfig(\n                CLUSTER.bootstrapServers(),\n                IntegerSerializer.class,\n                StringSerializer.class,\n                new Properties()),\n            mockTime.milliseconds()\n        );\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"B\")),\n            TestUtils.producerConfig(\n                CLUSTER.bootstrapServers(),\n                IntegerSerializer.class,\n                StringSerializer.class,\n                new Properties()),\n            mockTime.milliseconds()\n        );\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":264,"status":"M"},{"authorDate":"2020-04-21 02:00:58","commitOrder":3,"curCode":"    private void produceRecordsForClosingWindow(final Duration windowSize) throws Exception {\n        final MockTime mockTime = new MockTime(windowSize.toMillis() + 1);\n        final Properties props = TestUtils.producerConfig(\n            CLUSTER.bootstrapServers(),\n            IntegerSerializer.class,\n            StringSerializer.class,\n            new Properties());\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"A\")),\n            props,\n            mockTime.milliseconds()\n        );\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"B\")),\n            props,\n            mockTime.milliseconds()\n        );\n    }\n","date":"2020-04-21 02:00:58","endLine":304,"groupId":"511","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"produceRecordsForClosingWindow","params":"(finalDurationwindowSize)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/f8/389fc4cd27242a006919b7699317e9b735bcf2.src","preCode":"    private void produceRecordsForClosingWindow(final Duration windowSize) throws Exception {\n        final MockTime mockTime = new MockTime(windowSize.toMillis() + 1);\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"A\")),\n            TestUtils.producerConfig(\n                CLUSTER.bootstrapServers(),\n                IntegerSerializer.class,\n                StringSerializer.class,\n                new Properties()),\n            mockTime.milliseconds()\n        );\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"B\")),\n            TestUtils.producerConfig(\n                CLUSTER.bootstrapServers(),\n                IntegerSerializer.class,\n                StringSerializer.class,\n                new Properties()),\n            mockTime.milliseconds()\n        );\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":285,"status":"M"}],"commitId":"fcf45e1fac88238b1d3dbcfa1f324674939706f3","commitMessage":"@@@MINOR: Further reduce runtime for metrics integration tests (#8514)\n\n1. In both RocksDBMetrics and Metrics integration tests.  we do not need to wait for consumer to consume records from output topics since the sensors / metrics are registered upon task creation.\n\n2. Merged the two test cases of RocksDB with one app that creates two state stores (non-segmented and segmented).\n\nWith these two changes.  local runtime of these two tests reduced from 2min+ and 3min+ to under a minute.\n\nReviewers: Bruno Cadonna <bruno@confluent.io>.  Matthias J. Sax <matthias@confluent.io>","date":"2020-04-21 02:00:58","modifiedFileCount":"2","status":"M","submitter":"Guozhang Wang"},{"authorTime":"2020-08-25 08:37:49","codes":[{"authorDate":"2020-08-25 08:37:49","commitOrder":4,"curCode":"    private void produceRecordsForTwoSegments(final Duration segmentInterval) {\n        final MockTime mockTime = new MockTime(Math.max(segmentInterval.toMillis(), 60_000L));\n        final Properties props = TestUtils.producerConfig(\n            CLUSTER.bootstrapServers(),\n            IntegerSerializer.class,\n            StringSerializer.class,\n            new Properties());\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"A\")),\n            props,\n            mockTime.milliseconds()\n        );\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"B\")),\n            props,\n            mockTime.milliseconds()\n        );\n    }\n","date":"2020-08-25 08:37:49","endLine":296,"groupId":"102628","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"produceRecordsForTwoSegments","params":"(finalDurationsegmentInterval)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/8b/33d25bf8feb243d7e3404fcf241b8a4ab24af9.src","preCode":"    private void produceRecordsForTwoSegments(final Duration segmentInterval) throws Exception {\n        final MockTime mockTime = new MockTime(Math.max(segmentInterval.toMillis(), 60_000L));\n        final Properties props = TestUtils.producerConfig(\n            CLUSTER.bootstrapServers(),\n            IntegerSerializer.class,\n            StringSerializer.class,\n            new Properties());\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"A\")),\n            props,\n            mockTime.milliseconds()\n        );\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"B\")),\n            props,\n            mockTime.milliseconds()\n        );\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":277,"status":"M"},{"authorDate":"2020-08-25 08:37:49","commitOrder":4,"curCode":"    private void produceRecordsForClosingWindow(final Duration windowSize) {\n        final MockTime mockTime = new MockTime(windowSize.toMillis() + 1);\n        final Properties props = TestUtils.producerConfig(\n            CLUSTER.bootstrapServers(),\n            IntegerSerializer.class,\n            StringSerializer.class,\n            new Properties());\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"A\")),\n            props,\n            mockTime.milliseconds()\n        );\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"B\")),\n            props,\n            mockTime.milliseconds()\n        );\n    }\n","date":"2020-08-25 08:37:49","endLine":317,"groupId":"102628","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"produceRecordsForClosingWindow","params":"(finalDurationwindowSize)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/8b/33d25bf8feb243d7e3404fcf241b8a4ab24af9.src","preCode":"    private void produceRecordsForClosingWindow(final Duration windowSize) throws Exception {\n        final MockTime mockTime = new MockTime(windowSize.toMillis() + 1);\n        final Properties props = TestUtils.producerConfig(\n            CLUSTER.bootstrapServers(),\n            IntegerSerializer.class,\n            StringSerializer.class,\n            new Properties());\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"A\")),\n            props,\n            mockTime.milliseconds()\n        );\n        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n            STREAM_INPUT,\n            Collections.singletonList(new KeyValue<>(1, \"B\")),\n            props,\n            mockTime.milliseconds()\n        );\n    }\n","realPath":"streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":298,"status":"M"}],"commitId":"22bcd9fac3c988c15862d0b6c01930814b676253","commitMessage":"@@@KAFKA-10054: KIP-613.  add TRACE-level e2e latency metrics (#9094)\n\nAdds avg.  min.  and max e2e latency metrics at the new TRACE level. Also adds the missing avg task-level metric at the INFO level.\n\nI think where we left off with the KIP.  the TRACE-level metrics were still defined to be \"stateful-processor-level\". I realized this doesn't really make sense and would be pretty much impossible to define given the DFS processing approach of Streams.  and felt that store-level metrics made more sense to begin with. I haven't updated the KIP yet so I could get some initial feedback on this\n\nReviewers: Bruno Cadonna <bruno@confluent.io>.  Guozhang Wang <wangguoz@gmail.com>","date":"2020-08-25 08:37:49","modifiedFileCount":"18","status":"M","submitter":"A. Sophie Blee-Goldman"}]
