[{"authorTime":"2019-04-18 07:42:49","codes":[{"authorDate":"2019-04-18 07:42:49","commitOrder":1,"curCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(maxObservedTimestamp);\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = cache.range(cacheName, cacheKeyFrom, cacheKeyTo);\n        }\n","date":"2019-04-18 07:42:49","endLine":348,"groupId":"15820","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"getNextSegmentIterator","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/ad/baf4cde37f7ee708e56e17b197906d9bc7cb49.src","preCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(maxObservedTimestamp);\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = cache.range(cacheName, cacheKeyFrom, cacheKeyTo);\n        }\n","realPath":"streams/src/main/java/org/apache/kafka/streams/state/internals/CachingSessionStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":335,"status":"B"},{"authorDate":"2019-04-18 07:42:49","commitOrder":1,"curCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(Math.min(timeTo, maxObservedTimestamp));\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = cache.range(name, cacheKeyFrom, cacheKeyTo);\n        }\n","date":"2019-04-18 07:42:49","endLine":408,"groupId":"15820","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"getNextSegmentIterator","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/b6/4c0eb859de30f30b9cbebbfd812266f6262108.src","preCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(Math.min(timeTo, maxObservedTimestamp));\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = cache.range(name, cacheKeyFrom, cacheKeyTo);\n        }\n","realPath":"streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":395,"status":"B"}],"commitId":"0f7a87e93dd06659b886a75547ea576f374d01c2","commitMessage":"@@@KAFKA-7652: Restrict range of fetch/findSessions in cache (#6448)\n\nReduce the total key space cache iterators have to search for segmented byte stores by wrapping several single-segment iterators.\n\nSummary of Benchmarking Results (# records processed as primary indicator)\n\nSession Store:\nOnly single-key findSessions seems to benefit (~4x improvement) due to conservative scanning of potentially variable-sized keys in key-range findSessions. Could get improvement from key-range findSessions as well if we can tell when/if keys are a fixed size.  or pending an efficient custom comparator API from RocksDB\n\nWindow Store:\nBoth single and multi-key fetch saw some improvement; this depended on the size of the time-range in the fetch (in the DSL this would be window size) relative to the retention period. Performance benefits from this patch when the fetch spans multiple segments; hence the larger the time range being searched.  the better this will do.\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>.  Bill Bejeck <bbejeck@gmail.com>","date":"2019-04-18 07:42:49","modifiedFileCount":"4","status":"B","submitter":"A. Sophie Blee-Goldman"},{"authorTime":"2020-05-30 01:48:03","codes":[{"authorDate":"2020-05-30 01:48:03","commitOrder":2,"curCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(maxObservedTimestamp);\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = context.cache().range(cacheName, cacheKeyFrom, cacheKeyTo);\n        }\n","date":"2020-05-30 01:48:03","endLine":356,"groupId":"15820","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"getNextSegmentIterator","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/25/068c054f5e2eb63444f411593262b5f9e86013.src","preCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(maxObservedTimestamp);\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = cache.range(cacheName, cacheKeyFrom, cacheKeyTo);\n        }\n","realPath":"streams/src/main/java/org/apache/kafka/streams/state/internals/CachingSessionStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":343,"status":"M"},{"authorDate":"2020-05-30 01:48:03","commitOrder":2,"curCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(Math.min(timeTo, maxObservedTimestamp));\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = context.cache().range(name, cacheKeyFrom, cacheKeyTo);\n        }\n","date":"2020-05-30 01:48:03","endLine":420,"groupId":"15820","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"getNextSegmentIterator","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/78/e16a923fc020d35eb8159fbec3160e742edb81.src","preCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(Math.min(timeTo, maxObservedTimestamp));\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = cache.range(name, cacheKeyFrom, cacheKeyTo);\n        }\n","realPath":"streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":407,"status":"M"}],"commitId":"9d52deca247d9e16cf530d655891b2bbe474ffae","commitMessage":"@@@KAFKA-9501: convert between active and standby without closing stores (#8248)\n\nThis PR has gone through several significant transitions of its own.  but here's the latest:\n\n* TaskManager just collects the tasks to transition and refers to the active/standby task creator to handle closing & recycling the old task and creating the new one. If we ever hit an exception during the close.  we bail and close all the remaining tasks as dirty.\n\n* The task creators tell the task to \"close but recycle state\". If this is successful.  it tells the recycled processor context and state manager that they should transition to the new type.\n\n* During \"close and recycle\" the task just does a normal clean close.  but instead of closing the state manager it informs it to recycle itself: maintain all of its store information (most importantly the current store offsets) but unregister the changelogs from the changelog reader\n\n* The new task will (re-)register its changelogs during initialization.  but skip re-registering any stores. It will still read the checkpoint file.  but only use the written offsets if the store offsets are not already initialized from pre-transition\n\n* To ensure we don't end up with manual compaction disabled for standbys.  we have to call the state restore listener's onRestoreEnd for any active restoring stores that are switching to standbys\n\nReviewers: John Roesler <vvcephei@apache.org>.  Guozhang Wang <wangguoz@gmail.com>","date":"2020-05-30 01:48:03","modifiedFileCount":"41","status":"M","submitter":"A. Sophie Blee-Goldman"},{"authorTime":"2020-07-01 01:15:28","codes":[{"authorDate":"2020-05-30 01:48:03","commitOrder":3,"curCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(maxObservedTimestamp);\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = context.cache().range(cacheName, cacheKeyFrom, cacheKeyTo);\n        }\n","date":"2020-05-30 01:48:03","endLine":356,"groupId":"15820","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"getNextSegmentIterator","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/25/068c054f5e2eb63444f411593262b5f9e86013.src","preCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(maxObservedTimestamp);\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = context.cache().range(cacheName, cacheKeyFrom, cacheKeyTo);\n        }\n","realPath":"streams/src/main/java/org/apache/kafka/streams/state/internals/CachingSessionStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":343,"status":"N"},{"authorDate":"2020-07-01 01:15:28","commitOrder":3,"curCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(Math.min(timeTo, maxObservedTimestamp.get()));\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = context.cache().range(name, cacheKeyFrom, cacheKeyTo);\n        }\n","date":"2020-07-01 01:15:28","endLine":426,"groupId":"15820","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"getNextSegmentIterator","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/ee/31cf68f07b48808692ddeea21fc5ddc84e2e81.src","preCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(Math.min(timeTo, maxObservedTimestamp));\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = context.cache().range(name, cacheKeyFrom, cacheKeyTo);\n        }\n","realPath":"streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":413,"status":"M"}],"commitId":"c4ec765af51cfce8a09bcc2446c6ad33cae69ba1","commitMessage":"@@@KAFKA-4996: Fix findbugs multithreaded correctness warnings for streams (#8929)\n\nFix findbugs multithreaded correctness warnings for streams.  updated variables to be threadsafe\n\nReviewers: A. Sophie Blee-Goldman <sophie@confluent.io>.  Boyang Chen <boyang@confluent.io>.  John Roesler <vvcephei@apache.org>","date":"2020-07-01 01:15:28","modifiedFileCount":"2","status":"M","submitter":"leah"},{"authorTime":"2020-08-12 11:21:41","codes":[{"authorDate":"2020-05-30 01:48:03","commitOrder":4,"curCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(maxObservedTimestamp);\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = context.cache().range(cacheName, cacheKeyFrom, cacheKeyTo);\n        }\n","date":"2020-05-30 01:48:03","endLine":356,"groupId":"15820","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"getNextSegmentIterator","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/25/068c054f5e2eb63444f411593262b5f9e86013.src","preCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(maxObservedTimestamp);\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = context.cache().range(cacheName, cacheKeyFrom, cacheKeyTo);\n        }\n","realPath":"streams/src/main/java/org/apache/kafka/streams/state/internals/CachingSessionStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":343,"status":"N"},{"authorDate":"2020-08-12 11:21:41","commitOrder":4,"curCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(Math.min(timeTo, maxObservedTimestamp.get()));\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = context.cache().range(cacheName, cacheKeyFrom, cacheKeyTo);\n        }\n","date":"2020-08-12 11:21:41","endLine":430,"groupId":"15820","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"getNextSegmentIterator","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/7d/f0a4b7d0975217df33a24a017193e1d69acf98.src","preCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(Math.min(timeTo, maxObservedTimestamp.get()));\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = context.cache().range(name, cacheKeyFrom, cacheKeyTo);\n        }\n","realPath":"streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":417,"status":"M"}],"commitId":"7915d5e5f826a71c11e1c9183c84702410f7209a","commitMessage":"@@@KAFKA-9450: Decouple flushing state from commiting (#8964)\n\nIn Kafka Streams the source-of-truth of a state store is in its changelog.  therefore when committing a state store we only need to make sure its changelog records are all flushed and committed.  but we do not actually need to make sure that the materialized state have to be flushed and persisted since they can always be restored from changelog when necessary.\n\nOn the other hand.  flushing a state store too frequently may have side effects.  e.g. rocksDB flushing would gets the memtable into an L0 sstable.  leaving many small L0 files to be compacted later.  which introduces larger overhead.\n\nTherefore this PR decouples flushing from committing.  such that we do not always flush the state store upon committing.  but only when sufficient data has been written since last time flushed. The checkpoint file would then also be overwritten only along with flushing the state store indicating its current known snapshot. This is okay since: a) if EOS is not enabled.  then it is fine if the local persisted state is actually ahead of the checkpoint.  b) if EOS is enabled.  then we would never write a checkpoint file until close.\n\nHere's a more detailed change list of this PR:\n\n1. Do not always flush state stores when calling pre-commit; move stateMgr.flush into post-commit to couple together with checkpointing.\n\n2. In post-commit.  we checkpoint when: a) The state store's snapshot has progressed much further compared to the previous checkpoint.  b) When the task is being closed.  in which case we enforce checkpointing.\n\n3. There are some tricky obstacles that I'd have to work around in a bit hacky way: for cache / suppression buffer.  we still need to flush them in pre-commit to make sure all records sent via producers.  while the underlying state store should not be flushed. I've decided to introduce a new API in CachingStateStore to be triggered in pre-commit.\n\nI've also made some minor changes piggy-backed in this PR:\n\n4. Do not delete checkpoint file upon loading it.  and as a result simplify the checkpointNeeded logic.  initializing the snapshotLastFlush to the loaded offsets.\n\n5. In closing.  also follow the commit -> suspend -> close ordering as in revocation / assignment.\n\n6. If enforceCheckpoint == true during RUNNING.  still calls maybeCheckpoint even with EOS since that is the case for suspending / closing.\n\nReviewers: John Roesler <john@confluent.io>.  A. Sophie Blee-Goldman <sophie@confluent.io>.  Matthias J. Sax <matthias@confluent.io>","date":"2020-08-12 11:21:41","modifiedFileCount":"24","status":"M","submitter":"Guozhang Wang"},{"authorTime":"2020-09-03 06:17:07","codes":[{"authorDate":"2020-05-30 01:48:03","commitOrder":5,"curCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(maxObservedTimestamp);\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = context.cache().range(cacheName, cacheKeyFrom, cacheKeyTo);\n        }\n","date":"2020-05-30 01:48:03","endLine":356,"groupId":"15820","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"getNextSegmentIterator","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/25/068c054f5e2eb63444f411593262b5f9e86013.src","preCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(maxObservedTimestamp);\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = context.cache().range(cacheName, cacheKeyFrom, cacheKeyTo);\n        }\n","realPath":"streams/src/main/java/org/apache/kafka/streams/state/internals/CachingSessionStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":343,"status":"N"},{"authorDate":"2020-09-03 06:17:07","commitOrder":5,"curCode":"        private void getNextSegmentIterator() {\n            if (forward) {\n                ++currentSegmentId;\n                \r\n                lastSegmentId = cacheFunction.segmentId(Math.min(timeTo, maxObservedTimestamp.get()));\n\n                if (currentSegmentId > lastSegmentId) {\n                    current = null;\n                    return;\n                }\n\n                setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n                current.close();\n\n                current = context.cache().range(cacheName, cacheKeyFrom, cacheKeyTo);\n            } else {\n                --currentSegmentId;\n\n                \r\n                if (currentSegmentId < lastSegmentId) {\n                    current = null;\n                    return;\n                }\n\n                setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n                current.close();\n\n                current = context.cache().reverseRange(cacheName, cacheKeyFrom, cacheKeyTo);\n            }\n        }\n","date":"2020-09-03 06:17:07","endLine":574,"groupId":"15820","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"getNextSegmentIterator","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/20/be3a3a33bd4272619cea597b5203592ef7ff65.src","preCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(Math.min(timeTo, maxObservedTimestamp.get()));\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = context.cache().range(cacheName, cacheKeyFrom, cacheKeyTo);\n        }\n","realPath":"streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":543,"status":"M"}],"commitId":"4f06d9e7d083c30912c810f9deadea809fd74edf","commitMessage":"@@@KAFKA-9929: Support backward iterator on WindowStore (#9138)\n\nImplements KIP-617 on WindowStore that depends on #9137.\n\nTesting strategy: extend existing tests to validate reverse operations are supported.\n\nReviewers: A. Sophie Blee-Goldman <sophie@confluent.io>.  Guozhang Wang <wangguoz@gmail.com>","date":"2020-09-03 06:17:07","modifiedFileCount":"47","status":"M","submitter":"Jorge Esteban Quilcate Otoya"},{"authorTime":"2020-09-03 06:17:07","codes":[{"authorDate":"2020-10-08 21:08:24","commitOrder":6,"curCode":"        private void getNextSegmentIterator() {\n            if (forward) {\n                ++currentSegmentId;\n                lastSegmentId = cacheFunction.segmentId(maxObservedTimestamp);\n\n                if (currentSegmentId > lastSegmentId) {\n                    current = null;\n                    return;\n                }\n\n                setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n                current.close();\n\n                current = context.cache().range(cacheName, cacheKeyFrom, cacheKeyTo);\n            } else {\n                --currentSegmentId;\n\n                if (currentSegmentId < lastSegmentId) {\n                    current = null;\n                    return;\n                }\n\n                setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n                current.close();\n\n                current = context.cache().reverseRange(cacheName, cacheKeyFrom, cacheKeyTo);\n            }\n\n        }\n","date":"2020-10-08 21:08:24","endLine":478,"groupId":"102833","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"getNextSegmentIterator","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/d0/fe25a6050ca1cade3db3aef1270522e9805631.src","preCode":"        private void getNextSegmentIterator() {\n            ++currentSegmentId;\n            lastSegmentId = cacheFunction.segmentId(maxObservedTimestamp);\n\n            if (currentSegmentId > lastSegmentId) {\n                current = null;\n                return;\n            }\n\n            setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n            current.close();\n            current = context.cache().range(cacheName, cacheKeyFrom, cacheKeyTo);\n        }\n","realPath":"streams/src/main/java/org/apache/kafka/streams/state/internals/CachingSessionStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":448,"status":"M"},{"authorDate":"2020-09-03 06:17:07","commitOrder":6,"curCode":"        private void getNextSegmentIterator() {\n            if (forward) {\n                ++currentSegmentId;\n                \r\n                lastSegmentId = cacheFunction.segmentId(Math.min(timeTo, maxObservedTimestamp.get()));\n\n                if (currentSegmentId > lastSegmentId) {\n                    current = null;\n                    return;\n                }\n\n                setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n                current.close();\n\n                current = context.cache().range(cacheName, cacheKeyFrom, cacheKeyTo);\n            } else {\n                --currentSegmentId;\n\n                \r\n                if (currentSegmentId < lastSegmentId) {\n                    current = null;\n                    return;\n                }\n\n                setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n                current.close();\n\n                current = context.cache().reverseRange(cacheName, cacheKeyFrom, cacheKeyTo);\n            }\n        }\n","date":"2020-09-03 06:17:07","endLine":574,"groupId":"102833","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"getNextSegmentIterator","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/20/be3a3a33bd4272619cea597b5203592ef7ff65.src","preCode":"        private void getNextSegmentIterator() {\n            if (forward) {\n                ++currentSegmentId;\n                \r\n                lastSegmentId = cacheFunction.segmentId(Math.min(timeTo, maxObservedTimestamp.get()));\n\n                if (currentSegmentId > lastSegmentId) {\n                    current = null;\n                    return;\n                }\n\n                setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n                current.close();\n\n                current = context.cache().range(cacheName, cacheKeyFrom, cacheKeyTo);\n            } else {\n                --currentSegmentId;\n\n                \r\n                if (currentSegmentId < lastSegmentId) {\n                    current = null;\n                    return;\n                }\n\n                setCacheKeyRange(currentSegmentBeginTime(), currentSegmentLastTime());\n\n                current.close();\n\n                current = context.cache().reverseRange(cacheName, cacheKeyFrom, cacheKeyTo);\n            }\n        }\n","realPath":"streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":543,"status":"N"}],"commitId":"d0e6943bdd048aa6e0a4dbbdad3c8da460db16dc","commitMessage":"@@@KAFKA-9929: Support backward iterator on SessionStore (#9139)\n\nImplements KIP-617 for `SessionStore`\n\nReviewers: A. Sophie Blee-Goldman <sophie@confluent.io>.  John Roesler <vvcephei@apache.org>","date":"2020-10-08 21:08:24","modifiedFileCount":"14","status":"M","submitter":"Jorge Esteban Quilcate Otoya"}]
