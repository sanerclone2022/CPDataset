[{"authorTime":"2017-06-17 03:36:13","codes":[{"authorDate":"2017-06-17 03:36:13","commitOrder":5,"curCode":"    private void testAppendLarge(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n        RecordAccumulator accum = new RecordAccumulator(batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024,\n                compressionType, 0L, 100L, metrics, time, new ApiVersions(), null);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","date":"2017-06-17 03:36:28","endLine":172,"groupId":"6610","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testAppendLarge","params":"(CompressionTypecompressionType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/5b/fbdf0aa7f74ce4ea40be3461381b7d0ff5e3a6.src","preCode":"    private void testAppendLarge(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n        RecordAccumulator accum = new RecordAccumulator(batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024,\n                compressionType, 0L, 100L, metrics, time, new ApiVersions(), null);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":150,"status":"MB"},{"authorDate":"2017-06-17 03:36:13","commitOrder":5,"curCode":"    private void testAppendLargeOldMessageFormat(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n\n        ApiVersions apiVersions = new ApiVersions();\n        apiVersions.update(node1.idString(), NodeApiVersions.create(Collections.singleton(\n                new ApiVersionsResponse.ApiVersion(ApiKeys.PRODUCE.id, (short) 0, (short) 2))));\n\n        RecordAccumulator accum = new RecordAccumulator(batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024,\n                compressionType, 0L, 100L, metrics, time, apiVersions, null);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","date":"2017-06-17 03:36:28","endLine":211,"groupId":"6610","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testAppendLargeOldMessageFormat","params":"(CompressionTypecompressionType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/5b/fbdf0aa7f74ce4ea40be3461381b7d0ff5e3a6.src","preCode":"    private void testAppendLargeOldMessageFormat(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n\n        ApiVersions apiVersions = new ApiVersions();\n        apiVersions.update(node1.idString(), NodeApiVersions.create(Collections.singleton(\n                new ApiVersionsResponse.ApiVersion(ApiKeys.PRODUCE.id, (short) 0, (short) 2))));\n\n        RecordAccumulator accum = new RecordAccumulator(batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024,\n                compressionType, 0L, 100L, metrics, time, apiVersions, null);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":184,"status":"B"}],"commitId":"f49697a2796540452a21d4a29ab879ba04214046","commitMessage":"@@@KAFKA-5456; Ensure producer handles old format large compressed messages\n\nMore specifically.  fix the case where a compressed V0 or V1 message is\nlarger than the producer batch size.\n\nAuthor: Jason Gustafson <jason@confluent.io>\n\nReviewers: Apurva Mehta <apurva@confluent.io>.  Ismael Juma <ismael@juma.me.uk>\n\nCloses #3356 from hachikuji/KAFKA-5456\n","date":"2017-06-17 03:36:28","modifiedFileCount":"8","status":"M","submitter":"Jason Gustafson"},{"authorTime":"2017-08-26 01:38:15","codes":[{"authorDate":"2017-08-26 01:38:15","commitOrder":6,"curCode":"    private void testAppendLarge(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0L);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","date":"2017-08-26 01:42:40","endLine":174,"groupId":"6610","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testAppendLarge","params":"(CompressionTypecompressionType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/d4/86c10a94e1635be653b335c043ba495940a385.src","preCode":"    private void testAppendLarge(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n        RecordAccumulator accum = new RecordAccumulator(batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024,\n                compressionType, 0L, 100L, metrics, time, new ApiVersions(), null);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":152,"status":"M"},{"authorDate":"2017-08-26 01:38:15","commitOrder":6,"curCode":"    private void testAppendLargeOldMessageFormat(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n\n        ApiVersions apiVersions = new ApiVersions();\n        apiVersions.update(node1.idString(), NodeApiVersions.create(Collections.singleton(\n                new ApiVersionsResponse.ApiVersion(ApiKeys.PRODUCE.id, (short) 0, (short) 2))));\n\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0L);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","date":"2017-08-26 01:42:40","endLine":213,"groupId":"6610","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testAppendLargeOldMessageFormat","params":"(CompressionTypecompressionType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/d4/86c10a94e1635be653b335c043ba495940a385.src","preCode":"    private void testAppendLargeOldMessageFormat(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n\n        ApiVersions apiVersions = new ApiVersions();\n        apiVersions.update(node1.idString(), NodeApiVersions.create(Collections.singleton(\n                new ApiVersionsResponse.ApiVersion(ApiKeys.PRODUCE.id, (short) 0, (short) 2))));\n\n        RecordAccumulator accum = new RecordAccumulator(batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024,\n                compressionType, 0L, 100L, metrics, time, apiVersions, null);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":186,"status":"M"}],"commitId":"607c3c21f652a8f911d1efb8374d2eec313a4e2d","commitMessage":"@@@KAFKA-5755; KafkaProducer should be refactored to use LogContext\n\nWith LogContext.  each producer log item is automatically prefixed with client id and transactional id.\n\nAuthor: huxihx <huxi_2b@hotmail.com>\n\nReviewers: Jason Gustafson <jason@confluent.io>\n\nCloses #3703 from huxihx/KAFKA-5755\n","date":"2017-08-26 01:42:40","modifiedFileCount":"6","status":"M","submitter":"huxihx"},{"authorTime":"2019-04-29 23:59:18","codes":[{"authorDate":"2019-04-29 23:59:18","commitOrder":7,"curCode":"    private void testAppendLarge(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","date":"2019-04-29 23:59:17","endLine":173,"groupId":"6610","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testAppendLarge","params":"(CompressionTypecompressionType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/50/61447b54499191beaa38458c693c1eafc2ba36.src","preCode":"    private void testAppendLarge(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0L);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":151,"status":"M"},{"authorDate":"2019-04-29 23:59:18","commitOrder":7,"curCode":"    private void testAppendLargeOldMessageFormat(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n\n        ApiVersions apiVersions = new ApiVersions();\n        apiVersions.update(node1.idString(), NodeApiVersions.create(Collections.singleton(\n                new ApiVersionsResponse.ApiVersion(ApiKeys.PRODUCE.id, (short) 0, (short) 2))));\n\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","date":"2019-04-29 23:59:17","endLine":212,"groupId":"6610","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testAppendLargeOldMessageFormat","params":"(CompressionTypecompressionType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/50/61447b54499191beaa38458c693c1eafc2ba36.src","preCode":"    private void testAppendLargeOldMessageFormat(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n\n        ApiVersions apiVersions = new ApiVersions();\n        apiVersions.update(node1.idString(), NodeApiVersions.create(Collections.singleton(\n                new ApiVersionsResponse.ApiVersion(ApiKeys.PRODUCE.id, (short) 0, (short) 2))));\n\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0L);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":185,"status":"M"}],"commitId":"b4532a65f758448c763b65b2fdde1405db2f9d9d","commitMessage":"@@@KAFKA-8134: `linger.ms` must be a long\n\nReviewers: Ismael Juma <ismael@juma.me.uk>.  Colin P. McCabe <cmccabe@apache.org>","date":"2019-04-29 23:59:17","modifiedFileCount":"6","status":"M","submitter":"Dhruvil Shah"},{"authorTime":"2019-05-25 12:29:51","codes":[{"authorDate":"2019-04-29 23:59:18","commitOrder":8,"curCode":"    private void testAppendLarge(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","date":"2019-04-29 23:59:17","endLine":173,"groupId":"6610","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testAppendLarge","params":"(CompressionTypecompressionType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/50/61447b54499191beaa38458c693c1eafc2ba36.src","preCode":"    private void testAppendLarge(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":151,"status":"N"},{"authorDate":"2019-05-25 12:29:51","commitOrder":8,"curCode":"    private void testAppendLargeOldMessageFormat(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n\n        ApiVersions apiVersions = new ApiVersions();\n        apiVersions.update(node1.idString(), NodeApiVersions.create(Collections.singleton(\n                new ApiVersionsResponse.ApiVersion(ApiKeys.PRODUCE, (short) 0, (short) 2))));\n\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","date":"2019-05-25 12:29:51","endLine":212,"groupId":"6610","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testAppendLargeOldMessageFormat","params":"(CompressionTypecompressionType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/a5/8f3b5977befe72d87d8bb3e8d76d2e55ae09e7.src","preCode":"    private void testAppendLargeOldMessageFormat(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n\n        ApiVersions apiVersions = new ApiVersions();\n        apiVersions.update(node1.idString(), NodeApiVersions.create(Collections.singleton(\n                new ApiVersionsResponse.ApiVersion(ApiKeys.PRODUCE.id, (short) 0, (short) 2))));\n\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":185,"status":"M"}],"commitId":"a1808962e56eaaa3657298baad3fc09abcc49040","commitMessage":"@@@KAFKA-8422; Client should send OffsetForLeaderEpoch only if broker supports latest version (#6806)\n\nIn the olden days.  OffsetForLeaderEpoch was exclusively an inter-broker protocol and\nrequired Cluster level permission. With KIP-320.  clients can use this API as well and\nso we lowered the required permission to Topic Describe. The only way the client can\nbe sure that the new permissions are in use is to require version 3 of the protocol\nwhich was bumped for 2.3. If the broker does not support this version.  we skip the\nvalidation and revert to the old behavior.\n\nAdditionally.  this patch fixes a problem with the newly added replicaId field when\nparsed from older versions which did not have it. If the field was not present.  then\nwe used the consumer's sentinel value.  but this would limit the range of visible\noffsets by the high watermark. To get around this problem.  this patch adds a\nseparate \"debug\" sentinel similar to APIs like Fetch and ListOffsets.\n\nReviewers: Ismael Juma <ismael@juma.me.uk>","date":"2019-05-25 12:29:51","modifiedFileCount":"15","status":"M","submitter":"Jason Gustafson"},{"authorTime":"2019-08-02 05:36:12","codes":[{"authorDate":"2019-08-02 05:36:12","commitOrder":9,"curCode":"    private void testAppendLarge(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","date":"2019-08-02 05:36:12","endLine":174,"groupId":"6610","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testAppendLarge","params":"(CompressionTypecompressionType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/cd/b164af5b2dd161b3fef1f040dfa83272c21d6c.src","preCode":"    private void testAppendLarge(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":152,"status":"M"},{"authorDate":"2019-08-02 05:36:12","commitOrder":9,"curCode":"    private void testAppendLargeOldMessageFormat(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n\n        ApiVersions apiVersions = new ApiVersions();\n        apiVersions.update(node1.idString(), NodeApiVersions.create(Collections.singleton(\n                new ApiVersionsResponse.ApiVersion(ApiKeys.PRODUCE, (short) 0, (short) 2))));\n\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","date":"2019-08-02 05:36:12","endLine":213,"groupId":"6610","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testAppendLargeOldMessageFormat","params":"(CompressionTypecompressionType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/cd/b164af5b2dd161b3fef1f040dfa83272c21d6c.src","preCode":"    private void testAppendLargeOldMessageFormat(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n\n        ApiVersions apiVersions = new ApiVersions();\n        apiVersions.update(node1.idString(), NodeApiVersions.create(Collections.singleton(\n                new ApiVersionsResponse.ApiVersion(ApiKeys.PRODUCE, (short) 0, (short) 2))));\n\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":186,"status":"M"}],"commitId":"717c55be971df862c55f55d245b9997f1d6f998c","commitMessage":"@@@KAFKA-8601: Implement KIP-480: Sticky Partitioning for keyless records (#6997)\n\nImplement KIP-480.  which specifies that the default partitioner should use a \"sticky\" partitioning strategy for records that have a null key.\n\nReviewers: Colin P. McCabe <cmccabe@apache.org>.  Lucas Bradstreet <lucasbradstreet@gmail.com>.  Stanislav Kozlovski <stanislav_kozlovski@outlook.com>.  Jun Rao <junrao@gmail.com>.  Kamal Chandraprakash  <kamal.chandraprakash@gmail.com>","date":"2019-08-02 05:36:12","modifiedFileCount":"9","status":"M","submitter":"Justine Olshan"},{"authorTime":"2019-10-05 00:19:18","codes":[{"authorDate":"2019-08-02 05:36:12","commitOrder":10,"curCode":"    private void testAppendLarge(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","date":"2019-08-02 05:36:12","endLine":174,"groupId":"6610","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testAppendLarge","params":"(CompressionTypecompressionType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/cd/b164af5b2dd161b3fef1f040dfa83272c21d6c.src","preCode":"    private void testAppendLarge(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":152,"status":"N"},{"authorDate":"2019-10-05 00:19:18","commitOrder":10,"curCode":"    private void testAppendLargeOldMessageFormat(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n\n        ApiVersions apiVersions = new ApiVersions();\n        apiVersions.update(node1.idString(), NodeApiVersions.create(ApiKeys.PRODUCE.id, (short) 0, (short) 2));\n\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","date":"2019-10-05 00:19:18","endLine":211,"groupId":"6610","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testAppendLargeOldMessageFormat","params":"(CompressionTypecompressionType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/b0/4e5ddcfe530812b9c91b000361087eed7fced9.src","preCode":"    private void testAppendLargeOldMessageFormat(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n\n        ApiVersions apiVersions = new ApiVersions();\n        apiVersions.update(node1.idString(), NodeApiVersions.create(Collections.singleton(\n                new ApiVersionsResponse.ApiVersion(ApiKeys.PRODUCE, (short) 0, (short) 2))));\n\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":185,"status":"M"}],"commitId":"f98013cc501ab30c43ca707a1fbf46c4e9dc1215","commitMessage":"@@@Part 1 of KIP-511: Collect and Expose Client's Name and Version in the Brokers #7381\n\nReviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>.  David Arthur <mumrah@gmail.com>.  Colin P. McCabe <cmccabe@apache.org>","date":"2019-10-05 00:19:18","modifiedFileCount":"22","status":"M","submitter":"David Jacot"},{"authorTime":"2019-12-03 20:56:16","codes":[{"authorDate":"2019-12-03 20:56:16","commitOrder":11,"curCode":"    private void testAppendLarge(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false, time.milliseconds());\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","date":"2019-12-03 20:56:16","endLine":173,"groupId":"6610","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testAppendLarge","params":"(CompressionTypecompressionType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/08/b29b0996c864b1031a1b977aa53b2ec2135893.src","preCode":"    private void testAppendLarge(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":151,"status":"M"},{"authorDate":"2019-12-03 20:56:16","commitOrder":11,"curCode":"    private void testAppendLargeOldMessageFormat(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n\n        ApiVersions apiVersions = new ApiVersions();\n        apiVersions.update(node1.idString(), NodeApiVersions.create(ApiKeys.PRODUCE.id, (short) 0, (short) 2));\n\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false, time.milliseconds());\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","date":"2019-12-03 20:56:16","endLine":211,"groupId":"6610","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testAppendLargeOldMessageFormat","params":"(CompressionTypecompressionType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/08/b29b0996c864b1031a1b977aa53b2ec2135893.src","preCode":"    private void testAppendLargeOldMessageFormat(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n\n        ApiVersions apiVersions = new ApiVersions();\n        apiVersions.update(node1.idString(), NodeApiVersions.create(ApiKeys.PRODUCE.id, (short) 0, (short) 2));\n\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false);\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":185,"status":"M"}],"commitId":"38fde81132e0457e983eae60b3d4a9834ad84129","commitMessage":"@@@MINOR: Proactively update producer topic access time. (#7672)\n\nChanges the ProducerMetadata to longer record a sentinel TOPIC_EXPIRY_NEEDS_UPDATE upon topic map emplacement.  and instead set the expiry time directly. Previously the expiry time was being updated for all touched topics after a metadata fetch was processed.  which could be seconds/minutes in the future.\n\nAdditionally propagates the current time further in the Producer.  which should reduce the total number of current-time calls.\n\nReviewers: Ismael Juma <ismael@juma.me.uk>.   Rajini Sivaram <rajinisivaram@googlemail.com>","date":"2019-12-03 20:56:16","modifiedFileCount":"9","status":"M","submitter":"Brian Byrne"},{"authorTime":"2021-01-14 08:17:45","codes":[{"authorDate":"2021-01-14 08:17:45","commitOrder":12,"curCode":"    private void testAppendLarge(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false, time.milliseconds());\n        assertEquals(Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes, \"Our partition's leader should be ready\");\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","date":"2021-01-14 08:17:45","endLine":174,"groupId":"103703","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testAppendLarge","params":"(CompressionTypecompressionType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/36/08aabdebf6cda90c586957345bdf7e964fcf60.src","preCode":"    private void testAppendLarge(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false, time.milliseconds());\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":152,"status":"M"},{"authorDate":"2021-01-14 08:17:45","commitOrder":12,"curCode":"    private void testAppendLargeOldMessageFormat(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n\n        ApiVersions apiVersions = new ApiVersions();\n        apiVersions.update(node1.idString(), NodeApiVersions.create(ApiKeys.PRODUCE.id, (short) 0, (short) 2));\n\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false, time.milliseconds());\n        assertEquals(Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes, \"Our partition's leader should be ready\");\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","date":"2021-01-14 08:17:45","endLine":212,"groupId":"103703","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testAppendLargeOldMessageFormat","params":"(CompressionTypecompressionType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-kafka-10-0.7/blobInfo/CC_OUT/blobs/36/08aabdebf6cda90c586957345bdf7e964fcf60.src","preCode":"    private void testAppendLargeOldMessageFormat(CompressionType compressionType) throws Exception {\n        int batchSize = 512;\n        byte[] value = new byte[2 * batchSize];\n\n        ApiVersions apiVersions = new ApiVersions();\n        apiVersions.update(node1.idString(), NodeApiVersions.create(ApiKeys.PRODUCE.id, (short) 0, (short) 2));\n\n        RecordAccumulator accum = createTestRecordAccumulator(\n                batchSize + DefaultRecordBatch.RECORD_BATCH_OVERHEAD, 10 * 1024, compressionType, 0);\n        accum.append(tp1, 0L, key, value, Record.EMPTY_HEADERS, null, maxBlockTimeMs, false, time.milliseconds());\n        assertEquals(\"Our partition's leader should be ready\", Collections.singleton(node1), accum.ready(cluster, time.milliseconds()).readyNodes);\n\n        Deque<ProducerBatch> batches = accum.batches().get(tp1);\n        assertEquals(1, batches.size());\n        ProducerBatch producerBatch = batches.peek();\n        List<MutableRecordBatch> recordBatches = TestUtils.toList(producerBatch.records().batches());\n        assertEquals(1, recordBatches.size());\n        MutableRecordBatch recordBatch = recordBatches.get(0);\n        assertEquals(0L, recordBatch.baseOffset());\n        List<Record> records = TestUtils.toList(recordBatch);\n        assertEquals(1, records.size());\n        Record record = records.get(0);\n        assertEquals(0L, record.offset());\n        assertEquals(ByteBuffer.wrap(key), record.key());\n        assertEquals(ByteBuffer.wrap(value), record.value());\n        assertEquals(0L, record.timestamp());\n    }\n","realPath":"clients/src/test/java/org/apache/kafka/clients/producer/internals/RecordAccumulatorTest.java","repoName":"kafka","snippetEndLine":0,"snippetStartLine":0,"startLine":186,"status":"M"}],"commitId":"52b8aa0fdce1872b5b525b62dc3ac2241cfaa379","commitMessage":"@@@KAFKA-7340: Migrate clients module to JUnit 5 (#9874)\n\n* Use the packages/classes from JUnit 5\n* Move description in `assert` methods to last parameter\n* Convert parameterized tests so that they work with JUnit 5\n* Remove `hamcrest`.  it didn't seem to add much value\n* Fix `Utils.mkEntry` to have correct `equals` implementation\n* Add a missing `@Test` annotation in `SslSelectorTest` override\n* Adjust regex in `SaslAuthenticatorTest` due to small change in the\nassert failure string in JUnit 5\n\nReviewers: Chia-Ping Tsai <chia7712@gmail.com>","date":"2021-01-14 08:17:45","modifiedFileCount":"254","status":"M","submitter":"Ismael Juma"}]
