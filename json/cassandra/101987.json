[{"authorTime":"2014-11-14 02:01:19","codes":[{"authorDate":"2014-11-14 02:01:19","commitOrder":2,"curCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.count();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ICompactionScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.count());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.count());\n\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.count());\n        assertEquals(startStorageMetricsLoad - s.bytesOnDisk() + sum, StorageMetrics.load.count());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        Thread.sleep(1000);\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.count());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","date":"2014-11-14 02:01:19","endLine":219,"groupId":"20128","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testNumberOfFilesAndSizes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/8d/31b0416be7caf80125cd17ad1caba37430f4d9.src","preCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.count();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ICompactionScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.count());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.count());\n\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.count());\n        assertEquals(startStorageMetricsLoad - s.bytesOnDisk() + sum, StorageMetrics.load.count());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        Thread.sleep(1000);\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.count());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":173,"status":"MB"},{"authorDate":"2014-11-14 02:01:19","commitOrder":2,"curCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ICompactionScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        Thread.sleep(1000);\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        validateCFS(cfs);\n    }\n","date":"2014-11-14 02:01:19","endLine":431,"groupId":"29140","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testNumberOfFiles_truncate","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/8d/31b0416be7caf80125cd17ad1caba37430f4d9.src","preCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ICompactionScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        Thread.sleep(1000);\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":396,"status":"MB"}],"commitId":"9c66482e16b2fe45605dae8132fec8aff5eda470","commitMessage":"@@@Merge branch 'cassandra-2.1' into trunk\n","date":"2014-11-14 02:01:19","modifiedFileCount":"2","status":"M","submitter":"Joshua McKenzie"},{"authorTime":"2014-12-17 05:03:05","codes":[{"authorDate":"2014-12-17 05:03:05","commitOrder":3,"curCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.count();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.count());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.count());\n\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.count());\n        assertEquals(startStorageMetricsLoad - s.bytesOnDisk() + sum, StorageMetrics.load.count());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        Thread.sleep(1000);\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.count());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","date":"2014-12-17 05:03:05","endLine":227,"groupId":"20128","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testNumberOfFilesAndSizes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/24/a0091f3f26f985304cb122482636a93fd1c013.src","preCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.count();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ICompactionScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.count());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.count());\n\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.count());\n        assertEquals(startStorageMetricsLoad - s.bytesOnDisk() + sum, StorageMetrics.load.count());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        Thread.sleep(1000);\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.count());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":181,"status":"M"},{"authorDate":"2014-12-17 05:03:05","commitOrder":3,"curCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        Thread.sleep(1000);\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        validateCFS(cfs);\n    }\n","date":"2014-12-17 05:03:05","endLine":439,"groupId":"29140","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testNumberOfFiles_truncate","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/24/a0091f3f26f985304cb122482636a93fd1c013.src","preCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ICompactionScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        Thread.sleep(1000);\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":404,"status":"M"}],"commitId":"bee53d72a530aab2949d12bd9d2320b76811c85a","commitMessage":"@@@Merge branch 'cassandra-2.1' into trunk\n\nConflicts:\n\tsrc/java/org/apache/cassandra/db/compaction/AbstractCompactionStrategy.java\n\tsrc/java/org/apache/cassandra/db/compaction/CompactionIterable.java\n\tsrc/java/org/apache/cassandra/db/compaction/CompactionManager.java\n\tsrc/java/org/apache/cassandra/db/compaction/LeveledCompactionStrategy.java\n\tsrc/java/org/apache/cassandra/db/compaction/WrappingCompactionStrategy.java\n\tsrc/java/org/apache/cassandra/io/sstable/format/SSTableReader.java\n\tsrc/java/org/apache/cassandra/io/sstable/format/big/BigTableScanner.java\n\tsrc/java/org/apache/cassandra/tools/SSTableExport.java\n\ttest/unit/org/apache/cassandra/db/compaction/AntiCompactionTest.java\n\ttest/unit/org/apache/cassandra/db/compaction/CompactionsTest.java\n\ttest/unit/org/apache/cassandra/db/compaction/LeveledCompactionStrategyTest.java\n\ttest/unit/org/apache/cassandra/db/compaction/TTLExpiryTest.java\n\ttest/unit/org/apache/cassandra/io/sstable/SSTableReaderTest.java\n\ttest/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java\n\ttest/unit/org/apache/cassandra/io/sstable/SSTableScannerTest.java\n\ttest/unit/org/apache/cassandra/io/sstable/SSTableUtils.java\n","date":"2014-12-17 05:03:05","modifiedFileCount":"18","status":"M","submitter":"Joshua McKenzie"},{"authorTime":"2015-01-15 01:08:51","codes":[{"authorDate":"2014-12-17 05:03:05","commitOrder":4,"curCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.count();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.count());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.count());\n\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.count());\n        assertEquals(startStorageMetricsLoad - s.bytesOnDisk() + sum, StorageMetrics.load.count());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        Thread.sleep(1000);\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.count());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","date":"2014-12-17 05:03:05","endLine":227,"groupId":"20128","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testNumberOfFilesAndSizes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/24/a0091f3f26f985304cb122482636a93fd1c013.src","preCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.count();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.count());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.count());\n\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.count());\n        assertEquals(startStorageMetricsLoad - s.bytesOnDisk() + sum, StorageMetrics.load.count());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        Thread.sleep(1000);\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.count());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":181,"status":"N"},{"authorDate":"2015-01-15 01:08:51","commitOrder":4,"curCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        Thread.sleep(1000);\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        Thread.sleep(1000); \r\n        validateCFS(cfs);\n    }\n","date":"2015-01-15 01:08:51","endLine":498,"groupId":"29140","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testNumberOfFiles_truncate","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/af/ad97953fcfe729e6a905c00802615f0b8a5439.src","preCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        Thread.sleep(1000);\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":462,"status":"M"}],"commitId":"30a5597ca2eb5b13330cb5843a9ee036beac63de","commitMessage":"@@@Merge branch 'cassandra-2.1' into trunk\n","date":"2015-01-15 01:08:51","modifiedFileCount":"1","status":"M","submitter":"Marcus Eriksson"},{"authorTime":"2015-01-15 01:08:51","codes":[{"authorDate":"2015-01-05 22:34:08","commitOrder":5,"curCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - s.bytesOnDisk() + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        Thread.sleep(1000);\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","date":"2015-01-29 00:12:19","endLine":291,"groupId":"20128","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testNumberOfFilesAndSizes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/c8/23215b205ac711f1103aa1b4e62170529f9d33.src","preCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.count();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.count());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.count());\n\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.count());\n        assertEquals(startStorageMetricsLoad - s.bytesOnDisk() + sum, StorageMetrics.load.count());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        Thread.sleep(1000);\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.count());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":245,"status":"M"},{"authorDate":"2015-01-15 01:08:51","commitOrder":5,"curCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        Thread.sleep(1000);\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        Thread.sleep(1000); \r\n        validateCFS(cfs);\n    }\n","date":"2015-01-15 01:08:51","endLine":498,"groupId":"29140","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testNumberOfFiles_truncate","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/af/ad97953fcfe729e6a905c00802615f0b8a5439.src","preCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        Thread.sleep(1000);\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        Thread.sleep(1000); \r\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":462,"status":"N"}],"commitId":"8896a70b015102c212d0a27ed1f4e1f0fabe85c4","commitMessage":"@@@Upgrade metrics library and remove depricated metrics\n\nPatch by tjake; reviewed by aleksey for CASSANDRA-5657\n","date":"2015-01-29 00:12:19","modifiedFileCount":"72","status":"M","submitter":"T Jake Luciani"},{"authorTime":"2015-03-05 00:31:59","codes":[{"authorDate":"2015-01-05 22:34:08","commitOrder":6,"curCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - s.bytesOnDisk() + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        Thread.sleep(1000);\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","date":"2015-01-29 00:12:19","endLine":291,"groupId":"20128","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testNumberOfFilesAndSizes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/c8/23215b205ac711f1103aa1b4e62170529f9d33.src","preCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - s.bytesOnDisk() + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        Thread.sleep(1000);\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":245,"status":"N"},{"authorDate":"2015-03-05 00:31:59","commitOrder":6,"curCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n\n            List<SSTableReader> sstables = rewriter.finish();\n            cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n            Thread.sleep(1000);\n            assertFileCounts(s.descriptor.directory.list(), 0, 0);\n            cfs.truncateBlocking();\n            Thread.sleep(1000); \r\n            validateCFS(cfs);\n        }\n        catch (Throwable t)\n        {\n            rewriter.abort();\n            throw t;\n        }\n    }\n","date":"2015-03-05 00:31:59","endLine":552,"groupId":"29140","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testNumberOfFiles_truncate","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/77/c6df97a224829b9d81c075de445f1e92d4cb7f.src","preCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        Thread.sleep(1000);\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        Thread.sleep(1000); \r\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":510,"status":"M"}],"commitId":"36bd31d0578fedbcc1bfa2ca9b0dbccc4d9284d6","commitMessage":"@@@Merge branch 'cassandra-2.1' into trunk\n\nConflicts:\n\tsrc/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java\n\ttest/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java\n","date":"2015-03-05 00:31:59","modifiedFileCount":"4","status":"M","submitter":"Benedict Elliott Smith"},{"authorTime":"2015-04-10 04:05:45","codes":[{"authorDate":"2015-04-10 04:05:45","commitOrder":7,"curCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - s.bytesOnDisk() + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        SSTableDeletingTask.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","date":"2015-04-10 04:05:45","endLine":302,"groupId":"20128","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testNumberOfFilesAndSizes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/17/f33926d1d38ee8ff51d4d8571b61331a702ed4.src","preCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - s.bytesOnDisk() + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        Thread.sleep(1000);\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":255,"status":"M"},{"authorDate":"2015-04-10 04:05:45","commitOrder":7,"curCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n        }\n        catch (Throwable t)\n        {\n            rewriter.abort();\n            throw t;\n        }\n\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        SSTableDeletingTask.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        SSTableDeletingTask.waitForDeletions();\n        validateCFS(cfs);\n    }\n","date":"2015-04-10 04:05:45","endLine":564,"groupId":"29140","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testNumberOfFiles_truncate","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/17/f33926d1d38ee8ff51d4d8571b61331a702ed4.src","preCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n\n            List<SSTableReader> sstables = rewriter.finish();\n            cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n            Thread.sleep(1000);\n            assertFileCounts(s.descriptor.directory.list(), 0, 0);\n            cfs.truncateBlocking();\n            Thread.sleep(1000); \r\n            validateCFS(cfs);\n        }\n        catch (Throwable t)\n        {\n            rewriter.abort();\n            throw t;\n        }\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":522,"status":"M"}],"commitId":"d908bf431af7abe68aa69e6fd0ab50e30f01e607","commitMessage":"@@@Fix SSTableRewriter test on Windows\n\nPatch by stefania; reviewed by jmckenzie for CASSANDRA-8962\n","date":"2015-04-10 04:05:45","modifiedFileCount":"2","status":"M","submitter":"Stefania Alborghetti"},{"authorTime":"2015-05-17 20:50:44","codes":[{"authorDate":"2015-05-17 20:50:44","commitOrder":8,"curCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n             ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n            cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n            assertEquals(files, sstables.size());\n        }\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - s.bytesOnDisk() + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        SSTableDeletingTask.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","date":"2015-05-17 20:50:44","endLine":301,"groupId":"22829","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testNumberOfFilesAndSizes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/d3/9da611e93a6284cc24af66de42d9b4b1a2c539.src","preCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n        }\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - s.bytesOnDisk() + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        SSTableDeletingTask.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":252,"status":"M"},{"authorDate":"2015-05-17 20:50:44","commitOrder":8,"curCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n             ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n            cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        }\n\n        SSTableDeletingTask.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        SSTableDeletingTask.waitForDeletions();\n        validateCFS(cfs);\n    }\n","date":"2015-05-17 20:50:44","endLine":546,"groupId":"22837","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testNumberOfFiles_truncate","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/d3/9da611e93a6284cc24af66de42d9b4b1a2c539.src","preCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n        SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n        rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n        }\n        catch (Throwable t)\n        {\n            rewriter.abort();\n            throw t;\n        }\n\n        List<SSTableReader> sstables = rewriter.finish();\n        cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        SSTableDeletingTask.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        SSTableDeletingTask.waitForDeletions();\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":507,"status":"M"}],"commitId":"75a34879bb4aa01a6a65858c303f2a058af512a4","commitMessage":"@@@Merge branch 'cassandra-2.2' into trunk\n\nConflicts:\n\tCHANGES.txt\n","date":"2015-05-17 20:50:44","modifiedFileCount":"52","status":"M","submitter":"Benedict Elliott Smith"},{"authorTime":"2015-05-22 16:44:46","codes":[{"authorDate":"2015-05-22 16:44:46","commitOrder":9,"curCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false);)\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - s.bytesOnDisk() + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        SSTableDeletingTask.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","date":"2015-05-22 16:44:46","endLine":304,"groupId":"13403","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testNumberOfFilesAndSizes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/fa/91d00527f3607435c9967b40b65baf4b544403.src","preCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n             ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n            cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n            assertEquals(files, sstables.size());\n        }\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - s.bytesOnDisk() + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        SSTableDeletingTask.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":256,"status":"M"},{"authorDate":"2015-05-22 16:44:46","commitOrder":9,"curCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false);)\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        SSTableDeletingTask.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        SSTableDeletingTask.waitForDeletions();\n        validateCFS(cfs);\n    }\n","date":"2015-05-22 16:44:46","endLine":547,"groupId":"13409","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testNumberOfFiles_truncate","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/fa/91d00527f3607435c9967b40b65baf4b544403.src","preCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        cfs.truncateBlocking();\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (SSTableRewriter rewriter = new SSTableRewriter(cfs, compacting, 1000, false);\n             ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n            cfs.getDataTracker().markCompactedSSTablesReplaced(compacting, sstables, OperationType.COMPACTION);\n        }\n\n        SSTableDeletingTask.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        SSTableDeletingTask.waitForDeletions();\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":508,"status":"M"}],"commitId":"d96a02a12e20bc2e0b096904ebe7a1d68532d2f3","commitMessage":"@@@Merge branch 'cassandra-2.2' into trunk\n","date":"2015-05-22 16:44:46","modifiedFileCount":"48","status":"M","submitter":"Benedict Elliott Smith"},{"authorTime":"2015-05-22 16:44:46","codes":[{"authorDate":"2015-05-27 03:23:21","commitOrder":10,"curCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false);)\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        SSTableDeletingTask.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","date":"2015-05-27 03:23:21","endLine":305,"groupId":"13403","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"testNumberOfFilesAndSizes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/26/5bb6a5eb0a155324c79aeae32d779c92334aba.src","preCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false);)\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - s.bytesOnDisk() + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        SSTableDeletingTask.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":256,"status":"M"},{"authorDate":"2015-05-22 16:44:46","commitOrder":10,"curCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false);)\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        SSTableDeletingTask.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        SSTableDeletingTask.waitForDeletions();\n        validateCFS(cfs);\n    }\n","date":"2015-05-22 16:44:46","endLine":547,"groupId":"13409","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testNumberOfFiles_truncate","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/fa/91d00527f3607435c9967b40b65baf4b544403.src","preCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false);)\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        SSTableDeletingTask.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        SSTableDeletingTask.waitForDeletions();\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":508,"status":"N"}],"commitId":"3123e88e540b88767f1a2a62d4f649920b9af82c","commitMessage":"@@@Merge branch 'cassandra-2.2' into trunk\n","date":"2015-05-27 03:23:21","modifiedFileCount":"10","status":"M","submitter":"Tyler Hobbs"},{"authorTime":"2015-05-29 15:58:49","codes":[{"authorDate":"2015-05-29 15:58:49","commitOrder":11,"curCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        SSTableDeletingTask.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","date":"2015-05-29 15:58:49","endLine":305,"groupId":"13197","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"testNumberOfFilesAndSizes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/9e/1cb91d579923f2a39acc23d7e542e27af8aca4.src","preCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false);)\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        SSTableDeletingTask.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":257,"status":"M"},{"authorDate":"2015-05-29 15:58:49","commitOrder":11,"curCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        SSTableDeletingTask.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        SSTableDeletingTask.waitForDeletions();\n        validateCFS(cfs);\n    }\n","date":"2015-05-29 15:58:49","endLine":544,"groupId":"5132","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"testNumberOfFiles_truncate","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/9e/1cb91d579923f2a39acc23d7e542e27af8aca4.src","preCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n        SSTableRewriter.overrideOpenInterval(10000000);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false);)\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        SSTableDeletingTask.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        SSTableDeletingTask.waitForDeletions();\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":506,"status":"M"}],"commitId":"601944569cbe8f8aed5f1e3448828586ba3bc936","commitMessage":"@@@Merge branch 'cassandra-2.2' into trunk\n","date":"2015-05-29 15:58:49","modifiedFileCount":"4","status":"M","submitter":"Marcus Eriksson"},{"authorTime":"2014-09-02 00:54:46","codes":[{"authorDate":"2014-09-02 00:54:46","commitOrder":12,"curCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        SSTableDeletingTask.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","date":"2015-06-30 18:47:01","endLine":344,"groupId":"13197","id":21,"instanceNumber":1,"isCurCommit":0,"methodName":"testNumberOfFilesAndSizes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/9e/4c93910c03a2c8d186a765ff8a9c72c3705cc3.src","preCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        SSTableDeletingTask.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":294,"status":"M"},{"authorDate":"2014-09-02 00:54:46","commitOrder":12,"curCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        SSTableDeletingTask.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        SSTableDeletingTask.waitForDeletions();\n        validateCFS(cfs);\n    }\n","date":"2015-06-30 18:47:01","endLine":596,"groupId":"13197","id":22,"instanceNumber":2,"isCurCommit":0,"methodName":"testNumberOfFiles_truncate","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/9e/4c93910c03a2c8d186a765ff8a9c72c3705cc3.src","preCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(scanner.hasNext())\n            {\n                rewriter.append(new LazilyCompactedRow(controller, Arrays.asList(scanner.next())));\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        SSTableDeletingTask.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        SSTableDeletingTask.waitForDeletions();\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":557,"status":"M"}],"commitId":"a991b64811f4d6adb6c7b31c0df52288eb06cf19","commitMessage":"@@@Storage engine refactor.  a.k.a CASSANDRA-8099\n\nInitial patch.  see ticket for details\n","date":"2015-06-30 18:47:01","modifiedFileCount":"374","status":"M","submitter":"Sylvain Lebresne"},{"authorTime":"2015-07-06 19:14:50","codes":[{"authorDate":"2014-09-02 00:54:46","commitOrder":13,"curCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        SSTableDeletingTask.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","date":"2015-06-30 18:47:01","endLine":344,"groupId":"13197","id":23,"instanceNumber":1,"isCurCommit":0,"methodName":"testNumberOfFilesAndSizes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/9e/4c93910c03a2c8d186a765ff8a9c72c3705cc3.src","preCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        SSTableDeletingTask.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":294,"status":"N"},{"authorDate":"2015-07-06 19:14:50","commitOrder":13,"curCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        SSTableDeletingTask.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","date":"2015-07-06 19:14:50","endLine":604,"groupId":"13197","id":24,"instanceNumber":2,"isCurCommit":0,"methodName":"testNumberOfFiles_truncate","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/84/568c32a4fa0d3d8b4ff28f81d552154ce84388.src","preCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        SSTableDeletingTask.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        cfs.truncateBlocking();\n        SSTableDeletingTask.waitForDeletions();\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":567,"status":"M"}],"commitId":"2d7fec94f13dd8a7d2b14179fc89202413bdcbf7","commitMessage":"@@@Merge branch 'cassandra-2.2' into trunk\n\nConflicts:\n\ttest/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java\n","date":"2015-07-06 19:14:50","modifiedFileCount":"6","status":"M","submitter":"Benedict Elliott Smith"},{"authorTime":"2015-04-27 14:38:53","codes":[{"authorDate":"2015-04-27 14:38:53","commitOrder":14,"curCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n\n        TransactionLogs.waitForDeletions();\n\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        TransactionLogs.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","date":"2015-07-24 21:41:51","endLine":344,"groupId":"25776","id":25,"instanceNumber":1,"isCurCommit":0,"methodName":"testNumberOfFilesAndSizes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/0e/533c27e3a8602e74cbd3971eba0ec2dda0ca4b.src","preCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        SSTableDeletingTask.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":291,"status":"M"},{"authorDate":"2015-04-27 14:38:53","commitOrder":14,"curCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        TransactionLogs.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","date":"2015-07-24 21:41:51","endLine":613,"groupId":"25776","id":26,"instanceNumber":2,"isCurCommit":0,"methodName":"testNumberOfFiles_truncate","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/0e/533c27e3a8602e74cbd3971eba0ec2dda0ca4b.src","preCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        SSTableDeletingTask.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list(), 0, 0);\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":576,"status":"M"}],"commitId":"b09e60f72bb2f37235d9e9190c25db36371b3c18","commitMessage":"@@@Introduce safer durable sstable membership management\n(and simplify cleanup of compaction leftovers)\n\nInstead of using temporary files and system tables. \nthis patch introduces a simple transaction log for sstable\nmembership edits that can be committed/aborted atomically\nand simply replayed on startup.\n\npatch by stefania; reviewed by benedict for CASSANDRA-7066\n","date":"2015-07-24 21:41:51","modifiedFileCount":"75","status":"M","submitter":"Stefania Alborghetti"},{"authorTime":"2015-06-28 21:49:09","codes":[{"authorDate":"2015-06-28 21:49:09","commitOrder":15,"curCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n\n        TransactionLogs.waitForDeletions();\n\n        long sum = 0;\n        for (SSTableReader x : cfs.getLiveSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getLiveSSTables().size());\n        TransactionLogs.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","date":"2015-07-28 17:28:31","endLine":346,"groupId":"25776","id":27,"instanceNumber":1,"isCurCommit":0,"methodName":"testNumberOfFilesAndSizes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/cb/07d371630737e3fcf4c107b1e03696316428c5.src","preCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n\n        TransactionLogs.waitForDeletions();\n\n        long sum = 0;\n        for (SSTableReader x : cfs.getSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getSSTables().size());\n        TransactionLogs.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":293,"status":"M"},{"authorDate":"2015-06-28 21:49:09","commitOrder":15,"curCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        TransactionLogs.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","date":"2015-07-28 17:28:31","endLine":615,"groupId":"25776","id":28,"instanceNumber":2,"isCurCommit":0,"methodName":"testNumberOfFiles_truncate","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/cb/07d371630737e3fcf4c107b1e03696316428c5.src","preCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        TransactionLogs.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":578,"status":"M"}],"commitId":"ad8cad7c4d05fd5dea68fb274c81a102533ebe36","commitMessage":"@@@Make choice of SSTableReader types explicit\n\nAll accessors of a collection of SSTableReader must now\nspecify whether they desire the LIVE or CANONICAL set.  so\nthat no internal clients are accidentally exposed to a partial\nsstable they are not capable of safely handling.\n\npatch by benedict; reviewed by marcus for CASSANDRA-9699\n","date":"2015-07-28 17:28:31","modifiedFileCount":"60","status":"M","submitter":"Benedict Elliott Smith"},{"authorTime":"2015-08-17 17:20:15","codes":[{"authorDate":"2015-08-17 17:20:15","commitOrder":16,"curCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n\n        TransactionLog.waitForDeletions();\n\n        long sum = 0;\n        for (SSTableReader x : cfs.getLiveSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getLiveSSTables().size());\n        TransactionLog.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","date":"2015-08-17 17:20:15","endLine":346,"groupId":"25776","id":29,"instanceNumber":1,"isCurCommit":0,"methodName":"testNumberOfFilesAndSizes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/d9/516cb1f5c26f7cf4e9f37240eb4cc347f5d060.src","preCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n\n        TransactionLogs.waitForDeletions();\n\n        long sum = 0;\n        for (SSTableReader x : cfs.getLiveSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getLiveSSTables().size());\n        TransactionLogs.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":293,"status":"M"},{"authorDate":"2015-08-17 17:20:15","commitOrder":16,"curCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        TransactionLog.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","date":"2015-08-17 17:20:15","endLine":615,"groupId":"25776","id":30,"instanceNumber":2,"isCurCommit":0,"methodName":"testNumberOfFiles_truncate","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/d9/516cb1f5c26f7cf4e9f37240eb4cc347f5d060.src","preCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        TransactionLogs.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":578,"status":"M"}],"commitId":"13ec86ce9e4fe6616bff85aa4730686212a347c1","commitMessage":"@@@Merge branch 'cassandra-3.0' into trunk\n","date":"2015-08-17 17:20:15","modifiedFileCount":"41","status":"M","submitter":"Benedict Elliott Smith"},{"authorTime":"2015-09-08 18:53:45","codes":[{"authorDate":"2015-09-08 18:53:45","commitOrder":17,"curCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n\n        LifecycleTransaction.waitForDeletions();\n\n        long sum = 0;\n        for (SSTableReader x : cfs.getLiveSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getLiveSSTables().size());\n        LifecycleTransaction.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","date":"2015-09-08 18:53:45","endLine":345,"groupId":"25776","id":31,"instanceNumber":1,"isCurCommit":0,"methodName":"testNumberOfFilesAndSizes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/94/2c7f95e50ab58a780dbe175044c7cd93923d63.src","preCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n\n        TransactionLog.waitForDeletions();\n\n        long sum = 0;\n        for (SSTableReader x : cfs.getLiveSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getLiveSSTables().size());\n        TransactionLog.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":292,"status":"M"},{"authorDate":"2015-09-08 18:53:45","commitOrder":17,"curCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        LifecycleTransaction.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","date":"2015-09-08 18:53:45","endLine":614,"groupId":"25776","id":32,"instanceNumber":2,"isCurCommit":0,"methodName":"testNumberOfFiles_truncate","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/94/2c7f95e50ab58a780dbe175044c7cd93923d63.src","preCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        TransactionLog.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":577,"status":"M"}],"commitId":"322e21ec4ec4200d155ea5eb8e7545c446c3c726","commitMessage":"@@@Merge branch 'cassandra-3.0' into trunk\n","date":"2015-09-08 18:53:45","modifiedFileCount":"22","status":"M","submitter":"Benedict Elliott Smith"},{"authorTime":"2015-09-30 17:41:30","codes":[{"authorDate":"2015-09-30 17:41:30","commitOrder":18,"curCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(txn, 1000, false, 10000000, false);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n\n        LifecycleTransaction.waitForDeletions();\n\n        long sum = 0;\n        for (SSTableReader x : cfs.getLiveSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getLiveSSTables().size());\n        LifecycleTransaction.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","date":"2015-10-08 02:04:02","endLine":345,"groupId":"22111","id":33,"instanceNumber":1,"isCurCommit":0,"methodName":"testNumberOfFilesAndSizes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/09/3bffd5ec59a3ca2ceae6bf5cb877cda1275117.src","preCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n\n        LifecycleTransaction.waitForDeletions();\n\n        long sum = 0;\n        for (SSTableReader x : cfs.getLiveSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getLiveSSTables().size());\n        LifecycleTransaction.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":292,"status":"M"},{"authorDate":"2015-09-30 17:41:30","commitOrder":18,"curCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(txn, 1000, false, 10000000, false);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        LifecycleTransaction.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","date":"2015-10-08 02:04:02","endLine":614,"groupId":"22111","id":34,"instanceNumber":2,"isCurCommit":0,"methodName":"testNumberOfFiles_truncate","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/09/3bffd5ec59a3ca2ceae6bf5cb877cda1275117.src","preCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(cfs, txn, 1000, false, 10000000);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        LifecycleTransaction.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":577,"status":"M"}],"commitId":"127f7c5844f938649de0f3a5dec630b3c5cda256","commitMessage":"@@@Clean resource usage warnings.\n\nPatch by Branimir Lambov; reviewed by tjake for CASSANDRA-10385\n","date":"2015-10-08 02:04:02","modifiedFileCount":"40","status":"M","submitter":"Branimir Lambov"},{"authorTime":"2016-02-10 20:47:00","codes":[{"authorDate":"2016-02-10 20:47:00","commitOrder":19,"curCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(txn, 1000, 10000000, false);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n\n        LifecycleTransaction.waitForDeletions();\n\n        long sum = 0;\n        for (SSTableReader x : cfs.getLiveSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getLiveSSTables().size());\n        LifecycleTransaction.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","date":"2016-02-16 14:22:40","endLine":255,"groupId":"22111","id":35,"instanceNumber":1,"isCurCommit":0,"methodName":"testNumberOfFilesAndSizes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/7d/bc45e6f9db88ae99f92eba335cb6ef1857887d.src","preCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(txn, 1000, false, 10000000, false);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n\n        LifecycleTransaction.waitForDeletions();\n\n        long sum = 0;\n        for (SSTableReader x : cfs.getLiveSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getLiveSSTables().size());\n        LifecycleTransaction.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":202,"status":"M"},{"authorDate":"2016-02-10 20:47:00","commitOrder":19,"curCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(txn, 1000, 10000000, false);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        LifecycleTransaction.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","date":"2016-02-16 14:22:40","endLine":524,"groupId":"22111","id":36,"instanceNumber":2,"isCurCommit":0,"methodName":"testNumberOfFiles_truncate","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/7d/bc45e6f9db88ae99f92eba335cb6ef1857887d.src","preCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(txn, 1000, false, 10000000, false);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        LifecycleTransaction.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":487,"status":"M"}],"commitId":"f9a1a80af181e568240bb8a005cd53af8de00648","commitMessage":"@@@Remove duplicate offline compaction tracking\n\nPatch by marcuse; reviewed by Stefania for CASSANDRA-11148\n","date":"2016-02-16 14:22:40","modifiedFileCount":"12","status":"M","submitter":"Marcus Eriksson"},{"authorTime":"2020-12-15 04:42:50","codes":[{"authorDate":"2020-12-15 04:42:50","commitOrder":20,"curCode":"    public void testNumberOfFilesAndSizes()\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(txn, 1000, 10000000, false, true);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n\n        LifecycleTransaction.waitForDeletions();\n\n        long sum = 0;\n        for (SSTableReader x : cfs.getLiveSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getLiveSSTables().size());\n        LifecycleTransaction.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","date":"2021-01-21 06:22:42","endLine":249,"groupId":"101987","id":37,"instanceNumber":1,"isCurCommit":1,"methodName":"testNumberOfFilesAndSizes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/18/95653ccd0b34f97ab70c32bc8d83728b4ddc54.src","preCode":"    public void testNumberOfFilesAndSizes() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        long startStorageMetricsLoad = StorageMetrics.load.getCount();\n        long sBytesOnDisk = s.bytesOnDisk();\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(txn, 1000, 10000000, false);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                    assertEquals(s.bytesOnDisk(), cfs.metric.liveDiskSpaceUsed.getCount());\n                    assertEquals(s.bytesOnDisk(), cfs.metric.totalDiskSpaceUsed.getCount());\n\n                }\n            }\n            sstables = rewriter.finish();\n        }\n\n        LifecycleTransaction.waitForDeletions();\n\n        long sum = 0;\n        for (SSTableReader x : cfs.getLiveSSTables())\n            sum += x.bytesOnDisk();\n        assertEquals(sum, cfs.metric.liveDiskSpaceUsed.getCount());\n        assertEquals(startStorageMetricsLoad - sBytesOnDisk + sum, StorageMetrics.load.getCount());\n        assertEquals(files, sstables.size());\n        assertEquals(files, cfs.getLiveSSTables().size());\n        LifecycleTransaction.waitForDeletions();\n\n        \r\n        assertEquals(sum, cfs.metric.totalDiskSpaceUsed.getCount());\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":196,"status":"M"},{"authorDate":"2020-12-15 04:42:50","commitOrder":20,"curCode":"    public void testNumberOfFiles_truncate()\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(txn, 1000, 10000000, false, true);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                }\n            }\n\n            rewriter.finish();\n        }\n\n        LifecycleTransaction.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","date":"2021-01-21 06:22:42","endLine":518,"groupId":"101987","id":38,"instanceNumber":2,"isCurCommit":1,"methodName":"testNumberOfFiles_truncate","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-cassandra-10-0.7/blobInfo/CC_OUT/blobs/18/95653ccd0b34f97ab70c32bc8d83728b4ddc54.src","preCode":"    public void testNumberOfFiles_truncate() throws Exception\n    {\n        Keyspace keyspace = Keyspace.open(KEYSPACE);\n        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CF);\n        truncate(cfs);\n        cfs.disableAutoCompaction();\n\n        SSTableReader s = writeFile(cfs, 1000);\n        cfs.addSSTable(s);\n        Set<SSTableReader> compacting = Sets.newHashSet(s);\n\n        List<SSTableReader> sstables;\n        int files = 1;\n        try (ISSTableScanner scanner = s.getScanner();\n             CompactionController controller = new CompactionController(cfs, compacting, 0);\n             LifecycleTransaction txn = cfs.getTracker().tryModify(compacting, OperationType.UNKNOWN);\n             SSTableRewriter rewriter = new SSTableRewriter(txn, 1000, 10000000, false);\n             CompactionIterator ci = new CompactionIterator(OperationType.COMPACTION, Collections.singletonList(scanner), controller, FBUtilities.nowInSeconds(), UUIDGen.getTimeUUID()))\n        {\n            rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n            while(ci.hasNext())\n            {\n                rewriter.append(ci.next());\n                if (rewriter.currentWriter().getOnDiskFilePointer() > 25000000)\n                {\n                    rewriter.switchWriter(getWriter(cfs, s.descriptor.directory, txn));\n                    files++;\n                    assertEquals(cfs.getLiveSSTables().size(), files); \r\n                }\n            }\n\n            sstables = rewriter.finish();\n        }\n\n        LifecycleTransaction.waitForDeletions();\n        assertFileCounts(s.descriptor.directory.list());\n        validateCFS(cfs);\n    }\n","realPath":"test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java","repoName":"cassandra","snippetEndLine":0,"snippetStartLine":0,"startLine":481,"status":"M"}],"commitId":"5e8f7f591dfec5a61d8eb2e9e977ec29f3a2bbe4","commitMessage":"@@@Release StreamingTombstoneHistogramBuilder spool when switching writers\n\n patch by Adam Holmberg; reviewed by Berenguer Blasi.  Mick Semb Wever for CASSANDRA-14834\n","date":"2021-01-21 06:22:42","modifiedFileCount":"10","status":"M","submitter":"Adam Holmberg"}]
