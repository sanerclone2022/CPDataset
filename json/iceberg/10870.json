[{"authorTime":"2020-06-20 09:02:23","codes":[{"authorDate":"2020-06-30 08:56:05","commitOrder":2,"curCode":"  public InputPartition[] planInputPartitions() {\n    String tableSchemaString = SchemaParser.toJson(table.schema());\n    String expectedSchemaString = SchemaParser.toJson(expectedSchema);\n    String nameMappingString = table.properties().get(TableProperties.DEFAULT_NAME_MAPPING);\n\n    List<CombinedScanTask> scanTasks = tasks();\n    InputPartition[] readTasks = new InputPartition[scanTasks.size()];\n    for (int i = 0; i < scanTasks.size(); i++) {\n      readTasks[i] = new ReadTask(\n          scanTasks.get(i), tableSchemaString, expectedSchemaString, nameMappingString, io, encryptionManager,\n          caseSensitive, localityPreferred);\n    }\n\n    return readTasks;\n  }\n","date":"2020-06-30 08:56:05","endLine":147,"groupId":"909","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"planInputPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/02/6f6baccce41dfb5a47a0ca3d229038dc7c4478.src","preCode":"  public InputPartition[] planInputPartitions() {\n    String tableSchemaString = SchemaParser.toJson(table.schema());\n    String expectedSchemaString = SchemaParser.toJson(expectedSchema);\n    String nameMappingString = table.properties().get(TableProperties.DEFAULT_NAME_MAPPING);\n\n    List<CombinedScanTask> scanTasks = tasks();\n    InputPartition[] readTasks = new InputPartition[scanTasks.size()];\n    for (int i = 0; i < scanTasks.size(); i++) {\n      readTasks[i] = new ReadTask(\n          scanTasks.get(i), tableSchemaString, expectedSchemaString, nameMappingString, io, encryptionManager,\n          caseSensitive, localityPreferred);\n    }\n\n    return readTasks;\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":133,"status":"B"},{"authorDate":"2020-06-20 09:02:23","commitOrder":2,"curCode":"  public List<InputPartition<InternalRow>> planInputPartitions() {\n    String tableSchemaString = SchemaParser.toJson(table.schema());\n    String expectedSchemaString = SchemaParser.toJson(lazySchema());\n    String nameMappingString = table.properties().get(DEFAULT_NAME_MAPPING);\n\n    List<InputPartition<InternalRow>> readTasks = Lists.newArrayList();\n    for (CombinedScanTask task : tasks()) {\n      readTasks.add(new ReadTask<>(\n          task, tableSchemaString, expectedSchemaString, nameMappingString, io, encryptionManager, caseSensitive,\n          localityPreferred, InternalRowReaderFactory.INSTANCE));\n    }\n\n    return readTasks;\n  }\n","date":"2020-06-20 09:02:23","endLine":234,"groupId":"2572","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"planInputPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/51/859de75ac284ad0708719a77d5ded9611f72bb.src","preCode":"  public List<InputPartition<InternalRow>> planInputPartitions() {\n    String tableSchemaString = SchemaParser.toJson(table.schema());\n    String expectedSchemaString = SchemaParser.toJson(lazySchema());\n    String nameMappingString = table.properties().get(DEFAULT_NAME_MAPPING);\n\n    List<InputPartition<InternalRow>> readTasks = Lists.newArrayList();\n    for (CombinedScanTask task : tasks()) {\n      readTasks.add(new ReadTask<>(\n          task, tableSchemaString, expectedSchemaString, nameMappingString, io, encryptionManager, caseSensitive,\n          localityPreferred, InternalRowReaderFactory.INSTANCE));\n    }\n\n    return readTasks;\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":221,"status":"NB"}],"commitId":"51c930e33867e54b3d7e0159b11d6b9c4bc81f1a","commitMessage":"@@@Spark: Add Spark 3 data source classes (#1124)\n\n","date":"2020-06-30 08:56:05","modifiedFileCount":"7","status":"M","submitter":"Ryan Blue"},{"authorTime":"2021-04-20 14:32:55","codes":[{"authorDate":"2021-04-20 14:32:55","commitOrder":3,"curCode":"  public InputPartition[] planInputPartitions() {\n    String expectedSchemaString = SchemaParser.toJson(expectedSchema);\n\n    \r\n    Broadcast<Table> tableBroadcast = sparkContext.broadcast(SerializableTable.copyOf(table));\n\n    List<CombinedScanTask> scanTasks = tasks();\n    InputPartition[] readTasks = new InputPartition[scanTasks.size()];\n    for (int i = 0; i < scanTasks.size(); i++) {\n      readTasks[i] = new ReadTask(\n          scanTasks.get(i), tableBroadcast, expectedSchemaString,\n          caseSensitive, localityPreferred);\n    }\n\n    return readTasks;\n  }\n","date":"2021-04-20 14:32:55","endLine":135,"groupId":"909","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"planInputPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/19/99b568b9e0192a6c50080ddc3b338cd4d18e94.src","preCode":"  public InputPartition[] planInputPartitions() {\n    String tableSchemaString = SchemaParser.toJson(table.schema());\n    String expectedSchemaString = SchemaParser.toJson(expectedSchema);\n    String nameMappingString = table.properties().get(TableProperties.DEFAULT_NAME_MAPPING);\n\n    List<CombinedScanTask> scanTasks = tasks();\n    InputPartition[] readTasks = new InputPartition[scanTasks.size()];\n    for (int i = 0; i < scanTasks.size(); i++) {\n      readTasks[i] = new ReadTask(\n          scanTasks.get(i), tableSchemaString, expectedSchemaString, nameMappingString, io, encryptionManager,\n          caseSensitive, localityPreferred);\n    }\n\n    return readTasks;\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":120,"status":"M"},{"authorDate":"2021-04-20 14:32:55","commitOrder":3,"curCode":"  public List<InputPartition<InternalRow>> planInputPartitions() {\n    String expectedSchemaString = SchemaParser.toJson(lazySchema());\n\n    \r\n    Broadcast<Table> tableBroadcast = sparkContext.broadcast(SerializableTable.copyOf(table));\n\n    List<InputPartition<InternalRow>> readTasks = Lists.newArrayList();\n    for (CombinedScanTask task : tasks()) {\n      readTasks.add(new ReadTask<>(\n          task, tableBroadcast, expectedSchemaString, caseSensitive,\n          localityPreferred, InternalRowReaderFactory.INSTANCE));\n    }\n\n    return readTasks;\n  }\n","date":"2021-04-20 14:32:55","endLine":237,"groupId":"4204","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"planInputPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/45/a13f2762c4237e48d121f18144e5de3253e092.src","preCode":"  public List<InputPartition<InternalRow>> planInputPartitions() {\n    String tableSchemaString = SchemaParser.toJson(table.schema());\n    String expectedSchemaString = SchemaParser.toJson(lazySchema());\n    String nameMappingString = table.properties().get(DEFAULT_NAME_MAPPING);\n\n    List<InputPartition<InternalRow>> readTasks = Lists.newArrayList();\n    for (CombinedScanTask task : tasks()) {\n      readTasks.add(new ReadTask<>(\n          task, tableSchemaString, expectedSchemaString, nameMappingString, io, encryptionManager, caseSensitive,\n          localityPreferred, InternalRowReaderFactory.INSTANCE));\n    }\n\n    return readTasks;\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":223,"status":"M"}],"commitId":"a79de571860a290f6e96ac562d616c9c6be2071e","commitMessage":"@@@Spark: Pass Table to executors (#2362)\n\n","date":"2021-04-20 14:32:55","modifiedFileCount":"18","status":"M","submitter":"Anton Okolnychyi"},{"authorTime":"2021-07-13 01:24:19","codes":[{"authorDate":"2021-07-13 01:24:19","commitOrder":4,"curCode":"  public InputPartition[] planInputPartitions() {\n    String expectedSchemaString = SchemaParser.toJson(expectedSchema);\n\n    \r\n    Broadcast<Table> tableBroadcast = sparkContext.broadcast(SerializableTable.copyOf(table));\n\n    List<CombinedScanTask> scanTasks = tasks();\n    InputPartition[] readTasks = new InputPartition[scanTasks.size()];\n\n    Tasks.range(readTasks.length)\n        .stopOnFailure()\n        .executeWith(localityPreferred ? ThreadPools.getWorkerPool() : null)\n        .run(index -> readTasks[index] = new ReadTask(\n            scanTasks.get(index), tableBroadcast, expectedSchemaString,\n            caseSensitive, localityPreferred));\n\n    return readTasks;\n  }\n","date":"2021-07-13 01:24:19","endLine":146,"groupId":"10870","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"planInputPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/ba/f40341cece11f6d079533c9ece847d1a59a7c0.src","preCode":"  public InputPartition[] planInputPartitions() {\n    String expectedSchemaString = SchemaParser.toJson(expectedSchema);\n\n    \r\n    Broadcast<Table> tableBroadcast = sparkContext.broadcast(SerializableTable.copyOf(table));\n\n    List<CombinedScanTask> scanTasks = tasks();\n    InputPartition[] readTasks = new InputPartition[scanTasks.size()];\n    for (int i = 0; i < scanTasks.size(); i++) {\n      readTasks[i] = new ReadTask(\n          scanTasks.get(i), tableBroadcast, expectedSchemaString,\n          caseSensitive, localityPreferred);\n    }\n\n    return readTasks;\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":129,"status":"M"},{"authorDate":"2021-07-13 01:24:19","commitOrder":4,"curCode":"  public List<InputPartition<InternalRow>> planInputPartitions() {\n    String expectedSchemaString = SchemaParser.toJson(lazySchema());\n\n    \r\n    Broadcast<Table> tableBroadcast = sparkContext.broadcast(SerializableTable.copyOf(table));\n\n    List<CombinedScanTask> scanTasks = tasks();\n    InputPartition<InternalRow>[] readTasks = new InputPartition[scanTasks.size()];\n\n    Tasks.range(readTasks.length)\n        .stopOnFailure()\n        .executeWith(localityPreferred ? ThreadPools.getWorkerPool() : null)\n        .run(index -> readTasks[index] = new ReadTask<>(\n            scanTasks.get(index), tableBroadcast, expectedSchemaString, caseSensitive,\n            localityPreferred, InternalRowReaderFactory.INSTANCE));\n\n    return Arrays.asList(readTasks);\n  }\n","date":"2021-07-13 01:24:19","endLine":246,"groupId":"10870","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"planInputPartitions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/7a/9b73e82d4341b41ae3fb10ce9b7f1155fa3109.src","preCode":"  public List<InputPartition<InternalRow>> planInputPartitions() {\n    String expectedSchemaString = SchemaParser.toJson(lazySchema());\n\n    \r\n    Broadcast<Table> tableBroadcast = sparkContext.broadcast(SerializableTable.copyOf(table));\n\n    List<InputPartition<InternalRow>> readTasks = Lists.newArrayList();\n    for (CombinedScanTask task : tasks()) {\n      readTasks.add(new ReadTask<>(\n          task, tableBroadcast, expectedSchemaString, caseSensitive,\n          localityPreferred, InternalRowReaderFactory.INSTANCE));\n    }\n\n    return readTasks;\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":229,"status":"M"}],"commitId":"0bb89d024d73df5aca79368cc5c68d0e8d5aac15","commitMessage":"@@@Spark: Parallelize task init when fetching locality info (#2800)\n\n","date":"2021-07-13 01:24:19","modifiedFileCount":"3","status":"M","submitter":"jshmchenxi"}]
