[{"authorTime":"2020-06-20 09:02:23","codes":[{"authorDate":"2020-06-20 09:02:23","commitOrder":1,"curCode":"  public void testImportPartitionedTable() throws Exception {\n    File location = temp.newFolder(\"partitioned_table\");\n    spark.table(qualifiedTableName).write().mode(\"overwrite\").partitionBy(\"data\").format(\"parquet\")\n            .saveAsTable(\"test_partitioned_table\");\n    TableIdentifier source = spark.sessionState().sqlParser()\n            .parseTableIdentifier(\"test_partitioned_table\");\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Table table = tables.create(SparkSchemaUtil.schemaForTable(spark, qualifiedTableName),\n            SparkSchemaUtil.specForTable(spark, qualifiedTableName),\n            ImmutableMap.of(),\n            location.getCanonicalPath());\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n    long count = spark.read().format(\"iceberg\").load(location.toString()).count();\n    Assert.assertEquals(\"three values \", 3, count);\n  }\n","date":"2020-06-20 09:02:23","endLine":167,"groupId":"2705","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testImportPartitionedTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/37/f57d483364d35dd5ac100488e326ffc061552b.src","preCode":"  public void testImportPartitionedTable() throws Exception {\n    File location = temp.newFolder(\"partitioned_table\");\n    spark.table(qualifiedTableName).write().mode(\"overwrite\").partitionBy(\"data\").format(\"parquet\")\n            .saveAsTable(\"test_partitioned_table\");\n    TableIdentifier source = spark.sessionState().sqlParser()\n            .parseTableIdentifier(\"test_partitioned_table\");\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Table table = tables.create(SparkSchemaUtil.schemaForTable(spark, qualifiedTableName),\n            SparkSchemaUtil.specForTable(spark, qualifiedTableName),\n            ImmutableMap.of(),\n            location.getCanonicalPath());\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n    long count = spark.read().format(\"iceberg\").load(location.toString()).count();\n    Assert.assertEquals(\"three values \", 3, count);\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":152,"status":"B"},{"authorDate":"2020-06-20 09:02:23","commitOrder":1,"curCode":"  public void testImportUnpartitionedTable() throws Exception {\n    File location = temp.newFolder(\"unpartitioned_table\");\n    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n            .saveAsTable(\"test_unpartitioned_table\");\n    TableIdentifier source = spark.sessionState().sqlParser()\n            .parseTableIdentifier(\"test_unpartitioned_table\");\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Table table = tables.create(SparkSchemaUtil.schemaForTable(spark, qualifiedTableName),\n            SparkSchemaUtil.specForTable(spark, qualifiedTableName),\n            ImmutableMap.of(),\n            location.getCanonicalPath());\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n    long count = spark.read().format(\"iceberg\").load(location.toString()).count();\n    Assert.assertEquals(\"three values \", 3, count);\n  }\n","date":"2020-06-20 09:02:23","endLine":185,"groupId":"2705","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testImportUnpartitionedTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/37/f57d483364d35dd5ac100488e326ffc061552b.src","preCode":"  public void testImportUnpartitionedTable() throws Exception {\n    File location = temp.newFolder(\"unpartitioned_table\");\n    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n            .saveAsTable(\"test_unpartitioned_table\");\n    TableIdentifier source = spark.sessionState().sqlParser()\n            .parseTableIdentifier(\"test_unpartitioned_table\");\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Table table = tables.create(SparkSchemaUtil.schemaForTable(spark, qualifiedTableName),\n            SparkSchemaUtil.specForTable(spark, qualifiedTableName),\n            ImmutableMap.of(),\n            location.getCanonicalPath());\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n    long count = spark.read().format(\"iceberg\").load(location.toString()).count();\n    Assert.assertEquals(\"three values \", 3, count);\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":170,"status":"B"}],"commitId":"845de7bfe78bc8ba3480aad219fcef9f71477023","commitMessage":"@@@Spark: Move classes that depend on 2.x DSv2 to spark2 (#1122)\n\n","date":"2020-06-20 09:02:23","modifiedFileCount":"3","status":"B","submitter":"Ryan Blue"},{"authorTime":"2020-09-26 06:55:33","codes":[{"authorDate":"2020-09-26 06:55:33","commitOrder":2,"curCode":"    public void testImportPartitionedTable() throws Exception {\n      File location = temp.newFolder(\"partitioned_table\");\n      spark.table(QUALIFIED_TABLE_NAME).write().mode(\"overwrite\").partitionBy(\"data\").format(format.toString())\n          .saveAsTable(\"test_partitioned_table\");\n      TableIdentifier source = spark.sessionState().sqlParser()\n          .parseTableIdentifier(\"test_partitioned_table\");\n      HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n      Schema tableSchema = SparkSchemaUtil.schemaForTable(spark, QUALIFIED_TABLE_NAME);\n      Table table = tables.create(tableSchema,\n          SparkSchemaUtil.specForTable(spark, QUALIFIED_TABLE_NAME),\n          ImmutableMap.of(),\n          location.getCanonicalPath());\n      File stagingDir = temp.newFolder(\"staging-dir\");\n      SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n      long count = spark.read().format(\"iceberg\").load(location.toString()).count();\n      Assert.assertEquals(\"three values \", 3, count);\n    }\n","date":"2020-09-26 06:55:33","endLine":176,"groupId":"10950","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testImportPartitionedTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/fe/948a4ddaac357c6c86c8dc1f2ebd8ac85b57f8.src","preCode":"  public void testImportPartitionedTable() throws Exception {\n    File location = temp.newFolder(\"partitioned_table\");\n    spark.table(qualifiedTableName).write().mode(\"overwrite\").partitionBy(\"data\").format(\"parquet\")\n            .saveAsTable(\"test_partitioned_table\");\n    TableIdentifier source = spark.sessionState().sqlParser()\n            .parseTableIdentifier(\"test_partitioned_table\");\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Table table = tables.create(SparkSchemaUtil.schemaForTable(spark, qualifiedTableName),\n            SparkSchemaUtil.specForTable(spark, qualifiedTableName),\n            ImmutableMap.of(),\n            location.getCanonicalPath());\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n    long count = spark.read().format(\"iceberg\").load(location.toString()).count();\n    Assert.assertEquals(\"three values \", 3, count);\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":160,"status":"M"},{"authorDate":"2020-09-26 06:55:33","commitOrder":2,"curCode":"    public void testImportUnpartitionedTable() throws Exception {\n      File location = temp.newFolder(\"unpartitioned_table\");\n      spark.table(QUALIFIED_TABLE_NAME).write().mode(\"overwrite\").format(format.toString())\n          .saveAsTable(\"test_unpartitioned_table\");\n      TableIdentifier source = spark.sessionState().sqlParser()\n          .parseTableIdentifier(\"test_unpartitioned_table\");\n      HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n      Table table = tables.create(SparkSchemaUtil.schemaForTable(spark, QUALIFIED_TABLE_NAME),\n          SparkSchemaUtil.specForTable(spark, QUALIFIED_TABLE_NAME),\n          ImmutableMap.of(),\n          location.getCanonicalPath());\n      File stagingDir = temp.newFolder(\"staging-dir\");\n      SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n      long count = spark.read().format(\"iceberg\").load(location.toString()).count();\n      Assert.assertEquals(\"three values \", 3, count);\n    }\n","date":"2020-09-26 06:55:33","endLine":194,"groupId":"10950","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testImportUnpartitionedTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/fe/948a4ddaac357c6c86c8dc1f2ebd8ac85b57f8.src","preCode":"  public void testImportUnpartitionedTable() throws Exception {\n    File location = temp.newFolder(\"unpartitioned_table\");\n    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n            .saveAsTable(\"test_unpartitioned_table\");\n    TableIdentifier source = spark.sessionState().sqlParser()\n            .parseTableIdentifier(\"test_unpartitioned_table\");\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Table table = tables.create(SparkSchemaUtil.schemaForTable(spark, qualifiedTableName),\n            SparkSchemaUtil.specForTable(spark, qualifiedTableName),\n            ImmutableMap.of(),\n            location.getCanonicalPath());\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n    long count = spark.read().format(\"iceberg\").load(location.toString()).count();\n    Assert.assertEquals(\"three values \", 3, count);\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":179,"status":"M"}],"commitId":"c07b23b313e9992d16b8ea8a4eb89ed5a6b12985","commitMessage":"@@@Spark: Follow name mapping when importing ORC tables (#1399)\n\n","date":"2020-09-26 06:55:33","modifiedFileCount":"3","status":"M","submitter":"Edgar Rodriguez"}]
