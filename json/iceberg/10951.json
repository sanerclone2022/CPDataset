[{"authorTime":"2020-06-20 09:02:23","codes":[{"authorDate":"2020-06-20 09:02:23","commitOrder":1,"curCode":"  public void testImportWithNameMapping() throws Exception {\n    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n        .saveAsTable(\"original_table\");\n\n    \r\n    Schema filteredSchema = new Schema(\n        optional(1, \"data\", Types.StringType.get())\n    );\n\n    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n\n    TableIdentifier source = new TableIdentifier(\"original_table\");\n    Table table = catalog.createTable(\n        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"target_table\"),\n        filteredSchema,\n        SparkSchemaUtil.specForTable(spark, \"original_table\"));\n\n    table.updateProperties().set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping)).commit();\n\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n\n    \r\n    \r\n    List<String> actual = spark.read().format(\"iceberg\").load(DB_NAME + \".target_table\")\n        .select(\"data\")\n        .sort(\"data\")\n        .filter(\"data<'c'\")\n        .collectAsList()\n        .stream()\n        .map(r -> r.getString(0))\n        .collect(Collectors.toList());\n\n    List<SimpleRecord> expected = Lists.newArrayList(\n        new SimpleRecord(2, \"a\"),\n        new SimpleRecord(1, \"b\")\n    );\n\n    Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n  }\n","date":"2020-06-20 09:02:23","endLine":254,"groupId":"2019","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testImportWithNameMapping","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/37/f57d483364d35dd5ac100488e326ffc061552b.src","preCode":"  public void testImportWithNameMapping() throws Exception {\n    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n        .saveAsTable(\"original_table\");\n\n    \r\n    Schema filteredSchema = new Schema(\n        optional(1, \"data\", Types.StringType.get())\n    );\n\n    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n\n    TableIdentifier source = new TableIdentifier(\"original_table\");\n    Table table = catalog.createTable(\n        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"target_table\"),\n        filteredSchema,\n        SparkSchemaUtil.specForTable(spark, \"original_table\"));\n\n    table.updateProperties().set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping)).commit();\n\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n\n    \r\n    \r\n    List<String> actual = spark.read().format(\"iceberg\").load(DB_NAME + \".target_table\")\n        .select(\"data\")\n        .sort(\"data\")\n        .filter(\"data<'c'\")\n        .collectAsList()\n        .stream()\n        .map(r -> r.getString(0))\n        .collect(Collectors.toList());\n\n    List<SimpleRecord> expected = Lists.newArrayList(\n        new SimpleRecord(2, \"a\"),\n        new SimpleRecord(1, \"b\")\n    );\n\n    Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":215,"status":"B"},{"authorDate":"2020-06-20 09:02:23","commitOrder":1,"curCode":"  public void testImportWithNameMappingForVectorizedParquetReader() throws Exception {\n    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n        .saveAsTable(\"original_table\");\n\n    \r\n    Schema filteredSchema = new Schema(\n        optional(1, \"data\", Types.StringType.get())\n    );\n\n    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n\n    TableIdentifier source = new TableIdentifier(\"original_table\");\n    Table table = catalog.createTable(\n        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"target_table_for_vectorization\"),\n        filteredSchema,\n        SparkSchemaUtil.specForTable(spark, \"original_table\"));\n\n    table.updateProperties()\n        .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))\n        .set(PARQUET_VECTORIZATION_ENABLED, \"true\")\n        .commit();\n\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n\n    \r\n    \r\n    List<String> actual = spark.read().format(\"iceberg\")\n        .load(DB_NAME + \".target_table_for_vectorization\")\n        .select(\"data\")\n        .sort(\"data\")\n        .filter(\"data<'c'\")\n        .collectAsList()\n        .stream()\n        .map(r -> r.getString(0))\n        .collect(Collectors.toList());\n\n    List<SimpleRecord> expected = Lists.newArrayList(\n        new SimpleRecord(2, \"a\"),\n        new SimpleRecord(1, \"b\")\n    );\n\n    Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n  }\n","date":"2020-06-20 09:02:23","endLine":300,"groupId":"1984","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testImportWithNameMappingForVectorizedParquetReader","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/37/f57d483364d35dd5ac100488e326ffc061552b.src","preCode":"  public void testImportWithNameMappingForVectorizedParquetReader() throws Exception {\n    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n        .saveAsTable(\"original_table\");\n\n    \r\n    Schema filteredSchema = new Schema(\n        optional(1, \"data\", Types.StringType.get())\n    );\n\n    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n\n    TableIdentifier source = new TableIdentifier(\"original_table\");\n    Table table = catalog.createTable(\n        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"target_table_for_vectorization\"),\n        filteredSchema,\n        SparkSchemaUtil.specForTable(spark, \"original_table\"));\n\n    table.updateProperties()\n        .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))\n        .set(PARQUET_VECTORIZATION_ENABLED, \"true\")\n        .commit();\n\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n\n    \r\n    \r\n    List<String> actual = spark.read().format(\"iceberg\")\n        .load(DB_NAME + \".target_table_for_vectorization\")\n        .select(\"data\")\n        .sort(\"data\")\n        .filter(\"data<'c'\")\n        .collectAsList()\n        .stream()\n        .map(r -> r.getString(0))\n        .collect(Collectors.toList());\n\n    List<SimpleRecord> expected = Lists.newArrayList(\n        new SimpleRecord(2, \"a\"),\n        new SimpleRecord(1, \"b\")\n    );\n\n    Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":257,"status":"B"}],"commitId":"845de7bfe78bc8ba3480aad219fcef9f71477023","commitMessage":"@@@Spark: Move classes that depend on 2.x DSv2 to spark2 (#1122)\n\n","date":"2020-06-20 09:02:23","modifiedFileCount":"3","status":"B","submitter":"Ryan Blue"},{"authorTime":"2020-08-16 02:21:43","codes":[{"authorDate":"2020-08-16 02:21:43","commitOrder":2,"curCode":"  public void testImportWithNameMapping() throws Exception {\n    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n        .saveAsTable(\"original_table\");\n\n    \r\n    Schema filteredSchema = new Schema(\n        optional(1, \"data\", Types.StringType.get())\n    );\n\n    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n\n    TableIdentifier source = new TableIdentifier(\"original_table\");\n    Table table = catalog.createTable(\n        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"target_table\"),\n        filteredSchema,\n        SparkSchemaUtil.specForTable(spark, \"original_table\"));\n\n    table.updateProperties().set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping)).commit();\n\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n\n    \r\n    \r\n    List<String> actual = spark.read().format(\"iceberg\").load(DB_NAME + \".target_table\")\n        .select(\"data\")\n        .sort(\"data\")\n        .filter(\"data >= 'b'\")\n        .as(Encoders.STRING())\n        .collectAsList();\n\n    List<String> expected = Lists.newArrayList(\"b\", \"c\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","date":"2020-08-16 02:21:43","endLine":248,"groupId":"2019","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testImportWithNameMapping","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/3c/43c425008a585d66629861557e86bfd3c1826c.src","preCode":"  public void testImportWithNameMapping() throws Exception {\n    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n        .saveAsTable(\"original_table\");\n\n    \r\n    Schema filteredSchema = new Schema(\n        optional(1, \"data\", Types.StringType.get())\n    );\n\n    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n\n    TableIdentifier source = new TableIdentifier(\"original_table\");\n    Table table = catalog.createTable(\n        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"target_table\"),\n        filteredSchema,\n        SparkSchemaUtil.specForTable(spark, \"original_table\"));\n\n    table.updateProperties().set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping)).commit();\n\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n\n    \r\n    \r\n    List<String> actual = spark.read().format(\"iceberg\").load(DB_NAME + \".target_table\")\n        .select(\"data\")\n        .sort(\"data\")\n        .filter(\"data<'c'\")\n        .collectAsList()\n        .stream()\n        .map(r -> r.getString(0))\n        .collect(Collectors.toList());\n\n    List<SimpleRecord> expected = Lists.newArrayList(\n        new SimpleRecord(2, \"a\"),\n        new SimpleRecord(1, \"b\")\n    );\n\n    Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":214,"status":"M"},{"authorDate":"2020-08-16 02:21:43","commitOrder":2,"curCode":"  public void testImportWithNameMappingForVectorizedParquetReader() throws Exception {\n    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n        .saveAsTable(\"original_table\");\n\n    \r\n    Schema filteredSchema = new Schema(\n        optional(1, \"data\", Types.StringType.get())\n    );\n\n    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n\n    TableIdentifier source = new TableIdentifier(\"original_table\");\n    Table table = catalog.createTable(\n        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"target_table_for_vectorization\"),\n        filteredSchema,\n        SparkSchemaUtil.specForTable(spark, \"original_table\"));\n\n    table.updateProperties()\n        .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))\n        .set(PARQUET_VECTORIZATION_ENABLED, \"true\")\n        .commit();\n\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n\n    \r\n    \r\n    List<String> actual = spark.read().format(\"iceberg\")\n        .load(DB_NAME + \".target_table_for_vectorization\")\n        .select(\"data\")\n        .sort(\"data\")\n        .filter(\"data >= 'b'\")\n        .as(Encoders.STRING())\n        .collectAsList();\n\n    List<String> expected = Lists.newArrayList(\"b\", \"c\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","date":"2020-08-16 02:21:43","endLine":289,"groupId":"1984","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testImportWithNameMappingForVectorizedParquetReader","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/3c/43c425008a585d66629861557e86bfd3c1826c.src","preCode":"  public void testImportWithNameMappingForVectorizedParquetReader() throws Exception {\n    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n        .saveAsTable(\"original_table\");\n\n    \r\n    Schema filteredSchema = new Schema(\n        optional(1, \"data\", Types.StringType.get())\n    );\n\n    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n\n    TableIdentifier source = new TableIdentifier(\"original_table\");\n    Table table = catalog.createTable(\n        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"target_table_for_vectorization\"),\n        filteredSchema,\n        SparkSchemaUtil.specForTable(spark, \"original_table\"));\n\n    table.updateProperties()\n        .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))\n        .set(PARQUET_VECTORIZATION_ENABLED, \"true\")\n        .commit();\n\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n\n    \r\n    \r\n    List<String> actual = spark.read().format(\"iceberg\")\n        .load(DB_NAME + \".target_table_for_vectorization\")\n        .select(\"data\")\n        .sort(\"data\")\n        .filter(\"data<'c'\")\n        .collectAsList()\n        .stream()\n        .map(r -> r.getString(0))\n        .collect(Collectors.toList());\n\n    List<SimpleRecord> expected = Lists.newArrayList(\n        new SimpleRecord(2, \"a\"),\n        new SimpleRecord(1, \"b\")\n    );\n\n    Assert.assertEquals(expected.stream().map(SimpleRecord::getData).collect(Collectors.toList()), actual);\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":251,"status":"M"}],"commitId":"3990daae506822dc6b85c7b6d8461be28ca5362e","commitMessage":"@@@Spark: Follow name mapping while importing Parquet tables (#1335)\n\n","date":"2020-08-16 02:21:43","modifiedFileCount":"10","status":"M","submitter":"Anton Okolnychyi"},{"authorTime":"2020-09-26 06:55:33","codes":[{"authorDate":"2020-09-26 06:55:33","commitOrder":3,"curCode":"    public void testImportWithNameMapping() throws Exception {\n      spark.table(QUALIFIED_TABLE_NAME).write().mode(\"overwrite\").format(format.toString())\n          .saveAsTable(\"original_table\");\n\n      \r\n      Schema filteredSchema = new Schema(\n          optional(1, \"data\", Types.StringType.get())\n      );\n\n      NameMapping nameMapping = MappingUtil.create(filteredSchema);\n\n      String targetTableName = \"target_table_\" + format;\n      TableIdentifier source = new TableIdentifier(\"original_table\");\n      org.apache.iceberg.catalog.TableIdentifier targetTable =\n          org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, targetTableName);\n      Table table = catalog.createTable(\n          targetTable,\n          filteredSchema,\n          SparkSchemaUtil.specForTable(spark, \"original_table\"));\n\n      table.updateProperties().set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping)).commit();\n\n      File stagingDir = temp.newFolder(\"staging-dir\");\n      SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n\n      \r\n      \r\n      List<String> actual = spark.read().format(\"iceberg\").load(targetTable.toString())\n          .select(\"data\")\n          .sort(\"data\")\n          .filter(\"data >= 'b'\")\n          .as(Encoders.STRING())\n          .collectAsList();\n\n      List<String> expected = Lists.newArrayList(\"b\", \"c\");\n\n      Assert.assertEquals(expected, actual);\n    }\n","date":"2020-09-26 06:55:33","endLine":267,"groupId":"10951","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testImportWithNameMapping","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/fe/948a4ddaac357c6c86c8dc1f2ebd8ac85b57f8.src","preCode":"  public void testImportWithNameMapping() throws Exception {\n    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n        .saveAsTable(\"original_table\");\n\n    \r\n    Schema filteredSchema = new Schema(\n        optional(1, \"data\", Types.StringType.get())\n    );\n\n    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n\n    TableIdentifier source = new TableIdentifier(\"original_table\");\n    Table table = catalog.createTable(\n        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"target_table\"),\n        filteredSchema,\n        SparkSchemaUtil.specForTable(spark, \"original_table\"));\n\n    table.updateProperties().set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping)).commit();\n\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n\n    \r\n    \r\n    List<String> actual = spark.read().format(\"iceberg\").load(DB_NAME + \".target_table\")\n        .select(\"data\")\n        .sort(\"data\")\n        .filter(\"data >= 'b'\")\n        .as(Encoders.STRING())\n        .collectAsList();\n\n    List<String> expected = Lists.newArrayList(\"b\", \"c\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":230,"status":"M"},{"authorDate":"2020-09-26 06:55:33","commitOrder":3,"curCode":"    public void testImportWithNameMappingForVectorizedParquetReader() throws Exception {\n      Assume.assumeTrue(\"Applies only to parquet format.\",\n          FileFormat.PARQUET == format);\n      spark.table(QUALIFIED_TABLE_NAME).write().mode(\"overwrite\").format(format.toString())\n          .saveAsTable(\"original_table\");\n\n      \r\n      Schema filteredSchema = new Schema(\n          optional(1, \"data\", Types.StringType.get())\n      );\n\n      NameMapping nameMapping = MappingUtil.create(filteredSchema);\n\n      TableIdentifier source = new TableIdentifier(\"original_table\");\n      Table table = catalog.createTable(\n          org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"target_table_for_vectorization\"),\n          filteredSchema,\n          SparkSchemaUtil.specForTable(spark, \"original_table\"));\n\n      table.updateProperties()\n          .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))\n          .set(PARQUET_VECTORIZATION_ENABLED, \"true\")\n          .commit();\n\n      File stagingDir = temp.newFolder(\"staging-dir\");\n      SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n\n      \r\n      \r\n      List<String> actual = spark.read().format(\"iceberg\")\n          .load(DB_NAME + \".target_table_for_vectorization\")\n          .select(\"data\")\n          .sort(\"data\")\n          .filter(\"data >= 'b'\")\n          .as(Encoders.STRING())\n          .collectAsList();\n\n      List<String> expected = Lists.newArrayList(\"b\", \"c\");\n\n      Assert.assertEquals(expected, actual);\n    }\n","date":"2020-09-26 06:55:33","endLine":310,"groupId":"10951","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testImportWithNameMappingForVectorizedParquetReader","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/fe/948a4ddaac357c6c86c8dc1f2ebd8ac85b57f8.src","preCode":"  public void testImportWithNameMappingForVectorizedParquetReader() throws Exception {\n    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n        .saveAsTable(\"original_table\");\n\n    \r\n    Schema filteredSchema = new Schema(\n        optional(1, \"data\", Types.StringType.get())\n    );\n\n    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n\n    TableIdentifier source = new TableIdentifier(\"original_table\");\n    Table table = catalog.createTable(\n        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"target_table_for_vectorization\"),\n        filteredSchema,\n        SparkSchemaUtil.specForTable(spark, \"original_table\"));\n\n    table.updateProperties()\n        .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))\n        .set(PARQUET_VECTORIZATION_ENABLED, \"true\")\n        .commit();\n\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n\n    \r\n    \r\n    List<String> actual = spark.read().format(\"iceberg\")\n        .load(DB_NAME + \".target_table_for_vectorization\")\n        .select(\"data\")\n        .sort(\"data\")\n        .filter(\"data >= 'b'\")\n        .as(Encoders.STRING())\n        .collectAsList();\n\n    List<String> expected = Lists.newArrayList(\"b\", \"c\");\n\n    Assert.assertEquals(expected, actual);\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":270,"status":"M"}],"commitId":"c07b23b313e9992d16b8ea8a4eb89ed5a6b12985","commitMessage":"@@@Spark: Follow name mapping when importing ORC tables (#1399)\n\n","date":"2020-09-26 06:55:33","modifiedFileCount":"3","status":"M","submitter":"Edgar Rodriguez"}]
