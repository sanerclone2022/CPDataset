[{"authorTime":"2020-06-30 08:56:05","codes":[{"authorDate":"2020-06-30 08:56:05","commitOrder":2,"curCode":"  public Optional<DataSourceWriter> createWriter(String jobId, StructType dsStruct, SaveMode mode,\n                                                 DataSourceOptions options) {\n    Preconditions.checkArgument(mode == SaveMode.Append || mode == SaveMode.Overwrite,\n        \"Save mode %s is not supported\", mode);\n    Configuration conf = new Configuration(lazyBaseConf());\n    Table table = getTableAndResolveHadoopConfiguration(options, conf);\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsStruct);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema, checkNullability(options), checkOrdering(options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n    String appId = lazySparkSession().sparkContext().applicationId();\n    String wapId = lazySparkSession().conf().get(\"spark.wap.id\", null);\n    boolean replacePartitions = mode == SaveMode.Overwrite;\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    return Optional.of(new Writer(\n        table, io, encryptionManager, options, replacePartitions, appId, wapId, writeSchema, dsStruct));\n  }\n","date":"2020-06-30 08:56:05","endLine":108,"groupId":"3014","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"createWriter","params":"(StringjobId@StructTypedsStruct@SaveModemode@DataSourceOptionsoptions)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/58/9bbad2243546daba5ef44e71e881aefe37d4b9.src","preCode":"  public Optional<DataSourceWriter> createWriter(String jobId, StructType dsStruct, SaveMode mode,\n                                                 DataSourceOptions options) {\n    Preconditions.checkArgument(mode == SaveMode.Append || mode == SaveMode.Overwrite,\n        \"Save mode %s is not supported\", mode);\n    Configuration conf = new Configuration(lazyBaseConf());\n    Table table = getTableAndResolveHadoopConfiguration(options, conf);\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsStruct);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema, checkNullability(options), checkOrdering(options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n    String appId = lazySparkSession().sparkContext().applicationId();\n    String wapId = lazySparkSession().conf().get(\"spark.wap.id\", null);\n    boolean replacePartitions = mode == SaveMode.Overwrite;\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    return Optional.of(new Writer(\n        table, io, encryptionManager, options, replacePartitions, appId, wapId, writeSchema, dsStruct));\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":90,"status":"MB"},{"authorDate":"2020-06-30 08:56:05","commitOrder":2,"curCode":"  public StreamWriter createStreamWriter(String runId, StructType dsStruct,\n                                         OutputMode mode, DataSourceOptions options) {\n    Preconditions.checkArgument(\n        mode == OutputMode.Append() || mode == OutputMode.Complete(),\n        \"Output mode %s is not supported\", mode);\n    Configuration conf = new Configuration(lazyBaseConf());\n    Table table = getTableAndResolveHadoopConfiguration(options, conf);\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsStruct);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema, checkNullability(options), checkOrdering(options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n    \r\n    \r\n    String queryId = lazySparkSession().sparkContext().getLocalProperty(StreamExecution.QUERY_ID_KEY());\n    String appId = lazySparkSession().sparkContext().applicationId();\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    return new StreamingWriter(table, io, encryptionManager, options, queryId, mode, appId, writeSchema, dsStruct);\n  }\n","date":"2020-06-30 08:56:05","endLine":130,"groupId":"3015","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"createStreamWriter","params":"(StringrunId@StructTypedsStruct@OutputModemode@DataSourceOptionsoptions)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/58/9bbad2243546daba5ef44e71e881aefe37d4b9.src","preCode":"  public StreamWriter createStreamWriter(String runId, StructType dsStruct,\n                                         OutputMode mode, DataSourceOptions options) {\n    Preconditions.checkArgument(\n        mode == OutputMode.Append() || mode == OutputMode.Complete(),\n        \"Output mode %s is not supported\", mode);\n    Configuration conf = new Configuration(lazyBaseConf());\n    Table table = getTableAndResolveHadoopConfiguration(options, conf);\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsStruct);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema, checkNullability(options), checkOrdering(options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n    \r\n    \r\n    String queryId = lazySparkSession().sparkContext().getLocalProperty(StreamExecution.QUERY_ID_KEY());\n    String appId = lazySparkSession().sparkContext().applicationId();\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    return new StreamingWriter(table, io, encryptionManager, options, queryId, mode, appId, writeSchema, dsStruct);\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":111,"status":"MB"}],"commitId":"51c930e33867e54b3d7e0159b11d6b9c4bc81f1a","commitMessage":"@@@Spark: Add Spark 3 data source classes (#1124)\n\n","date":"2020-06-30 08:56:05","modifiedFileCount":"7","status":"M","submitter":"Ryan Blue"},{"authorTime":"2021-04-20 14:32:55","codes":[{"authorDate":"2021-04-20 14:32:55","commitOrder":3,"curCode":"  public Optional<DataSourceWriter> createWriter(String jobId, StructType dsStruct, SaveMode mode,\n                                                 DataSourceOptions options) {\n    Preconditions.checkArgument(mode == SaveMode.Append || mode == SaveMode.Overwrite,\n        \"Save mode %s is not supported\", mode);\n    Configuration conf = new Configuration(lazyBaseConf());\n    Table table = getTableAndResolveHadoopConfiguration(options, conf);\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsStruct);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema, checkNullability(options), checkOrdering(options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n    String appId = lazySparkSession().sparkContext().applicationId();\n    String wapId = lazySparkSession().conf().get(\"spark.wap.id\", null);\n    boolean replacePartitions = mode == SaveMode.Overwrite;\n\n    return Optional.of(new Writer(\n        lazySparkSession(), table, options, replacePartitions, appId, wapId, writeSchema, dsStruct));\n  }\n","date":"2021-04-20 14:32:55","endLine":95,"groupId":"3014","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"createWriter","params":"(StringjobId@StructTypedsStruct@SaveModemode@DataSourceOptionsoptions)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/9b/86e004c06d18e8c13ae4f04a97d09cbe7d85b6.src","preCode":"  public Optional<DataSourceWriter> createWriter(String jobId, StructType dsStruct, SaveMode mode,\n                                                 DataSourceOptions options) {\n    Preconditions.checkArgument(mode == SaveMode.Append || mode == SaveMode.Overwrite,\n        \"Save mode %s is not supported\", mode);\n    Configuration conf = new Configuration(lazyBaseConf());\n    Table table = getTableAndResolveHadoopConfiguration(options, conf);\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsStruct);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema, checkNullability(options), checkOrdering(options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n    String appId = lazySparkSession().sparkContext().applicationId();\n    String wapId = lazySparkSession().conf().get(\"spark.wap.id\", null);\n    boolean replacePartitions = mode == SaveMode.Overwrite;\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    return Optional.of(new Writer(\n        table, io, encryptionManager, options, replacePartitions, appId, wapId, writeSchema, dsStruct));\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":80,"status":"M"},{"authorDate":"2021-04-20 14:32:55","commitOrder":3,"curCode":"  public StreamWriter createStreamWriter(String runId, StructType dsStruct,\n                                         OutputMode mode, DataSourceOptions options) {\n    Preconditions.checkArgument(\n        mode == OutputMode.Append() || mode == OutputMode.Complete(),\n        \"Output mode %s is not supported\", mode);\n    Configuration conf = new Configuration(lazyBaseConf());\n    Table table = getTableAndResolveHadoopConfiguration(options, conf);\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsStruct);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema, checkNullability(options), checkOrdering(options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n    \r\n    \r\n    String queryId = lazySparkSession().sparkContext().getLocalProperty(StreamExecution.QUERY_ID_KEY());\n    String appId = lazySparkSession().sparkContext().applicationId();\n\n    return new StreamingWriter(lazySparkSession(), table, options, queryId, mode, appId, writeSchema, dsStruct);\n  }\n","date":"2021-04-20 14:32:55","endLine":114,"groupId":"3015","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"createStreamWriter","params":"(StringrunId@StructTypedsStruct@OutputModemode@DataSourceOptionsoptions)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/9b/86e004c06d18e8c13ae4f04a97d09cbe7d85b6.src","preCode":"  public StreamWriter createStreamWriter(String runId, StructType dsStruct,\n                                         OutputMode mode, DataSourceOptions options) {\n    Preconditions.checkArgument(\n        mode == OutputMode.Append() || mode == OutputMode.Complete(),\n        \"Output mode %s is not supported\", mode);\n    Configuration conf = new Configuration(lazyBaseConf());\n    Table table = getTableAndResolveHadoopConfiguration(options, conf);\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsStruct);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema, checkNullability(options), checkOrdering(options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n    \r\n    \r\n    String queryId = lazySparkSession().sparkContext().getLocalProperty(StreamExecution.QUERY_ID_KEY());\n    String appId = lazySparkSession().sparkContext().applicationId();\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    return new StreamingWriter(table, io, encryptionManager, options, queryId, mode, appId, writeSchema, dsStruct);\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":98,"status":"M"}],"commitId":"a79de571860a290f6e96ac562d616c9c6be2071e","commitMessage":"@@@Spark: Pass Table to executors (#2362)\n\n","date":"2021-04-20 14:32:55","modifiedFileCount":"18","status":"M","submitter":"Anton Okolnychyi"},{"authorTime":"2021-04-20 14:32:55","codes":[{"authorDate":"2021-07-16 01:06:44","commitOrder":4,"curCode":"  public Optional<DataSourceWriter> createWriter(String jobId, StructType dsStruct, SaveMode mode,\n                                                 DataSourceOptions options) {\n    Preconditions.checkArgument(mode == SaveMode.Append || mode == SaveMode.Overwrite,\n        \"Save mode %s is not supported\", mode);\n    Configuration conf = new Configuration(lazyBaseConf());\n    Table table = getTableAndResolveHadoopConfiguration(options, conf);\n    boolean handleTimestampWithoutZone =\n            SparkUtil.canHandleTimestampWithoutZone(options.asMap(), lazySparkSession().conf());\n    Preconditions.checkArgument(handleTimestampWithoutZone || !SparkUtil.hasTimestampWithoutZone(table.schema()),\n            SparkUtil.TIMESTAMP_WITHOUT_TIMEZONE_ERROR);\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsStruct);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema, checkNullability(options), checkOrdering(options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n    String appId = lazySparkSession().sparkContext().applicationId();\n    String wapId = lazySparkSession().conf().get(\"spark.wap.id\", null);\n    boolean replacePartitions = mode == SaveMode.Overwrite;\n\n    return Optional.of(new Writer(\n        lazySparkSession(), table, options, replacePartitions, appId, wapId, writeSchema, dsStruct));\n  }\n","date":"2021-07-16 01:06:44","endLine":99,"groupId":"10970","id":5,"instanceNumber":1,"isCurCommit":1,"methodName":"createWriter","params":"(StringjobId@StructTypedsStruct@SaveModemode@DataSourceOptionsoptions)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/02/2b81d4e9b166d47714e0991f2cb1b670ae50b1.src","preCode":"  public Optional<DataSourceWriter> createWriter(String jobId, StructType dsStruct, SaveMode mode,\n                                                 DataSourceOptions options) {\n    Preconditions.checkArgument(mode == SaveMode.Append || mode == SaveMode.Overwrite,\n        \"Save mode %s is not supported\", mode);\n    Configuration conf = new Configuration(lazyBaseConf());\n    Table table = getTableAndResolveHadoopConfiguration(options, conf);\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsStruct);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema, checkNullability(options), checkOrdering(options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n    String appId = lazySparkSession().sparkContext().applicationId();\n    String wapId = lazySparkSession().conf().get(\"spark.wap.id\", null);\n    boolean replacePartitions = mode == SaveMode.Overwrite;\n\n    return Optional.of(new Writer(\n        lazySparkSession(), table, options, replacePartitions, appId, wapId, writeSchema, dsStruct));\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":80,"status":"M"},{"authorDate":"2021-04-20 14:32:55","commitOrder":4,"curCode":"  public StreamWriter createStreamWriter(String runId, StructType dsStruct,\n                                         OutputMode mode, DataSourceOptions options) {\n    Preconditions.checkArgument(\n        mode == OutputMode.Append() || mode == OutputMode.Complete(),\n        \"Output mode %s is not supported\", mode);\n    Configuration conf = new Configuration(lazyBaseConf());\n    Table table = getTableAndResolveHadoopConfiguration(options, conf);\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsStruct);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema, checkNullability(options), checkOrdering(options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n    \r\n    \r\n    String queryId = lazySparkSession().sparkContext().getLocalProperty(StreamExecution.QUERY_ID_KEY());\n    String appId = lazySparkSession().sparkContext().applicationId();\n\n    return new StreamingWriter(lazySparkSession(), table, options, queryId, mode, appId, writeSchema, dsStruct);\n  }\n","date":"2021-04-20 14:32:55","endLine":114,"groupId":"10970","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"createStreamWriter","params":"(StringrunId@StructTypedsStruct@OutputModemode@DataSourceOptionsoptions)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/9b/86e004c06d18e8c13ae4f04a97d09cbe7d85b6.src","preCode":"  public StreamWriter createStreamWriter(String runId, StructType dsStruct,\n                                         OutputMode mode, DataSourceOptions options) {\n    Preconditions.checkArgument(\n        mode == OutputMode.Append() || mode == OutputMode.Complete(),\n        \"Output mode %s is not supported\", mode);\n    Configuration conf = new Configuration(lazyBaseConf());\n    Table table = getTableAndResolveHadoopConfiguration(options, conf);\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsStruct);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema, checkNullability(options), checkOrdering(options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n    \r\n    \r\n    String queryId = lazySparkSession().sparkContext().getLocalProperty(StreamExecution.QUERY_ID_KEY());\n    String appId = lazySparkSession().sparkContext().applicationId();\n\n    return new StreamingWriter(lazySparkSession(), table, options, queryId, mode, appId, writeSchema, dsStruct);\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":98,"status":"N"}],"commitId":"9a0d154b0ba5e6d10d79e30470295c91c89c1e09","commitMessage":"@@@Add support for reading/writing timestamps without timezone.  (#2757)\n\nPreviously Spark could not handle Iceberg tables which contained Timestamp.withoutTimeZone. New parameters are introduced to allow Timestamp without TimeZone to be treated as Timestamp with Timezone.  \n\nCo-authored-by: bkahloon <kahlonbakht@gmail.com>\nCo-authored-by: shardulm94 ","date":"2021-07-16 01:06:44","modifiedFileCount":"15","status":"M","submitter":"sshkvar"}]
