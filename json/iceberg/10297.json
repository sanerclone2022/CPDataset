[{"authorTime":"2020-04-11 23:51:33","codes":[{"authorDate":"2019-11-14 04:59:37","commitOrder":3,"curCode":"  public void testPartitionValueTypes() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(SUPPORTED_PRIMITIVES, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.fromInputFile(Files.localInput(avroData), 10))\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\").load(sourceLocation);\n\n    try {\n      for (String column : columnNames) {\n        String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n        File parent = temp.newFolder(desc);\n        File location = new File(parent, \"test\");\n        File dataFolder = new File(location, \"data\");\n        Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n        PartitionSpec spec = PartitionSpec.builderFor(SUPPORTED_PRIMITIVES).identity(column).build();\n\n        Table table = tables.create(SUPPORTED_PRIMITIVES, spec, location.toString());\n        table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n        sourceDF.write()\n            .format(\"iceberg\")\n            .mode(\"append\")\n            .save(location.toString());\n\n        List<Row> actual = spark.read()\n            .format(\"iceberg\")\n            .load(location.toString())\n            .collectAsList();\n\n        Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n        for (int i = 0; i < expected.size(); i += 1) {\n          TestHelpers.assertEqualsSafe(\n              SUPPORTED_PRIMITIVES.asStruct(), expected.get(i), actual.get(i));\n        }\n      }\n    } finally {\n      TestTables.clearTables();\n    }\n  }\n","date":"2019-11-14 04:59:37","endLine":220,"groupId":"1625","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testPartitionValueTypes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a3/7aed9e986e69e760534451696125ba51736034.src","preCode":"  public void testPartitionValueTypes() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(SUPPORTED_PRIMITIVES, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.fromInputFile(Files.localInput(avroData), 10))\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\").load(sourceLocation);\n\n    try {\n      for (String column : columnNames) {\n        String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n        File parent = temp.newFolder(desc);\n        File location = new File(parent, \"test\");\n        File dataFolder = new File(location, \"data\");\n        Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n        PartitionSpec spec = PartitionSpec.builderFor(SUPPORTED_PRIMITIVES).identity(column).build();\n\n        Table table = tables.create(SUPPORTED_PRIMITIVES, spec, location.toString());\n        table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n        sourceDF.write()\n            .format(\"iceberg\")\n            .mode(\"append\")\n            .save(location.toString());\n\n        List<Row> actual = spark.read()\n            .format(\"iceberg\")\n            .load(location.toString())\n            .collectAsList();\n\n        Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n        for (int i = 0; i < expected.size(); i += 1) {\n          TestHelpers.assertEqualsSafe(\n              SUPPORTED_PRIMITIVES.asStruct(), expected.get(i), actual.get(i));\n        }\n      }\n    } finally {\n      TestTables.clearTables();\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":158,"status":"NB"},{"authorDate":"2020-04-11 23:51:33","commitOrder":3,"curCode":"  public void testNestedPartitionValues() throws Exception {\n    Assume.assumeTrue(\"ORC can't project nested partition values\", !format.equalsIgnoreCase(\"orc\"));\n\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Schema nestedSchema = new Schema(optional(1, \"nested\", SUPPORTED_PRIMITIVES.asStruct()));\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(nestedSchema, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.fromInputFile(Files.localInput(avroData), 10))\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\").load(sourceLocation);\n\n    try {\n      for (String column : columnNames) {\n        String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n        File parent = temp.newFolder(desc);\n        File location = new File(parent, \"test\");\n        File dataFolder = new File(location, \"data\");\n        Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n        PartitionSpec spec = PartitionSpec.builderFor(nestedSchema).identity(\"nested.\" + column).build();\n\n        Table table = tables.create(nestedSchema, spec, location.toString());\n        table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n        sourceDF.write()\n            .format(\"iceberg\")\n            .mode(\"append\")\n            .save(location.toString());\n\n        List<Row> actual = spark.read()\n            .format(\"iceberg\")\n            .load(location.toString())\n            .collectAsList();\n\n        Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n        for (int i = 0; i < expected.size(); i += 1) {\n          TestHelpers.assertEqualsSafe(\n              nestedSchema.asStruct(), expected.get(i), actual.get(i));\n        }\n      }\n    } finally {\n      TestTables.clearTables();\n    }\n  }\n","date":"2020-04-11 23:51:33","endLine":378,"groupId":"4443","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testNestedPartitionValues","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/e6/c7621de2b016a934416ef3caefb052dad33d3a.src","preCode":"  public void testNestedPartitionValues() throws Exception {\n    Assume.assumeTrue(\"ORC can't project nested partition values\", !format.equalsIgnoreCase(\"orc\"));\n\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Schema nestedSchema = new Schema(optional(1, \"nested\", SUPPORTED_PRIMITIVES.asStruct()));\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(nestedSchema, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.fromInputFile(Files.localInput(avroData), 10))\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\").load(sourceLocation);\n\n    try {\n      for (String column : columnNames) {\n        String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n        File parent = temp.newFolder(desc);\n        File location = new File(parent, \"test\");\n        File dataFolder = new File(location, \"data\");\n        Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n        PartitionSpec spec = PartitionSpec.builderFor(nestedSchema).identity(\"nested.\" + column).build();\n\n        Table table = tables.create(nestedSchema, spec, location.toString());\n        table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n        sourceDF.write()\n            .format(\"iceberg\")\n            .mode(\"append\")\n            .save(location.toString());\n\n        List<Row> actual = spark.read()\n            .format(\"iceberg\")\n            .load(location.toString())\n            .collectAsList();\n\n        Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n        for (int i = 0; i < expected.size(); i += 1) {\n          TestHelpers.assertEqualsSafe(\n              nestedSchema.asStruct(), expected.get(i), actual.get(i));\n        }\n      }\n    } finally {\n      TestTables.clearTables();\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":313,"status":"B"}],"commitId":"3a35c0764a51008fa5c5ecea109c87f92106dbcf","commitMessage":"@@@Parquet: Support constant map for partition values (#909)\n\nThis is a follow-up to #896.  which added the same constant map support for Avro.\n\nFixes #575 for Parquet and replaces #585. Andrei did a lot of the work for this in #585.\n\nCo-authored-by: Andrei Ionescu <webdev.andrei@gmail.com>","date":"2020-04-11 23:51:33","modifiedFileCount":"9","status":"M","submitter":"Ryan Blue"},{"authorTime":"2020-05-23 02:41:45","codes":[{"authorDate":"2019-11-14 04:59:37","commitOrder":4,"curCode":"  public void testPartitionValueTypes() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(SUPPORTED_PRIMITIVES, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.fromInputFile(Files.localInput(avroData), 10))\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\").load(sourceLocation);\n\n    try {\n      for (String column : columnNames) {\n        String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n        File parent = temp.newFolder(desc);\n        File location = new File(parent, \"test\");\n        File dataFolder = new File(location, \"data\");\n        Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n        PartitionSpec spec = PartitionSpec.builderFor(SUPPORTED_PRIMITIVES).identity(column).build();\n\n        Table table = tables.create(SUPPORTED_PRIMITIVES, spec, location.toString());\n        table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n        sourceDF.write()\n            .format(\"iceberg\")\n            .mode(\"append\")\n            .save(location.toString());\n\n        List<Row> actual = spark.read()\n            .format(\"iceberg\")\n            .load(location.toString())\n            .collectAsList();\n\n        Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n        for (int i = 0; i < expected.size(); i += 1) {\n          TestHelpers.assertEqualsSafe(\n              SUPPORTED_PRIMITIVES.asStruct(), expected.get(i), actual.get(i));\n        }\n      }\n    } finally {\n      TestTables.clearTables();\n    }\n  }\n","date":"2019-11-14 04:59:37","endLine":220,"groupId":"1625","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testPartitionValueTypes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a3/7aed9e986e69e760534451696125ba51736034.src","preCode":"  public void testPartitionValueTypes() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(SUPPORTED_PRIMITIVES, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.fromInputFile(Files.localInput(avroData), 10))\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\").load(sourceLocation);\n\n    try {\n      for (String column : columnNames) {\n        String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n        File parent = temp.newFolder(desc);\n        File location = new File(parent, \"test\");\n        File dataFolder = new File(location, \"data\");\n        Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n        PartitionSpec spec = PartitionSpec.builderFor(SUPPORTED_PRIMITIVES).identity(column).build();\n\n        Table table = tables.create(SUPPORTED_PRIMITIVES, spec, location.toString());\n        table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n        sourceDF.write()\n            .format(\"iceberg\")\n            .mode(\"append\")\n            .save(location.toString());\n\n        List<Row> actual = spark.read()\n            .format(\"iceberg\")\n            .load(location.toString())\n            .collectAsList();\n\n        Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n        for (int i = 0; i < expected.size(); i += 1) {\n          TestHelpers.assertEqualsSafe(\n              SUPPORTED_PRIMITIVES.asStruct(), expected.get(i), actual.get(i));\n        }\n      }\n    } finally {\n      TestTables.clearTables();\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":158,"status":"N"},{"authorDate":"2020-05-23 02:41:45","commitOrder":4,"curCode":"  public void testNestedPartitionValues() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Schema nestedSchema = new Schema(optional(1, \"nested\", SUPPORTED_PRIMITIVES.asStruct()));\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(nestedSchema, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.fromInputFile(Files.localInput(avroData), 10))\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\").load(sourceLocation);\n\n    try {\n      for (String column : columnNames) {\n        String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n        File parent = temp.newFolder(desc);\n        File location = new File(parent, \"test\");\n        File dataFolder = new File(location, \"data\");\n        Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n        PartitionSpec spec = PartitionSpec.builderFor(nestedSchema).identity(\"nested.\" + column).build();\n\n        Table table = tables.create(nestedSchema, spec, location.toString());\n        table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n        sourceDF.write()\n            .format(\"iceberg\")\n            .mode(\"append\")\n            .save(location.toString());\n\n        List<Row> actual = spark.read()\n            .format(\"iceberg\")\n            .load(location.toString())\n            .collectAsList();\n\n        Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n        for (int i = 0; i < expected.size(); i += 1) {\n          TestHelpers.assertEqualsSafe(\n              nestedSchema.asStruct(), expected.get(i), actual.get(i));\n        }\n      }\n    } finally {\n      TestTables.clearTables();\n    }\n  }\n","date":"2020-05-23 02:41:45","endLine":375,"groupId":"4443","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testNestedPartitionValues","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/44/686d69d7890aeca828836ab55ceafd625094c4.src","preCode":"  public void testNestedPartitionValues() throws Exception {\n    Assume.assumeTrue(\"ORC can't project nested partition values\", !format.equalsIgnoreCase(\"orc\"));\n\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Schema nestedSchema = new Schema(optional(1, \"nested\", SUPPORTED_PRIMITIVES.asStruct()));\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(nestedSchema, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.fromInputFile(Files.localInput(avroData), 10))\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\").load(sourceLocation);\n\n    try {\n      for (String column : columnNames) {\n        String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n        File parent = temp.newFolder(desc);\n        File location = new File(parent, \"test\");\n        File dataFolder = new File(location, \"data\");\n        Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n        PartitionSpec spec = PartitionSpec.builderFor(nestedSchema).identity(\"nested.\" + column).build();\n\n        Table table = tables.create(nestedSchema, spec, location.toString());\n        table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n        sourceDF.write()\n            .format(\"iceberg\")\n            .mode(\"append\")\n            .save(location.toString());\n\n        List<Row> actual = spark.read()\n            .format(\"iceberg\")\n            .load(location.toString())\n            .collectAsList();\n\n        Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n        for (int i = 0; i < expected.size(); i += 1) {\n          TestHelpers.assertEqualsSafe(\n              nestedSchema.asStruct(), expected.get(i), actual.get(i));\n        }\n      }\n    } finally {\n      TestTables.clearTables();\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":312,"status":"M"}],"commitId":"17caf95e95c86c5ef290c496f7e96d0d06635924","commitMessage":"@@@ORC: Supported nested identity partition data (#989)\n\n","date":"2020-05-23 02:41:45","modifiedFileCount":"9","status":"M","submitter":"Ratandeep Ratti"},{"authorTime":"2020-06-30 08:56:05","codes":[{"authorDate":"2020-06-30 08:56:05","commitOrder":5,"curCode":"  public void testPartitionValueTypes() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(SUPPORTED_PRIMITIVES, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.fromInputFile(Files.localInput(avroData), 10))\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\").load(sourceLocation);\n\n    for (String column : columnNames) {\n      String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n      File parent = temp.newFolder(desc);\n      File location = new File(parent, \"test\");\n      File dataFolder = new File(location, \"data\");\n      Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n      PartitionSpec spec = PartitionSpec.builderFor(SUPPORTED_PRIMITIVES).identity(column).build();\n\n      Table table = tables.create(SUPPORTED_PRIMITIVES, spec, location.toString());\n      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n      sourceDF.write()\n          .format(\"iceberg\")\n          .mode(\"append\")\n          .save(location.toString());\n\n      List<Row> actual = spark.read()\n          .format(\"iceberg\")\n          .load(location.toString())\n          .collectAsList();\n\n      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n      for (int i = 0; i < expected.size(); i += 1) {\n        TestHelpers.assertEqualsSafe(\n            SUPPORTED_PRIMITIVES.asStruct(), expected.get(i), actual.get(i));\n      }\n    }\n  }\n","date":"2020-06-30 08:56:05","endLine":296,"groupId":"1625","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testPartitionValueTypes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/07/b21748d51d5f9f3c333a03ef2ca1c33dd0c21b.src","preCode":"  public void testPartitionValueTypes() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(SUPPORTED_PRIMITIVES, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.fromInputFile(Files.localInput(avroData), 10))\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\").load(sourceLocation);\n\n    try {\n      for (String column : columnNames) {\n        String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n        File parent = temp.newFolder(desc);\n        File location = new File(parent, \"test\");\n        File dataFolder = new File(location, \"data\");\n        Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n        PartitionSpec spec = PartitionSpec.builderFor(SUPPORTED_PRIMITIVES).identity(column).build();\n\n        Table table = tables.create(SUPPORTED_PRIMITIVES, spec, location.toString());\n        table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n        sourceDF.write()\n            .format(\"iceberg\")\n            .mode(\"append\")\n            .save(location.toString());\n\n        List<Row> actual = spark.read()\n            .format(\"iceberg\")\n            .load(location.toString())\n            .collectAsList();\n\n        Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n        for (int i = 0; i < expected.size(); i += 1) {\n          TestHelpers.assertEqualsSafe(\n              SUPPORTED_PRIMITIVES.asStruct(), expected.get(i), actual.get(i));\n        }\n      }\n    } finally {\n      TestTables.clearTables();\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":238,"status":"M"},{"authorDate":"2020-06-30 08:56:05","commitOrder":5,"curCode":"  public void testNestedPartitionValues() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Schema nestedSchema = new Schema(optional(1, \"nested\", SUPPORTED_PRIMITIVES.asStruct()));\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(nestedSchema, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.fromInputFile(Files.localInput(avroData), 10))\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\").load(sourceLocation);\n\n    for (String column : columnNames) {\n      String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n      File parent = temp.newFolder(desc);\n      File location = new File(parent, \"test\");\n      File dataFolder = new File(location, \"data\");\n      Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n      PartitionSpec spec = PartitionSpec.builderFor(nestedSchema).identity(\"nested.\" + column).build();\n\n      Table table = tables.create(nestedSchema, spec, location.toString());\n      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n      sourceDF.write()\n          .format(\"iceberg\")\n          .mode(\"append\")\n          .save(location.toString());\n\n      List<Row> actual = spark.read()\n          .format(\"iceberg\")\n          .load(location.toString())\n          .collectAsList();\n\n      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n      for (int i = 0; i < expected.size(); i += 1) {\n        TestHelpers.assertEqualsSafe(\n            nestedSchema.asStruct(), expected.get(i), actual.get(i));\n      }\n    }\n  }\n","date":"2020-06-30 08:56:05","endLine":358,"groupId":"4443","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testNestedPartitionValues","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/07/b21748d51d5f9f3c333a03ef2ca1c33dd0c21b.src","preCode":"  public void testNestedPartitionValues() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Schema nestedSchema = new Schema(optional(1, \"nested\", SUPPORTED_PRIMITIVES.asStruct()));\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(nestedSchema, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.fromInputFile(Files.localInput(avroData), 10))\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\").load(sourceLocation);\n\n    try {\n      for (String column : columnNames) {\n        String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n        File parent = temp.newFolder(desc);\n        File location = new File(parent, \"test\");\n        File dataFolder = new File(location, \"data\");\n        Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n        PartitionSpec spec = PartitionSpec.builderFor(nestedSchema).identity(\"nested.\" + column).build();\n\n        Table table = tables.create(nestedSchema, spec, location.toString());\n        table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n        sourceDF.write()\n            .format(\"iceberg\")\n            .mode(\"append\")\n            .save(location.toString());\n\n        List<Row> actual = spark.read()\n            .format(\"iceberg\")\n            .load(location.toString())\n            .collectAsList();\n\n        Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n        for (int i = 0; i < expected.size(); i += 1) {\n          TestHelpers.assertEqualsSafe(\n              nestedSchema.asStruct(), expected.get(i), actual.get(i));\n        }\n      }\n    } finally {\n      TestTables.clearTables();\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":299,"status":"M"}],"commitId":"51c930e33867e54b3d7e0159b11d6b9c4bc81f1a","commitMessage":"@@@Spark: Add Spark 3 data source classes (#1124)\n\n","date":"2020-06-30 08:56:05","modifiedFileCount":"7","status":"M","submitter":"Ryan Blue"},{"authorTime":"2020-07-14 05:27:36","codes":[{"authorDate":"2020-07-14 05:27:36","commitOrder":6,"curCode":"  public void testPartitionValueTypes() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(SUPPORTED_PRIMITIVES, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.fromInputFile(Files.localInput(avroData), 10))\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\")\n        .option(\"vectorization-enabled\", String.valueOf(vectorized))\n        .load(sourceLocation);\n\n    for (String column : columnNames) {\n      String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n      File parent = temp.newFolder(desc);\n      File location = new File(parent, \"test\");\n      File dataFolder = new File(location, \"data\");\n      Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n      PartitionSpec spec = PartitionSpec.builderFor(SUPPORTED_PRIMITIVES).identity(column).build();\n\n      Table table = tables.create(SUPPORTED_PRIMITIVES, spec, location.toString());\n      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n      sourceDF.write()\n          .format(\"iceberg\")\n          .mode(\"append\")\n          .save(location.toString());\n\n      List<Row> actual = spark.read()\n          .format(\"iceberg\")\n          .option(\"vectorization-enabled\", String.valueOf(vectorized))\n          .load(location.toString())\n          .collectAsList();\n\n      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n      for (int i = 0; i < expected.size(); i += 1) {\n        TestHelpers.assertEqualsSafe(\n            SUPPORTED_PRIMITIVES.asStruct(), expected.get(i), actual.get(i));\n      }\n    }\n  }\n","date":"2020-07-14 05:27:36","endLine":306,"groupId":"0","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testPartitionValueTypes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/c4/6b19166ae56e09df48c4f6dfe218217dbcce0c.src","preCode":"  public void testPartitionValueTypes() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(SUPPORTED_PRIMITIVES, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.fromInputFile(Files.localInput(avroData), 10))\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\").load(sourceLocation);\n\n    for (String column : columnNames) {\n      String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n      File parent = temp.newFolder(desc);\n      File location = new File(parent, \"test\");\n      File dataFolder = new File(location, \"data\");\n      Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n      PartitionSpec spec = PartitionSpec.builderFor(SUPPORTED_PRIMITIVES).identity(column).build();\n\n      Table table = tables.create(SUPPORTED_PRIMITIVES, spec, location.toString());\n      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n      sourceDF.write()\n          .format(\"iceberg\")\n          .mode(\"append\")\n          .save(location.toString());\n\n      List<Row> actual = spark.read()\n          .format(\"iceberg\")\n          .load(location.toString())\n          .collectAsList();\n\n      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n      for (int i = 0; i < expected.size(); i += 1) {\n        TestHelpers.assertEqualsSafe(\n            SUPPORTED_PRIMITIVES.asStruct(), expected.get(i), actual.get(i));\n      }\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":245,"status":"M"},{"authorDate":"2020-07-14 05:27:36","commitOrder":6,"curCode":"  public void testNestedPartitionValues() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Schema nestedSchema = new Schema(optional(1, \"nested\", SUPPORTED_PRIMITIVES.asStruct()));\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(nestedSchema, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.fromInputFile(Files.localInput(avroData), 10))\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\")\n        .option(\"vectorization-enabled\", String.valueOf(vectorized))\n        .load(sourceLocation);\n\n    for (String column : columnNames) {\n      String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n      File parent = temp.newFolder(desc);\n      File location = new File(parent, \"test\");\n      File dataFolder = new File(location, \"data\");\n      Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n      PartitionSpec spec = PartitionSpec.builderFor(nestedSchema).identity(\"nested.\" + column).build();\n\n      Table table = tables.create(nestedSchema, spec, location.toString());\n      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n      sourceDF.write()\n          .format(\"iceberg\")\n          .mode(\"append\")\n          .save(location.toString());\n\n      List<Row> actual = spark.read()\n          .format(\"iceberg\")\n          .option(\"vectorization-enabled\", String.valueOf(vectorized))\n          .load(location.toString())\n          .collectAsList();\n\n      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n      for (int i = 0; i < expected.size(); i += 1) {\n        TestHelpers.assertEqualsSafe(\n            nestedSchema.asStruct(), expected.get(i), actual.get(i));\n      }\n    }\n  }\n","date":"2020-07-14 05:27:36","endLine":371,"groupId":"0","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testNestedPartitionValues","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/c4/6b19166ae56e09df48c4f6dfe218217dbcce0c.src","preCode":"  public void testNestedPartitionValues() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Schema nestedSchema = new Schema(optional(1, \"nested\", SUPPORTED_PRIMITIVES.asStruct()));\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(nestedSchema, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.fromInputFile(Files.localInput(avroData), 10))\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\").load(sourceLocation);\n\n    for (String column : columnNames) {\n      String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n      File parent = temp.newFolder(desc);\n      File location = new File(parent, \"test\");\n      File dataFolder = new File(location, \"data\");\n      Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n      PartitionSpec spec = PartitionSpec.builderFor(nestedSchema).identity(\"nested.\" + column).build();\n\n      Table table = tables.create(nestedSchema, spec, location.toString());\n      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n      sourceDF.write()\n          .format(\"iceberg\")\n          .mode(\"append\")\n          .save(location.toString());\n\n      List<Row> actual = spark.read()\n          .format(\"iceberg\")\n          .load(location.toString())\n          .collectAsList();\n\n      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n      for (int i = 0; i < expected.size(); i += 1) {\n        TestHelpers.assertEqualsSafe(\n            nestedSchema.asStruct(), expected.get(i), actual.get(i));\n      }\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":309,"status":"M"}],"commitId":"6fab8f57bdb7e5fe7eadc3ff41558581338e1b69","commitMessage":"@@@Spark: Support ORC vectorized reads (#1189)\n\n","date":"2020-07-14 05:27:36","modifiedFileCount":"25","status":"M","submitter":"Shardul Mahadik"},{"authorTime":"2020-08-12 07:02:14","codes":[{"authorDate":"2020-08-12 07:02:14","commitOrder":7,"curCode":"  public void testPartitionValueTypes() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(SUPPORTED_PRIMITIVES, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.builder(PartitionSpec.unpartitioned())\n            .withRecordCount(10)\n            .withInputFile(Files.localInput(avroData))\n            .build())\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\")\n        .option(\"vectorization-enabled\", String.valueOf(vectorized))\n        .load(sourceLocation);\n\n    for (String column : columnNames) {\n      String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n      File parent = temp.newFolder(desc);\n      File location = new File(parent, \"test\");\n      File dataFolder = new File(location, \"data\");\n      Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n      PartitionSpec spec = PartitionSpec.builderFor(SUPPORTED_PRIMITIVES).identity(column).build();\n\n      Table table = tables.create(SUPPORTED_PRIMITIVES, spec, location.toString());\n      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n      sourceDF.write()\n          .format(\"iceberg\")\n          .mode(\"append\")\n          .save(location.toString());\n\n      List<Row> actual = spark.read()\n          .format(\"iceberg\")\n          .option(\"vectorization-enabled\", String.valueOf(vectorized))\n          .load(location.toString())\n          .collectAsList();\n\n      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n      for (int i = 0; i < expected.size(); i += 1) {\n        TestHelpers.assertEqualsSafe(\n            SUPPORTED_PRIMITIVES.asStruct(), expected.get(i), actual.get(i));\n      }\n    }\n  }\n","date":"2020-08-12 07:02:14","endLine":309,"groupId":"3271","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testPartitionValueTypes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/3a/60167a5f4e8ffe41300797091cd10cf75b4acc.src","preCode":"  public void testPartitionValueTypes() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(SUPPORTED_PRIMITIVES, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.fromInputFile(Files.localInput(avroData), 10))\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\")\n        .option(\"vectorization-enabled\", String.valueOf(vectorized))\n        .load(sourceLocation);\n\n    for (String column : columnNames) {\n      String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n      File parent = temp.newFolder(desc);\n      File location = new File(parent, \"test\");\n      File dataFolder = new File(location, \"data\");\n      Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n      PartitionSpec spec = PartitionSpec.builderFor(SUPPORTED_PRIMITIVES).identity(column).build();\n\n      Table table = tables.create(SUPPORTED_PRIMITIVES, spec, location.toString());\n      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n      sourceDF.write()\n          .format(\"iceberg\")\n          .mode(\"append\")\n          .save(location.toString());\n\n      List<Row> actual = spark.read()\n          .format(\"iceberg\")\n          .option(\"vectorization-enabled\", String.valueOf(vectorized))\n          .load(location.toString())\n          .collectAsList();\n\n      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n      for (int i = 0; i < expected.size(); i += 1) {\n        TestHelpers.assertEqualsSafe(\n            SUPPORTED_PRIMITIVES.asStruct(), expected.get(i), actual.get(i));\n      }\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":245,"status":"M"},{"authorDate":"2020-08-12 07:02:14","commitOrder":7,"curCode":"  public void testNestedPartitionValues() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Schema nestedSchema = new Schema(optional(1, \"nested\", SUPPORTED_PRIMITIVES.asStruct()));\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(nestedSchema, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.builder(PartitionSpec.unpartitioned())\n            .withRecordCount(10)\n            .withInputFile(Files.localInput(avroData))\n            .build())\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\")\n        .option(\"vectorization-enabled\", String.valueOf(vectorized))\n        .load(sourceLocation);\n\n    for (String column : columnNames) {\n      String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n      File parent = temp.newFolder(desc);\n      File location = new File(parent, \"test\");\n      File dataFolder = new File(location, \"data\");\n      Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n      PartitionSpec spec = PartitionSpec.builderFor(nestedSchema).identity(\"nested.\" + column).build();\n\n      Table table = tables.create(nestedSchema, spec, location.toString());\n      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n      sourceDF.write()\n          .format(\"iceberg\")\n          .mode(\"append\")\n          .save(location.toString());\n\n      List<Row> actual = spark.read()\n          .format(\"iceberg\")\n          .option(\"vectorization-enabled\", String.valueOf(vectorized))\n          .load(location.toString())\n          .collectAsList();\n\n      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n      for (int i = 0; i < expected.size(); i += 1) {\n        TestHelpers.assertEqualsSafe(\n            nestedSchema.asStruct(), expected.get(i), actual.get(i));\n      }\n    }\n  }\n","date":"2020-08-12 07:02:14","endLine":377,"groupId":"3272","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testNestedPartitionValues","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/3a/60167a5f4e8ffe41300797091cd10cf75b4acc.src","preCode":"  public void testNestedPartitionValues() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Schema nestedSchema = new Schema(optional(1, \"nested\", SUPPORTED_PRIMITIVES.asStruct()));\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(nestedSchema, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.fromInputFile(Files.localInput(avroData), 10))\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\")\n        .option(\"vectorization-enabled\", String.valueOf(vectorized))\n        .load(sourceLocation);\n\n    for (String column : columnNames) {\n      String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n      File parent = temp.newFolder(desc);\n      File location = new File(parent, \"test\");\n      File dataFolder = new File(location, \"data\");\n      Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n      PartitionSpec spec = PartitionSpec.builderFor(nestedSchema).identity(\"nested.\" + column).build();\n\n      Table table = tables.create(nestedSchema, spec, location.toString());\n      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n      sourceDF.write()\n          .format(\"iceberg\")\n          .mode(\"append\")\n          .save(location.toString());\n\n      List<Row> actual = spark.read()\n          .format(\"iceberg\")\n          .option(\"vectorization-enabled\", String.valueOf(vectorized))\n          .load(location.toString())\n          .collectAsList();\n\n      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n      for (int i = 0; i < expected.size(); i += 1) {\n        TestHelpers.assertEqualsSafe(\n            nestedSchema.asStruct(), expected.get(i), actual.get(i));\n      }\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":312,"status":"M"}],"commitId":"700fc9eaf91317e08287bbc23c5088494e74184a","commitMessage":"@@@Core: Add specId to DataFile (#1317)\n\n","date":"2020-08-12 07:02:14","modifiedFileCount":"18","status":"M","submitter":"Ryan Blue"},{"authorTime":"2020-12-29 05:55:09","codes":[{"authorDate":"2020-12-29 05:55:09","commitOrder":8,"curCode":"  public void testPartitionValueTypes() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(SUPPORTED_PRIMITIVES, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.builder(PartitionSpec.unpartitioned())\n            .withRecordCount(10)\n            .withInputFile(Files.localInput(avroData))\n            .build())\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\")\n        .option(SparkReadOptions.VECTORIZATION_ENABLED, String.valueOf(vectorized))\n        .load(sourceLocation);\n\n    for (String column : columnNames) {\n      String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n      File parent = temp.newFolder(desc);\n      File location = new File(parent, \"test\");\n      File dataFolder = new File(location, \"data\");\n      Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n      PartitionSpec spec = PartitionSpec.builderFor(SUPPORTED_PRIMITIVES).identity(column).build();\n\n      Table table = tables.create(SUPPORTED_PRIMITIVES, spec, location.toString());\n      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n      sourceDF.write()\n          .format(\"iceberg\")\n          .mode(SaveMode.Append)\n          .save(location.toString());\n\n      List<Row> actual = spark.read()\n          .format(\"iceberg\")\n          .option(SparkReadOptions.VECTORIZATION_ENABLED, String.valueOf(vectorized))\n          .load(location.toString())\n          .collectAsList();\n\n      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n      for (int i = 0; i < expected.size(); i += 1) {\n        TestHelpers.assertEqualsSafe(\n            SUPPORTED_PRIMITIVES.asStruct(), expected.get(i), actual.get(i));\n      }\n    }\n  }\n","date":"2020-12-29 05:55:09","endLine":312,"groupId":"10297","id":11,"instanceNumber":1,"isCurCommit":1,"methodName":"testPartitionValueTypes","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/3c/ee81ea49e80a9b284c704e481c79559a08d22c.src","preCode":"  public void testPartitionValueTypes() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(SUPPORTED_PRIMITIVES, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.builder(PartitionSpec.unpartitioned())\n            .withRecordCount(10)\n            .withInputFile(Files.localInput(avroData))\n            .build())\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\")\n        .option(\"vectorization-enabled\", String.valueOf(vectorized))\n        .load(sourceLocation);\n\n    for (String column : columnNames) {\n      String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n      File parent = temp.newFolder(desc);\n      File location = new File(parent, \"test\");\n      File dataFolder = new File(location, \"data\");\n      Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n      PartitionSpec spec = PartitionSpec.builderFor(SUPPORTED_PRIMITIVES).identity(column).build();\n\n      Table table = tables.create(SUPPORTED_PRIMITIVES, spec, location.toString());\n      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n      sourceDF.write()\n          .format(\"iceberg\")\n          .mode(\"append\")\n          .save(location.toString());\n\n      List<Row> actual = spark.read()\n          .format(\"iceberg\")\n          .option(\"vectorization-enabled\", String.valueOf(vectorized))\n          .load(location.toString())\n          .collectAsList();\n\n      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n      for (int i = 0; i < expected.size(); i += 1) {\n        TestHelpers.assertEqualsSafe(\n            SUPPORTED_PRIMITIVES.asStruct(), expected.get(i), actual.get(i));\n      }\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":248,"status":"M"},{"authorDate":"2020-12-29 05:55:09","commitOrder":8,"curCode":"  public void testNestedPartitionValues() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Schema nestedSchema = new Schema(optional(1, \"nested\", SUPPORTED_PRIMITIVES.asStruct()));\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(nestedSchema, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.builder(PartitionSpec.unpartitioned())\n            .withRecordCount(10)\n            .withInputFile(Files.localInput(avroData))\n            .build())\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\")\n        .option(SparkReadOptions.VECTORIZATION_ENABLED, String.valueOf(vectorized))\n        .load(sourceLocation);\n\n    for (String column : columnNames) {\n      String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n      File parent = temp.newFolder(desc);\n      File location = new File(parent, \"test\");\n      File dataFolder = new File(location, \"data\");\n      Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n      PartitionSpec spec = PartitionSpec.builderFor(nestedSchema).identity(\"nested.\" + column).build();\n\n      Table table = tables.create(nestedSchema, spec, location.toString());\n      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n      sourceDF.write()\n          .format(\"iceberg\")\n          .mode(SaveMode.Append)\n          .save(location.toString());\n\n      List<Row> actual = spark.read()\n          .format(\"iceberg\")\n          .option(SparkReadOptions.VECTORIZATION_ENABLED, String.valueOf(vectorized))\n          .load(location.toString())\n          .collectAsList();\n\n      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n      for (int i = 0; i < expected.size(); i += 1) {\n        TestHelpers.assertEqualsSafe(\n            nestedSchema.asStruct(), expected.get(i), actual.get(i));\n      }\n    }\n  }\n","date":"2020-12-29 05:55:09","endLine":380,"groupId":"10297","id":12,"instanceNumber":2,"isCurCommit":1,"methodName":"testNestedPartitionValues","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/3c/ee81ea49e80a9b284c704e481c79559a08d22c.src","preCode":"  public void testNestedPartitionValues() throws Exception {\n    String[] columnNames = new String[] {\n        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n    };\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Schema nestedSchema = new Schema(optional(1, \"nested\", SUPPORTED_PRIMITIVES.asStruct()));\n\n    \r\n    String sourceLocation = temp.newFolder(\"source_table\").toString();\n    Table source = tables.create(nestedSchema, sourceLocation);\n\n    \r\n    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n    File avroData = temp.newFile(\"data.avro\");\n    Assert.assertTrue(avroData.delete());\n    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n        .schema(source.schema())\n        .build()) {\n      appender.addAll(expected);\n    }\n\n    \r\n    source.newAppend()\n        .appendFile(DataFiles.builder(PartitionSpec.unpartitioned())\n            .withRecordCount(10)\n            .withInputFile(Files.localInput(avroData))\n            .build())\n        .commit();\n\n    Dataset<Row> sourceDF = spark.read().format(\"iceberg\")\n        .option(\"vectorization-enabled\", String.valueOf(vectorized))\n        .load(sourceLocation);\n\n    for (String column : columnNames) {\n      String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n\n      File parent = temp.newFolder(desc);\n      File location = new File(parent, \"test\");\n      File dataFolder = new File(location, \"data\");\n      Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n\n      PartitionSpec spec = PartitionSpec.builderFor(nestedSchema).identity(\"nested.\" + column).build();\n\n      Table table = tables.create(nestedSchema, spec, location.toString());\n      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n\n      sourceDF.write()\n          .format(\"iceberg\")\n          .mode(\"append\")\n          .save(location.toString());\n\n      List<Row> actual = spark.read()\n          .format(\"iceberg\")\n          .option(\"vectorization-enabled\", String.valueOf(vectorized))\n          .load(location.toString())\n          .collectAsList();\n\n      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n\n      for (int i = 0; i < expected.size(); i += 1) {\n        TestHelpers.assertEqualsSafe(\n            nestedSchema.asStruct(), expected.get(i), actual.get(i));\n      }\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":315,"status":"M"}],"commitId":"cbb244759ae218b85a7e68a1568fddaf815aab3c","commitMessage":"@@@Spark: Use constants for DF read and write options (#1933)\n\n","date":"2020-12-29 05:55:09","modifiedFileCount":"22","status":"M","submitter":"Karuppayya"}]
