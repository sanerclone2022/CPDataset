[{"authorTime":"2020-07-14 05:27:36","codes":[{"authorDate":"2020-07-14 05:27:36","commitOrder":3,"curCode":"  public PartitionReaderFactory createReaderFactory() {\n    boolean allParquetFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.PARQUET)));\n\n    boolean allOrcFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.ORC)));\n\n    boolean atLeastOneColumn = expectedSchema.columns().size() > 0;\n\n    boolean hasNoIdentityProjections = tasks().stream()\n        .allMatch(combinedScanTask -> combinedScanTask.files()\n            .stream()\n            .allMatch(fileScanTask -> fileScanTask.spec().identitySourceIds().isEmpty()));\n\n    boolean onlyPrimitives = expectedSchema.columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n    boolean readUsingBatch = batchReadsEnabled && (allOrcFileScanTasks ||\n        (allParquetFileScanTasks && atLeastOneColumn && hasNoIdentityProjections && onlyPrimitives));\n\n    return new ReaderFactory(readUsingBatch ? batchSize : 0);\n  }\n","date":"2020-07-14 05:27:36","endLine":178,"groupId":"2438","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"createReaderFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/91/2d90cafb53a6b09a55d44ef11d3e4e40bbaa92.src","preCode":"  public PartitionReaderFactory createReaderFactory() {\n    boolean allParquetFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.PARQUET)));\n\n    boolean allOrcFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.ORC)));\n\n    boolean atLeastOneColumn = expectedSchema.columns().size() > 0;\n\n    boolean hasNoIdentityProjections = tasks().stream()\n        .allMatch(combinedScanTask -> combinedScanTask.files()\n            .stream()\n            .allMatch(fileScanTask -> fileScanTask.spec().identitySourceIds().isEmpty()));\n\n    boolean onlyPrimitives = expectedSchema.columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n    boolean readUsingBatch = batchReadsEnabled && (allOrcFileScanTasks ||\n        (allParquetFileScanTasks && atLeastOneColumn && hasNoIdentityProjections && onlyPrimitives));\n\n    return new ReaderFactory(readUsingBatch ? batchSize : 0);\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":150,"status":"MB"},{"authorDate":"2020-07-14 05:27:36","commitOrder":3,"curCode":"  public boolean enableBatchRead() {\n    if (readUsingBatch == null) {\n      boolean allParquetFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.PARQUET)));\n\n      boolean allOrcFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.ORC)));\n\n      boolean atLeastOneColumn = lazySchema().columns().size() > 0;\n\n      boolean hasNoIdentityProjections = tasks().stream()\n          .allMatch(combinedScanTask -> combinedScanTask.files()\n              .stream()\n              .allMatch(fileScanTask -> fileScanTask.spec().identitySourceIds().isEmpty()));\n\n      boolean onlyPrimitives = lazySchema().columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n      this.readUsingBatch = batchReadsEnabled && (allOrcFileScanTasks ||\n          (allParquetFileScanTasks && atLeastOneColumn && hasNoIdentityProjections && onlyPrimitives));\n    }\n    return readUsingBatch;\n  }\n","date":"2020-07-14 05:27:36","endLine":322,"groupId":"2704","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"enableBatchRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/9f/b475bcd55190cc8bcc9170f4ab00264ad6865c.src","preCode":"  public boolean enableBatchRead() {\n    if (readUsingBatch == null) {\n      boolean allParquetFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.PARQUET)));\n\n      boolean allOrcFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.ORC)));\n\n      boolean atLeastOneColumn = lazySchema().columns().size() > 0;\n\n      boolean hasNoIdentityProjections = tasks().stream()\n          .allMatch(combinedScanTask -> combinedScanTask.files()\n              .stream()\n              .allMatch(fileScanTask -> fileScanTask.spec().identitySourceIds().isEmpty()));\n\n      boolean onlyPrimitives = lazySchema().columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n      this.readUsingBatch = batchReadsEnabled && (allOrcFileScanTasks ||\n          (allParquetFileScanTasks && atLeastOneColumn && hasNoIdentityProjections && onlyPrimitives));\n    }\n    return readUsingBatch;\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":293,"status":"MB"}],"commitId":"6fab8f57bdb7e5fe7eadc3ff41558581338e1b69","commitMessage":"@@@Spark: Support ORC vectorized reads (#1189)\n\n","date":"2020-07-14 05:27:36","modifiedFileCount":"25","status":"M","submitter":"Shardul Mahadik"},{"authorTime":"2020-08-05 09:08:03","codes":[{"authorDate":"2020-07-14 05:27:36","commitOrder":4,"curCode":"  public PartitionReaderFactory createReaderFactory() {\n    boolean allParquetFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.PARQUET)));\n\n    boolean allOrcFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.ORC)));\n\n    boolean atLeastOneColumn = expectedSchema.columns().size() > 0;\n\n    boolean hasNoIdentityProjections = tasks().stream()\n        .allMatch(combinedScanTask -> combinedScanTask.files()\n            .stream()\n            .allMatch(fileScanTask -> fileScanTask.spec().identitySourceIds().isEmpty()));\n\n    boolean onlyPrimitives = expectedSchema.columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n    boolean readUsingBatch = batchReadsEnabled && (allOrcFileScanTasks ||\n        (allParquetFileScanTasks && atLeastOneColumn && hasNoIdentityProjections && onlyPrimitives));\n\n    return new ReaderFactory(readUsingBatch ? batchSize : 0);\n  }\n","date":"2020-07-14 05:27:36","endLine":178,"groupId":"2438","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"createReaderFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/91/2d90cafb53a6b09a55d44ef11d3e4e40bbaa92.src","preCode":"  public PartitionReaderFactory createReaderFactory() {\n    boolean allParquetFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.PARQUET)));\n\n    boolean allOrcFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.ORC)));\n\n    boolean atLeastOneColumn = expectedSchema.columns().size() > 0;\n\n    boolean hasNoIdentityProjections = tasks().stream()\n        .allMatch(combinedScanTask -> combinedScanTask.files()\n            .stream()\n            .allMatch(fileScanTask -> fileScanTask.spec().identitySourceIds().isEmpty()));\n\n    boolean onlyPrimitives = expectedSchema.columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n    boolean readUsingBatch = batchReadsEnabled && (allOrcFileScanTasks ||\n        (allParquetFileScanTasks && atLeastOneColumn && hasNoIdentityProjections && onlyPrimitives));\n\n    return new ReaderFactory(readUsingBatch ? batchSize : 0);\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":150,"status":"N"},{"authorDate":"2020-08-05 09:08:03","commitOrder":4,"curCode":"  public boolean enableBatchRead() {\n    if (readUsingBatch == null) {\n      boolean allParquetFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.PARQUET)));\n\n      boolean allOrcFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.ORC)));\n\n      boolean atLeastOneColumn = lazySchema().columns().size() > 0;\n\n      boolean onlyPrimitives = lazySchema().columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n      this.readUsingBatch = batchReadsEnabled && (allOrcFileScanTasks ||\n          (allParquetFileScanTasks && atLeastOneColumn && onlyPrimitives));\n    }\n    return readUsingBatch;\n  }\n","date":"2020-08-05 09:08:03","endLine":324,"groupId":"2704","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"enableBatchRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/23/2acc614eebd028aff849175c6171fc3f476112.src","preCode":"  public boolean enableBatchRead() {\n    if (readUsingBatch == null) {\n      boolean allParquetFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.PARQUET)));\n\n      boolean allOrcFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.ORC)));\n\n      boolean atLeastOneColumn = lazySchema().columns().size() > 0;\n\n      boolean hasNoIdentityProjections = tasks().stream()\n          .allMatch(combinedScanTask -> combinedScanTask.files()\n              .stream()\n              .allMatch(fileScanTask -> fileScanTask.spec().identitySourceIds().isEmpty()));\n\n      boolean onlyPrimitives = lazySchema().columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n      this.readUsingBatch = batchReadsEnabled && (allOrcFileScanTasks ||\n          (allParquetFileScanTasks && atLeastOneColumn && hasNoIdentityProjections && onlyPrimitives));\n    }\n    return readUsingBatch;\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":300,"status":"M"}],"commitId":"71de51805be7954dd423ec29fd4c7d6b303adc1f","commitMessage":"@@@Parquet: Support vectorized reads with identity partition values (#1287)\n\n","date":"2020-08-05 09:08:03","modifiedFileCount":"7","status":"M","submitter":"Russell Spitzer"},{"authorTime":"2020-08-05 09:08:03","codes":[{"authorDate":"2020-08-10 23:34:00","commitOrder":5,"curCode":"  public PartitionReaderFactory createReaderFactory() {\n    boolean allParquetFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.PARQUET)));\n\n    boolean allOrcFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.ORC)));\n\n    boolean atLeastOneColumn = expectedSchema.columns().size() > 0;\n\n    boolean onlyPrimitives = expectedSchema.columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n    boolean readUsingBatch = batchReadsEnabled && (allOrcFileScanTasks ||\n        (allParquetFileScanTasks && atLeastOneColumn && onlyPrimitives));\n\n    return new ReaderFactory(readUsingBatch ? batchSize : 0);\n  }\n","date":"2020-08-10 23:34:00","endLine":175,"groupId":"2438","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"createReaderFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/5d/47eb92942fc2d45f52c3cc766c70ea9d9dc4be.src","preCode":"  public PartitionReaderFactory createReaderFactory() {\n    boolean allParquetFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.PARQUET)));\n\n    boolean allOrcFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.ORC)));\n\n    boolean atLeastOneColumn = expectedSchema.columns().size() > 0;\n\n    boolean hasNoIdentityProjections = tasks().stream()\n        .allMatch(combinedScanTask -> combinedScanTask.files()\n            .stream()\n            .allMatch(fileScanTask -> fileScanTask.spec().identitySourceIds().isEmpty()));\n\n    boolean onlyPrimitives = expectedSchema.columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n    boolean readUsingBatch = batchReadsEnabled && (allOrcFileScanTasks ||\n        (allParquetFileScanTasks && atLeastOneColumn && hasNoIdentityProjections && onlyPrimitives));\n\n    return new ReaderFactory(readUsingBatch ? batchSize : 0);\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":152,"status":"M"},{"authorDate":"2020-08-05 09:08:03","commitOrder":5,"curCode":"  public boolean enableBatchRead() {\n    if (readUsingBatch == null) {\n      boolean allParquetFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.PARQUET)));\n\n      boolean allOrcFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.ORC)));\n\n      boolean atLeastOneColumn = lazySchema().columns().size() > 0;\n\n      boolean onlyPrimitives = lazySchema().columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n      this.readUsingBatch = batchReadsEnabled && (allOrcFileScanTasks ||\n          (allParquetFileScanTasks && atLeastOneColumn && onlyPrimitives));\n    }\n    return readUsingBatch;\n  }\n","date":"2020-08-05 09:08:03","endLine":324,"groupId":"2704","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"enableBatchRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/23/2acc614eebd028aff849175c6171fc3f476112.src","preCode":"  public boolean enableBatchRead() {\n    if (readUsingBatch == null) {\n      boolean allParquetFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.PARQUET)));\n\n      boolean allOrcFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.ORC)));\n\n      boolean atLeastOneColumn = lazySchema().columns().size() > 0;\n\n      boolean onlyPrimitives = lazySchema().columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n      this.readUsingBatch = batchReadsEnabled && (allOrcFileScanTasks ||\n          (allParquetFileScanTasks && atLeastOneColumn && onlyPrimitives));\n    }\n    return readUsingBatch;\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":300,"status":"N"}],"commitId":"c2f2f27d41c0c63414969479316225b0d6c7eace","commitMessage":"@@@Spark3: Enable Parquet vectorized reads with identity partition values (#1312)\n\n","date":"2020-08-10 23:34:00","modifiedFileCount":"1","status":"M","submitter":"Shardul Mahadik"},{"authorTime":"2020-09-18 01:14:41","codes":[{"authorDate":"2020-09-18 01:14:41","commitOrder":6,"curCode":"  public PartitionReaderFactory createReaderFactory() {\n    boolean allParquetFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.PARQUET)));\n\n    boolean allOrcFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.ORC)));\n\n    boolean atLeastOneColumn = expectedSchema.columns().size() > 0;\n\n    boolean onlyPrimitives = expectedSchema.columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n    boolean hasNoDeleteFiles = tasks().stream().noneMatch(TableScanUtil::hasDeletes);\n\n    boolean readUsingBatch = batchReadsEnabled && hasNoDeleteFiles && (allOrcFileScanTasks ||\n        (allParquetFileScanTasks && atLeastOneColumn && onlyPrimitives));\n\n    return new ReaderFactory(readUsingBatch ? batchSize : 0);\n  }\n","date":"2020-09-18 01:14:41","endLine":178,"groupId":"2438","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"createReaderFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/b8/4b56df990fb4248ce64b7f9f04c913caa6ff87.src","preCode":"  public PartitionReaderFactory createReaderFactory() {\n    boolean allParquetFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.PARQUET)));\n\n    boolean allOrcFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.ORC)));\n\n    boolean atLeastOneColumn = expectedSchema.columns().size() > 0;\n\n    boolean onlyPrimitives = expectedSchema.columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n    boolean readUsingBatch = batchReadsEnabled && (allOrcFileScanTasks ||\n        (allParquetFileScanTasks && atLeastOneColumn && onlyPrimitives));\n\n    return new ReaderFactory(readUsingBatch ? batchSize : 0);\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":153,"status":"M"},{"authorDate":"2020-09-18 01:14:41","commitOrder":6,"curCode":"  public boolean enableBatchRead() {\n    if (readUsingBatch == null) {\n      boolean allParquetFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.PARQUET)));\n\n      boolean allOrcFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.ORC)));\n\n      boolean atLeastOneColumn = lazySchema().columns().size() > 0;\n\n      boolean onlyPrimitives = lazySchema().columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n      boolean hasNoDeleteFiles = tasks().stream().noneMatch(TableScanUtil::hasDeletes);\n\n      this.readUsingBatch = batchReadsEnabled && hasNoDeleteFiles && (allOrcFileScanTasks ||\n          (allParquetFileScanTasks && atLeastOneColumn && onlyPrimitives));\n    }\n    return readUsingBatch;\n  }\n","date":"2020-09-18 01:14:41","endLine":338,"groupId":"2704","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"enableBatchRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/82/6c4508381b93a3c4cac5477fea85269f766e24.src","preCode":"  public boolean enableBatchRead() {\n    if (readUsingBatch == null) {\n      boolean allParquetFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.PARQUET)));\n\n      boolean allOrcFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.ORC)));\n\n      boolean atLeastOneColumn = lazySchema().columns().size() > 0;\n\n      boolean onlyPrimitives = lazySchema().columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n      this.readUsingBatch = batchReadsEnabled && (allOrcFileScanTasks ||\n          (allParquetFileScanTasks && atLeastOneColumn && onlyPrimitives));\n    }\n    return readUsingBatch;\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":312,"status":"M"}],"commitId":"1772f4f27b8a12d3e89a7f65b8b600b717e1f09d","commitMessage":"@@@Spark: Apply row-level delete files when reading (#1444)\n\n","date":"2020-09-18 01:14:41","modifiedFileCount":"9","status":"M","submitter":"Ryan Blue"},{"authorTime":"2021-03-06 09:53:28","codes":[{"authorDate":"2021-03-06 09:53:28","commitOrder":7,"curCode":"  public PartitionReaderFactory createReaderFactory() {\n    boolean allParquetFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.PARQUET)));\n\n    boolean allOrcFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.ORC)));\n\n    boolean atLeastOneColumn = expectedSchema.columns().size() > 0;\n\n    boolean onlyPrimitives = expectedSchema.columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n    boolean hasNoDeleteFiles = tasks().stream().noneMatch(TableScanUtil::hasDeletes);\n\n    boolean batchReadsEnabled = batchReadsEnabled(allParquetFileScanTasks, allOrcFileScanTasks);\n\n    boolean readUsingBatch = batchReadsEnabled && hasNoDeleteFiles && (allOrcFileScanTasks ||\n        (allParquetFileScanTasks && atLeastOneColumn && onlyPrimitives));\n\n    return new ReaderFactory(readUsingBatch ? batchSize : 0);\n  }\n","date":"2021-03-06 09:53:28","endLine":168,"groupId":"2438","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"createReaderFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/e2/6f8c6209196b1c64e5218e9f0eadce5ba1fcef.src","preCode":"  public PartitionReaderFactory createReaderFactory() {\n    boolean allParquetFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.PARQUET)));\n\n    boolean allOrcFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.ORC)));\n\n    boolean atLeastOneColumn = expectedSchema.columns().size() > 0;\n\n    boolean onlyPrimitives = expectedSchema.columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n    boolean hasNoDeleteFiles = tasks().stream().noneMatch(TableScanUtil::hasDeletes);\n\n    boolean readUsingBatch = batchReadsEnabled && hasNoDeleteFiles && (allOrcFileScanTasks ||\n        (allParquetFileScanTasks && atLeastOneColumn && onlyPrimitives));\n\n    return new ReaderFactory(readUsingBatch ? batchSize : 0);\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":141,"status":"M"},{"authorDate":"2021-03-06 09:53:28","commitOrder":7,"curCode":"  public boolean enableBatchRead() {\n    if (readUsingBatch == null) {\n      boolean allParquetFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.PARQUET)));\n\n      boolean allOrcFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.ORC)));\n\n      boolean atLeastOneColumn = lazySchema().columns().size() > 0;\n\n      boolean onlyPrimitives = lazySchema().columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n      boolean hasNoDeleteFiles = tasks().stream().noneMatch(TableScanUtil::hasDeletes);\n\n      boolean batchReadsEnabled = batchReadsEnabled(allParquetFileScanTasks, allOrcFileScanTasks);\n\n      this.readUsingBatch = batchReadsEnabled && hasNoDeleteFiles && (allOrcFileScanTasks ||\n          (allParquetFileScanTasks && atLeastOneColumn && onlyPrimitives));\n    }\n    return readUsingBatch;\n  }\n","date":"2021-03-06 09:53:28","endLine":339,"groupId":"2704","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"enableBatchRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/ff/dcc4727c9072432300cda716a888b3834d08e8.src","preCode":"  public boolean enableBatchRead() {\n    if (readUsingBatch == null) {\n      boolean allParquetFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.PARQUET)));\n\n      boolean allOrcFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.ORC)));\n\n      boolean atLeastOneColumn = lazySchema().columns().size() > 0;\n\n      boolean onlyPrimitives = lazySchema().columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n      boolean hasNoDeleteFiles = tasks().stream().noneMatch(TableScanUtil::hasDeletes);\n\n      this.readUsingBatch = batchReadsEnabled && hasNoDeleteFiles && (allOrcFileScanTasks ||\n          (allParquetFileScanTasks && atLeastOneColumn && onlyPrimitives));\n    }\n    return readUsingBatch;\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":311,"status":"M"}],"commitId":"5099e1eb1ac5d802badf55c8d1306d42a0118250","commitMessage":"@@@Spark: Fix vectorization flags (#2248)\n\n","date":"2021-03-06 09:53:28","modifiedFileCount":"3","status":"M","submitter":"Anton Okolnychyi"},{"authorTime":"2021-09-18 02:42:22","codes":[{"authorDate":"2021-09-18 02:42:22","commitOrder":8,"curCode":"  public PartitionReaderFactory createReaderFactory() {\n    boolean allParquetFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.PARQUET)));\n\n    boolean allOrcFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.ORC)));\n\n    boolean atLeastOneColumn = expectedSchema.columns().size() > 0;\n\n    boolean onlyPrimitives = expectedSchema.columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n    boolean hasNoDeleteFiles = tasks().stream().noneMatch(TableScanUtil::hasDeletes);\n\n    boolean batchReadsEnabled = batchReadsEnabled(allParquetFileScanTasks, allOrcFileScanTasks);\n\n    boolean readUsingBatch = batchReadsEnabled && hasNoDeleteFiles && (allOrcFileScanTasks ||\n        (allParquetFileScanTasks && atLeastOneColumn && onlyPrimitives));\n\n    int batchSize = readUsingBatch ? batchSize(allParquetFileScanTasks, allOrcFileScanTasks) : 0;\n\n    return new ReaderFactory(batchSize);\n  }\n","date":"2021-09-18 02:42:22","endLine":184,"groupId":"10872","id":11,"instanceNumber":1,"isCurCommit":1,"methodName":"createReaderFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/3c/256bc49248acd5618a3ba734839307c0eacdbb.src","preCode":"  public PartitionReaderFactory createReaderFactory() {\n    boolean allParquetFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.PARQUET)));\n\n    boolean allOrcFileScanTasks =\n        tasks().stream()\n            .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                .stream()\n                .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                    FileFormat.ORC)));\n\n    boolean atLeastOneColumn = expectedSchema.columns().size() > 0;\n\n    boolean onlyPrimitives = expectedSchema.columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n    boolean hasNoDeleteFiles = tasks().stream().noneMatch(TableScanUtil::hasDeletes);\n\n    boolean batchReadsEnabled = batchReadsEnabled(allParquetFileScanTasks, allOrcFileScanTasks);\n\n    boolean readUsingBatch = batchReadsEnabled && hasNoDeleteFiles && (allOrcFileScanTasks ||\n        (allParquetFileScanTasks && atLeastOneColumn && onlyPrimitives));\n\n    return new ReaderFactory(readUsingBatch ? batchSize : 0);\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":155,"status":"M"},{"authorDate":"2021-09-18 02:42:22","commitOrder":8,"curCode":"  public boolean enableBatchRead() {\n    if (readUsingBatch == null) {\n      boolean allParquetFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.PARQUET)));\n\n      boolean allOrcFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.ORC)));\n\n      boolean atLeastOneColumn = lazySchema().columns().size() > 0;\n\n      boolean onlyPrimitives = lazySchema().columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n      boolean hasNoDeleteFiles = tasks().stream().noneMatch(TableScanUtil::hasDeletes);\n\n      boolean batchReadsEnabled = batchReadsEnabled(allParquetFileScanTasks, allOrcFileScanTasks);\n\n      this.readUsingBatch = batchReadsEnabled && hasNoDeleteFiles && (allOrcFileScanTasks ||\n          (allParquetFileScanTasks && atLeastOneColumn && onlyPrimitives));\n\n      if (readUsingBatch) {\n        this.batchSize = batchSize(allParquetFileScanTasks, allOrcFileScanTasks);\n      }\n    }\n    return readUsingBatch;\n  }\n","date":"2021-09-18 02:42:22","endLine":354,"groupId":"10872","id":12,"instanceNumber":2,"isCurCommit":1,"methodName":"enableBatchRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/f3/a600e3298ffa11c5f68234973d6c033440dfc6.src","preCode":"  public boolean enableBatchRead() {\n    if (readUsingBatch == null) {\n      boolean allParquetFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.PARQUET)));\n\n      boolean allOrcFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.ORC)));\n\n      boolean atLeastOneColumn = lazySchema().columns().size() > 0;\n\n      boolean onlyPrimitives = lazySchema().columns().stream().allMatch(c -> c.type().isPrimitiveType());\n\n      boolean hasNoDeleteFiles = tasks().stream().noneMatch(TableScanUtil::hasDeletes);\n\n      boolean batchReadsEnabled = batchReadsEnabled(allParquetFileScanTasks, allOrcFileScanTasks);\n\n      this.readUsingBatch = batchReadsEnabled && hasNoDeleteFiles && (allOrcFileScanTasks ||\n          (allParquetFileScanTasks && atLeastOneColumn && onlyPrimitives));\n    }\n    return readUsingBatch;\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":322,"status":"M"}],"commitId":"604fd28146ef60fb2aa8d331ba0a8436250476a8","commitMessage":"@@@Core: Add table property for ORC batch size (#3133)\n\n","date":"2021-09-18 02:42:22","modifiedFileCount":"4","status":"M","submitter":"Anton Okolnychyi"}]
