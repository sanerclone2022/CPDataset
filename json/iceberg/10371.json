[{"authorTime":"2019-06-11 01:36:27","codes":[{"authorDate":"2019-06-11 01:36:27","commitOrder":1,"curCode":"  public void testSnapshotSelectionById() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    Table table = tables.create(SCHEMA, spec, tableLocation);\n\n    \r\n    List<SimpleRecord> firstBatchRecords = Lists.newArrayList(\n        new SimpleRecord(1, \"a\"),\n        new SimpleRecord(2, \"b\"),\n        new SimpleRecord(3, \"c\")\n    );\n    Dataset<Row> firstDf = spark.createDataFrame(firstBatchRecords, SimpleRecord.class);\n    firstDf.select(\"id\", \"data\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n\n    \r\n    List<SimpleRecord> secondBatchRecords = Lists.newArrayList(\n        new SimpleRecord(4, \"d\"),\n        new SimpleRecord(5, \"e\"),\n        new SimpleRecord(6, \"f\")\n    );\n    Dataset<Row> secondDf = spark.createDataFrame(secondBatchRecords, SimpleRecord.class);\n    secondDf.select(\"id\", \"data\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n\n    Assert.assertEquals(\"Expected 2 snapshots\", 2, Iterables.size(table.snapshots()));\n\n    \r\n    Dataset<Row> currentSnapshotResult = spark.read()\n        .format(\"iceberg\")\n        .load(tableLocation);\n    List<SimpleRecord> currentSnapshotRecords = currentSnapshotResult.orderBy(\"id\")\n        .as(Encoders.bean(SimpleRecord.class))\n        .collectAsList();\n    List<SimpleRecord> expectedRecords = Lists.newArrayList();\n    expectedRecords.addAll(firstBatchRecords);\n    expectedRecords.addAll(secondBatchRecords);\n    Assert.assertEquals(\"Current snapshot rows should match\", expectedRecords, currentSnapshotRecords);\n\n    \r\n    Snapshot currentSnapshot = table.currentSnapshot();\n    Long parentSnapshotId = currentSnapshot.parentId();\n    Dataset<Row> previousSnapshotResult = spark.read()\n        .format(\"iceberg\")\n        .option(\"snapshot-id\", parentSnapshotId)\n        .load(tableLocation);\n    List<SimpleRecord> previousSnapshotRecords = previousSnapshotResult.orderBy(\"id\")\n        .as(Encoders.bean(SimpleRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Previous snapshot rows should match\", firstBatchRecords, previousSnapshotRecords);\n  }\n","date":"2019-06-11 01:36:27","endLine":122,"groupId":"3720","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testSnapshotSelectionById","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/cb/d9b1d310fa2b9edb740ba9e61f880ef8c8de2a.src","preCode":"  public void testSnapshotSelectionById() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    Table table = tables.create(SCHEMA, spec, tableLocation);\n\n    \r\n    List<SimpleRecord> firstBatchRecords = Lists.newArrayList(\n        new SimpleRecord(1, \"a\"),\n        new SimpleRecord(2, \"b\"),\n        new SimpleRecord(3, \"c\")\n    );\n    Dataset<Row> firstDf = spark.createDataFrame(firstBatchRecords, SimpleRecord.class);\n    firstDf.select(\"id\", \"data\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n\n    \r\n    List<SimpleRecord> secondBatchRecords = Lists.newArrayList(\n        new SimpleRecord(4, \"d\"),\n        new SimpleRecord(5, \"e\"),\n        new SimpleRecord(6, \"f\")\n    );\n    Dataset<Row> secondDf = spark.createDataFrame(secondBatchRecords, SimpleRecord.class);\n    secondDf.select(\"id\", \"data\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n\n    Assert.assertEquals(\"Expected 2 snapshots\", 2, Iterables.size(table.snapshots()));\n\n    \r\n    Dataset<Row> currentSnapshotResult = spark.read()\n        .format(\"iceberg\")\n        .load(tableLocation);\n    List<SimpleRecord> currentSnapshotRecords = currentSnapshotResult.orderBy(\"id\")\n        .as(Encoders.bean(SimpleRecord.class))\n        .collectAsList();\n    List<SimpleRecord> expectedRecords = Lists.newArrayList();\n    expectedRecords.addAll(firstBatchRecords);\n    expectedRecords.addAll(secondBatchRecords);\n    Assert.assertEquals(\"Current snapshot rows should match\", expectedRecords, currentSnapshotRecords);\n\n    \r\n    Snapshot currentSnapshot = table.currentSnapshot();\n    Long parentSnapshotId = currentSnapshot.parentId();\n    Dataset<Row> previousSnapshotResult = spark.read()\n        .format(\"iceberg\")\n        .option(\"snapshot-id\", parentSnapshotId)\n        .load(tableLocation);\n    List<SimpleRecord> previousSnapshotRecords = previousSnapshotResult.orderBy(\"id\")\n        .as(Encoders.bean(SimpleRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Previous snapshot rows should match\", firstBatchRecords, previousSnapshotRecords);\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":72,"status":"B"},{"authorDate":"2019-06-11 01:36:27","commitOrder":1,"curCode":"  public void testSnapshotSelectionByTimestamp() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    Table table = tables.create(SCHEMA, spec, tableLocation);\n\n    \r\n    List<SimpleRecord> firstBatchRecords = Lists.newArrayList(\n        new SimpleRecord(1, \"a\"),\n        new SimpleRecord(2, \"b\"),\n        new SimpleRecord(3, \"c\")\n    );\n    Dataset<Row> firstDf = spark.createDataFrame(firstBatchRecords, SimpleRecord.class);\n    firstDf.select(\"id\", \"data\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n\n    \r\n    long firstSnapshotTimestamp = System.currentTimeMillis();\n\n    \r\n    List<SimpleRecord> secondBatchRecords = Lists.newArrayList(\n        new SimpleRecord(4, \"d\"),\n        new SimpleRecord(5, \"e\"),\n        new SimpleRecord(6, \"f\")\n    );\n    Dataset<Row> secondDf = spark.createDataFrame(secondBatchRecords, SimpleRecord.class);\n    secondDf.select(\"id\", \"data\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n\n    Assert.assertEquals(\"Expected 2 snapshots\", 2, Iterables.size(table.snapshots()));\n\n    \r\n    Dataset<Row> currentSnapshotResult = spark.read()\n        .format(\"iceberg\")\n        .load(tableLocation);\n    List<SimpleRecord> currentSnapshotRecords = currentSnapshotResult.orderBy(\"id\")\n        .as(Encoders.bean(SimpleRecord.class))\n        .collectAsList();\n    List<SimpleRecord> expectedRecords = Lists.newArrayList();\n    expectedRecords.addAll(firstBatchRecords);\n    expectedRecords.addAll(secondBatchRecords);\n    Assert.assertEquals(\"Current snapshot rows should match\", expectedRecords, currentSnapshotRecords);\n\n    \r\n    Dataset<Row> previousSnapshotResult = spark.read()\n        .format(\"iceberg\")\n        .option(\"as-of-timestamp\", firstSnapshotTimestamp)\n        .load(tableLocation);\n    List<SimpleRecord> previousSnapshotRecords = previousSnapshotResult.orderBy(\"id\")\n        .as(Encoders.bean(SimpleRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Previous snapshot rows should match\", firstBatchRecords, previousSnapshotRecords);\n  }\n","date":"2019-06-11 01:36:27","endLine":176,"groupId":"3720","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testSnapshotSelectionByTimestamp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/cb/d9b1d310fa2b9edb740ba9e61f880ef8c8de2a.src","preCode":"  public void testSnapshotSelectionByTimestamp() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    Table table = tables.create(SCHEMA, spec, tableLocation);\n\n    \r\n    List<SimpleRecord> firstBatchRecords = Lists.newArrayList(\n        new SimpleRecord(1, \"a\"),\n        new SimpleRecord(2, \"b\"),\n        new SimpleRecord(3, \"c\")\n    );\n    Dataset<Row> firstDf = spark.createDataFrame(firstBatchRecords, SimpleRecord.class);\n    firstDf.select(\"id\", \"data\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n\n    \r\n    long firstSnapshotTimestamp = System.currentTimeMillis();\n\n    \r\n    List<SimpleRecord> secondBatchRecords = Lists.newArrayList(\n        new SimpleRecord(4, \"d\"),\n        new SimpleRecord(5, \"e\"),\n        new SimpleRecord(6, \"f\")\n    );\n    Dataset<Row> secondDf = spark.createDataFrame(secondBatchRecords, SimpleRecord.class);\n    secondDf.select(\"id\", \"data\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n\n    Assert.assertEquals(\"Expected 2 snapshots\", 2, Iterables.size(table.snapshots()));\n\n    \r\n    Dataset<Row> currentSnapshotResult = spark.read()\n        .format(\"iceberg\")\n        .load(tableLocation);\n    List<SimpleRecord> currentSnapshotRecords = currentSnapshotResult.orderBy(\"id\")\n        .as(Encoders.bean(SimpleRecord.class))\n        .collectAsList();\n    List<SimpleRecord> expectedRecords = Lists.newArrayList();\n    expectedRecords.addAll(firstBatchRecords);\n    expectedRecords.addAll(secondBatchRecords);\n    Assert.assertEquals(\"Current snapshot rows should match\", expectedRecords, currentSnapshotRecords);\n\n    \r\n    Dataset<Row> previousSnapshotResult = spark.read()\n        .format(\"iceberg\")\n        .option(\"as-of-timestamp\", firstSnapshotTimestamp)\n        .load(tableLocation);\n    List<SimpleRecord> previousSnapshotRecords = previousSnapshotResult.orderBy(\"id\")\n        .as(Encoders.bean(SimpleRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Previous snapshot rows should match\", firstBatchRecords, previousSnapshotRecords);\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":125,"status":"B"}],"commitId":"d687f96beb4e0b523ae86d814a720fdbc0299b4b","commitMessage":"@@@Spark: Add snapshot selection options to reads (#61)\n\n","date":"2019-06-11 01:36:27","modifiedFileCount":"2","status":"B","submitter":"Ryan Blue"},{"authorTime":"2020-12-29 05:55:09","codes":[{"authorDate":"2019-06-11 01:36:27","commitOrder":2,"curCode":"  public void testSnapshotSelectionById() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    Table table = tables.create(SCHEMA, spec, tableLocation);\n\n    \r\n    List<SimpleRecord> firstBatchRecords = Lists.newArrayList(\n        new SimpleRecord(1, \"a\"),\n        new SimpleRecord(2, \"b\"),\n        new SimpleRecord(3, \"c\")\n    );\n    Dataset<Row> firstDf = spark.createDataFrame(firstBatchRecords, SimpleRecord.class);\n    firstDf.select(\"id\", \"data\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n\n    \r\n    List<SimpleRecord> secondBatchRecords = Lists.newArrayList(\n        new SimpleRecord(4, \"d\"),\n        new SimpleRecord(5, \"e\"),\n        new SimpleRecord(6, \"f\")\n    );\n    Dataset<Row> secondDf = spark.createDataFrame(secondBatchRecords, SimpleRecord.class);\n    secondDf.select(\"id\", \"data\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n\n    Assert.assertEquals(\"Expected 2 snapshots\", 2, Iterables.size(table.snapshots()));\n\n    \r\n    Dataset<Row> currentSnapshotResult = spark.read()\n        .format(\"iceberg\")\n        .load(tableLocation);\n    List<SimpleRecord> currentSnapshotRecords = currentSnapshotResult.orderBy(\"id\")\n        .as(Encoders.bean(SimpleRecord.class))\n        .collectAsList();\n    List<SimpleRecord> expectedRecords = Lists.newArrayList();\n    expectedRecords.addAll(firstBatchRecords);\n    expectedRecords.addAll(secondBatchRecords);\n    Assert.assertEquals(\"Current snapshot rows should match\", expectedRecords, currentSnapshotRecords);\n\n    \r\n    Snapshot currentSnapshot = table.currentSnapshot();\n    Long parentSnapshotId = currentSnapshot.parentId();\n    Dataset<Row> previousSnapshotResult = spark.read()\n        .format(\"iceberg\")\n        .option(\"snapshot-id\", parentSnapshotId)\n        .load(tableLocation);\n    List<SimpleRecord> previousSnapshotRecords = previousSnapshotResult.orderBy(\"id\")\n        .as(Encoders.bean(SimpleRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Previous snapshot rows should match\", firstBatchRecords, previousSnapshotRecords);\n  }\n","date":"2019-06-11 01:36:27","endLine":122,"groupId":"10371","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testSnapshotSelectionById","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/cb/d9b1d310fa2b9edb740ba9e61f880ef8c8de2a.src","preCode":"  public void testSnapshotSelectionById() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    Table table = tables.create(SCHEMA, spec, tableLocation);\n\n    \r\n    List<SimpleRecord> firstBatchRecords = Lists.newArrayList(\n        new SimpleRecord(1, \"a\"),\n        new SimpleRecord(2, \"b\"),\n        new SimpleRecord(3, \"c\")\n    );\n    Dataset<Row> firstDf = spark.createDataFrame(firstBatchRecords, SimpleRecord.class);\n    firstDf.select(\"id\", \"data\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n\n    \r\n    List<SimpleRecord> secondBatchRecords = Lists.newArrayList(\n        new SimpleRecord(4, \"d\"),\n        new SimpleRecord(5, \"e\"),\n        new SimpleRecord(6, \"f\")\n    );\n    Dataset<Row> secondDf = spark.createDataFrame(secondBatchRecords, SimpleRecord.class);\n    secondDf.select(\"id\", \"data\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n\n    Assert.assertEquals(\"Expected 2 snapshots\", 2, Iterables.size(table.snapshots()));\n\n    \r\n    Dataset<Row> currentSnapshotResult = spark.read()\n        .format(\"iceberg\")\n        .load(tableLocation);\n    List<SimpleRecord> currentSnapshotRecords = currentSnapshotResult.orderBy(\"id\")\n        .as(Encoders.bean(SimpleRecord.class))\n        .collectAsList();\n    List<SimpleRecord> expectedRecords = Lists.newArrayList();\n    expectedRecords.addAll(firstBatchRecords);\n    expectedRecords.addAll(secondBatchRecords);\n    Assert.assertEquals(\"Current snapshot rows should match\", expectedRecords, currentSnapshotRecords);\n\n    \r\n    Snapshot currentSnapshot = table.currentSnapshot();\n    Long parentSnapshotId = currentSnapshot.parentId();\n    Dataset<Row> previousSnapshotResult = spark.read()\n        .format(\"iceberg\")\n        .option(\"snapshot-id\", parentSnapshotId)\n        .load(tableLocation);\n    List<SimpleRecord> previousSnapshotRecords = previousSnapshotResult.orderBy(\"id\")\n        .as(Encoders.bean(SimpleRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Previous snapshot rows should match\", firstBatchRecords, previousSnapshotRecords);\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":72,"status":"N"},{"authorDate":"2020-12-29 05:55:09","commitOrder":2,"curCode":"  public void testSnapshotSelectionByTimestamp() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    Table table = tables.create(SCHEMA, spec, tableLocation);\n\n    \r\n    List<SimpleRecord> firstBatchRecords = Lists.newArrayList(\n        new SimpleRecord(1, \"a\"),\n        new SimpleRecord(2, \"b\"),\n        new SimpleRecord(3, \"c\")\n    );\n    Dataset<Row> firstDf = spark.createDataFrame(firstBatchRecords, SimpleRecord.class);\n    firstDf.select(\"id\", \"data\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n\n    \r\n    long firstSnapshotTimestamp = System.currentTimeMillis();\n\n    \r\n    List<SimpleRecord> secondBatchRecords = Lists.newArrayList(\n        new SimpleRecord(4, \"d\"),\n        new SimpleRecord(5, \"e\"),\n        new SimpleRecord(6, \"f\")\n    );\n    Dataset<Row> secondDf = spark.createDataFrame(secondBatchRecords, SimpleRecord.class);\n    secondDf.select(\"id\", \"data\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n\n    Assert.assertEquals(\"Expected 2 snapshots\", 2, Iterables.size(table.snapshots()));\n\n    \r\n    Dataset<Row> currentSnapshotResult = spark.read()\n        .format(\"iceberg\")\n        .load(tableLocation);\n    List<SimpleRecord> currentSnapshotRecords = currentSnapshotResult.orderBy(\"id\")\n        .as(Encoders.bean(SimpleRecord.class))\n        .collectAsList();\n    List<SimpleRecord> expectedRecords = Lists.newArrayList();\n    expectedRecords.addAll(firstBatchRecords);\n    expectedRecords.addAll(secondBatchRecords);\n    Assert.assertEquals(\"Current snapshot rows should match\", expectedRecords, currentSnapshotRecords);\n\n    \r\n    Dataset<Row> previousSnapshotResult = spark.read()\n        .format(\"iceberg\")\n        .option(SparkReadOptions.AS_OF_TIMESTAMP, firstSnapshotTimestamp)\n        .load(tableLocation);\n    List<SimpleRecord> previousSnapshotRecords = previousSnapshotResult.orderBy(\"id\")\n        .as(Encoders.bean(SimpleRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Previous snapshot rows should match\", firstBatchRecords, previousSnapshotRecords);\n  }\n","date":"2020-12-29 05:55:09","endLine":177,"groupId":"10371","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"testSnapshotSelectionByTimestamp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/8a/adfb858ca71aa6df1cf7f902674d98382aed05.src","preCode":"  public void testSnapshotSelectionByTimestamp() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    Table table = tables.create(SCHEMA, spec, tableLocation);\n\n    \r\n    List<SimpleRecord> firstBatchRecords = Lists.newArrayList(\n        new SimpleRecord(1, \"a\"),\n        new SimpleRecord(2, \"b\"),\n        new SimpleRecord(3, \"c\")\n    );\n    Dataset<Row> firstDf = spark.createDataFrame(firstBatchRecords, SimpleRecord.class);\n    firstDf.select(\"id\", \"data\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n\n    \r\n    long firstSnapshotTimestamp = System.currentTimeMillis();\n\n    \r\n    List<SimpleRecord> secondBatchRecords = Lists.newArrayList(\n        new SimpleRecord(4, \"d\"),\n        new SimpleRecord(5, \"e\"),\n        new SimpleRecord(6, \"f\")\n    );\n    Dataset<Row> secondDf = spark.createDataFrame(secondBatchRecords, SimpleRecord.class);\n    secondDf.select(\"id\", \"data\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n\n    Assert.assertEquals(\"Expected 2 snapshots\", 2, Iterables.size(table.snapshots()));\n\n    \r\n    Dataset<Row> currentSnapshotResult = spark.read()\n        .format(\"iceberg\")\n        .load(tableLocation);\n    List<SimpleRecord> currentSnapshotRecords = currentSnapshotResult.orderBy(\"id\")\n        .as(Encoders.bean(SimpleRecord.class))\n        .collectAsList();\n    List<SimpleRecord> expectedRecords = Lists.newArrayList();\n    expectedRecords.addAll(firstBatchRecords);\n    expectedRecords.addAll(secondBatchRecords);\n    Assert.assertEquals(\"Current snapshot rows should match\", expectedRecords, currentSnapshotRecords);\n\n    \r\n    Dataset<Row> previousSnapshotResult = spark.read()\n        .format(\"iceberg\")\n        .option(\"as-of-timestamp\", firstSnapshotTimestamp)\n        .load(tableLocation);\n    List<SimpleRecord> previousSnapshotRecords = previousSnapshotResult.orderBy(\"id\")\n        .as(Encoders.bean(SimpleRecord.class))\n        .collectAsList();\n    Assert.assertEquals(\"Previous snapshot rows should match\", firstBatchRecords, previousSnapshotRecords);\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":126,"status":"M"}],"commitId":"cbb244759ae218b85a7e68a1568fddaf815aab3c","commitMessage":"@@@Spark: Use constants for DF read and write options (#1933)\n\n","date":"2020-12-29 05:55:09","modifiedFileCount":"22","status":"M","submitter":"Karuppayya"}]
