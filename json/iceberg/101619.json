[{"authorTime":"2020-12-11 02:34:51","codes":[{"authorDate":"2020-12-11 02:34:51","commitOrder":1,"curCode":"  public void commitJob(JobContext jobContext) throws IOException {\n    JobConf conf = jobContext.getJobConf();\n    Table table = Catalogs.loadTable(conf);\n\n    long startTime = System.currentTimeMillis();\n    LOG.info(\"Committing job has started for table: {}, using location: {}\", table,\n        generateJobLocation(conf, jobContext.getJobID()));\n\n    FileIO io = HiveIcebergStorageHandler.io(jobContext.getJobConf());\n    List<DataFile> dataFiles = dataFiles(jobContext, io, true);\n\n    if (dataFiles.size() > 0) {\n      \r\n      AppendFiles append = table.newAppend();\n      dataFiles.forEach(append::appendFile);\n      append.commit();\n      LOG.info(\"Commit took {} ms for table: {} with {} file(s)\", System.currentTimeMillis() - startTime, table,\n          dataFiles.size());\n      LOG.debug(\"Added files {}\", dataFiles);\n    } else {\n      LOG.info(\"Commit took {} ms for table: {} with no new files\", System.currentTimeMillis() - startTime, table);\n    }\n\n    cleanup(jobContext);\n  }\n","date":"2020-12-11 02:34:51","endLine":151,"groupId":"2271","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"commitJob","params":"(JobContextjobContext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/7c/34f5ad8feb3debb9e501737a1e158471d1653e.src","preCode":"  public void commitJob(JobContext jobContext) throws IOException {\n    JobConf conf = jobContext.getJobConf();\n    Table table = Catalogs.loadTable(conf);\n\n    long startTime = System.currentTimeMillis();\n    LOG.info(\"Committing job has started for table: {}, using location: {}\", table,\n        generateJobLocation(conf, jobContext.getJobID()));\n\n    FileIO io = HiveIcebergStorageHandler.io(jobContext.getJobConf());\n    List<DataFile> dataFiles = dataFiles(jobContext, io, true);\n\n    if (dataFiles.size() > 0) {\n      \r\n      AppendFiles append = table.newAppend();\n      dataFiles.forEach(append::appendFile);\n      append.commit();\n      LOG.info(\"Commit took {} ms for table: {} with {} file(s)\", System.currentTimeMillis() - startTime, table,\n          dataFiles.size());\n      LOG.debug(\"Added files {}\", dataFiles);\n    } else {\n      LOG.info(\"Commit took {} ms for table: {} with no new files\", System.currentTimeMillis() - startTime, table);\n    }\n\n    cleanup(jobContext);\n  }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":127,"status":"B"},{"authorDate":"2020-12-11 02:34:51","commitOrder":1,"curCode":"  public void abortJob(JobContext jobContext, int status) throws IOException {\n    String location = generateJobLocation(jobContext.getJobConf(), jobContext.getJobID());\n    LOG.info(\"Job {} is aborted. Cleaning job location {}\", jobContext.getJobID(), location);\n\n    FileIO io = HiveIcebergStorageHandler.io(jobContext.getJobConf());\n    List<DataFile> dataFiles = dataFiles(jobContext, io, false);\n\n    \r\n    if (dataFiles.size() > 0) {\n      Tasks.foreach(dataFiles)\n          .retry(3)\n          .suppressFailureWhenFinished()\n          .onFailure((file, exc) -> LOG.debug(\"Failed on to remove data file {} on abort job\", file.path(), exc))\n          .run(file -> io.deleteFile(file.path().toString()));\n    }\n\n    cleanup(jobContext);\n  }\n","date":"2020-12-11 02:34:51","endLine":178,"groupId":"2272","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"abortJob","params":"(JobContextjobContext@intstatus)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/7c/34f5ad8feb3debb9e501737a1e158471d1653e.src","preCode":"  public void abortJob(JobContext jobContext, int status) throws IOException {\n    String location = generateJobLocation(jobContext.getJobConf(), jobContext.getJobID());\n    LOG.info(\"Job {} is aborted. Cleaning job location {}\", jobContext.getJobID(), location);\n\n    FileIO io = HiveIcebergStorageHandler.io(jobContext.getJobConf());\n    List<DataFile> dataFiles = dataFiles(jobContext, io, false);\n\n    \r\n    if (dataFiles.size() > 0) {\n      Tasks.foreach(dataFiles)\n          .retry(3)\n          .suppressFailureWhenFinished()\n          .onFailure((file, exc) -> LOG.debug(\"Failed on to remove data file {} on abort job\", file.path(), exc))\n          .run(file -> io.deleteFile(file.path().toString()));\n    }\n\n    cleanup(jobContext);\n  }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":161,"status":"B"}],"commitId":"05aa9011cb2b6c0e24427be652bb825d5f8488ac","commitMessage":"@@@Hive: Add OutputCommitter implementation (#1861)\n\n","date":"2020-12-11 02:34:51","modifiedFileCount":"4","status":"B","submitter":"pvary"},{"authorTime":"2021-02-05 06:34:05","codes":[{"authorDate":"2021-02-05 06:34:05","commitOrder":2,"curCode":"  public void commitJob(JobContext originalContext) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n\n    JobConf conf = jobContext.getJobConf();\n    Table table = Catalogs.loadTable(conf);\n\n    long startTime = System.currentTimeMillis();\n    LOG.info(\"Committing job has started for table: {}, using location: {}\", table,\n        generateJobLocation(conf, jobContext.getJobID()));\n\n    FileIO io = HiveIcebergStorageHandler.io(jobContext.getJobConf());\n    List<DataFile> dataFiles = dataFiles(jobContext, io, true);\n\n    if (dataFiles.size() > 0) {\n      \r\n      AppendFiles append = table.newAppend();\n      dataFiles.forEach(append::appendFile);\n      append.commit();\n      LOG.info(\"Commit took {} ms for table: {} with {} file(s)\", System.currentTimeMillis() - startTime, table,\n          dataFiles.size());\n      LOG.debug(\"Added files {}\", dataFiles);\n    } else {\n      LOG.info(\"Commit took {} ms for table: {} with no new files\", System.currentTimeMillis() - startTime, table);\n    }\n\n    cleanup(jobContext);\n  }\n","date":"2021-02-05 06:34:05","endLine":162,"groupId":"2271","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"commitJob","params":"(JobContextoriginalContext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/47/cbf7db85b2cb0e5a5f9fcf267dc4e0f07888e6.src","preCode":"  public void commitJob(JobContext jobContext) throws IOException {\n    JobConf conf = jobContext.getJobConf();\n    Table table = Catalogs.loadTable(conf);\n\n    long startTime = System.currentTimeMillis();\n    LOG.info(\"Committing job has started for table: {}, using location: {}\", table,\n        generateJobLocation(conf, jobContext.getJobID()));\n\n    FileIO io = HiveIcebergStorageHandler.io(jobContext.getJobConf());\n    List<DataFile> dataFiles = dataFiles(jobContext, io, true);\n\n    if (dataFiles.size() > 0) {\n      \r\n      AppendFiles append = table.newAppend();\n      dataFiles.forEach(append::appendFile);\n      append.commit();\n      LOG.info(\"Commit took {} ms for table: {} with {} file(s)\", System.currentTimeMillis() - startTime, table,\n          dataFiles.size());\n      LOG.debug(\"Added files {}\", dataFiles);\n    } else {\n      LOG.info(\"Commit took {} ms for table: {} with no new files\", System.currentTimeMillis() - startTime, table);\n    }\n\n    cleanup(jobContext);\n  }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":136,"status":"M"},{"authorDate":"2021-02-05 06:34:05","commitOrder":2,"curCode":"  public void abortJob(JobContext originalContext, int status) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n\n    String location = generateJobLocation(jobContext.getJobConf(), jobContext.getJobID());\n    LOG.info(\"Job {} is aborted. Cleaning job location {}\", jobContext.getJobID(), location);\n\n    FileIO io = HiveIcebergStorageHandler.io(jobContext.getJobConf());\n    List<DataFile> dataFiles = dataFiles(jobContext, io, false);\n\n    \r\n    if (dataFiles.size() > 0) {\n      Tasks.foreach(dataFiles)\n          .retry(3)\n          .suppressFailureWhenFinished()\n          .onFailure((file, exc) -> LOG.debug(\"Failed on to remove data file {} on abort job\", file.path(), exc))\n          .run(file -> io.deleteFile(file.path().toString()));\n    }\n\n    cleanup(jobContext);\n  }\n","date":"2021-02-05 06:34:05","endLine":191,"groupId":"2272","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"abortJob","params":"(JobContextoriginalContext@intstatus)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/47/cbf7db85b2cb0e5a5f9fcf267dc4e0f07888e6.src","preCode":"  public void abortJob(JobContext jobContext, int status) throws IOException {\n    String location = generateJobLocation(jobContext.getJobConf(), jobContext.getJobID());\n    LOG.info(\"Job {} is aborted. Cleaning job location {}\", jobContext.getJobID(), location);\n\n    FileIO io = HiveIcebergStorageHandler.io(jobContext.getJobConf());\n    List<DataFile> dataFiles = dataFiles(jobContext, io, false);\n\n    \r\n    if (dataFiles.size() > 0) {\n      Tasks.foreach(dataFiles)\n          .retry(3)\n          .suppressFailureWhenFinished()\n          .onFailure((file, exc) -> LOG.debug(\"Failed on to remove data file {} on abort job\", file.path(), exc))\n          .run(file -> io.deleteFile(file.path().toString()));\n    }\n\n    cleanup(jobContext);\n  }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":172,"status":"M"}],"commitId":"3ab15bf15c22f5d11dee6b2126b2e72bd99a4b3d","commitMessage":"@@@Hive: Update Hive write path for Tez (#2163)\n\nCo-authored-by: Marton Bod <mbod@cloudera.com>","date":"2021-02-05 06:34:05","modifiedFileCount":"4","status":"M","submitter":"Marton Bod"},{"authorTime":"2021-03-22 19:46:21","codes":[{"authorDate":"2021-03-22 19:46:21","commitOrder":3,"curCode":"  public void commitJob(JobContext originalContext) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n\n    JobConf conf = jobContext.getJobConf();\n    Table table = Catalogs.loadTable(conf);\n\n    long startTime = System.currentTimeMillis();\n    LOG.info(\"Committing job has started for table: {}, using location: {}\", table,\n        generateJobLocation(conf, jobContext.getJobID()));\n\n    FileIO io = HiveIcebergStorageHandler.table(jobContext.getJobConf()).io();\n    List<DataFile> dataFiles = dataFiles(jobContext, io, true);\n\n    if (dataFiles.size() > 0) {\n      \r\n      AppendFiles append = table.newAppend();\n      dataFiles.forEach(append::appendFile);\n      append.commit();\n      LOG.info(\"Commit took {} ms for table: {} with {} file(s)\", System.currentTimeMillis() - startTime, table,\n          dataFiles.size());\n      LOG.debug(\"Added files {}\", dataFiles);\n    } else {\n      LOG.info(\"Commit took {} ms for table: {} with no new files\", System.currentTimeMillis() - startTime, table);\n    }\n\n    cleanup(jobContext);\n  }\n","date":"2021-03-22 19:46:21","endLine":162,"groupId":"2271","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"commitJob","params":"(JobContextoriginalContext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/d5/5a142365dfaef1af4edb3a8a91ce4447f15a7d.src","preCode":"  public void commitJob(JobContext originalContext) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n\n    JobConf conf = jobContext.getJobConf();\n    Table table = Catalogs.loadTable(conf);\n\n    long startTime = System.currentTimeMillis();\n    LOG.info(\"Committing job has started for table: {}, using location: {}\", table,\n        generateJobLocation(conf, jobContext.getJobID()));\n\n    FileIO io = HiveIcebergStorageHandler.io(jobContext.getJobConf());\n    List<DataFile> dataFiles = dataFiles(jobContext, io, true);\n\n    if (dataFiles.size() > 0) {\n      \r\n      AppendFiles append = table.newAppend();\n      dataFiles.forEach(append::appendFile);\n      append.commit();\n      LOG.info(\"Commit took {} ms for table: {} with {} file(s)\", System.currentTimeMillis() - startTime, table,\n          dataFiles.size());\n      LOG.debug(\"Added files {}\", dataFiles);\n    } else {\n      LOG.info(\"Commit took {} ms for table: {} with no new files\", System.currentTimeMillis() - startTime, table);\n    }\n\n    cleanup(jobContext);\n  }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":136,"status":"M"},{"authorDate":"2021-03-22 19:46:21","commitOrder":3,"curCode":"  public void abortJob(JobContext originalContext, int status) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n\n    String location = generateJobLocation(jobContext.getJobConf(), jobContext.getJobID());\n    LOG.info(\"Job {} is aborted. Cleaning job location {}\", jobContext.getJobID(), location);\n\n    FileIO io = HiveIcebergStorageHandler.table(jobContext.getJobConf()).io();\n    List<DataFile> dataFiles = dataFiles(jobContext, io, false);\n\n    \r\n    if (dataFiles.size() > 0) {\n      Tasks.foreach(dataFiles)\n          .retry(3)\n          .suppressFailureWhenFinished()\n          .onFailure((file, exc) -> LOG.debug(\"Failed on to remove data file {} on abort job\", file.path(), exc))\n          .run(file -> io.deleteFile(file.path().toString()));\n    }\n\n    cleanup(jobContext);\n  }\n","date":"2021-03-22 19:46:21","endLine":191,"groupId":"2272","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"abortJob","params":"(JobContextoriginalContext@intstatus)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/d5/5a142365dfaef1af4edb3a8a91ce4447f15a7d.src","preCode":"  public void abortJob(JobContext originalContext, int status) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n\n    String location = generateJobLocation(jobContext.getJobConf(), jobContext.getJobID());\n    LOG.info(\"Job {} is aborted. Cleaning job location {}\", jobContext.getJobID(), location);\n\n    FileIO io = HiveIcebergStorageHandler.io(jobContext.getJobConf());\n    List<DataFile> dataFiles = dataFiles(jobContext, io, false);\n\n    \r\n    if (dataFiles.size() > 0) {\n      Tasks.foreach(dataFiles)\n          .retry(3)\n          .suppressFailureWhenFinished()\n          .onFailure((file, exc) -> LOG.debug(\"Failed on to remove data file {} on abort job\", file.path(), exc))\n          .run(file -> io.deleteFile(file.path().toString()));\n    }\n\n    cleanup(jobContext);\n  }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":172,"status":"M"}],"commitId":"05d39c5dd09d604336ab21c34e78553e5a6b13ef","commitMessage":"@@@Hive: Remove unnecessary SerializationUtil.serializeToBase64 from HiveIcebergStorageHandler.overlayTableProperties (#2340)\n\n","date":"2021-03-22 19:46:21","modifiedFileCount":"8","status":"M","submitter":"L?szl? Pint?r"},{"authorTime":"2021-03-30 20:07:23","codes":[{"authorDate":"2021-03-30 20:07:23","commitOrder":4,"curCode":"  public void commitJob(JobContext originalContext) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n    JobConf jobConf = jobContext.getJobConf();\n\n    long startTime = System.currentTimeMillis();\n    LOG.info(\"Committing job {} has started\", jobContext.getJobID());\n\n    Collection<String> outputs = HiveIcebergStorageHandler.outputTables(jobContext.getJobConf());\n    Collection<String> jobLocations = new ConcurrentLinkedQueue<>();\n\n    ExecutorService fileExecutor = fileExecutor(jobConf);\n    ExecutorService tableExecutor = tableExecutor(jobConf, outputs.size());\n    try {\n      \r\n      Tasks.foreach(outputs)\n          .throwFailureWhenFinished()\n          .stopOnFailure()\n          .executeWith(tableExecutor)\n          .run(output -> {\n            Table table = HiveIcebergStorageHandler.table(jobConf, output);\n            jobLocations.add(generateJobLocation(table.location(), jobConf, jobContext.getJobID()));\n            commitTable(table.io(), fileExecutor, jobContext, output, table.location());\n          });\n    } finally {\n      fileExecutor.shutdown();\n      if (tableExecutor != null) {\n        tableExecutor.shutdown();\n      }\n    }\n\n    LOG.info(\"Commit took {} ms for job {}\", System.currentTimeMillis() - startTime, jobContext.getJobID());\n\n    cleanup(jobContext, jobLocations);\n  }\n","date":"2021-03-30 20:07:23","endLine":186,"groupId":"0","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"commitJob","params":"(JobContextoriginalContext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/34/79b362a82c3d41448f175ac4cb68c6bccaf96f.src","preCode":"  public void commitJob(JobContext originalContext) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n\n    JobConf conf = jobContext.getJobConf();\n    Table table = Catalogs.loadTable(conf);\n\n    long startTime = System.currentTimeMillis();\n    LOG.info(\"Committing job has started for table: {}, using location: {}\", table,\n        generateJobLocation(conf, jobContext.getJobID()));\n\n    FileIO io = HiveIcebergStorageHandler.table(jobContext.getJobConf()).io();\n    List<DataFile> dataFiles = dataFiles(jobContext, io, true);\n\n    if (dataFiles.size() > 0) {\n      \r\n      AppendFiles append = table.newAppend();\n      dataFiles.forEach(append::appendFile);\n      append.commit();\n      LOG.info(\"Commit took {} ms for table: {} with {} file(s)\", System.currentTimeMillis() - startTime, table,\n          dataFiles.size());\n      LOG.debug(\"Added files {}\", dataFiles);\n    } else {\n      LOG.info(\"Commit took {} ms for table: {} with no new files\", System.currentTimeMillis() - startTime, table);\n    }\n\n    cleanup(jobContext);\n  }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":153,"status":"M"},{"authorDate":"2021-03-30 20:07:23","commitOrder":4,"curCode":"  public void abortJob(JobContext originalContext, int status) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n    JobConf jobConf = jobContext.getJobConf();\n\n    LOG.info(\"Job {} is aborted. Data file cleaning started\", jobContext.getJobID());\n    Collection<String> outputs = HiveIcebergStorageHandler.outputTables(jobContext.getJobConf());\n    Collection<String> jobLocations = new ConcurrentLinkedQueue<>();\n\n    ExecutorService fileExecutor = fileExecutor(jobConf);\n    ExecutorService tableExecutor = tableExecutor(jobConf, outputs.size());\n    try {\n      \r\n      Tasks.foreach(outputs)\n          .suppressFailureWhenFinished()\n          .executeWith(tableExecutor)\n          .onFailure((output, exc) -> LOG.warn(\"Failed cleanup table {} on abort job\", output, exc))\n          .run(output -> {\n            LOG.info(\"Cleaning job for table {}\", jobContext.getJobID(), output);\n\n            Table table = HiveIcebergStorageHandler.table(jobConf, output);\n            jobLocations.add(generateJobLocation(table.location(), jobConf, jobContext.getJobID()));\n            Collection<DataFile> dataFiles = dataFiles(fileExecutor, table.location(), jobContext, table.io(), false);\n\n            \r\n            if (dataFiles.size() > 0) {\n              Tasks.foreach(dataFiles)\n                  .retry(3)\n                  .suppressFailureWhenFinished()\n                  .executeWith(fileExecutor)\n                  .onFailure((file, exc) -> LOG.warn(\"Failed to remove data file {} on abort job\", file.path(), exc))\n                  .run(file -> table.io().deleteFile(file.path().toString()));\n            }\n          });\n    } finally {\n      fileExecutor.shutdown();\n      if (tableExecutor != null) {\n        tableExecutor.shutdown();\n      }\n    }\n\n    LOG.info(\"Job {} is aborted. Data file cleaning finished\", jobContext.getJobID());\n\n    cleanup(jobContext, jobLocations);\n  }\n","date":"2021-03-30 20:07:23","endLine":239,"groupId":"1920","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"abortJob","params":"(JobContextoriginalContext@intstatus)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/34/79b362a82c3d41448f175ac4cb68c6bccaf96f.src","preCode":"  public void abortJob(JobContext originalContext, int status) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n\n    String location = generateJobLocation(jobContext.getJobConf(), jobContext.getJobID());\n    LOG.info(\"Job {} is aborted. Cleaning job location {}\", jobContext.getJobID(), location);\n\n    FileIO io = HiveIcebergStorageHandler.table(jobContext.getJobConf()).io();\n    List<DataFile> dataFiles = dataFiles(jobContext, io, false);\n\n    \r\n    if (dataFiles.size() > 0) {\n      Tasks.foreach(dataFiles)\n          .retry(3)\n          .suppressFailureWhenFinished()\n          .onFailure((file, exc) -> LOG.debug(\"Failed on to remove data file {} on abort job\", file.path(), exc))\n          .run(file -> io.deleteFile(file.path().toString()));\n    }\n\n    cleanup(jobContext);\n  }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":196,"status":"M"}],"commitId":"d1510340eaff68d88a2e8194d58e7e493af02bcc","commitMessage":"@@@Hive: Implement multi-table inserts (#2228)\n\n","date":"2021-03-30 20:07:23","modifiedFileCount":"9","status":"M","submitter":"pvary"},{"authorTime":"2021-03-30 20:07:23","codes":[{"authorDate":"2021-04-12 15:36:22","commitOrder":5,"curCode":"  public void commitJob(JobContext originalContext) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n    JobConf jobConf = jobContext.getJobConf();\n\n    long startTime = System.currentTimeMillis();\n    LOG.info(\"Committing job {} has started\", jobContext.getJobID());\n\n    Collection<String> outputs = HiveIcebergStorageHandler.outputTables(jobContext.getJobConf());\n    Collection<String> jobLocations = new ConcurrentLinkedQueue<>();\n\n    ExecutorService fileExecutor = fileExecutor(jobConf);\n    ExecutorService tableExecutor = tableExecutor(jobConf, outputs.size());\n    try {\n      \r\n      Tasks.foreach(outputs)\n          .throwFailureWhenFinished()\n          .stopOnFailure()\n          .executeWith(tableExecutor)\n          .run(output -> {\n            Table table = HiveIcebergStorageHandler.table(jobConf, output);\n            String catalogName = HiveIcebergStorageHandler.catalogName(jobConf, output);\n            jobLocations.add(generateJobLocation(table.location(), jobConf, jobContext.getJobID()));\n            commitTable(table.io(), fileExecutor, jobContext, output, table.location(), catalogName);\n          });\n    } finally {\n      fileExecutor.shutdown();\n      if (tableExecutor != null) {\n        tableExecutor.shutdown();\n      }\n    }\n\n    LOG.info(\"Commit took {} ms for job {}\", System.currentTimeMillis() - startTime, jobContext.getJobID());\n\n    cleanup(jobContext, jobLocations);\n  }\n","date":"2021-04-12 15:36:22","endLine":187,"groupId":"1919","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"commitJob","params":"(JobContextoriginalContext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/bf/298ad35bed2520bbf65cc0fb3a349857ec5428.src","preCode":"  public void commitJob(JobContext originalContext) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n    JobConf jobConf = jobContext.getJobConf();\n\n    long startTime = System.currentTimeMillis();\n    LOG.info(\"Committing job {} has started\", jobContext.getJobID());\n\n    Collection<String> outputs = HiveIcebergStorageHandler.outputTables(jobContext.getJobConf());\n    Collection<String> jobLocations = new ConcurrentLinkedQueue<>();\n\n    ExecutorService fileExecutor = fileExecutor(jobConf);\n    ExecutorService tableExecutor = tableExecutor(jobConf, outputs.size());\n    try {\n      \r\n      Tasks.foreach(outputs)\n          .throwFailureWhenFinished()\n          .stopOnFailure()\n          .executeWith(tableExecutor)\n          .run(output -> {\n            Table table = HiveIcebergStorageHandler.table(jobConf, output);\n            jobLocations.add(generateJobLocation(table.location(), jobConf, jobContext.getJobID()));\n            commitTable(table.io(), fileExecutor, jobContext, output, table.location());\n          });\n    } finally {\n      fileExecutor.shutdown();\n      if (tableExecutor != null) {\n        tableExecutor.shutdown();\n      }\n    }\n\n    LOG.info(\"Commit took {} ms for job {}\", System.currentTimeMillis() - startTime, jobContext.getJobID());\n\n    cleanup(jobContext, jobLocations);\n  }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":153,"status":"M"},{"authorDate":"2021-03-30 20:07:23","commitOrder":5,"curCode":"  public void abortJob(JobContext originalContext, int status) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n    JobConf jobConf = jobContext.getJobConf();\n\n    LOG.info(\"Job {} is aborted. Data file cleaning started\", jobContext.getJobID());\n    Collection<String> outputs = HiveIcebergStorageHandler.outputTables(jobContext.getJobConf());\n    Collection<String> jobLocations = new ConcurrentLinkedQueue<>();\n\n    ExecutorService fileExecutor = fileExecutor(jobConf);\n    ExecutorService tableExecutor = tableExecutor(jobConf, outputs.size());\n    try {\n      \r\n      Tasks.foreach(outputs)\n          .suppressFailureWhenFinished()\n          .executeWith(tableExecutor)\n          .onFailure((output, exc) -> LOG.warn(\"Failed cleanup table {} on abort job\", output, exc))\n          .run(output -> {\n            LOG.info(\"Cleaning job for table {}\", jobContext.getJobID(), output);\n\n            Table table = HiveIcebergStorageHandler.table(jobConf, output);\n            jobLocations.add(generateJobLocation(table.location(), jobConf, jobContext.getJobID()));\n            Collection<DataFile> dataFiles = dataFiles(fileExecutor, table.location(), jobContext, table.io(), false);\n\n            \r\n            if (dataFiles.size() > 0) {\n              Tasks.foreach(dataFiles)\n                  .retry(3)\n                  .suppressFailureWhenFinished()\n                  .executeWith(fileExecutor)\n                  .onFailure((file, exc) -> LOG.warn(\"Failed to remove data file {} on abort job\", file.path(), exc))\n                  .run(file -> table.io().deleteFile(file.path().toString()));\n            }\n          });\n    } finally {\n      fileExecutor.shutdown();\n      if (tableExecutor != null) {\n        tableExecutor.shutdown();\n      }\n    }\n\n    LOG.info(\"Job {} is aborted. Data file cleaning finished\", jobContext.getJobID());\n\n    cleanup(jobContext, jobLocations);\n  }\n","date":"2021-03-30 20:07:23","endLine":239,"groupId":"1920","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"abortJob","params":"(JobContextoriginalContext@intstatus)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/34/79b362a82c3d41448f175ac4cb68c6bccaf96f.src","preCode":"  public void abortJob(JobContext originalContext, int status) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n    JobConf jobConf = jobContext.getJobConf();\n\n    LOG.info(\"Job {} is aborted. Data file cleaning started\", jobContext.getJobID());\n    Collection<String> outputs = HiveIcebergStorageHandler.outputTables(jobContext.getJobConf());\n    Collection<String> jobLocations = new ConcurrentLinkedQueue<>();\n\n    ExecutorService fileExecutor = fileExecutor(jobConf);\n    ExecutorService tableExecutor = tableExecutor(jobConf, outputs.size());\n    try {\n      \r\n      Tasks.foreach(outputs)\n          .suppressFailureWhenFinished()\n          .executeWith(tableExecutor)\n          .onFailure((output, exc) -> LOG.warn(\"Failed cleanup table {} on abort job\", output, exc))\n          .run(output -> {\n            LOG.info(\"Cleaning job for table {}\", jobContext.getJobID(), output);\n\n            Table table = HiveIcebergStorageHandler.table(jobConf, output);\n            jobLocations.add(generateJobLocation(table.location(), jobConf, jobContext.getJobID()));\n            Collection<DataFile> dataFiles = dataFiles(fileExecutor, table.location(), jobContext, table.io(), false);\n\n            \r\n            if (dataFiles.size() > 0) {\n              Tasks.foreach(dataFiles)\n                  .retry(3)\n                  .suppressFailureWhenFinished()\n                  .executeWith(fileExecutor)\n                  .onFailure((file, exc) -> LOG.warn(\"Failed to remove data file {} on abort job\", file.path(), exc))\n                  .run(file -> table.io().deleteFile(file.path().toString()));\n            }\n          });\n    } finally {\n      fileExecutor.shutdown();\n      if (tableExecutor != null) {\n        tableExecutor.shutdown();\n      }\n    }\n\n    LOG.info(\"Job {} is aborted. Data file cleaning finished\", jobContext.getJobID());\n\n    cleanup(jobContext, jobLocations);\n  }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":196,"status":"N"}],"commitId":"db8248c16e99c435ff7eed8fa86bc3913af2756a","commitMessage":"@@@Hive: Configure catalog type on table level. (#2129)\n\n","date":"2021-04-12 15:36:22","modifiedFileCount":"14","status":"M","submitter":"L?szl? Pint?r"},{"authorTime":"2021-03-30 20:07:23","codes":[{"authorDate":"2021-04-28 15:45:09","commitOrder":6,"curCode":"  public void commitJob(JobContext originalContext) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n    JobConf jobConf = jobContext.getJobConf();\n\n    long startTime = System.currentTimeMillis();\n    LOG.info(\"Committing job {} has started\", jobContext.getJobID());\n\n    Collection<String> outputs = HiveIcebergStorageHandler.outputTables(jobContext.getJobConf());\n    Collection<String> jobLocations = new ConcurrentLinkedQueue<>();\n\n    ExecutorService fileExecutor = fileExecutor(jobConf);\n    ExecutorService tableExecutor = tableExecutor(jobConf, outputs.size());\n    try {\n      \r\n      Tasks.foreach(outputs)\n          .throwFailureWhenFinished()\n          .stopOnFailure()\n          .executeWith(tableExecutor)\n          .run(output -> {\n            Table table = HiveIcebergStorageHandler.table(jobConf, output);\n            if (table != null) {\n              String catalogName = HiveIcebergStorageHandler.catalogName(jobConf, output);\n              jobLocations.add(generateJobLocation(table.location(), jobConf, jobContext.getJobID()));\n              commitTable(table.io(), fileExecutor, jobContext, output, table.location(), catalogName);\n            } else {\n              LOG.info(\"CommitJob found no serialized table in config for table: {}. Skipping job commit.\", output);\n            }\n          });\n    } finally {\n      fileExecutor.shutdown();\n      if (tableExecutor != null) {\n        tableExecutor.shutdown();\n      }\n    }\n\n    LOG.info(\"Commit took {} ms for job {}\", System.currentTimeMillis() - startTime, jobContext.getJobID());\n\n    cleanup(jobContext, jobLocations);\n  }\n","date":"2021-04-28 15:45:09","endLine":208,"groupId":"1919","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"commitJob","params":"(JobContextoriginalContext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a6/ea8e21dd8487afb10e5443a5a511235a1aacee.src","preCode":"  public void commitJob(JobContext originalContext) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n    JobConf jobConf = jobContext.getJobConf();\n\n    long startTime = System.currentTimeMillis();\n    LOG.info(\"Committing job {} has started\", jobContext.getJobID());\n\n    Collection<String> outputs = HiveIcebergStorageHandler.outputTables(jobContext.getJobConf());\n    Collection<String> jobLocations = new ConcurrentLinkedQueue<>();\n\n    ExecutorService fileExecutor = fileExecutor(jobConf);\n    ExecutorService tableExecutor = tableExecutor(jobConf, outputs.size());\n    try {\n      \r\n      Tasks.foreach(outputs)\n          .throwFailureWhenFinished()\n          .stopOnFailure()\n          .executeWith(tableExecutor)\n          .run(output -> {\n            Table table = HiveIcebergStorageHandler.table(jobConf, output);\n            String catalogName = HiveIcebergStorageHandler.catalogName(jobConf, output);\n            jobLocations.add(generateJobLocation(table.location(), jobConf, jobContext.getJobID()));\n            commitTable(table.io(), fileExecutor, jobContext, output, table.location(), catalogName);\n          });\n    } finally {\n      fileExecutor.shutdown();\n      if (tableExecutor != null) {\n        tableExecutor.shutdown();\n      }\n    }\n\n    LOG.info(\"Commit took {} ms for job {}\", System.currentTimeMillis() - startTime, jobContext.getJobID());\n\n    cleanup(jobContext, jobLocations);\n  }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":170,"status":"M"},{"authorDate":"2021-03-30 20:07:23","commitOrder":6,"curCode":"  public void abortJob(JobContext originalContext, int status) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n    JobConf jobConf = jobContext.getJobConf();\n\n    LOG.info(\"Job {} is aborted. Data file cleaning started\", jobContext.getJobID());\n    Collection<String> outputs = HiveIcebergStorageHandler.outputTables(jobContext.getJobConf());\n    Collection<String> jobLocations = new ConcurrentLinkedQueue<>();\n\n    ExecutorService fileExecutor = fileExecutor(jobConf);\n    ExecutorService tableExecutor = tableExecutor(jobConf, outputs.size());\n    try {\n      \r\n      Tasks.foreach(outputs)\n          .suppressFailureWhenFinished()\n          .executeWith(tableExecutor)\n          .onFailure((output, exc) -> LOG.warn(\"Failed cleanup table {} on abort job\", output, exc))\n          .run(output -> {\n            LOG.info(\"Cleaning job for table {}\", jobContext.getJobID(), output);\n\n            Table table = HiveIcebergStorageHandler.table(jobConf, output);\n            jobLocations.add(generateJobLocation(table.location(), jobConf, jobContext.getJobID()));\n            Collection<DataFile> dataFiles = dataFiles(fileExecutor, table.location(), jobContext, table.io(), false);\n\n            \r\n            if (dataFiles.size() > 0) {\n              Tasks.foreach(dataFiles)\n                  .retry(3)\n                  .suppressFailureWhenFinished()\n                  .executeWith(fileExecutor)\n                  .onFailure((file, exc) -> LOG.warn(\"Failed to remove data file {} on abort job\", file.path(), exc))\n                  .run(file -> table.io().deleteFile(file.path().toString()));\n            }\n          });\n    } finally {\n      fileExecutor.shutdown();\n      if (tableExecutor != null) {\n        tableExecutor.shutdown();\n      }\n    }\n\n    LOG.info(\"Job {} is aborted. Data file cleaning finished\", jobContext.getJobID());\n\n    cleanup(jobContext, jobLocations);\n  }\n","date":"2021-03-30 20:07:23","endLine":239,"groupId":"1920","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"abortJob","params":"(JobContextoriginalContext@intstatus)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/34/79b362a82c3d41448f175ac4cb68c6bccaf96f.src","preCode":"  public void abortJob(JobContext originalContext, int status) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n    JobConf jobConf = jobContext.getJobConf();\n\n    LOG.info(\"Job {} is aborted. Data file cleaning started\", jobContext.getJobID());\n    Collection<String> outputs = HiveIcebergStorageHandler.outputTables(jobContext.getJobConf());\n    Collection<String> jobLocations = new ConcurrentLinkedQueue<>();\n\n    ExecutorService fileExecutor = fileExecutor(jobConf);\n    ExecutorService tableExecutor = tableExecutor(jobConf, outputs.size());\n    try {\n      \r\n      Tasks.foreach(outputs)\n          .suppressFailureWhenFinished()\n          .executeWith(tableExecutor)\n          .onFailure((output, exc) -> LOG.warn(\"Failed cleanup table {} on abort job\", output, exc))\n          .run(output -> {\n            LOG.info(\"Cleaning job for table {}\", jobContext.getJobID(), output);\n\n            Table table = HiveIcebergStorageHandler.table(jobConf, output);\n            jobLocations.add(generateJobLocation(table.location(), jobConf, jobContext.getJobID()));\n            Collection<DataFile> dataFiles = dataFiles(fileExecutor, table.location(), jobContext, table.io(), false);\n\n            \r\n            if (dataFiles.size() > 0) {\n              Tasks.foreach(dataFiles)\n                  .retry(3)\n                  .suppressFailureWhenFinished()\n                  .executeWith(fileExecutor)\n                  .onFailure((file, exc) -> LOG.warn(\"Failed to remove data file {} on abort job\", file.path(), exc))\n                  .run(file -> table.io().deleteFile(file.path().toString()));\n            }\n          });\n    } finally {\n      fileExecutor.shutdown();\n      if (tableExecutor != null) {\n        tableExecutor.shutdown();\n      }\n    }\n\n    LOG.info(\"Job {} is aborted. Data file cleaning finished\", jobContext.getJobID());\n\n    cleanup(jobContext, jobLocations);\n  }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":196,"status":"N"}],"commitId":"90bc3297fbde62150281698ad5b61dc0dfb2be9c","commitMessage":"@@@Hive: fix issue of inserting empty data on Tez (#2516)\n\n","date":"2021-04-28 15:45:09","modifiedFileCount":"2","status":"M","submitter":"Marton Bod"},{"authorTime":"2021-05-28 03:11:08","codes":[{"authorDate":"2021-04-28 15:45:09","commitOrder":7,"curCode":"  public void commitJob(JobContext originalContext) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n    JobConf jobConf = jobContext.getJobConf();\n\n    long startTime = System.currentTimeMillis();\n    LOG.info(\"Committing job {} has started\", jobContext.getJobID());\n\n    Collection<String> outputs = HiveIcebergStorageHandler.outputTables(jobContext.getJobConf());\n    Collection<String> jobLocations = new ConcurrentLinkedQueue<>();\n\n    ExecutorService fileExecutor = fileExecutor(jobConf);\n    ExecutorService tableExecutor = tableExecutor(jobConf, outputs.size());\n    try {\n      \r\n      Tasks.foreach(outputs)\n          .throwFailureWhenFinished()\n          .stopOnFailure()\n          .executeWith(tableExecutor)\n          .run(output -> {\n            Table table = HiveIcebergStorageHandler.table(jobConf, output);\n            if (table != null) {\n              String catalogName = HiveIcebergStorageHandler.catalogName(jobConf, output);\n              jobLocations.add(generateJobLocation(table.location(), jobConf, jobContext.getJobID()));\n              commitTable(table.io(), fileExecutor, jobContext, output, table.location(), catalogName);\n            } else {\n              LOG.info(\"CommitJob found no serialized table in config for table: {}. Skipping job commit.\", output);\n            }\n          });\n    } finally {\n      fileExecutor.shutdown();\n      if (tableExecutor != null) {\n        tableExecutor.shutdown();\n      }\n    }\n\n    LOG.info(\"Commit took {} ms for job {}\", System.currentTimeMillis() - startTime, jobContext.getJobID());\n\n    cleanup(jobContext, jobLocations);\n  }\n","date":"2021-04-28 15:45:09","endLine":208,"groupId":"101619","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"commitJob","params":"(JobContextoriginalContext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a6/ea8e21dd8487afb10e5443a5a511235a1aacee.src","preCode":"  public void commitJob(JobContext originalContext) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n    JobConf jobConf = jobContext.getJobConf();\n\n    long startTime = System.currentTimeMillis();\n    LOG.info(\"Committing job {} has started\", jobContext.getJobID());\n\n    Collection<String> outputs = HiveIcebergStorageHandler.outputTables(jobContext.getJobConf());\n    Collection<String> jobLocations = new ConcurrentLinkedQueue<>();\n\n    ExecutorService fileExecutor = fileExecutor(jobConf);\n    ExecutorService tableExecutor = tableExecutor(jobConf, outputs.size());\n    try {\n      \r\n      Tasks.foreach(outputs)\n          .throwFailureWhenFinished()\n          .stopOnFailure()\n          .executeWith(tableExecutor)\n          .run(output -> {\n            Table table = HiveIcebergStorageHandler.table(jobConf, output);\n            if (table != null) {\n              String catalogName = HiveIcebergStorageHandler.catalogName(jobConf, output);\n              jobLocations.add(generateJobLocation(table.location(), jobConf, jobContext.getJobID()));\n              commitTable(table.io(), fileExecutor, jobContext, output, table.location(), catalogName);\n            } else {\n              LOG.info(\"CommitJob found no serialized table in config for table: {}. Skipping job commit.\", output);\n            }\n          });\n    } finally {\n      fileExecutor.shutdown();\n      if (tableExecutor != null) {\n        tableExecutor.shutdown();\n      }\n    }\n\n    LOG.info(\"Commit took {} ms for job {}\", System.currentTimeMillis() - startTime, jobContext.getJobID());\n\n    cleanup(jobContext, jobLocations);\n  }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":170,"status":"N"},{"authorDate":"2021-05-28 03:11:08","commitOrder":7,"curCode":"  public void abortJob(JobContext originalContext, int status) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n    JobConf jobConf = jobContext.getJobConf();\n\n    LOG.info(\"Job {} is aborted. Data file cleaning started\", jobContext.getJobID());\n    Collection<String> outputs = HiveIcebergStorageHandler.outputTables(jobContext.getJobConf());\n    Collection<String> jobLocations = new ConcurrentLinkedQueue<>();\n\n    ExecutorService fileExecutor = fileExecutor(jobConf);\n    ExecutorService tableExecutor = tableExecutor(jobConf, outputs.size());\n    try {\n      \r\n      Tasks.foreach(outputs)\n          .suppressFailureWhenFinished()\n          .executeWith(tableExecutor)\n          .onFailure((output, exc) -> LOG.warn(\"Failed cleanup table {} on abort job\", output, exc))\n          .run(output -> {\n            LOG.info(\"Cleaning table {} with job id {}\", output, jobContext.getJobID());\n            Table table = HiveIcebergStorageHandler.table(jobConf, output);\n            jobLocations.add(generateJobLocation(table.location(), jobConf, jobContext.getJobID()));\n            Collection<DataFile> dataFiles = dataFiles(fileExecutor, table.location(), jobContext, table.io(), false);\n\n            \r\n            if (dataFiles.size() > 0) {\n              Tasks.foreach(dataFiles)\n                  .retry(3)\n                  .suppressFailureWhenFinished()\n                  .executeWith(fileExecutor)\n                  .onFailure((file, exc) -> LOG.warn(\"Failed to remove data file {} on abort job\", file.path(), exc))\n                  .run(file -> table.io().deleteFile(file.path().toString()));\n            }\n          });\n    } finally {\n      fileExecutor.shutdown();\n      if (tableExecutor != null) {\n        tableExecutor.shutdown();\n      }\n    }\n\n    LOG.info(\"Job {} is aborted. Data file cleaning finished\", jobContext.getJobID());\n\n    cleanup(jobContext, jobLocations);\n  }\n","date":"2021-05-28 03:11:08","endLine":260,"groupId":"101619","id":14,"instanceNumber":2,"isCurCommit":1,"methodName":"abortJob","params":"(JobContextoriginalContext@intstatus)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/4b/3157b2c8aa099e96eee494b61626ecde044153.src","preCode":"  public void abortJob(JobContext originalContext, int status) throws IOException {\n    JobContext jobContext = TezUtil.enrichContextWithVertexId(originalContext);\n    JobConf jobConf = jobContext.getJobConf();\n\n    LOG.info(\"Job {} is aborted. Data file cleaning started\", jobContext.getJobID());\n    Collection<String> outputs = HiveIcebergStorageHandler.outputTables(jobContext.getJobConf());\n    Collection<String> jobLocations = new ConcurrentLinkedQueue<>();\n\n    ExecutorService fileExecutor = fileExecutor(jobConf);\n    ExecutorService tableExecutor = tableExecutor(jobConf, outputs.size());\n    try {\n      \r\n      Tasks.foreach(outputs)\n          .suppressFailureWhenFinished()\n          .executeWith(tableExecutor)\n          .onFailure((output, exc) -> LOG.warn(\"Failed cleanup table {} on abort job\", output, exc))\n          .run(output -> {\n            LOG.info(\"Cleaning job for table {}\", jobContext.getJobID(), output);\n\n            Table table = HiveIcebergStorageHandler.table(jobConf, output);\n            jobLocations.add(generateJobLocation(table.location(), jobConf, jobContext.getJobID()));\n            Collection<DataFile> dataFiles = dataFiles(fileExecutor, table.location(), jobContext, table.io(), false);\n\n            \r\n            if (dataFiles.size() > 0) {\n              Tasks.foreach(dataFiles)\n                  .retry(3)\n                  .suppressFailureWhenFinished()\n                  .executeWith(fileExecutor)\n                  .onFailure((file, exc) -> LOG.warn(\"Failed to remove data file {} on abort job\", file.path(), exc))\n                  .run(file -> table.io().deleteFile(file.path().toString()));\n            }\n          });\n    } finally {\n      fileExecutor.shutdown();\n      if (tableExecutor != null) {\n        tableExecutor.shutdown();\n      }\n    }\n\n    LOG.info(\"Job {} is aborted. Data file cleaning finished\", jobContext.getJobID());\n\n    cleanup(jobContext, jobLocations);\n  }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":218,"status":"M"}],"commitId":"15616941461aaad3f03568ce4880171b86a06547","commitMessage":"@@@Hive: Improve code style (#2641)\n\nCo-authored-by: xiepengjie <xiepengjie@didiglobal.com>","date":"2021-05-28 03:11:08","modifiedFileCount":"4","status":"M","submitter":"Jeff.r"}]
