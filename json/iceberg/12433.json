[{"authorTime":"2019-03-21 07:25:05","codes":[{"authorDate":"2019-03-21 07:25:05","commitOrder":1,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName()))-1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","date":"2019-03-21 07:25:05","endLine":122,"groupId":"224","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/01/d2311ea20a40ee5a9bfb2d18cdea423550cdd9.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName()))-1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","realPath":"parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroValueReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":88,"status":"B"},{"authorDate":"2019-03-21 07:25:05","commitOrder":1,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName()))-1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new InternalRowReader(types, reorderedFields);\n    }\n","date":"2019-03-21 07:25:05","endLine":160,"groupId":"224","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/29/4a849fd8e8678106a13921aa4c7396ab7d17d7.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName()))-1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new InternalRowReader(types, reorderedFields);\n    }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":128,"status":"B"}],"commitId":"c20927801a369104e5ea510470e1cf7c8e28b808","commitMessage":"@@@Rename packages to org.apache.iceberg (#138)\n\n* Move all packages by directory (but don't change references)\n* Rename all references from com.netflix.iceberg to org.apache.iceberg\n* Reorganize all imports due to new package name.\n  Previous commit only did a string find-replace.  which made all the imports out of order. Use an IDE to auto-sort all imports.\n\n","date":"2019-03-21 07:25:05","modifiedFileCount":"0","status":"B","submitter":"mccheah"},{"authorTime":"2019-06-24 23:57:49","codes":[{"authorDate":"2019-03-21 07:25:05","commitOrder":2,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName()))-1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","date":"2019-03-21 07:25:05","endLine":122,"groupId":"224","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/01/d2311ea20a40ee5a9bfb2d18cdea423550cdd9.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName()))-1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","realPath":"parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroValueReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":88,"status":"N"},{"authorDate":"2019-06-24 23:57:49","commitOrder":2,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new InternalRowReader(types, reorderedFields);\n    }\n","date":"2019-06-24 23:57:49","endLine":160,"groupId":"224","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/9a/36266ffdf222c4dac237d44b43c458ba5bfe9b.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName()))-1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new InternalRowReader(types, reorderedFields);\n    }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":128,"status":"M"}],"commitId":"5f6fc3be3400cfae858a938213ba8516acc983a5","commitMessage":"@@@[Baseline] Apply Baseline plugin to iceberg-spark (#226)\n\n","date":"2019-06-24 23:57:49","modifiedFileCount":"36","status":"M","submitter":"Anton Okolnychyi"},{"authorTime":"2019-06-24 23:57:49","codes":[{"authorDate":"2019-10-23 02:17:28","commitOrder":3,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","date":"2019-10-23 02:17:28","endLine":120,"groupId":"224","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a2/6fe92a12b858edf1b08dd88c1c0dff344a62f4.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName()))-1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","realPath":"parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroValueReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"M"},{"authorDate":"2019-06-24 23:57:49","commitOrder":3,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new InternalRowReader(types, reorderedFields);\n    }\n","date":"2019-06-24 23:57:49","endLine":160,"groupId":"224","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/9a/36266ffdf222c4dac237d44b43c458ba5bfe9b.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new InternalRowReader(types, reorderedFields);\n    }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":128,"status":"N"}],"commitId":"336174b0a4438ed68cdb8e208833e380adaa15fc","commitMessage":"@@@Baseline: Add Baseline to iceberg-parquet (#526)\n\n","date":"2019-10-23 02:17:28","modifiedFileCount":"29","status":"M","submitter":"Fokko Driesprong"},{"authorTime":"2020-04-11 23:51:33","codes":[{"authorDate":"2019-10-23 02:17:28","commitOrder":4,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","date":"2019-10-23 02:17:28","endLine":120,"groupId":"224","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a2/6fe92a12b858edf1b08dd88c1c0dff344a62f4.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","realPath":"parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroValueReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"N"},{"authorDate":"2020-04-11 23:51:33","commitOrder":4,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (idToConstant.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new InternalRowReader(types, reorderedFields);\n    }\n","date":"2020-04-11 23:51:33","endLine":175,"groupId":"1606","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/19/0b708db14de22530815ebb38b7953c2e6e0e31.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new InternalRowReader(types, reorderedFields);\n    }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":137,"status":"M"}],"commitId":"3a35c0764a51008fa5c5ecea109c87f92106dbcf","commitMessage":"@@@Parquet: Support constant map for partition values (#909)\n\nThis is a follow-up to #896.  which added the same constant map support for Avro.\n\nFixes #575 for Parquet and replaces #585. Andrei did a lot of the work for this in #585.\n\nCo-authored-by: Andrei Ionescu <webdev.andrei@gmail.com>","date":"2020-04-11 23:51:33","modifiedFileCount":"9","status":"M","submitter":"Ryan Blue"},{"authorTime":"2020-06-18 00:57:08","codes":[{"authorDate":"2019-10-23 02:17:28","commitOrder":5,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","date":"2019-10-23 02:17:28","endLine":120,"groupId":"224","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a2/6fe92a12b858edf1b08dd88c1c0dff344a62f4.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","realPath":"parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroValueReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"N"},{"authorDate":"2020-06-18 00:57:08","commitOrder":5,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        if (fieldType.getId() != null) {\n          int id = fieldType.getId().intValue();\n          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n          typesById.put(id, fieldType);\n        }\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (idToConstant.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new InternalRowReader(types, reorderedFields);\n    }\n","date":"2020-06-18 00:57:08","endLine":176,"groupId":"1606","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/51/ddc9432bc3f10a1a87339bd98743d25f21b4ec.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (idToConstant.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new InternalRowReader(types, reorderedFields);\n    }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":136,"status":"M"}],"commitId":"5a3cd22e775dfa8bf79deab675390aad48ba79a5","commitMessage":"@@@Parquet: Support name mappings to recover field IDs (#830)\n\n","date":"2020-06-18 00:57:08","modifiedFileCount":"16","status":"M","submitter":"Chen Junjie"},{"authorTime":"2020-07-31 09:13:43","codes":[{"authorDate":"2019-10-23 02:17:28","commitOrder":6,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","date":"2019-10-23 02:17:28","endLine":120,"groupId":"224","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a2/6fe92a12b858edf1b08dd88c1c0dff344a62f4.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","realPath":"parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroValueReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"N"},{"authorDate":"2020-07-31 09:13:43","commitOrder":6,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        if (fieldType.getId() != null) {\n          int id = fieldType.getId().intValue();\n          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n          typesById.put(id, fieldType);\n        }\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (idToConstant.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n          types.add(null);\n        } else if (id == MetadataColumns.ROW_POSITION.fieldId()) {\n          reorderedFields.add(ParquetValueReaders.position());\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new InternalRowReader(types, reorderedFields);\n    }\n","date":"2020-07-31 09:13:43","endLine":182,"groupId":"1606","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/ca/3a77f71ecc47b592edde346ec38a54f2fa5d54.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        if (fieldType.getId() != null) {\n          int id = fieldType.getId().intValue();\n          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n          typesById.put(id, fieldType);\n        }\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (idToConstant.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new InternalRowReader(types, reorderedFields);\n    }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":139,"status":"M"}],"commitId":"7060c928390c59e24dc207ec86f99132f6c1a828","commitMessage":"@@@Parquet: Add row position reader (#1254)\n\n","date":"2020-07-31 09:13:43","modifiedFileCount":"6","status":"M","submitter":"Chen Junjie"},{"authorTime":"2021-06-19 07:38:18","codes":[{"authorDate":"2019-10-23 02:17:28","commitOrder":7,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","date":"2019-10-23 02:17:28","endLine":120,"groupId":"12433","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a2/6fe92a12b858edf1b08dd88c1c0dff344a62f4.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","realPath":"parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroValueReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"N"},{"authorDate":"2021-06-19 07:38:18","commitOrder":7,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        if (fieldType.getId() != null) {\n          int id = fieldType.getId().intValue();\n          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n          typesById.put(id, fieldType);\n        }\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (idToConstant.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n          types.add(null);\n        } else if (id == MetadataColumns.ROW_POSITION.fieldId()) {\n          reorderedFields.add(ParquetValueReaders.position());\n          types.add(null);\n        } else if (id == MetadataColumns.IS_DELETED.fieldId()) {\n          reorderedFields.add(ParquetValueReaders.constant(false));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new InternalRowReader(types, reorderedFields);\n    }\n","date":"2021-06-19 07:38:18","endLine":185,"groupId":"12433","id":14,"instanceNumber":2,"isCurCommit":1,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/8a/bee4a575e1abfd7b6b8b4fb4adc1d005fb611c.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        if (fieldType.getId() != null) {\n          int id = fieldType.getId().intValue();\n          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n          typesById.put(id, fieldType);\n        }\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (idToConstant.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n          types.add(null);\n        } else if (id == MetadataColumns.ROW_POSITION.fieldId()) {\n          reorderedFields.add(ParquetValueReaders.position());\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new InternalRowReader(types, reorderedFields);\n    }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":139,"status":"M"}],"commitId":"a9f43636028a67eed39e3ed5903bc4c148ebaf76","commitMessage":"@@@Core: Add delete marker metadata column (#2538)\n\n","date":"2021-06-19 07:38:18","modifiedFileCount":"12","status":"M","submitter":"Chen Junjie"}]
