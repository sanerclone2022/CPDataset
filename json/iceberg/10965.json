[{"authorTime":"2020-06-20 09:02:23","codes":[{"authorDate":"2020-06-20 09:02:23","commitOrder":1,"curCode":"  public void before() throws IOException {\n    spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n    tableLocation = Files.createTempDirectory(\"temp\").toFile();\n    Schema schema = new Schema(\n        optional(1, \"title\", Types.StringType.get()),\n        optional(2, \"price\", Types.IntegerType.get()),\n        optional(3, \"author\", Types.StringType.get()),\n        optional(4, \"published\", Types.TimestampType.withZone()),\n        optional(5, \"genre\", Types.StringType.get())\n    );\n    PartitionSpec spec = PartitionSpec.builderFor(schema)\n        .year(\"published\")\n        .build();\n\n    HadoopTables tables = new HadoopTables(spark.sparkContext().hadoopConfiguration());\n    table = tables.create(schema, spec, tableLocation.toString());\n\n    Dataset<Row> df = spark.read().json(dataLocation + \"/books.json\");\n\n    df.select(df.col(\"title\"), df.col(\"price\").cast(DataTypes.IntegerType),\n        df.col(\"author\"), df.col(\"published\").cast(DataTypes.TimestampType),\n        df.col(\"genre\")).write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation.toString());\n\n    table.refresh();\n  }\n","date":"2020-06-20 09:02:23","endLine":83,"groupId":"293","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"before","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/f6/89f5359ed563c4a41a8a844567677f17638c43.src","preCode":"  public void before() throws IOException {\n    spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n    tableLocation = Files.createTempDirectory(\"temp\").toFile();\n    Schema schema = new Schema(\n        optional(1, \"title\", Types.StringType.get()),\n        optional(2, \"price\", Types.IntegerType.get()),\n        optional(3, \"author\", Types.StringType.get()),\n        optional(4, \"published\", Types.TimestampType.withZone()),\n        optional(5, \"genre\", Types.StringType.get())\n    );\n    PartitionSpec spec = PartitionSpec.builderFor(schema)\n        .year(\"published\")\n        .build();\n\n    HadoopTables tables = new HadoopTables(spark.sparkContext().hadoopConfiguration());\n    table = tables.create(schema, spec, tableLocation.toString());\n\n    Dataset<Row> df = spark.read().json(dataLocation + \"/books.json\");\n\n    df.select(df.col(\"title\"), df.col(\"price\").cast(DataTypes.IntegerType),\n        df.col(\"author\"), df.col(\"published\").cast(DataTypes.TimestampType),\n        df.col(\"genre\")).write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation.toString());\n\n    table.refresh();\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":56,"status":"B"},{"authorDate":"2020-06-20 09:02:23","commitOrder":1,"curCode":"  public void writeDataFromJsonFile() {\n    Schema bookSchema = new Schema(\n        optional(1, \"title\", Types.StringType.get()),\n        optional(2, \"price\", Types.LongType.get()),\n        optional(3, \"author\", Types.StringType.get()),\n        optional(4, \"published\", Types.TimestampType.withZone()),\n        optional(5, \"genre\", Types.StringType.get())\n    );\n\n    table = tables.create(bookSchema, pathToTable.toString());\n\n    Dataset<Row> df = spark.read().json(\"src/test/resources/data/books.json\");\n\n    df.select(df.col(\"title\"), df.col(\"price\"), df.col(\"author\"),\n        df.col(\"published\").cast(DataTypes.TimestampType), df.col(\"genre\")).write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(pathToTable.toString());\n\n    table.refresh();\n  }\n","date":"2020-06-20 09:02:23","endLine":132,"groupId":"2957","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"writeDataFromJsonFile","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/0f/d68cebd4cdde54f9a2606ea61a05d379061421.src","preCode":"  public void writeDataFromJsonFile() {\n    Schema bookSchema = new Schema(\n        optional(1, \"title\", Types.StringType.get()),\n        optional(2, \"price\", Types.LongType.get()),\n        optional(3, \"author\", Types.StringType.get()),\n        optional(4, \"published\", Types.TimestampType.withZone()),\n        optional(5, \"genre\", Types.StringType.get())\n    );\n\n    table = tables.create(bookSchema, pathToTable.toString());\n\n    Dataset<Row> df = spark.read().json(\"src/test/resources/data/books.json\");\n\n    df.select(df.col(\"title\"), df.col(\"price\"), df.col(\"author\"),\n        df.col(\"published\").cast(DataTypes.TimestampType), df.col(\"genre\")).write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(pathToTable.toString());\n\n    table.refresh();\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/examples/ReadAndWriteTablesTest.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":112,"status":"B"}],"commitId":"845de7bfe78bc8ba3480aad219fcef9f71477023","commitMessage":"@@@Spark: Move classes that depend on 2.x DSv2 to spark2 (#1122)\n\n","date":"2020-06-20 09:02:23","modifiedFileCount":"3","status":"B","submitter":"Ryan Blue"},{"authorTime":"2020-06-20 09:02:23","codes":[{"authorDate":"2020-08-10 23:31:09","commitOrder":2,"curCode":"  public void before() throws IOException {\n    spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n    tableLocation = Files.createTempDirectory(\"temp\").toFile();\n    Schema schema = new Schema(\n        optional(1, \"title\", Types.StringType.get()),\n        optional(2, \"price\", Types.IntegerType.get()),\n        optional(3, \"author\", Types.StringType.get()),\n        optional(4, \"published\", Types.TimestampType.withZone()),\n        optional(5, \"genre\", Types.StringType.get())\n    );\n    PartitionSpec spec = PartitionSpec.builderFor(schema)\n        .year(\"published\")\n        .build();\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    table = tables.create(schema, spec, tableLocation.toString());\n\n    Dataset<Row> df = spark.read().json(dataLocation + \"/books.json\");\n\n    df.select(df.col(\"title\"), df.col(\"price\").cast(DataTypes.IntegerType),\n        df.col(\"author\"), df.col(\"published\").cast(DataTypes.TimestampType),\n        df.col(\"genre\")).write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation.toString());\n\n    table.refresh();\n  }\n","date":"2020-08-10 23:31:09","endLine":83,"groupId":"293","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"before","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/95/81a802396a1c8f39211cdd76d1fdc8871372c8.src","preCode":"  public void before() throws IOException {\n    spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n    tableLocation = Files.createTempDirectory(\"temp\").toFile();\n    Schema schema = new Schema(\n        optional(1, \"title\", Types.StringType.get()),\n        optional(2, \"price\", Types.IntegerType.get()),\n        optional(3, \"author\", Types.StringType.get()),\n        optional(4, \"published\", Types.TimestampType.withZone()),\n        optional(5, \"genre\", Types.StringType.get())\n    );\n    PartitionSpec spec = PartitionSpec.builderFor(schema)\n        .year(\"published\")\n        .build();\n\n    HadoopTables tables = new HadoopTables(spark.sparkContext().hadoopConfiguration());\n    table = tables.create(schema, spec, tableLocation.toString());\n\n    Dataset<Row> df = spark.read().json(dataLocation + \"/books.json\");\n\n    df.select(df.col(\"title\"), df.col(\"price\").cast(DataTypes.IntegerType),\n        df.col(\"author\"), df.col(\"published\").cast(DataTypes.TimestampType),\n        df.col(\"genre\")).write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation.toString());\n\n    table.refresh();\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":56,"status":"M"},{"authorDate":"2020-06-20 09:02:23","commitOrder":2,"curCode":"  public void writeDataFromJsonFile() {\n    Schema bookSchema = new Schema(\n        optional(1, \"title\", Types.StringType.get()),\n        optional(2, \"price\", Types.LongType.get()),\n        optional(3, \"author\", Types.StringType.get()),\n        optional(4, \"published\", Types.TimestampType.withZone()),\n        optional(5, \"genre\", Types.StringType.get())\n    );\n\n    table = tables.create(bookSchema, pathToTable.toString());\n\n    Dataset<Row> df = spark.read().json(\"src/test/resources/data/books.json\");\n\n    df.select(df.col(\"title\"), df.col(\"price\"), df.col(\"author\"),\n        df.col(\"published\").cast(DataTypes.TimestampType), df.col(\"genre\")).write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(pathToTable.toString());\n\n    table.refresh();\n  }\n","date":"2020-06-20 09:02:23","endLine":132,"groupId":"2957","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"writeDataFromJsonFile","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/0f/d68cebd4cdde54f9a2606ea61a05d379061421.src","preCode":"  public void writeDataFromJsonFile() {\n    Schema bookSchema = new Schema(\n        optional(1, \"title\", Types.StringType.get()),\n        optional(2, \"price\", Types.LongType.get()),\n        optional(3, \"author\", Types.StringType.get()),\n        optional(4, \"published\", Types.TimestampType.withZone()),\n        optional(5, \"genre\", Types.StringType.get())\n    );\n\n    table = tables.create(bookSchema, pathToTable.toString());\n\n    Dataset<Row> df = spark.read().json(\"src/test/resources/data/books.json\");\n\n    df.select(df.col(\"title\"), df.col(\"price\"), df.col(\"author\"),\n        df.col(\"published\").cast(DataTypes.TimestampType), df.col(\"genre\")).write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(pathToTable.toString());\n\n    table.refresh();\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/examples/ReadAndWriteTablesTest.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":112,"status":"N"}],"commitId":"4bb2b0893a5de0b0452cb9db32401729c098ca8b","commitMessage":"@@@Spark: Use SessionState to load Hadoop config (#1310)\n\n","date":"2020-08-10 23:31:09","modifiedFileCount":"8","status":"M","submitter":"Anton Okolnychyi"},{"authorTime":"2020-06-20 09:02:23","codes":[{"authorDate":"2020-12-16 05:08:42","commitOrder":3,"curCode":"  public void before() throws IOException {\n    tableLocation = Files.createTempDirectory(\"temp\").toFile();\n    Schema schema = new Schema(\n        optional(1, \"title\", Types.StringType.get()),\n        optional(2, \"price\", Types.IntegerType.get()),\n        optional(3, \"author\", Types.StringType.get()),\n        optional(4, \"published\", Types.TimestampType.withZone()),\n        optional(5, \"genre\", Types.StringType.get())\n    );\n    PartitionSpec spec = PartitionSpec.builderFor(schema)\n        .year(\"published\")\n        .build();\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    table = tables.create(schema, spec, tableLocation.toString());\n\n    Dataset<Row> df = spark.read().json(dataLocation + \"/books.json\");\n\n    df.select(df.col(\"title\"), df.col(\"price\").cast(DataTypes.IntegerType),\n        df.col(\"author\"), df.col(\"published\").cast(DataTypes.TimestampType),\n        df.col(\"genre\")).write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation.toString());\n\n    table.refresh();\n  }\n","date":"2020-12-16 05:08:42","endLine":97,"groupId":"10965","id":5,"instanceNumber":1,"isCurCommit":1,"methodName":"before","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/af/a77f076ec1499cff6f6d2d9294bc4ce1260626.src","preCode":"  public void before() throws IOException {\n    spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n    tableLocation = Files.createTempDirectory(\"temp\").toFile();\n    Schema schema = new Schema(\n        optional(1, \"title\", Types.StringType.get()),\n        optional(2, \"price\", Types.IntegerType.get()),\n        optional(3, \"author\", Types.StringType.get()),\n        optional(4, \"published\", Types.TimestampType.withZone()),\n        optional(5, \"genre\", Types.StringType.get())\n    );\n    PartitionSpec spec = PartitionSpec.builderFor(schema)\n        .year(\"published\")\n        .build();\n\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    table = tables.create(schema, spec, tableLocation.toString());\n\n    Dataset<Row> df = spark.read().json(dataLocation + \"/books.json\");\n\n    df.select(df.col(\"title\"), df.col(\"price\").cast(DataTypes.IntegerType),\n        df.col(\"author\"), df.col(\"published\").cast(DataTypes.TimestampType),\n        df.col(\"genre\")).write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation.toString());\n\n    table.refresh();\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/examples/SchemaEvolutionTest.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":71,"status":"M"},{"authorDate":"2020-06-20 09:02:23","commitOrder":3,"curCode":"  public void writeDataFromJsonFile() {\n    Schema bookSchema = new Schema(\n        optional(1, \"title\", Types.StringType.get()),\n        optional(2, \"price\", Types.LongType.get()),\n        optional(3, \"author\", Types.StringType.get()),\n        optional(4, \"published\", Types.TimestampType.withZone()),\n        optional(5, \"genre\", Types.StringType.get())\n    );\n\n    table = tables.create(bookSchema, pathToTable.toString());\n\n    Dataset<Row> df = spark.read().json(\"src/test/resources/data/books.json\");\n\n    df.select(df.col(\"title\"), df.col(\"price\"), df.col(\"author\"),\n        df.col(\"published\").cast(DataTypes.TimestampType), df.col(\"genre\")).write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(pathToTable.toString());\n\n    table.refresh();\n  }\n","date":"2020-06-20 09:02:23","endLine":132,"groupId":"10965","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"writeDataFromJsonFile","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/0f/d68cebd4cdde54f9a2606ea61a05d379061421.src","preCode":"  public void writeDataFromJsonFile() {\n    Schema bookSchema = new Schema(\n        optional(1, \"title\", Types.StringType.get()),\n        optional(2, \"price\", Types.LongType.get()),\n        optional(3, \"author\", Types.StringType.get()),\n        optional(4, \"published\", Types.TimestampType.withZone()),\n        optional(5, \"genre\", Types.StringType.get())\n    );\n\n    table = tables.create(bookSchema, pathToTable.toString());\n\n    Dataset<Row> df = spark.read().json(\"src/test/resources/data/books.json\");\n\n    df.select(df.col(\"title\"), df.col(\"price\"), df.col(\"author\"),\n        df.col(\"published\").cast(DataTypes.TimestampType), df.col(\"genre\")).write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(pathToTable.toString());\n\n    table.refresh();\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/examples/ReadAndWriteTablesTest.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":112,"status":"N"}],"commitId":"13ee93a5e33a252cfe7aa8c2c4a51060c2c42238","commitMessage":"@@@Spark: Refactor schema evolution examples  (#1932)\n\n","date":"2020-12-16 05:08:42","modifiedFileCount":"1","status":"M","submitter":"Karuppayya"}]
