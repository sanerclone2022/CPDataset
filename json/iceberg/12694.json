[{"authorTime":"2020-08-21 00:17:18","codes":[{"authorDate":"2020-08-07 07:07:19","commitOrder":2,"curCode":"    public void nonNullWrite(int rowId, int column, SpecializedGetters data, ColumnVector output) {\n      ArrayData value = data.getArray(column);\n      ListColumnVector cv = (ListColumnVector) output;\n      \r\n      cv.lengths[rowId] = value.numElements();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount += cv.lengths[rowId];\n      \r\n      cv.child.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        writer.write((int) (e + cv.offsets[rowId]), e, value, cv.child);\n      }\n    }\n","date":"2020-08-07 07:07:19","endLine":246,"groupId":"1334","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"nonNullWrite","params":"(introwId@intcolumn@SpecializedGettersdata@ColumnVectoroutput)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/8b/b0f53f83cbd483cdcb57e3ac0a145b3532d11b.src","preCode":"    public void nonNullWrite(int rowId, int column, SpecializedGetters data, ColumnVector output) {\n      ArrayData value = data.getArray(column);\n      ListColumnVector cv = (ListColumnVector) output;\n      \r\n      cv.lengths[rowId] = value.numElements();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount += cv.lengths[rowId];\n      \r\n      cv.child.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        writer.write((int) (e + cv.offsets[rowId]), e, value, cv.child);\n      }\n    }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueWriters.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":233,"status":"NB"},{"authorDate":"2020-08-21 00:17:18","commitOrder":2,"curCode":"    public void nonNullWrite(int rowId, MapData data, ColumnVector output) {\n      MapColumnVector cv = (MapColumnVector) output;\n      ArrayData keyArray = data.keyArray();\n      ArrayData valArray = data.valueArray();\n\n      \r\n      cv.lengths[rowId] = data.size();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount += cv.lengths[rowId];\n      \r\n      cv.keys.ensureSize(cv.childCount, true);\n      cv.values.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        int pos = (int) (e + cv.offsets[rowId]);\n        keyWriter.write(pos, (K) keyGetter.getElementOrNull(keyArray, e), cv.keys);\n        valueWriter.write(pos, (V) valueGetter.getElementOrNull(valArray, e), cv.values);\n      }\n    }\n","date":"2020-08-21 00:17:18","endLine":298,"groupId":"4030","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"nonNullWrite","params":"(introwId@MapDatadata@ColumnVectoroutput)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a3/919c988c21921fea052ca187d10c2843213058.src","preCode":"    public void nonNullWrite(int rowId, MapData data, ColumnVector output) {\n      MapColumnVector cv = (MapColumnVector) output;\n      ArrayData keyArray = data.keyArray();\n      ArrayData valArray = data.valueArray();\n\n      \r\n      cv.lengths[rowId] = data.size();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount += cv.lengths[rowId];\n      \r\n      cv.keys.ensureSize(cv.childCount, true);\n      cv.values.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        int pos = (int) (e + cv.offsets[rowId]);\n        keyWriter.write(pos, (K) keyGetter.getElementOrNull(keyArray, e), cv.keys);\n        valueWriter.write(pos, (V) valueGetter.getElementOrNull(valArray, e), cv.values);\n      }\n    }\n","realPath":"flink/src/main/java/org/apache/iceberg/flink/data/FlinkOrcWriters.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":280,"status":"B"}],"commitId":"311f2a107fb2fac0103d9af871493f1cc1b9828b","commitMessage":"@@@Flink: Add Orc reader.  writer implementations (#1255)\n\n","date":"2020-08-21 00:17:18","modifiedFileCount":"6","status":"M","submitter":"openinx"},{"authorTime":"2021-01-23 02:17:49","codes":[{"authorDate":"2021-01-23 02:17:49","commitOrder":3,"curCode":"    public void nonNullWrite(int rowId, int column, SpecializedGetters data, ColumnVector output) {\n      ArrayData value = data.getArray(column);\n      ListColumnVector cv = (ListColumnVector) output;\n      \r\n      cv.lengths[rowId] = value.numElements();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount = (int) (cv.childCount + cv.lengths[rowId]);\n      \r\n      cv.child.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        writer.write((int) (e + cv.offsets[rowId]), e, value, cv.child);\n      }\n    }\n","date":"2021-01-23 02:17:49","endLine":246,"groupId":"1334","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"nonNullWrite","params":"(introwId@intcolumn@SpecializedGettersdata@ColumnVectoroutput)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/9d/517983e43799dc98590e665aebb7d4584c3948.src","preCode":"    public void nonNullWrite(int rowId, int column, SpecializedGetters data, ColumnVector output) {\n      ArrayData value = data.getArray(column);\n      ListColumnVector cv = (ListColumnVector) output;\n      \r\n      cv.lengths[rowId] = value.numElements();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount += cv.lengths[rowId];\n      \r\n      cv.child.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        writer.write((int) (e + cv.offsets[rowId]), e, value, cv.child);\n      }\n    }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueWriters.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":233,"status":"M"},{"authorDate":"2021-01-23 02:17:49","commitOrder":3,"curCode":"    public void nonNullWrite(int rowId, MapData data, ColumnVector output) {\n      MapColumnVector cv = (MapColumnVector) output;\n      ArrayData keyArray = data.keyArray();\n      ArrayData valArray = data.valueArray();\n\n      \r\n      cv.lengths[rowId] = data.size();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount = (int) (cv.childCount + cv.lengths[rowId]);\n      \r\n      cv.keys.ensureSize(cv.childCount, true);\n      cv.values.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        int pos = (int) (e + cv.offsets[rowId]);\n        keyWriter.write(pos, (K) keyGetter.getElementOrNull(keyArray, e), cv.keys);\n        valueWriter.write(pos, (V) valueGetter.getElementOrNull(valArray, e), cv.values);\n      }\n    }\n","date":"2021-01-23 02:17:49","endLine":298,"groupId":"4030","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"nonNullWrite","params":"(introwId@MapDatadata@ColumnVectoroutput)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/80/0ad2071274f21f1134edb9535c53d8d7049fd8.src","preCode":"    public void nonNullWrite(int rowId, MapData data, ColumnVector output) {\n      MapColumnVector cv = (MapColumnVector) output;\n      ArrayData keyArray = data.keyArray();\n      ArrayData valArray = data.valueArray();\n\n      \r\n      cv.lengths[rowId] = data.size();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount += cv.lengths[rowId];\n      \r\n      cv.keys.ensureSize(cv.childCount, true);\n      cv.values.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        int pos = (int) (e + cv.offsets[rowId]);\n        keyWriter.write(pos, (K) keyGetter.getElementOrNull(keyArray, e), cv.keys);\n        valueWriter.write(pos, (V) valueGetter.getElementOrNull(valArray, e), cv.values);\n      }\n    }\n","realPath":"flink/src/main/java/org/apache/iceberg/flink/data/FlinkOrcWriters.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":280,"status":"M"}],"commitId":"503c0eff8ba483a370f2c904fc696175102e8603","commitMessage":"@@@ORC: Fix NarrowingCompoundAssignment warnings (#2103)\n\n","date":"2021-01-23 02:17:49","modifiedFileCount":"3","status":"M","submitter":"Kyle Bendickson"},{"authorTime":"2021-02-06 10:04:47","codes":[{"authorDate":"2021-02-06 10:04:47","commitOrder":4,"curCode":"    public void nonNullWrite(int rowId, int column, SpecializedGetters data, ColumnVector output) {\n      ArrayData value = data.getArray(column);\n      ListColumnVector cv = (ListColumnVector) output;\n      \r\n      cv.lengths[rowId] = value.numElements();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount = (int) (cv.childCount + cv.lengths[rowId]);\n      \r\n      growColumnVector(cv.child, cv.childCount);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        writer.write((int) (e + cv.offsets[rowId]), e, value, cv.child);\n      }\n    }\n","date":"2021-02-06 10:04:47","endLine":281,"groupId":"12694","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"nonNullWrite","params":"(introwId@intcolumn@SpecializedGettersdata@ColumnVectoroutput)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/4b/4075070f6f835d6aedec753c349dacc26243f8.src","preCode":"    public void nonNullWrite(int rowId, int column, SpecializedGetters data, ColumnVector output) {\n      ArrayData value = data.getArray(column);\n      ListColumnVector cv = (ListColumnVector) output;\n      \r\n      cv.lengths[rowId] = value.numElements();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount = (int) (cv.childCount + cv.lengths[rowId]);\n      \r\n      cv.child.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        writer.write((int) (e + cv.offsets[rowId]), e, value, cv.child);\n      }\n    }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueWriters.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":268,"status":"M"},{"authorDate":"2021-02-06 10:04:47","commitOrder":4,"curCode":"    public void nonNullWrite(int rowId, MapData data, ColumnVector output) {\n      MapColumnVector cv = (MapColumnVector) output;\n      ArrayData keyArray = data.keyArray();\n      ArrayData valArray = data.valueArray();\n\n      \r\n      cv.lengths[rowId] = data.size();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount = (int) (cv.childCount + cv.lengths[rowId]);\n      \r\n      growColumnVector(cv.keys, cv.childCount);\n      growColumnVector(cv.values, cv.childCount);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        int pos = (int) (e + cv.offsets[rowId]);\n        keyWriter.write(pos, (K) keyGetter.getElementOrNull(keyArray, e), cv.keys);\n        valueWriter.write(pos, (V) valueGetter.getElementOrNull(valArray, e), cv.values);\n      }\n    }\n","date":"2021-02-06 10:04:47","endLine":306,"groupId":"12694","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"nonNullWrite","params":"(introwId@MapDatadata@ColumnVectoroutput)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/75/8d73d87e9b2f629236ff2e9c5d2aab6e0854e3.src","preCode":"    public void nonNullWrite(int rowId, MapData data, ColumnVector output) {\n      MapColumnVector cv = (MapColumnVector) output;\n      ArrayData keyArray = data.keyArray();\n      ArrayData valArray = data.valueArray();\n\n      \r\n      cv.lengths[rowId] = data.size();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount = (int) (cv.childCount + cv.lengths[rowId]);\n      \r\n      cv.keys.ensureSize(cv.childCount, true);\n      cv.values.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        int pos = (int) (e + cv.offsets[rowId]);\n        keyWriter.write(pos, (K) keyGetter.getElementOrNull(keyArray, e), cv.keys);\n        valueWriter.write(pos, (V) valueGetter.getElementOrNull(valArray, e), cv.values);\n      }\n    }\n","realPath":"flink/src/main/java/org/apache/iceberg/flink/data/FlinkOrcWriters.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":288,"status":"M"}],"commitId":"592e97f5daf0716ec6f57b4f049dd191d8094fad","commitMessage":"@@@ORC: Grow list and map child vectors with a growth factor of 3 (#2218)\n\n","date":"2021-02-06 10:04:47","modifiedFileCount":"3","status":"M","submitter":"Shardul Mahadik"}]
