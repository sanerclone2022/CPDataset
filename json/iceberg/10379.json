[{"authorTime":"2019-06-19 06:14:37","codes":[{"authorDate":"2019-06-19 06:14:37","commitOrder":1,"curCode":"  public void testWriteFormatOptionOverridesTableProperties() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    Map<String, String> options = Maps.newHashMap();\n    options.put(TableProperties.DEFAULT_FILE_FORMAT, \"avro\");\n    Table table = tables.create(SCHEMA, spec, options, tableLocation);\n\n    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n        new SimpleRecord(1, \"a\"),\n        new SimpleRecord(2, \"b\"),\n        new SimpleRecord(3, \"c\")\n    );\n    Dataset<Row> df = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n    df.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .option(\"write-format\", \"parquet\")\n        .mode(\"append\")\n        .save(tableLocation);\n\n    try (CloseableIterable<FileScanTask> tasks = table.newScan().planFiles()) {\n      tasks.forEach(task -> {\n        FileFormat fileFormat = FileFormat.fromFileName(task.file().path());\n        Assert.assertEquals(FileFormat.PARQUET, fileFormat);\n      });\n    }\n  }\n","date":"2019-06-19 06:14:37","endLine":103,"groupId":"4915","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testWriteFormatOptionOverridesTableProperties","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/ee/2190231873354ca546cfa263ac713a74fa56eb.src","preCode":"  public void testWriteFormatOptionOverridesTableProperties() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    Map<String, String> options = Maps.newHashMap();\n    options.put(TableProperties.DEFAULT_FILE_FORMAT, \"avro\");\n    Table table = tables.create(SCHEMA, spec, options, tableLocation);\n\n    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n        new SimpleRecord(1, \"a\"),\n        new SimpleRecord(2, \"b\"),\n        new SimpleRecord(3, \"c\")\n    );\n    Dataset<Row> df = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n    df.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .option(\"write-format\", \"parquet\")\n        .mode(\"append\")\n        .save(tableLocation);\n\n    try (CloseableIterable<FileScanTask> tasks = table.newScan().planFiles()) {\n      tasks.forEach(task -> {\n        FileFormat fileFormat = FileFormat.fromFileName(task.file().path());\n        Assert.assertEquals(FileFormat.PARQUET, fileFormat);\n      });\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":76,"status":"B"},{"authorDate":"2019-06-19 06:14:37","commitOrder":1,"curCode":"  public void testNoWriteFormatOption() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    Map<String, String> options = Maps.newHashMap();\n    options.put(TableProperties.DEFAULT_FILE_FORMAT, \"avro\");\n    Table table = tables.create(SCHEMA, spec, options, tableLocation);\n\n    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n        new SimpleRecord(1, \"a\"),\n        new SimpleRecord(2, \"b\"),\n        new SimpleRecord(3, \"c\")\n    );\n    Dataset<Row> df = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n    df.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation);\n\n    try (CloseableIterable<FileScanTask> tasks = table.newScan().planFiles()) {\n      tasks.forEach(task -> {\n        FileFormat fileFormat = FileFormat.fromFileName(task.file().path());\n        Assert.assertEquals(FileFormat.AVRO, fileFormat);\n      });\n    }\n  }\n","date":"2019-06-19 06:14:37","endLine":132,"groupId":"3893","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testNoWriteFormatOption","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/ee/2190231873354ca546cfa263ac713a74fa56eb.src","preCode":"  public void testNoWriteFormatOption() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    Map<String, String> options = Maps.newHashMap();\n    options.put(TableProperties.DEFAULT_FILE_FORMAT, \"avro\");\n    Table table = tables.create(SCHEMA, spec, options, tableLocation);\n\n    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n        new SimpleRecord(1, \"a\"),\n        new SimpleRecord(2, \"b\"),\n        new SimpleRecord(3, \"c\")\n    );\n    Dataset<Row> df = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n    df.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation);\n\n    try (CloseableIterable<FileScanTask> tasks = table.newScan().planFiles()) {\n      tasks.forEach(task -> {\n        FileFormat fileFormat = FileFormat.fromFileName(task.file().path());\n        Assert.assertEquals(FileFormat.AVRO, fileFormat);\n      });\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":106,"status":"B"}],"commitId":"9881f21a28a3bb189c8df94791389ccbc35390ab","commitMessage":"@@@Make Spark options use a consistent style (#224)\n\n","date":"2019-06-19 06:14:37","modifiedFileCount":"1","status":"B","submitter":"Anton Okolnychyi"},{"authorTime":"2019-06-19 06:14:37","codes":[{"authorDate":"2020-12-29 05:55:09","commitOrder":2,"curCode":"  public void testWriteFormatOptionOverridesTableProperties() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    Map<String, String> options = Maps.newHashMap();\n    options.put(TableProperties.DEFAULT_FILE_FORMAT, \"avro\");\n    Table table = tables.create(SCHEMA, spec, options, tableLocation);\n\n    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n        new SimpleRecord(1, \"a\"),\n        new SimpleRecord(2, \"b\"),\n        new SimpleRecord(3, \"c\")\n    );\n    Dataset<Row> df = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n    df.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .option(SparkWriteOptions.WRITE_FORMAT, \"parquet\")\n        .mode(SaveMode.Append)\n        .save(tableLocation);\n\n    try (CloseableIterable<FileScanTask> tasks = table.newScan().planFiles()) {\n      tasks.forEach(task -> {\n        FileFormat fileFormat = FileFormat.fromFileName(task.file().path());\n        Assert.assertEquals(FileFormat.PARQUET, fileFormat);\n      });\n    }\n  }\n","date":"2020-12-29 05:55:09","endLine":108,"groupId":"10379","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testWriteFormatOptionOverridesTableProperties","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/ae/a16879033d16ede5f8e2fd19a05dddfd472ae6.src","preCode":"  public void testWriteFormatOptionOverridesTableProperties() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    Map<String, String> options = Maps.newHashMap();\n    options.put(TableProperties.DEFAULT_FILE_FORMAT, \"avro\");\n    Table table = tables.create(SCHEMA, spec, options, tableLocation);\n\n    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n        new SimpleRecord(1, \"a\"),\n        new SimpleRecord(2, \"b\"),\n        new SimpleRecord(3, \"c\")\n    );\n    Dataset<Row> df = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n    df.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .option(\"write-format\", \"parquet\")\n        .mode(\"append\")\n        .save(tableLocation);\n\n    try (CloseableIterable<FileScanTask> tasks = table.newScan().planFiles()) {\n      tasks.forEach(task -> {\n        FileFormat fileFormat = FileFormat.fromFileName(task.file().path());\n        Assert.assertEquals(FileFormat.PARQUET, fileFormat);\n      });\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":81,"status":"M"},{"authorDate":"2019-06-19 06:14:37","commitOrder":2,"curCode":"  public void testNoWriteFormatOption() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    Map<String, String> options = Maps.newHashMap();\n    options.put(TableProperties.DEFAULT_FILE_FORMAT, \"avro\");\n    Table table = tables.create(SCHEMA, spec, options, tableLocation);\n\n    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n        new SimpleRecord(1, \"a\"),\n        new SimpleRecord(2, \"b\"),\n        new SimpleRecord(3, \"c\")\n    );\n    Dataset<Row> df = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n    df.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation);\n\n    try (CloseableIterable<FileScanTask> tasks = table.newScan().planFiles()) {\n      tasks.forEach(task -> {\n        FileFormat fileFormat = FileFormat.fromFileName(task.file().path());\n        Assert.assertEquals(FileFormat.AVRO, fileFormat);\n      });\n    }\n  }\n","date":"2019-06-19 06:14:37","endLine":132,"groupId":"10379","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testNoWriteFormatOption","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/ee/2190231873354ca546cfa263ac713a74fa56eb.src","preCode":"  public void testNoWriteFormatOption() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    Map<String, String> options = Maps.newHashMap();\n    options.put(TableProperties.DEFAULT_FILE_FORMAT, \"avro\");\n    Table table = tables.create(SCHEMA, spec, options, tableLocation);\n\n    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n        new SimpleRecord(1, \"a\"),\n        new SimpleRecord(2, \"b\"),\n        new SimpleRecord(3, \"c\")\n    );\n    Dataset<Row> df = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n    df.select(\"id\", \"data\").write()\n        .format(\"iceberg\")\n        .mode(\"append\")\n        .save(tableLocation);\n\n    try (CloseableIterable<FileScanTask> tasks = table.newScan().planFiles()) {\n      tasks.forEach(task -> {\n        FileFormat fileFormat = FileFormat.fromFileName(task.file().path());\n        Assert.assertEquals(FileFormat.AVRO, fileFormat);\n      });\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":106,"status":"N"}],"commitId":"cbb244759ae218b85a7e68a1568fddaf815aab3c","commitMessage":"@@@Spark: Use constants for DF read and write options (#1933)\n\n","date":"2020-12-29 05:55:09","modifiedFileCount":"22","status":"M","submitter":"Karuppayya"}]
