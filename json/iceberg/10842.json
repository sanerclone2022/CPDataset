[{"authorTime":"2020-11-21 07:02:06","codes":[{"authorDate":"2020-11-21 07:02:06","commitOrder":1,"curCode":"  public void testSparkSessionCatalogHadoopTable() throws Exception {\n    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\");\n    spark.conf().set(\"spark.sql.catalog.spark_catalog.warehouse\", tableLocation);\n    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n\n    String[] database = {\"default\"};\n    Identifier id = Identifier.of(database, \"table\");\n    Map<String, String> options = Maps.newHashMap();\n    Transform[] transforms = {};\n    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n    SparkTable table = (SparkTable) cat.loadTable(id);\n\n    spark.sql(\"INSERT INTO default.table VALUES (1,1,1)\");\n\n    String location = table.table().location().replaceFirst(\"file:\", \"\");\n    new File(location + \"/data/trashfile\").createNewFile();\n\n    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n        .olderThan(System.currentTimeMillis() + 1000).execute();\n    Assert.assertTrue(\"trash file should be removed\",\n        results.contains(\"file:\" + location + \"/data/trashfile\"));\n  }\n","date":"2020-11-21 07:02:06","endLine":134,"groupId":"392","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testSparkSessionCatalogHadoopTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/99/97538e9c27d8b4d7b275429f7b2aa398736801.src","preCode":"  public void testSparkSessionCatalogHadoopTable() throws Exception {\n    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\");\n    spark.conf().set(\"spark.sql.catalog.spark_catalog.warehouse\", tableLocation);\n    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n\n    String[] database = {\"default\"};\n    Identifier id = Identifier.of(database, \"table\");\n    Map<String, String> options = Maps.newHashMap();\n    Transform[] transforms = {};\n    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n    SparkTable table = (SparkTable) cat.loadTable(id);\n\n    spark.sql(\"INSERT INTO default.table VALUES (1,1,1)\");\n\n    String location = table.table().location().replaceFirst(\"file:\", \"\");\n    new File(location + \"/data/trashfile\").createNewFile();\n\n    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n        .olderThan(System.currentTimeMillis() + 1000).execute();\n    Assert.assertTrue(\"trash file should be removed\",\n        results.contains(\"file:\" + location + \"/data/trashfile\"));\n  }\n","realPath":"spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":112,"status":"B"},{"authorDate":"2020-11-21 07:02:06","commitOrder":1,"curCode":"  public void testSparkSessionCatalogHiveTable() throws Exception {\n    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hive\");\n    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n\n    String[] database = {\"default\"};\n    Identifier id = Identifier.of(database, \"sessioncattest\");\n    Map<String, String> options = Maps.newHashMap();\n    Transform[] transforms = {};\n    cat.dropTable(id);\n    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n    SparkTable table = (SparkTable) cat.loadTable(id);\n\n    spark.sql(\"INSERT INTO default.sessioncattest VALUES (1,1,1)\");\n\n    String location = table.table().location().replaceFirst(\"file:\", \"\");\n    new File(location + \"/data/trashfile\").createNewFile();\n\n    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n        .olderThan(System.currentTimeMillis() + 1000).execute();\n    Assert.assertTrue(\"trash file should be removed\",\n        results.contains(\"file:\" + location + \"/data/trashfile\"));\n  }\n","date":"2020-11-21 07:02:06","endLine":159,"groupId":"2428","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testSparkSessionCatalogHiveTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/99/97538e9c27d8b4d7b275429f7b2aa398736801.src","preCode":"  public void testSparkSessionCatalogHiveTable() throws Exception {\n    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hive\");\n    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n\n    String[] database = {\"default\"};\n    Identifier id = Identifier.of(database, \"sessioncattest\");\n    Map<String, String> options = Maps.newHashMap();\n    Transform[] transforms = {};\n    cat.dropTable(id);\n    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n    SparkTable table = (SparkTable) cat.loadTable(id);\n\n    spark.sql(\"INSERT INTO default.sessioncattest VALUES (1,1,1)\");\n\n    String location = table.table().location().replaceFirst(\"file:\", \"\");\n    new File(location + \"/data/trashfile\").createNewFile();\n\n    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n        .olderThan(System.currentTimeMillis() + 1000).execute();\n    Assert.assertTrue(\"trash file should be removed\",\n        results.contains(\"file:\" + location + \"/data/trashfile\"));\n  }\n","realPath":"spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":137,"status":"B"}],"commitId":"064d8028be0a77b57d66baad7bc7fe3b9f5ebf77","commitMessage":"@@@Spark: Fix resolution of metadata tables in actions (#1784)\n\n","date":"2020-11-21 07:02:06","modifiedFileCount":"3","status":"B","submitter":"Russell Spitzer"},{"authorTime":"2020-12-23 02:09:13","codes":[{"authorDate":"2020-12-23 02:09:13","commitOrder":2,"curCode":"  public void testSparkSessionCatalogHadoopTable() throws Exception {\n    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\");\n    spark.conf().set(\"spark.sql.catalog.spark_catalog.warehouse\", tableLocation);\n    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n\n    String[] database = {\"default\"};\n    Identifier id = Identifier.of(database, \"table\");\n    Map<String, String> options = Maps.newHashMap();\n    Transform[] transforms = {};\n    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n    SparkTable table = (SparkTable) cat.loadTable(id);\n\n    spark.sql(\"INSERT INTO default.table VALUES (1,1,1)\");\n\n    String location = table.table().location().replaceFirst(\"file:\", \"\");\n    new File(location + \"/data/trashfile\").createNewFile();\n\n    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n        .olderThan(System.currentTimeMillis() + 1000).execute();\n    Assert.assertTrue(\"trash file should be removed\",\n        results.contains(\"file:\" + location + \"/data/trashfile\"));\n    \r\n    spark.conf().unset(\"spark.sql.catalog.spark_catalog\");\n    spark.conf().unset(\"spark.sql.catalog.spark_catalog.type\");\n    spark.conf().unset(\"spark.sql.catalog.spark_catalog.warehouse\");\n  }\n","date":"2020-12-23 02:09:13","endLine":138,"groupId":"392","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testSparkSessionCatalogHadoopTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/fa/429ff0cc1c3cd4ce07449332e3acf12c75bffb.src","preCode":"  public void testSparkSessionCatalogHadoopTable() throws Exception {\n    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\");\n    spark.conf().set(\"spark.sql.catalog.spark_catalog.warehouse\", tableLocation);\n    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n\n    String[] database = {\"default\"};\n    Identifier id = Identifier.of(database, \"table\");\n    Map<String, String> options = Maps.newHashMap();\n    Transform[] transforms = {};\n    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n    SparkTable table = (SparkTable) cat.loadTable(id);\n\n    spark.sql(\"INSERT INTO default.table VALUES (1,1,1)\");\n\n    String location = table.table().location().replaceFirst(\"file:\", \"\");\n    new File(location + \"/data/trashfile\").createNewFile();\n\n    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n        .olderThan(System.currentTimeMillis() + 1000).execute();\n    Assert.assertTrue(\"trash file should be removed\",\n        results.contains(\"file:\" + location + \"/data/trashfile\"));\n  }\n","realPath":"spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":112,"status":"M"},{"authorDate":"2020-12-23 02:09:13","commitOrder":2,"curCode":"  public void testSparkSessionCatalogHiveTable() throws Exception {\n    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hive\");\n    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n\n    String[] database = {\"default\"};\n    Identifier id = Identifier.of(database, \"sessioncattest\");\n    Map<String, String> options = Maps.newHashMap();\n    Transform[] transforms = {};\n    cat.dropTable(id);\n    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n    SparkTable table = (SparkTable) cat.loadTable(id);\n\n    spark.sql(\"INSERT INTO default.sessioncattest VALUES (1,1,1)\");\n\n    String location = table.table().location().replaceFirst(\"file:\", \"\");\n    new File(location + \"/data/trashfile\").createNewFile();\n\n    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n        .olderThan(System.currentTimeMillis() + 1000).execute();\n    Assert.assertTrue(\"trash file should be removed\",\n        results.contains(\"file:\" + location + \"/data/trashfile\"));\n    \r\n    spark.conf().unset(\"spark.sql.catalog.spark_catalog\");\n    spark.conf().unset(\"spark.sql.catalog.spark_catalog.type\");\n  }\n","date":"2020-12-23 02:09:13","endLine":166,"groupId":"2428","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testSparkSessionCatalogHiveTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/fa/429ff0cc1c3cd4ce07449332e3acf12c75bffb.src","preCode":"  public void testSparkSessionCatalogHiveTable() throws Exception {\n    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hive\");\n    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n\n    String[] database = {\"default\"};\n    Identifier id = Identifier.of(database, \"sessioncattest\");\n    Map<String, String> options = Maps.newHashMap();\n    Transform[] transforms = {};\n    cat.dropTable(id);\n    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n    SparkTable table = (SparkTable) cat.loadTable(id);\n\n    spark.sql(\"INSERT INTO default.sessioncattest VALUES (1,1,1)\");\n\n    String location = table.table().location().replaceFirst(\"file:\", \"\");\n    new File(location + \"/data/trashfile\").createNewFile();\n\n    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n        .olderThan(System.currentTimeMillis() + 1000).execute();\n    Assert.assertTrue(\"trash file should be removed\",\n        results.contains(\"file:\" + location + \"/data/trashfile\"));\n  }\n","realPath":"spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":141,"status":"M"}],"commitId":"2f136e9da5630e1f4f402f847b1d9e6a40c7401a","commitMessage":"@@@Spark: 3.x: Support custom catalogs in IcebergSource (#1783)\n\n","date":"2020-12-23 02:09:13","modifiedFileCount":"7","status":"M","submitter":"Ryan Murray"},{"authorTime":"2021-06-22 05:48:49","codes":[{"authorDate":"2021-06-22 05:48:49","commitOrder":3,"curCode":"  public void testSparkSessionCatalogHadoopTable() throws Exception {\n    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\");\n    spark.conf().set(\"spark.sql.catalog.spark_catalog.warehouse\", tableLocation);\n    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n\n    String[] database = {\"default\"};\n    Identifier id = Identifier.of(database, \"table\");\n    Map<String, String> options = Maps.newHashMap();\n    Transform[] transforms = {};\n    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n    SparkTable table = (SparkTable) cat.loadTable(id);\n\n    spark.sql(\"INSERT INTO default.table VALUES (1,1,1)\");\n\n    String location = table.table().location().replaceFirst(\"file:\", \"\");\n    new File(location + \"/data/trashfile\").createNewFile();\n\n    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n        .olderThan(System.currentTimeMillis() + 1000).execute();\n    Assert.assertTrue(\"trash file should be removed\",\n        results.contains(\"file:\" + location + \"/data/trashfile\"));\n  }\n","date":"2021-06-22 05:48:49","endLine":135,"groupId":"10842","id":5,"instanceNumber":1,"isCurCommit":1,"methodName":"testSparkSessionCatalogHadoopTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/02/a66424b07db91f5f75b58e807f7dd98c96d021.src","preCode":"  public void testSparkSessionCatalogHadoopTable() throws Exception {\n    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\");\n    spark.conf().set(\"spark.sql.catalog.spark_catalog.warehouse\", tableLocation);\n    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n\n    String[] database = {\"default\"};\n    Identifier id = Identifier.of(database, \"table\");\n    Map<String, String> options = Maps.newHashMap();\n    Transform[] transforms = {};\n    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n    SparkTable table = (SparkTable) cat.loadTable(id);\n\n    spark.sql(\"INSERT INTO default.table VALUES (1,1,1)\");\n\n    String location = table.table().location().replaceFirst(\"file:\", \"\");\n    new File(location + \"/data/trashfile\").createNewFile();\n\n    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n        .olderThan(System.currentTimeMillis() + 1000).execute();\n    Assert.assertTrue(\"trash file should be removed\",\n        results.contains(\"file:\" + location + \"/data/trashfile\"));\n    \r\n    spark.conf().unset(\"spark.sql.catalog.spark_catalog\");\n    spark.conf().unset(\"spark.sql.catalog.spark_catalog.type\");\n    spark.conf().unset(\"spark.sql.catalog.spark_catalog.warehouse\");\n  }\n","realPath":"spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":113,"status":"M"},{"authorDate":"2021-06-22 05:48:49","commitOrder":3,"curCode":"  public void testSparkSessionCatalogHiveTable() throws Exception {\n    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hive\");\n    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n\n    String[] database = {\"default\"};\n    Identifier id = Identifier.of(database, \"sessioncattest\");\n    Map<String, String> options = Maps.newHashMap();\n    Transform[] transforms = {};\n    cat.dropTable(id);\n    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n    SparkTable table = (SparkTable) cat.loadTable(id);\n\n    spark.sql(\"INSERT INTO default.sessioncattest VALUES (1,1,1)\");\n\n    String location = table.table().location().replaceFirst(\"file:\", \"\");\n    new File(location + \"/data/trashfile\").createNewFile();\n\n    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n        .olderThan(System.currentTimeMillis() + 1000).execute();\n    Assert.assertTrue(\"trash file should be removed\",\n        results.contains(\"file:\" + location + \"/data/trashfile\"));\n  }\n","date":"2021-06-22 05:48:49","endLine":160,"groupId":"10842","id":6,"instanceNumber":2,"isCurCommit":1,"methodName":"testSparkSessionCatalogHiveTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/02/a66424b07db91f5f75b58e807f7dd98c96d021.src","preCode":"  public void testSparkSessionCatalogHiveTable() throws Exception {\n    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hive\");\n    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n\n    String[] database = {\"default\"};\n    Identifier id = Identifier.of(database, \"sessioncattest\");\n    Map<String, String> options = Maps.newHashMap();\n    Transform[] transforms = {};\n    cat.dropTable(id);\n    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n    SparkTable table = (SparkTable) cat.loadTable(id);\n\n    spark.sql(\"INSERT INTO default.sessioncattest VALUES (1,1,1)\");\n\n    String location = table.table().location().replaceFirst(\"file:\", \"\");\n    new File(location + \"/data/trashfile\").createNewFile();\n\n    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n        .olderThan(System.currentTimeMillis() + 1000).execute();\n    Assert.assertTrue(\"trash file should be removed\",\n        results.contains(\"file:\" + location + \"/data/trashfile\"));\n    \r\n    spark.conf().unset(\"spark.sql.catalog.spark_catalog\");\n    spark.conf().unset(\"spark.sql.catalog.spark_catalog.type\");\n  }\n","realPath":"spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":138,"status":"M"}],"commitId":"0c784fae03e792d98aee46f4c71da5e180611f10","commitMessage":"@@@Core: Add JDBC catalog implementation (#1870)\n\n","date":"2021-06-22 05:48:49","modifiedFileCount":"2","status":"M","submitter":"ismail simsek"}]
