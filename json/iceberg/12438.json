[{"authorTime":"2020-08-20 08:36:46","codes":[{"authorDate":"2019-10-28 05:06:22","commitOrder":4,"curCode":"    public ParquetValueReader<?> struct(\n        Types.StructType expected, GroupType struct, List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (partitionValues.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(partitionValues.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new TupleReader(types, reorderedFields);\n    }\n","date":"2019-10-28 05:06:22","endLine":173,"groupId":"224","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/aa/3f1dc7d6a423647980e090c67f868e9174860f.src","preCode":"    public ParquetValueReader<?> struct(\n        Types.StructType expected, GroupType struct, List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (partitionValues.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(partitionValues.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new TupleReader(types, reorderedFields);\n    }\n","realPath":"pig/src/main/java/org/apache/iceberg/pig/PigParquetReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":135,"status":"NB"},{"authorDate":"2020-08-20 08:36:46","commitOrder":4,"curCode":"    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n                                              List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        if (fieldType.getId() != null) {\n          int id = fieldType.getId().intValue();\n          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n          typesById.put(id, fieldType);\n        }\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (idToConstant.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new RowDataReader(types, reorderedFields);\n    }\n","date":"2020-08-20 08:36:46","endLine":128,"groupId":"1606","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/30/12544cba8367880b92039dd5d6a9552d50f001.src","preCode":"    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n                                              List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        if (fieldType.getId() != null) {\n          int id = fieldType.getId().intValue();\n          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n          typesById.put(id, fieldType);\n        }\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (idToConstant.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new RowDataReader(types, reorderedFields);\n    }\n","realPath":"flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":88,"status":"B"}],"commitId":"6cb2db7acb06891502ac4af1845b239ed7cb521d","commitMessage":"@@@Flink: Read Parquet as RowData using a schema visitor (#1266)\n\n","date":"2020-08-20 08:36:46","modifiedFileCount":"5","status":"M","submitter":"Chen Junjie"},{"authorTime":"2020-10-15 07:11:41","codes":[{"authorDate":"2019-10-28 05:06:22","commitOrder":5,"curCode":"    public ParquetValueReader<?> struct(\n        Types.StructType expected, GroupType struct, List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (partitionValues.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(partitionValues.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new TupleReader(types, reorderedFields);\n    }\n","date":"2019-10-28 05:06:22","endLine":173,"groupId":"224","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/aa/3f1dc7d6a423647980e090c67f868e9174860f.src","preCode":"    public ParquetValueReader<?> struct(\n        Types.StructType expected, GroupType struct, List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (partitionValues.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(partitionValues.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new TupleReader(types, reorderedFields);\n    }\n","realPath":"pig/src/main/java/org/apache/iceberg/pig/PigParquetReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":135,"status":"N"},{"authorDate":"2020-10-15 07:11:41","commitOrder":5,"curCode":"    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n                                              List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        if (fieldType.getId() != null) {\n          int id = fieldType.getId().intValue();\n          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n          typesById.put(id, fieldType);\n        }\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (idToConstant.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n          types.add(null);\n        } else if (id == MetadataColumns.ROW_POSITION.fieldId()) {\n          reorderedFields.add(ParquetValueReaders.position());\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new RowDataReader(types, reorderedFields);\n    }\n","date":"2020-10-15 07:11:41","endLine":132,"groupId":"1606","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/3c/9bd5cd56c13d9f6e2e4e251fa1a0f8a48d7e05.src","preCode":"    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n                                              List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        if (fieldType.getId() != null) {\n          int id = fieldType.getId().intValue();\n          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n          typesById.put(id, fieldType);\n        }\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (idToConstant.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new RowDataReader(types, reorderedFields);\n    }\n","realPath":"flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":89,"status":"M"}],"commitId":"a238a90eb987bfbf5d14b5cab8d109e53a75e861","commitMessage":"@@@Flink: Apply row-level deletes when reading (#1517)\n\n","date":"2020-10-15 07:11:41","modifiedFileCount":"12","status":"M","submitter":"Chen Junjie"},{"authorTime":"2020-12-08 10:16:47","codes":[{"authorDate":"2019-10-28 05:06:22","commitOrder":6,"curCode":"    public ParquetValueReader<?> struct(\n        Types.StructType expected, GroupType struct, List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (partitionValues.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(partitionValues.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new TupleReader(types, reorderedFields);\n    }\n","date":"2019-10-28 05:06:22","endLine":173,"groupId":"224","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/aa/3f1dc7d6a423647980e090c67f868e9174860f.src","preCode":"    public ParquetValueReader<?> struct(\n        Types.StructType expected, GroupType struct, List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (partitionValues.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(partitionValues.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new TupleReader(types, reorderedFields);\n    }\n","realPath":"pig/src/main/java/org/apache/iceberg/pig/PigParquetReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":135,"status":"N"},{"authorDate":"2020-12-08 10:16:47","commitOrder":6,"curCode":"    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n                                              List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        if (fieldReaders.get(i) != null) {\n          int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n          if (fieldType.getId() != null) {\n            int id = fieldType.getId().intValue();\n            readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n            typesById.put(id, fieldType);\n          }\n        }\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (idToConstant.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n          types.add(null);\n        } else if (id == MetadataColumns.ROW_POSITION.fieldId()) {\n          reorderedFields.add(ParquetValueReaders.position());\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new RowDataReader(types, reorderedFields);\n    }\n","date":"2020-12-08 10:16:47","endLine":134,"groupId":"1606","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/6f/95d652ceab5aa73312fb1adc5c22471d9a946d.src","preCode":"    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n                                              List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        if (fieldType.getId() != null) {\n          int id = fieldType.getId().intValue();\n          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n          typesById.put(id, fieldType);\n        }\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (idToConstant.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n          types.add(null);\n        } else if (id == MetadataColumns.ROW_POSITION.fieldId()) {\n          reorderedFields.add(ParquetValueReaders.position());\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new RowDataReader(types, reorderedFields);\n    }\n","realPath":"flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":89,"status":"M"}],"commitId":"c3dd3e19a766b2111fb8378eb7834587dd65eb1d","commitMessage":"@@@Flink: fix projection NPE caused by timestamp type (#1882)\n\n","date":"2020-12-08 10:16:47","modifiedFileCount":"2","status":"M","submitter":"Chen Junjie"},{"authorTime":"2021-06-19 07:38:18","codes":[{"authorDate":"2019-10-28 05:06:22","commitOrder":7,"curCode":"    public ParquetValueReader<?> struct(\n        Types.StructType expected, GroupType struct, List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (partitionValues.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(partitionValues.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new TupleReader(types, reorderedFields);\n    }\n","date":"2019-10-28 05:06:22","endLine":173,"groupId":"12438","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/aa/3f1dc7d6a423647980e090c67f868e9174860f.src","preCode":"    public ParquetValueReader<?> struct(\n        Types.StructType expected, GroupType struct, List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (partitionValues.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(partitionValues.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new TupleReader(types, reorderedFields);\n    }\n","realPath":"pig/src/main/java/org/apache/iceberg/pig/PigParquetReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":135,"status":"N"},{"authorDate":"2021-06-19 07:38:18","commitOrder":7,"curCode":"    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n                                              List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        if (fieldReaders.get(i) != null) {\n          int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n          if (fieldType.getId() != null) {\n            int id = fieldType.getId().intValue();\n            readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n            typesById.put(id, fieldType);\n          }\n        }\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (idToConstant.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n          types.add(null);\n        } else if (id == MetadataColumns.ROW_POSITION.fieldId()) {\n          reorderedFields.add(ParquetValueReaders.position());\n          types.add(null);\n        } else if (id == MetadataColumns.IS_DELETED.fieldId()) {\n          reorderedFields.add(ParquetValueReaders.constant(false));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new RowDataReader(types, reorderedFields);\n    }\n","date":"2021-06-19 07:38:18","endLine":137,"groupId":"12438","id":8,"instanceNumber":2,"isCurCommit":1,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/d7/088b4700e5b1507961058ecb54569fea5130ff.src","preCode":"    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n                                              List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        if (fieldReaders.get(i) != null) {\n          int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n          if (fieldType.getId() != null) {\n            int id = fieldType.getId().intValue();\n            readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n            typesById.put(id, fieldType);\n          }\n        }\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (idToConstant.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n          types.add(null);\n        } else if (id == MetadataColumns.ROW_POSITION.fieldId()) {\n          reorderedFields.add(ParquetValueReaders.position());\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new RowDataReader(types, reorderedFields);\n    }\n","realPath":"flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":89,"status":"M"}],"commitId":"a9f43636028a67eed39e3ed5903bc4c148ebaf76","commitMessage":"@@@Core: Add delete marker metadata column (#2538)\n\n","date":"2021-06-19 07:38:18","modifiedFileCount":"12","status":"M","submitter":"Chen Junjie"}]
