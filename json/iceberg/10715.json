[{"authorTime":"2020-06-30 08:56:05","codes":[{"authorDate":"2020-06-30 08:56:05","commitOrder":1,"curCode":"  public void testUnpartitionedTimestampFilter() {\n    CaseInsensitiveStringMap options = new CaseInsensitiveStringMap(ImmutableMap.of(\n        \"path\", unpartitioned.toString())\n    );\n\n    SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options);\n\n    pushFilters(builder, LessThan.apply(\"ts\", \"2017-12-22T00:00:00+00:00\"));\n    Batch scan = builder.build().toBatch();\n\n    InputPartition[] tasks = scan.planInputPartitions();\n    Assert.assertEquals(\"Should only create one task for a small file\", 1, tasks.length);\n\n    assertEqualsSafe(SCHEMA.asStruct(), expected(5, 6, 7, 8, 9),\n        read(unpartitioned.toString(), \"ts < cast('2017-12-22 00:00:00+00:00' as timestamp)\"));\n  }\n","date":"2020-06-30 08:56:05","endLine":279,"groupId":"4950","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testUnpartitionedTimestampFilter","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/7d/d308dcbed19b9615af2fe851dd00df86c8c855.src","preCode":"  public void testUnpartitionedTimestampFilter() {\n    CaseInsensitiveStringMap options = new CaseInsensitiveStringMap(ImmutableMap.of(\n        \"path\", unpartitioned.toString())\n    );\n\n    SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options);\n\n    pushFilters(builder, LessThan.apply(\"ts\", \"2017-12-22T00:00:00+00:00\"));\n    Batch scan = builder.build().toBatch();\n\n    InputPartition[] tasks = scan.planInputPartitions();\n    Assert.assertEquals(\"Should only create one task for a small file\", 1, tasks.length);\n\n    assertEqualsSafe(SCHEMA.asStruct(), expected(5, 6, 7, 8, 9),\n        read(unpartitioned.toString(), \"ts < cast('2017-12-22 00:00:00+00:00' as timestamp)\"));\n  }\n","realPath":"spark3/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":264,"status":"B"},{"authorDate":"2020-06-30 08:56:05","commitOrder":1,"curCode":"  public void testHourPartitionedTimestampFilters() {\n    Table table = buildPartitionedTable(\"partitioned_by_hour\", PARTITION_BY_HOUR, \"ts_hour\", \"ts\");\n\n    CaseInsensitiveStringMap options = new CaseInsensitiveStringMap(ImmutableMap.of(\"path\", table.location()));\n    Batch unfiltered = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options).build().toBatch();\n\n    Assert.assertEquals(\"Unfiltered table should created 9 read tasks\",\n        9, unfiltered.planInputPartitions().length);\n\n    {\n      SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options);\n\n      pushFilters(builder, LessThan.apply(\"ts\", \"2017-12-22T00:00:00+00:00\"));\n      Batch scan = builder.build().toBatch();\n\n      InputPartition[] tasks = scan.planInputPartitions();\n      Assert.assertEquals(\"Should create 4 tasks for 2017-12-21: 15, 17, 21, 22\", 4, tasks.length);\n\n      assertEqualsSafe(SCHEMA.asStruct(), expected(8, 9, 7, 6, 5),\n          read(table.location(), \"ts < cast('2017-12-22 00:00:00+00:00' as timestamp)\"));\n    }\n\n    {\n      SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options);\n\n      pushFilters(builder, And.apply(\n          GreaterThan.apply(\"ts\", \"2017-12-22T06:00:00+00:00\"),\n          LessThan.apply(\"ts\", \"2017-12-22T08:00:00+00:00\")));\n      Batch scan = builder.build().toBatch();\n\n      InputPartition[] tasks = scan.planInputPartitions();\n      Assert.assertEquals(\"Should create 2 tasks for 2017-12-22: 6, 7\", 2, tasks.length);\n\n      assertEqualsSafe(SCHEMA.asStruct(), expected(2, 1), read(table.location(),\n          \"ts > cast('2017-12-22 06:00:00+00:00' as timestamp) and \" +\n              \"ts < cast('2017-12-22 08:00:00+00:00' as timestamp)\"));\n    }\n  }\n","date":"2020-06-30 08:56:05","endLine":385,"groupId":"579","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testHourPartitionedTimestampFilters","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/7d/d308dcbed19b9615af2fe851dd00df86c8c855.src","preCode":"  public void testHourPartitionedTimestampFilters() {\n    Table table = buildPartitionedTable(\"partitioned_by_hour\", PARTITION_BY_HOUR, \"ts_hour\", \"ts\");\n\n    CaseInsensitiveStringMap options = new CaseInsensitiveStringMap(ImmutableMap.of(\"path\", table.location()));\n    Batch unfiltered = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options).build().toBatch();\n\n    Assert.assertEquals(\"Unfiltered table should created 9 read tasks\",\n        9, unfiltered.planInputPartitions().length);\n\n    {\n      SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options);\n\n      pushFilters(builder, LessThan.apply(\"ts\", \"2017-12-22T00:00:00+00:00\"));\n      Batch scan = builder.build().toBatch();\n\n      InputPartition[] tasks = scan.planInputPartitions();\n      Assert.assertEquals(\"Should create 4 tasks for 2017-12-21: 15, 17, 21, 22\", 4, tasks.length);\n\n      assertEqualsSafe(SCHEMA.asStruct(), expected(8, 9, 7, 6, 5),\n          read(table.location(), \"ts < cast('2017-12-22 00:00:00+00:00' as timestamp)\"));\n    }\n\n    {\n      SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options);\n\n      pushFilters(builder, And.apply(\n          GreaterThan.apply(\"ts\", \"2017-12-22T06:00:00+00:00\"),\n          LessThan.apply(\"ts\", \"2017-12-22T08:00:00+00:00\")));\n      Batch scan = builder.build().toBatch();\n\n      InputPartition[] tasks = scan.planInputPartitions();\n      Assert.assertEquals(\"Should create 2 tasks for 2017-12-22: 6, 7\", 2, tasks.length);\n\n      assertEqualsSafe(SCHEMA.asStruct(), expected(2, 1), read(table.location(),\n          \"ts > cast('2017-12-22 06:00:00+00:00' as timestamp) and \" +\n              \"ts < cast('2017-12-22 08:00:00+00:00' as timestamp)\"));\n    }\n  }\n","realPath":"spark3/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":348,"status":"B"}],"commitId":"51c930e33867e54b3d7e0159b11d6b9c4bc81f1a","commitMessage":"@@@Spark: Add Spark 3 data source classes (#1124)\n\n","date":"2020-06-30 08:56:05","modifiedFileCount":"7","status":"B","submitter":"Ryan Blue"},{"authorTime":"2020-07-14 05:27:36","codes":[{"authorDate":"2020-07-14 05:27:36","commitOrder":2,"curCode":"  public void testUnpartitionedTimestampFilter() {\n    CaseInsensitiveStringMap options = new CaseInsensitiveStringMap(ImmutableMap.of(\n        \"path\", unpartitioned.toString())\n    );\n\n    SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options);\n\n    pushFilters(builder, LessThan.apply(\"ts\", \"2017-12-22T00:00:00+00:00\"));\n    Batch scan = builder.build().toBatch();\n\n    InputPartition[] tasks = scan.planInputPartitions();\n    Assert.assertEquals(\"Should only create one task for a small file\", 1, tasks.length);\n\n    assertEqualsSafe(SCHEMA.asStruct(), expected(5, 6, 7, 8, 9),\n        read(unpartitioned.toString(), vectorized, \"ts < cast('2017-12-22 00:00:00+00:00' as timestamp)\"));\n  }\n","date":"2020-07-14 05:27:36","endLine":297,"groupId":"10715","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testUnpartitionedTimestampFilter","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/9b/e99383873ff072f8b9a1577c9034bf17a6d465.src","preCode":"  public void testUnpartitionedTimestampFilter() {\n    CaseInsensitiveStringMap options = new CaseInsensitiveStringMap(ImmutableMap.of(\n        \"path\", unpartitioned.toString())\n    );\n\n    SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options);\n\n    pushFilters(builder, LessThan.apply(\"ts\", \"2017-12-22T00:00:00+00:00\"));\n    Batch scan = builder.build().toBatch();\n\n    InputPartition[] tasks = scan.planInputPartitions();\n    Assert.assertEquals(\"Should only create one task for a small file\", 1, tasks.length);\n\n    assertEqualsSafe(SCHEMA.asStruct(), expected(5, 6, 7, 8, 9),\n        read(unpartitioned.toString(), \"ts < cast('2017-12-22 00:00:00+00:00' as timestamp)\"));\n  }\n","realPath":"spark3/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":282,"status":"M"},{"authorDate":"2020-07-14 05:27:36","commitOrder":2,"curCode":"  public void testHourPartitionedTimestampFilters() {\n    Table table = buildPartitionedTable(\"partitioned_by_hour\", PARTITION_BY_HOUR, \"ts_hour\", \"ts\");\n\n    CaseInsensitiveStringMap options = new CaseInsensitiveStringMap(ImmutableMap.of(\"path\", table.location()));\n    Batch unfiltered = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options).build().toBatch();\n\n    Assert.assertEquals(\"Unfiltered table should created 9 read tasks\",\n        9, unfiltered.planInputPartitions().length);\n\n    {\n      SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options);\n\n      pushFilters(builder, LessThan.apply(\"ts\", \"2017-12-22T00:00:00+00:00\"));\n      Batch scan = builder.build().toBatch();\n\n      InputPartition[] tasks = scan.planInputPartitions();\n      Assert.assertEquals(\"Should create 4 tasks for 2017-12-21: 15, 17, 21, 22\", 4, tasks.length);\n\n      assertEqualsSafe(SCHEMA.asStruct(), expected(8, 9, 7, 6, 5),\n          read(table.location(), vectorized, \"ts < cast('2017-12-22 00:00:00+00:00' as timestamp)\"));\n    }\n\n    {\n      SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options);\n\n      pushFilters(builder, And.apply(\n          GreaterThan.apply(\"ts\", \"2017-12-22T06:00:00+00:00\"),\n          LessThan.apply(\"ts\", \"2017-12-22T08:00:00+00:00\")));\n      Batch scan = builder.build().toBatch();\n\n      InputPartition[] tasks = scan.planInputPartitions();\n      Assert.assertEquals(\"Should create 2 tasks for 2017-12-22: 6, 7\", 2, tasks.length);\n\n      assertEqualsSafe(SCHEMA.asStruct(), expected(2, 1), read(table.location(), vectorized,\n          \"ts > cast('2017-12-22 06:00:00+00:00' as timestamp) and \" +\n              \"ts < cast('2017-12-22 08:00:00+00:00' as timestamp)\"));\n    }\n  }\n","date":"2020-07-14 05:27:36","endLine":403,"groupId":"10715","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testHourPartitionedTimestampFilters","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/9b/e99383873ff072f8b9a1577c9034bf17a6d465.src","preCode":"  public void testHourPartitionedTimestampFilters() {\n    Table table = buildPartitionedTable(\"partitioned_by_hour\", PARTITION_BY_HOUR, \"ts_hour\", \"ts\");\n\n    CaseInsensitiveStringMap options = new CaseInsensitiveStringMap(ImmutableMap.of(\"path\", table.location()));\n    Batch unfiltered = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options).build().toBatch();\n\n    Assert.assertEquals(\"Unfiltered table should created 9 read tasks\",\n        9, unfiltered.planInputPartitions().length);\n\n    {\n      SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options);\n\n      pushFilters(builder, LessThan.apply(\"ts\", \"2017-12-22T00:00:00+00:00\"));\n      Batch scan = builder.build().toBatch();\n\n      InputPartition[] tasks = scan.planInputPartitions();\n      Assert.assertEquals(\"Should create 4 tasks for 2017-12-21: 15, 17, 21, 22\", 4, tasks.length);\n\n      assertEqualsSafe(SCHEMA.asStruct(), expected(8, 9, 7, 6, 5),\n          read(table.location(), \"ts < cast('2017-12-22 00:00:00+00:00' as timestamp)\"));\n    }\n\n    {\n      SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options);\n\n      pushFilters(builder, And.apply(\n          GreaterThan.apply(\"ts\", \"2017-12-22T06:00:00+00:00\"),\n          LessThan.apply(\"ts\", \"2017-12-22T08:00:00+00:00\")));\n      Batch scan = builder.build().toBatch();\n\n      InputPartition[] tasks = scan.planInputPartitions();\n      Assert.assertEquals(\"Should create 2 tasks for 2017-12-22: 6, 7\", 2, tasks.length);\n\n      assertEqualsSafe(SCHEMA.asStruct(), expected(2, 1), read(table.location(),\n          \"ts > cast('2017-12-22 06:00:00+00:00' as timestamp) and \" +\n              \"ts < cast('2017-12-22 08:00:00+00:00' as timestamp)\"));\n    }\n  }\n","realPath":"spark3/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":366,"status":"M"}],"commitId":"6fab8f57bdb7e5fe7eadc3ff41558581338e1b69","commitMessage":"@@@Spark: Support ORC vectorized reads (#1189)\n\n","date":"2020-07-14 05:27:36","modifiedFileCount":"25","status":"M","submitter":"Shardul Mahadik"}]
