[{"authorTime":"2020-06-20 09:02:23","codes":[{"authorDate":"2020-06-30 08:56:05","commitOrder":2,"curCode":"  public Statistics estimateStatistics() {\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","date":"2020-06-30 08:56:05","endLine":186,"groupId":"1034","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"estimateStatistics","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/02/6f6baccce41dfb5a47a0ca3d229038dc7c4478.src","preCode":"  public Statistics estimateStatistics() {\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":174,"status":"B"},{"authorDate":"2020-06-20 09:02:23","commitOrder":2,"curCode":"  public Statistics estimateStatistics() {\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","date":"2020-06-20 09:02:23","endLine":290,"groupId":"1034","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"estimateStatistics","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/51/859de75ac284ad0708719a77d5ded9611f72bb.src","preCode":"  public Statistics estimateStatistics() {\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":278,"status":"NB"}],"commitId":"51c930e33867e54b3d7e0159b11d6b9c4bc81f1a","commitMessage":"@@@Spark: Add Spark 3 data source classes (#1124)\n\n","date":"2020-06-30 08:56:05","modifiedFileCount":"7","status":"M","submitter":"Ryan Blue"},{"authorTime":"2020-07-29 03:34:54","codes":[{"authorDate":"2020-07-29 03:34:54","commitOrder":3,"curCode":"  public Statistics estimateStatistics() {\n    if (filterExpressions == null || filterExpressions.isEmpty()) {\n      LOG.debug(\"using table metadata to estimate table statistics\");\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      Schema projectedSchema = expectedSchema != null ? expectedSchema : table.schema();\n      return new Stats(\n          SparkSchemaUtil.estimateSize(SparkSchemaUtil.convert(projectedSchema), totalRecords),\n          totalRecords);\n    }\n\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","date":"2020-07-29 03:34:54","endLine":205,"groupId":"1034","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"estimateStatistics","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/e7/b33852b1fa243667340df8582703cc6c097c31.src","preCode":"  public Statistics estimateStatistics() {\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":183,"status":"M"},{"authorDate":"2020-07-29 03:34:54","commitOrder":3,"curCode":"  public Statistics estimateStatistics() {\n    if (filterExpressions == null || filterExpressions == Expressions.alwaysTrue()) {\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      return new Stats(SparkSchemaUtil.estimateSize(lazyType(), totalRecords), totalRecords);\n    }\n\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","date":"2020-07-29 03:34:54","endLine":297,"groupId":"1034","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"estimateStatistics","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/35/b137c08a27b74bfd8d3383b77ec5bd34de849f.src","preCode":"  public Statistics estimateStatistics() {\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":279,"status":"M"}],"commitId":"ef801726bd6627ddcda1bd238894ea8ccdae5f39","commitMessage":"@@@Spark: Use snapshot summary data in estimateStatistics (#1221)\n\n","date":"2020-07-29 03:34:54","modifiedFileCount":"3","status":"M","submitter":"Sudarshan S"},{"authorTime":"2020-08-25 13:59:20","codes":[{"authorDate":"2020-08-25 13:59:20","commitOrder":4,"curCode":"  public Statistics estimateStatistics() {\n    \r\n    if (table.currentSnapshot() == null) {\n      return new Stats(0L, 0L);\n    }\n\n    if (filterExpressions == null || filterExpressions.isEmpty()) {\n      LOG.debug(\"using table metadata to estimate table statistics\");\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      Schema projectedSchema = expectedSchema != null ? expectedSchema : table.schema();\n      return new Stats(\n          SparkSchemaUtil.estimateSize(SparkSchemaUtil.convert(projectedSchema), totalRecords),\n          totalRecords);\n    }\n\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","date":"2020-08-25 13:59:20","endLine":205,"groupId":"1034","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"estimateStatistics","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/98/898eb2b8a847a5bb42d457b75f3fa64cd9dce8.src","preCode":"  public Statistics estimateStatistics() {\n    if (filterExpressions == null || filterExpressions.isEmpty()) {\n      LOG.debug(\"using table metadata to estimate table statistics\");\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      Schema projectedSchema = expectedSchema != null ? expectedSchema : table.schema();\n      return new Stats(\n          SparkSchemaUtil.estimateSize(SparkSchemaUtil.convert(projectedSchema), totalRecords),\n          totalRecords);\n    }\n\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":178,"status":"M"},{"authorDate":"2020-08-25 13:59:20","commitOrder":4,"curCode":"  public Statistics estimateStatistics() {\n    \r\n    if (table.currentSnapshot() == null) {\n      return new Stats(0L, 0L);\n    }\n\n    if (filterExpressions == null || filterExpressions == Expressions.alwaysTrue()) {\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      return new Stats(SparkSchemaUtil.estimateSize(lazyType(), totalRecords), totalRecords);\n    }\n\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","date":"2020-08-25 13:59:20","endLine":303,"groupId":"1034","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"estimateStatistics","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/52/c0338bffa247ec9d408471c58c9fd1dd88b293.src","preCode":"  public Statistics estimateStatistics() {\n    if (filterExpressions == null || filterExpressions == Expressions.alwaysTrue()) {\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      return new Stats(SparkSchemaUtil.estimateSize(lazyType(), totalRecords), totalRecords);\n    }\n\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":280,"status":"M"}],"commitId":"017ca465baf30c655cb289e03d2f34c8a3916aa5","commitMessage":"@@@Spark: Fix NPE while estimating statistics on an empty table (#1376)\n\n","date":"2020-08-25 13:59:20","modifiedFileCount":"3","status":"M","submitter":"manishmalhotrawork"},{"authorTime":"2020-08-26 03:04:50","codes":[{"authorDate":"2020-08-26 03:04:50","commitOrder":5,"curCode":"  public Statistics estimateStatistics() {\n    \r\n    if (table.currentSnapshot() == null) {\n      return new Stats(0L, 0L);\n    }\n\n    \r\n    if (!table.spec().isUnpartitioned() && (filterExpressions == null || filterExpressions.isEmpty())) {\n      LOG.debug(\"using table metadata to estimate table statistics\");\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      Schema projectedSchema = expectedSchema != null ? expectedSchema : table.schema();\n      return new Stats(\n          SparkSchemaUtil.estimateSize(SparkSchemaUtil.convert(projectedSchema), totalRecords),\n          totalRecords);\n    }\n\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","date":"2020-08-26 03:04:50","endLine":206,"groupId":"1034","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"estimateStatistics","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/6c/d166b72758d956a7eab324bc89f85a5685c10f.src","preCode":"  public Statistics estimateStatistics() {\n    \r\n    if (table.currentSnapshot() == null) {\n      return new Stats(0L, 0L);\n    }\n\n    if (filterExpressions == null || filterExpressions.isEmpty()) {\n      LOG.debug(\"using table metadata to estimate table statistics\");\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      Schema projectedSchema = expectedSchema != null ? expectedSchema : table.schema();\n      return new Stats(\n          SparkSchemaUtil.estimateSize(SparkSchemaUtil.convert(projectedSchema), totalRecords),\n          totalRecords);\n    }\n\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":178,"status":"M"},{"authorDate":"2020-08-26 03:04:50","commitOrder":5,"curCode":"  public Statistics estimateStatistics() {\n    \r\n    if (table.currentSnapshot() == null) {\n      return new Stats(0L, 0L);\n    }\n\n    \r\n    if (!table.spec().isUnpartitioned() && filterExpression() == Expressions.alwaysTrue()) {\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      return new Stats(SparkSchemaUtil.estimateSize(lazyType(), totalRecords), totalRecords);\n    }\n\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","date":"2020-08-26 03:04:50","endLine":304,"groupId":"1034","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"estimateStatistics","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/56/4f8f2168ff33a407e265b7c432457553c3bb4c.src","preCode":"  public Statistics estimateStatistics() {\n    \r\n    if (table.currentSnapshot() == null) {\n      return new Stats(0L, 0L);\n    }\n\n    if (filterExpressions == null || filterExpressions == Expressions.alwaysTrue()) {\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      return new Stats(SparkSchemaUtil.estimateSize(lazyType(), totalRecords), totalRecords);\n    }\n\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":280,"status":"M"}],"commitId":"6aa4c72c7a55e8bee12496a6005e160b47e146dd","commitMessage":"@@@Spark: Estimate stats using snapshot summary only for partitioned tables (#1379)\n\n","date":"2020-08-26 03:04:50","modifiedFileCount":"2","status":"M","submitter":"Anton Okolnychyi"},{"authorTime":"2020-08-26 03:04:50","codes":[{"authorDate":"2020-11-21 06:37:57","commitOrder":6,"curCode":"  public Statistics estimateStatistics() {\n    \r\n    if (table.currentSnapshot() == null) {\n      return new Stats(0L, 0L);\n    }\n\n    \r\n    if (!table.spec().isUnpartitioned() && filterExpressions.isEmpty()) {\n      LOG.debug(\"using table metadata to estimate table statistics\");\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      Schema projectedSchema = expectedSchema != null ? expectedSchema : table.schema();\n      return new Stats(\n          SparkSchemaUtil.estimateSize(SparkSchemaUtil.convert(projectedSchema), totalRecords),\n          totalRecords);\n    }\n\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","date":"2020-11-21 06:37:57","endLine":196,"groupId":"1034","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"estimateStatistics","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a6/265e18c40b012eefa7d715f48cd7fb475ed118.src","preCode":"  public Statistics estimateStatistics() {\n    \r\n    if (table.currentSnapshot() == null) {\n      return new Stats(0L, 0L);\n    }\n\n    \r\n    if (!table.spec().isUnpartitioned() && (filterExpressions == null || filterExpressions.isEmpty())) {\n      LOG.debug(\"using table metadata to estimate table statistics\");\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      Schema projectedSchema = expectedSchema != null ? expectedSchema : table.schema();\n      return new Stats(\n          SparkSchemaUtil.estimateSize(SparkSchemaUtil.convert(projectedSchema), totalRecords),\n          totalRecords);\n    }\n\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":168,"status":"M"},{"authorDate":"2020-08-26 03:04:50","commitOrder":6,"curCode":"  public Statistics estimateStatistics() {\n    \r\n    if (table.currentSnapshot() == null) {\n      return new Stats(0L, 0L);\n    }\n\n    \r\n    if (!table.spec().isUnpartitioned() && filterExpression() == Expressions.alwaysTrue()) {\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      return new Stats(SparkSchemaUtil.estimateSize(lazyType(), totalRecords), totalRecords);\n    }\n\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","date":"2020-08-26 03:04:50","endLine":304,"groupId":"1034","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"estimateStatistics","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/56/4f8f2168ff33a407e265b7c432457553c3bb4c.src","preCode":"  public Statistics estimateStatistics() {\n    \r\n    if (table.currentSnapshot() == null) {\n      return new Stats(0L, 0L);\n    }\n\n    \r\n    if (!table.spec().isUnpartitioned() && filterExpression() == Expressions.alwaysTrue()) {\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      return new Stats(SparkSchemaUtil.estimateSize(lazyType(), totalRecords), totalRecords);\n    }\n\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":280,"status":"N"}],"commitId":"1e52c817d05add2f3106e6e6767bb06f566b1c04","commitMessage":"@@@Spark: Add SparkMergeScan (#1782)\n\n","date":"2020-11-21 06:37:57","modifiedFileCount":"2","status":"M","submitter":"Anton Okolnychyi"},{"authorTime":"2020-08-26 03:04:50","codes":[{"authorDate":"2021-09-13 00:35:17","commitOrder":7,"curCode":"  public Statistics estimateStatistics() {\n    \r\n    if (table.currentSnapshot() == null) {\n      return new Stats(0L, 0L);\n    }\n\n    \r\n    if (!table.spec().isUnpartitioned() && filterExpressions.isEmpty()) {\n      LOG.debug(\"using table metadata to estimate table statistics\");\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      return new Stats(\n          SparkSchemaUtil.estimateSize(readSchema(), totalRecords),\n          totalRecords);\n    }\n\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        numRows += file.file().recordCount();\n      }\n    }\n\n    long sizeInBytes = SparkSchemaUtil.estimateSize(readSchema(), numRows);\n    return new Stats(sizeInBytes, numRows);\n  }\n","date":"2021-09-13 00:35:17","endLine":225,"groupId":"1034","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"estimateStatistics","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/e9/eda0b293943efca425adff3839e8fb0476ee05.src","preCode":"  public Statistics estimateStatistics() {\n    \r\n    if (table.currentSnapshot() == null) {\n      return new Stats(0L, 0L);\n    }\n\n    \r\n    if (!table.spec().isUnpartitioned() && filterExpressions.isEmpty()) {\n      LOG.debug(\"using table metadata to estimate table statistics\");\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      Schema projectedSchema = expectedSchema != null ? expectedSchema : table.schema();\n      return new Stats(\n          SparkSchemaUtil.estimateSize(SparkSchemaUtil.convert(projectedSchema), totalRecords),\n          totalRecords);\n    }\n\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":199,"status":"M"},{"authorDate":"2020-08-26 03:04:50","commitOrder":7,"curCode":"  public Statistics estimateStatistics() {\n    \r\n    if (table.currentSnapshot() == null) {\n      return new Stats(0L, 0L);\n    }\n\n    \r\n    if (!table.spec().isUnpartitioned() && filterExpression() == Expressions.alwaysTrue()) {\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      return new Stats(SparkSchemaUtil.estimateSize(lazyType(), totalRecords), totalRecords);\n    }\n\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","date":"2020-08-26 03:04:50","endLine":304,"groupId":"1034","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"estimateStatistics","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/56/4f8f2168ff33a407e265b7c432457553c3bb4c.src","preCode":"  public Statistics estimateStatistics() {\n    \r\n    if (table.currentSnapshot() == null) {\n      return new Stats(0L, 0L);\n    }\n\n    \r\n    if (!table.spec().isUnpartitioned() && filterExpression() == Expressions.alwaysTrue()) {\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      return new Stats(SparkSchemaUtil.estimateSize(lazyType(), totalRecords), totalRecords);\n    }\n\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":280,"status":"N"}],"commitId":"b1d5a58602620f3ce6aa684b1178bce8e4430ad9","commitMessage":"@@@Spark: Improve SparkBatchScan statistics estimation (#3038)\n\n","date":"2021-09-13 00:35:17","modifiedFileCount":"2","status":"M","submitter":"Wing Yew Poon"},{"authorTime":"2021-09-18 02:37:44","codes":[{"authorDate":"2021-09-13 00:35:17","commitOrder":8,"curCode":"  public Statistics estimateStatistics() {\n    \r\n    if (table.currentSnapshot() == null) {\n      return new Stats(0L, 0L);\n    }\n\n    \r\n    if (!table.spec().isUnpartitioned() && filterExpressions.isEmpty()) {\n      LOG.debug(\"using table metadata to estimate table statistics\");\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      return new Stats(\n          SparkSchemaUtil.estimateSize(readSchema(), totalRecords),\n          totalRecords);\n    }\n\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        numRows += file.file().recordCount();\n      }\n    }\n\n    long sizeInBytes = SparkSchemaUtil.estimateSize(readSchema(), numRows);\n    return new Stats(sizeInBytes, numRows);\n  }\n","date":"2021-09-13 00:35:17","endLine":225,"groupId":"10873","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"estimateStatistics","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/e9/eda0b293943efca425adff3839e8fb0476ee05.src","preCode":"  public Statistics estimateStatistics() {\n    \r\n    if (table.currentSnapshot() == null) {\n      return new Stats(0L, 0L);\n    }\n\n    \r\n    if (!table.spec().isUnpartitioned() && filterExpressions.isEmpty()) {\n      LOG.debug(\"using table metadata to estimate table statistics\");\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      return new Stats(\n          SparkSchemaUtil.estimateSize(readSchema(), totalRecords),\n          totalRecords);\n    }\n\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        numRows += file.file().recordCount();\n      }\n    }\n\n    long sizeInBytes = SparkSchemaUtil.estimateSize(readSchema(), numRows);\n    return new Stats(sizeInBytes, numRows);\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":199,"status":"N"},{"authorDate":"2021-09-18 02:37:44","commitOrder":8,"curCode":"  public Statistics estimateStatistics() {\n    \r\n    if (table.currentSnapshot() == null) {\n      return new Stats(0L, 0L);\n    }\n\n    \r\n    if (!table.spec().isUnpartitioned() && filterExpression() == Expressions.alwaysTrue()) {\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      return new Stats(SparkSchemaUtil.estimateSize(lazyType(), totalRecords), totalRecords);\n    }\n\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        numRows += file.file().recordCount();\n      }\n    }\n\n    long sizeInBytes = SparkSchemaUtil.estimateSize(lazyType(), numRows);\n    return new Stats(sizeInBytes, numRows);\n  }\n","date":"2021-09-18 02:37:44","endLine":319,"groupId":"10873","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"estimateStatistics","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/36/31e94acb59339a162f74ef1edf6f4763acae22.src","preCode":"  public Statistics estimateStatistics() {\n    \r\n    if (table.currentSnapshot() == null) {\n      return new Stats(0L, 0L);\n    }\n\n    \r\n    if (!table.spec().isUnpartitioned() && filterExpression() == Expressions.alwaysTrue()) {\n      long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n          SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n      return new Stats(SparkSchemaUtil.estimateSize(lazyType(), totalRecords), totalRecords);\n    }\n\n    long sizeInBytes = 0L;\n    long numRows = 0L;\n\n    for (CombinedScanTask task : tasks()) {\n      for (FileScanTask file : task.files()) {\n        sizeInBytes += file.length();\n        numRows += file.file().recordCount();\n      }\n    }\n\n    return new Stats(sizeInBytes, numRows);\n  }\n","realPath":"spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":296,"status":"M"}],"commitId":"ec2716edfbc74206528dc96fa19397d7fe69fa9e","commitMessage":"@@@Spark: Better statistics estimation for Spark 2 Reader (#3134)\n\nFollow-up to #3038.\nUse (estimated) row size * number of rows to estimate the size instead of adding up file sizes.\nThe row size is estimated from the pruned schema if we prune columns.","date":"2021-09-18 02:37:44","modifiedFileCount":"1","status":"M","submitter":"Wing Yew Poon"}]
