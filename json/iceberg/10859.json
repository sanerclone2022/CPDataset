[{"authorTime":"2020-06-30 08:56:05","codes":[{"authorDate":"2020-06-30 08:56:05","commitOrder":1,"curCode":"  public BatchWrite buildForBatch() {\n    \r\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    return new SparkBatchWrite(\n        table, io, encryptionManager, options, overwriteDynamic, overwriteByFilter, overwriteExpr, appId, wapId,\n        writeSchema, dsSchema);\n  }\n","date":"2020-06-30 08:56:05","endLine":119,"groupId":"4285","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"buildForBatch","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/ed/c7a312d24163a4453c68b42c1ef32d2bab717e.src","preCode":"  public BatchWrite buildForBatch() {\n    \r\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    return new SparkBatchWrite(\n        table, io, encryptionManager, options, overwriteDynamic, overwriteByFilter, overwriteExpr, appId, wapId,\n        writeSchema, dsSchema);\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkWriteBuilder.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":100,"status":"B"},{"authorDate":"2020-06-30 08:56:05","commitOrder":1,"curCode":"  public StreamingWrite buildForStreaming() {\n    \r\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    Preconditions.checkState(!overwriteDynamic,\n        \"Unsupported streaming operation: dynamic partition overwrite\");\n    Preconditions.checkState(!overwriteByFilter || overwriteExpr == Expressions.alwaysTrue(),\n        \"Unsupported streaming operation: overwrite by filter: %s\", overwriteExpr);\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    return new SparkStreamingWrite(\n        table, io, encryptionManager, options, overwriteByFilter, writeQueryId, appId, wapId, writeSchema, dsSchema);\n  }\n","date":"2020-06-30 08:56:05","endLine":146,"groupId":"4286","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"buildForStreaming","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/ed/c7a312d24163a4453c68b42c1ef32d2bab717e.src","preCode":"  public StreamingWrite buildForStreaming() {\n    \r\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    Preconditions.checkState(!overwriteDynamic,\n        \"Unsupported streaming operation: dynamic partition overwrite\");\n    Preconditions.checkState(!overwriteByFilter || overwriteExpr == Expressions.alwaysTrue(),\n        \"Unsupported streaming operation: overwrite by filter: %s\", overwriteExpr);\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    return new SparkStreamingWrite(\n        table, io, encryptionManager, options, overwriteByFilter, writeQueryId, appId, wapId, writeSchema, dsSchema);\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkWriteBuilder.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":122,"status":"B"}],"commitId":"51c930e33867e54b3d7e0159b11d6b9c4bc81f1a","commitMessage":"@@@Spark: Add Spark 3 data source classes (#1124)\n\n","date":"2020-06-30 08:56:05","modifiedFileCount":"7","status":"B","submitter":"Ryan Blue"},{"authorTime":"2020-11-18 00:35:55","codes":[{"authorDate":"2020-11-18 00:35:55","commitOrder":2,"curCode":"  public BatchWrite buildForBatch() {\n    \r\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    SparkWrite write = new SparkWrite(table, io, encryptionManager, writeInfo, appId, wapId, writeSchema, dsSchema);\n    if (overwriteByFilter) {\n      return write.asOverwriteByFilter(overwriteExpr);\n    } else if (overwriteDynamic) {\n      return write.asDynamicOverwrite();\n    } else {\n      return write.asBatchAppend();\n    }\n  }\n","date":"2020-11-18 00:35:55","endLine":124,"groupId":"4285","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"buildForBatch","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/45/f4ce4474ce9f66b652626bd9e5269856e4f7da.src","preCode":"  public BatchWrite buildForBatch() {\n    \r\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    return new SparkBatchWrite(\n        table, io, encryptionManager, options, overwriteDynamic, overwriteByFilter, overwriteExpr, appId, wapId,\n        writeSchema, dsSchema);\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkWriteBuilder.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":100,"status":"M"},{"authorDate":"2020-11-18 00:35:55","commitOrder":2,"curCode":"  public StreamingWrite buildForStreaming() {\n    \r\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    Preconditions.checkState(!overwriteDynamic,\n        \"Unsupported streaming operation: dynamic partition overwrite\");\n    Preconditions.checkState(!overwriteByFilter || overwriteExpr == Expressions.alwaysTrue(),\n        \"Unsupported streaming operation: overwrite by filter: %s\", overwriteExpr);\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    SparkWrite write = new SparkWrite(table, io, encryptionManager, writeInfo, appId, wapId, writeSchema, dsSchema);\n    if (overwriteByFilter) {\n      return write.asStreamingOverwrite();\n    } else {\n      return write.asStreamingAppend();\n    }\n  }\n","date":"2020-11-18 00:35:55","endLine":155,"groupId":"4286","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"buildForStreaming","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/45/f4ce4474ce9f66b652626bd9e5269856e4f7da.src","preCode":"  public StreamingWrite buildForStreaming() {\n    \r\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    Preconditions.checkState(!overwriteDynamic,\n        \"Unsupported streaming operation: dynamic partition overwrite\");\n    Preconditions.checkState(!overwriteByFilter || overwriteExpr == Expressions.alwaysTrue(),\n        \"Unsupported streaming operation: overwrite by filter: %s\", overwriteExpr);\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    return new SparkStreamingWrite(\n        table, io, encryptionManager, options, overwriteByFilter, writeQueryId, appId, wapId, writeSchema, dsSchema);\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkWriteBuilder.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":127,"status":"M"}],"commitId":"8e2952c5809a8b2e82093a5665439570798798ce","commitMessage":"@@@Spark: Refactor Spark writes into separate classes (#1776)\n\n","date":"2020-11-18 00:35:55","modifiedFileCount":"1","status":"M","submitter":"Anton Okolnychyi"},{"authorTime":"2020-11-18 00:35:55","codes":[{"authorDate":"2020-12-05 03:28:59","commitOrder":3,"curCode":"  public BatchWrite buildForBatch() {\n    \r\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    SparkWrite write = new SparkWrite(table, io, encryptionManager, writeInfo, appId, wapId, writeSchema, dsSchema);\n    if (overwriteByFilter) {\n      return write.asOverwriteByFilter(overwriteExpr);\n    } else if (overwriteDynamic) {\n      return write.asDynamicOverwrite();\n    } else if (overwriteFiles) {\n      return write.asCopyOnWriteMergeWrite(mergeScan, isolationLevel);\n    } else {\n      return write.asBatchAppend();\n    }\n  }\n","date":"2020-12-05 03:28:59","endLine":143,"groupId":"4285","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"buildForBatch","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/21/11552693bbc242fc96bb67ae96a341a9db369b.src","preCode":"  public BatchWrite buildForBatch() {\n    \r\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    SparkWrite write = new SparkWrite(table, io, encryptionManager, writeInfo, appId, wapId, writeSchema, dsSchema);\n    if (overwriteByFilter) {\n      return write.asOverwriteByFilter(overwriteExpr);\n    } else if (overwriteDynamic) {\n      return write.asDynamicOverwrite();\n    } else {\n      return write.asBatchAppend();\n    }\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkWriteBuilder.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":117,"status":"M"},{"authorDate":"2020-11-18 00:35:55","commitOrder":3,"curCode":"  public StreamingWrite buildForStreaming() {\n    \r\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    Preconditions.checkState(!overwriteDynamic,\n        \"Unsupported streaming operation: dynamic partition overwrite\");\n    Preconditions.checkState(!overwriteByFilter || overwriteExpr == Expressions.alwaysTrue(),\n        \"Unsupported streaming operation: overwrite by filter: %s\", overwriteExpr);\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    SparkWrite write = new SparkWrite(table, io, encryptionManager, writeInfo, appId, wapId, writeSchema, dsSchema);\n    if (overwriteByFilter) {\n      return write.asStreamingOverwrite();\n    } else {\n      return write.asStreamingAppend();\n    }\n  }\n","date":"2020-11-18 00:35:55","endLine":155,"groupId":"4286","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"buildForStreaming","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/45/f4ce4474ce9f66b652626bd9e5269856e4f7da.src","preCode":"  public StreamingWrite buildForStreaming() {\n    \r\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    Preconditions.checkState(!overwriteDynamic,\n        \"Unsupported streaming operation: dynamic partition overwrite\");\n    Preconditions.checkState(!overwriteByFilter || overwriteExpr == Expressions.alwaysTrue(),\n        \"Unsupported streaming operation: overwrite by filter: %s\", overwriteExpr);\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    SparkWrite write = new SparkWrite(table, io, encryptionManager, writeInfo, appId, wapId, writeSchema, dsSchema);\n    if (overwriteByFilter) {\n      return write.asStreamingOverwrite();\n    } else {\n      return write.asStreamingAppend();\n    }\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkWriteBuilder.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":127,"status":"N"}],"commitId":"af5f60068c75195c0bb41ca85065e05282fe2fe8","commitMessage":"@@@Spark: Implement copy-on-write DELETE (#1862)\n\n","date":"2020-12-05 03:28:59","modifiedFileCount":"6","status":"M","submitter":"Anton Okolnychyi"},{"authorTime":"2021-04-20 14:32:55","codes":[{"authorDate":"2021-04-20 14:32:55","commitOrder":4,"curCode":"  public BatchWrite buildForBatch() {\n    \r\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    SparkWrite write = new SparkWrite(spark, table, writeInfo, appId, wapId, writeSchema, dsSchema);\n    if (overwriteByFilter) {\n      return write.asOverwriteByFilter(overwriteExpr);\n    } else if (overwriteDynamic) {\n      return write.asDynamicOverwrite();\n    } else if (overwriteFiles) {\n      return write.asCopyOnWriteMergeWrite(mergeScan, isolationLevel);\n    } else {\n      return write.asBatchAppend();\n    }\n  }\n","date":"2021-04-20 14:32:55","endLine":127,"groupId":"1617","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"buildForBatch","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/19/debd53c9cbe9b82840af6019acfb48ad915b22.src","preCode":"  public BatchWrite buildForBatch() {\n    \r\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    SparkWrite write = new SparkWrite(table, io, encryptionManager, writeInfo, appId, wapId, writeSchema, dsSchema);\n    if (overwriteByFilter) {\n      return write.asOverwriteByFilter(overwriteExpr);\n    } else if (overwriteDynamic) {\n      return write.asDynamicOverwrite();\n    } else if (overwriteFiles) {\n      return write.asCopyOnWriteMergeWrite(mergeScan, isolationLevel);\n    } else {\n      return write.asBatchAppend();\n    }\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkWriteBuilder.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":104,"status":"M"},{"authorDate":"2021-04-20 14:32:55","commitOrder":4,"curCode":"  public StreamingWrite buildForStreaming() {\n    \r\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    Preconditions.checkState(!overwriteDynamic,\n        \"Unsupported streaming operation: dynamic partition overwrite\");\n    Preconditions.checkState(!overwriteByFilter || overwriteExpr == Expressions.alwaysTrue(),\n        \"Unsupported streaming operation: overwrite by filter: %s\", overwriteExpr);\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    SparkWrite write = new SparkWrite(spark, table, writeInfo, appId, wapId, writeSchema, dsSchema);\n    if (overwriteByFilter) {\n      return write.asStreamingOverwrite();\n    } else {\n      return write.asStreamingAppend();\n    }\n  }\n","date":"2021-04-20 14:32:55","endLine":155,"groupId":"4286","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"buildForStreaming","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/19/debd53c9cbe9b82840af6019acfb48ad915b22.src","preCode":"  public StreamingWrite buildForStreaming() {\n    \r\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    Preconditions.checkState(!overwriteDynamic,\n        \"Unsupported streaming operation: dynamic partition overwrite\");\n    Preconditions.checkState(!overwriteByFilter || overwriteExpr == Expressions.alwaysTrue(),\n        \"Unsupported streaming operation: overwrite by filter: %s\", overwriteExpr);\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    Broadcast<FileIO> io = lazySparkContext().broadcast(SparkUtil.serializableFileIO(table));\n    Broadcast<EncryptionManager> encryptionManager = lazySparkContext().broadcast(table.encryption());\n\n    SparkWrite write = new SparkWrite(table, io, encryptionManager, writeInfo, appId, wapId, writeSchema, dsSchema);\n    if (overwriteByFilter) {\n      return write.asStreamingOverwrite();\n    } else {\n      return write.asStreamingAppend();\n    }\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkWriteBuilder.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":130,"status":"M"}],"commitId":"a79de571860a290f6e96ac562d616c9c6be2071e","commitMessage":"@@@Spark: Pass Table to executors (#2362)\n\n","date":"2021-04-20 14:32:55","modifiedFileCount":"18","status":"M","submitter":"Anton Okolnychyi"},{"authorTime":"2021-04-20 14:32:55","codes":[{"authorDate":"2021-07-16 01:06:44","commitOrder":5,"curCode":"  public BatchWrite buildForBatch() {\n    \r\n    Preconditions.checkArgument(canHandleTimestampWithoutZone || !SparkUtil.hasTimestampWithoutZone(table.schema()),\n            SparkUtil.TIMESTAMP_WITHOUT_TIMEZONE_ERROR);\n\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    SparkWrite write = new SparkWrite(spark, table, writeInfo, appId, wapId, writeSchema, dsSchema);\n    if (overwriteByFilter) {\n      return write.asOverwriteByFilter(overwriteExpr);\n    } else if (overwriteDynamic) {\n      return write.asDynamicOverwrite();\n    } else if (overwriteFiles) {\n      return write.asCopyOnWriteMergeWrite(mergeScan, isolationLevel);\n    } else {\n      return write.asBatchAppend();\n    }\n  }\n","date":"2021-07-16 01:06:44","endLine":132,"groupId":"10859","id":9,"instanceNumber":1,"isCurCommit":1,"methodName":"buildForBatch","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/b2/3e0a7935cf72359fb397e4f9cd88ae89c89886.src","preCode":"  public BatchWrite buildForBatch() {\n    \r\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    SparkWrite write = new SparkWrite(spark, table, writeInfo, appId, wapId, writeSchema, dsSchema);\n    if (overwriteByFilter) {\n      return write.asOverwriteByFilter(overwriteExpr);\n    } else if (overwriteDynamic) {\n      return write.asDynamicOverwrite();\n    } else if (overwriteFiles) {\n      return write.asCopyOnWriteMergeWrite(mergeScan, isolationLevel);\n    } else {\n      return write.asBatchAppend();\n    }\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkWriteBuilder.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":106,"status":"M"},{"authorDate":"2021-04-20 14:32:55","commitOrder":5,"curCode":"  public StreamingWrite buildForStreaming() {\n    \r\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    Preconditions.checkState(!overwriteDynamic,\n        \"Unsupported streaming operation: dynamic partition overwrite\");\n    Preconditions.checkState(!overwriteByFilter || overwriteExpr == Expressions.alwaysTrue(),\n        \"Unsupported streaming operation: overwrite by filter: %s\", overwriteExpr);\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    SparkWrite write = new SparkWrite(spark, table, writeInfo, appId, wapId, writeSchema, dsSchema);\n    if (overwriteByFilter) {\n      return write.asStreamingOverwrite();\n    } else {\n      return write.asStreamingAppend();\n    }\n  }\n","date":"2021-04-20 14:32:55","endLine":155,"groupId":"10859","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"buildForStreaming","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/19/debd53c9cbe9b82840af6019acfb48ad915b22.src","preCode":"  public StreamingWrite buildForStreaming() {\n    \r\n    Schema writeSchema = SparkSchemaUtil.convert(table.schema(), dsSchema);\n    TypeUtil.validateWriteSchema(table.schema(), writeSchema,\n        checkNullability(spark, options), checkOrdering(spark, options));\n    SparkUtil.validatePartitionTransforms(table.spec());\n\n    \r\n    Preconditions.checkState(!overwriteDynamic,\n        \"Unsupported streaming operation: dynamic partition overwrite\");\n    Preconditions.checkState(!overwriteByFilter || overwriteExpr == Expressions.alwaysTrue(),\n        \"Unsupported streaming operation: overwrite by filter: %s\", overwriteExpr);\n\n    \r\n    String appId = spark.sparkContext().applicationId();\n\n    \r\n    String wapId = spark.conf().get(\"spark.wap.id\", null);\n\n    SparkWrite write = new SparkWrite(spark, table, writeInfo, appId, wapId, writeSchema, dsSchema);\n    if (overwriteByFilter) {\n      return write.asStreamingOverwrite();\n    } else {\n      return write.asStreamingAppend();\n    }\n  }\n","realPath":"spark3/src/main/java/org/apache/iceberg/spark/source/SparkWriteBuilder.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":130,"status":"N"}],"commitId":"9a0d154b0ba5e6d10d79e30470295c91c89c1e09","commitMessage":"@@@Add support for reading/writing timestamps without timezone.  (#2757)\n\nPreviously Spark could not handle Iceberg tables which contained Timestamp.withoutTimeZone. New parameters are introduced to allow Timestamp without TimeZone to be treated as Timestamp with Timezone.  \n\nCo-authored-by: bkahloon <kahlonbakht@gmail.com>\nCo-authored-by: shardulm94 ","date":"2021-07-16 01:06:44","modifiedFileCount":"15","status":"M","submitter":"sshkvar"}]
