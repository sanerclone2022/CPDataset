[{"authorTime":"2020-08-21 00:17:18","codes":[{"authorDate":"2020-08-22 01:04:38","commitOrder":3,"curCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    RowType flinkSchema = FlinkSchemaUtil.convert(schema);\n    List<Record> expectedRecords = RandomGenericData.generate(schema, NUM_RECORDS, 1991L);\n    List<RowData> expectedRows = Lists.newArrayList(RandomRowData.convert(schema, expectedRecords));\n\n    File recordsFile = temp.newFile();\n    Assert.assertTrue(\"Delete should succeed\", recordsFile.delete());\n\n    \r\n    try (FileAppender<Record> writer = Avro.write(Files.localOutput(recordsFile))\n        .schema(schema)\n        .createWriterFunc(DataWriter::create)\n        .build()) {\n      writer.addAll(expectedRecords);\n    }\n\n    try (CloseableIterable<RowData> reader = Avro.read(Files.localInput(recordsFile))\n        .project(schema)\n        .createReaderFunc(FlinkAvroReader::new)\n        .build()) {\n      Iterator<Record> expected = expectedRecords.iterator();\n      Iterator<RowData> rows = reader.iterator();\n      for (int i = 0; i < NUM_RECORDS; i++) {\n        Assert.assertTrue(\"Should have expected number of records\", rows.hasNext());\n        TestHelpers.assertRowData(schema.asStruct(), flinkSchema, expected.next(), rows.next());\n      }\n      Assert.assertFalse(\"Should not have extra records\", rows.hasNext());\n    }\n\n    File rowDataFile = temp.newFile();\n    Assert.assertTrue(\"Delete should succeed\", rowDataFile.delete());\n\n    \r\n    try (FileAppender<RowData> writer = Avro.write(Files.localOutput(rowDataFile))\n        .schema(schema)\n        .createWriterFunc(ignore -> new FlinkAvroWriter(flinkSchema))\n        .build()) {\n      writer.addAll(expectedRows);\n    }\n\n    try (CloseableIterable<Record> reader = Avro.read(Files.localInput(rowDataFile))\n        .project(schema)\n        .createReaderFunc(DataReader::create)\n        .build()) {\n      Iterator<RowData> expected = expectedRows.iterator();\n      Iterator<Record> records = reader.iterator();\n      for (int i = 0; i < NUM_RECORDS; i += 1) {\n        Assert.assertTrue(\"Should have expected number of records\", records.hasNext());\n        TestHelpers.assertRowData(schema.asStruct(), flinkSchema, records.next(), expected.next());\n      }\n      Assert.assertFalse(\"Should not have extra records\", records.hasNext());\n    }\n  }\n","date":"2020-08-22 01:04:38","endLine":99,"groupId":"197","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"writeAndValidate","params":"(Schemaschema)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/4d/ef6dbe94656069fc6a0c3eb817e85bf8abeafe.src","preCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    RowType flinkSchema = FlinkSchemaUtil.convert(schema);\n    List<Record> expectedRecords = RandomGenericData.generate(schema, NUM_RECORDS, 1991L);\n    List<RowData> expectedRows = Lists.newArrayList(RandomRowData.convert(schema, expectedRecords));\n\n    File recordsFile = temp.newFile();\n    Assert.assertTrue(\"Delete should succeed\", recordsFile.delete());\n\n    \r\n    try (FileAppender<Record> writer = Avro.write(Files.localOutput(recordsFile))\n        .schema(schema)\n        .createWriterFunc(DataWriter::create)\n        .build()) {\n      writer.addAll(expectedRecords);\n    }\n\n    try (CloseableIterable<RowData> reader = Avro.read(Files.localInput(recordsFile))\n        .project(schema)\n        .createReaderFunc(FlinkAvroReader::new)\n        .build()) {\n      Iterator<Record> expected = expectedRecords.iterator();\n      Iterator<RowData> rows = reader.iterator();\n      for (int i = 0; i < NUM_RECORDS; i++) {\n        Assert.assertTrue(\"Should have expected number of records\", rows.hasNext());\n        TestHelpers.assertRowData(schema.asStruct(), flinkSchema, expected.next(), rows.next());\n      }\n      Assert.assertFalse(\"Should not have extra records\", rows.hasNext());\n    }\n\n    File rowDataFile = temp.newFile();\n    Assert.assertTrue(\"Delete should succeed\", rowDataFile.delete());\n\n    \r\n    try (FileAppender<RowData> writer = Avro.write(Files.localOutput(rowDataFile))\n        .schema(schema)\n        .createWriterFunc(ignore -> new FlinkAvroWriter(flinkSchema))\n        .build()) {\n      writer.addAll(expectedRows);\n    }\n\n    try (CloseableIterable<Record> reader = Avro.read(Files.localInput(rowDataFile))\n        .project(schema)\n        .createReaderFunc(DataReader::create)\n        .build()) {\n      Iterator<RowData> expected = expectedRows.iterator();\n      Iterator<Record> records = reader.iterator();\n      for (int i = 0; i < NUM_RECORDS; i += 1) {\n        Assert.assertTrue(\"Should have expected number of records\", records.hasNext());\n        TestHelpers.assertRowData(schema.asStruct(), flinkSchema, records.next(), expected.next());\n      }\n      Assert.assertFalse(\"Should not have extra records\", records.hasNext());\n    }\n  }\n","realPath":"flink/src/test/java/org/apache/iceberg/flink/data/TestFlinkAvroReaderWriter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":47,"status":"MB"},{"authorDate":"2020-08-21 00:17:18","commitOrder":3,"curCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    RowType flinkSchema = FlinkSchemaUtil.convert(schema);\n    List<Record> expectedRecords = RandomGenericData.generate(schema, NUM_RECORDS, 1990L);\n    List<RowData> expectedRows = Lists.newArrayList(RandomRowData.convert(schema, expectedRecords));\n\n    File recordsFile = temp.newFile();\n    Assert.assertTrue(\"Delete should succeed\", recordsFile.delete());\n\n    \r\n    try (FileAppender<Record> writer = ORC.write(Files.localOutput(recordsFile))\n        .schema(schema)\n        .createWriterFunc(GenericOrcWriter::buildWriter)\n        .build()) {\n      writer.addAll(expectedRecords);\n    }\n\n    try (CloseableIterable<RowData> reader = ORC.read(Files.localInput(recordsFile))\n        .project(schema)\n        .createReaderFunc(type -> FlinkOrcReader.buildReader(schema, type))\n        .build()) {\n      Iterator<Record> expected = expectedRecords.iterator();\n      Iterator<RowData> rows = reader.iterator();\n      for (int i = 0; i < NUM_RECORDS; i++) {\n        Assert.assertTrue(\"Should have expected number of records\", rows.hasNext());\n        TestHelpers.assertRowData(schema.asStruct(), flinkSchema, expected.next(), rows.next());\n      }\n      Assert.assertFalse(\"Should not have extra records\", rows.hasNext());\n    }\n\n    File rowDataFile = temp.newFile();\n    Assert.assertTrue(\"Delete should succeed\", rowDataFile.delete());\n\n    \r\n    RowType rowType = FlinkSchemaUtil.convert(schema);\n    try (FileAppender<RowData> writer = ORC.write(Files.localOutput(rowDataFile))\n        .schema(schema)\n        .createWriterFunc((iSchema, typeDesc) -> FlinkOrcWriter.buildWriter(rowType, iSchema))\n        .build()) {\n      writer.addAll(expectedRows);\n    }\n\n    try (CloseableIterable<Record> reader = ORC.read(Files.localInput(rowDataFile))\n        .project(schema)\n        .createReaderFunc(type -> GenericOrcReader.buildReader(schema, type))\n        .build()) {\n      Iterator<RowData> expected = expectedRows.iterator();\n      Iterator<Record> records = reader.iterator();\n      for (int i = 0; i < NUM_RECORDS; i += 1) {\n        Assert.assertTrue(\"Should have expected number of records\", records.hasNext());\n        TestHelpers.assertRowData(schema.asStruct(), flinkSchema, records.next(), expected.next());\n      }\n      Assert.assertFalse(\"Should not have extra records\", records.hasNext());\n    }\n  }\n","date":"2020-08-21 00:17:18","endLine":104,"groupId":"3402","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"writeAndValidate","params":"(Schemaschema)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/79/f1c61f905a9bb75c7f654b9e1dfdd2c597198f.src","preCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    RowType flinkSchema = FlinkSchemaUtil.convert(schema);\n    List<Record> expectedRecords = RandomGenericData.generate(schema, NUM_RECORDS, 1990L);\n    List<RowData> expectedRows = Lists.newArrayList(RandomRowData.convert(schema, expectedRecords));\n\n    File recordsFile = temp.newFile();\n    Assert.assertTrue(\"Delete should succeed\", recordsFile.delete());\n\n    \r\n    try (FileAppender<Record> writer = ORC.write(Files.localOutput(recordsFile))\n        .schema(schema)\n        .createWriterFunc(GenericOrcWriter::buildWriter)\n        .build()) {\n      writer.addAll(expectedRecords);\n    }\n\n    try (CloseableIterable<RowData> reader = ORC.read(Files.localInput(recordsFile))\n        .project(schema)\n        .createReaderFunc(type -> FlinkOrcReader.buildReader(schema, type))\n        .build()) {\n      Iterator<Record> expected = expectedRecords.iterator();\n      Iterator<RowData> rows = reader.iterator();\n      for (int i = 0; i < NUM_RECORDS; i++) {\n        Assert.assertTrue(\"Should have expected number of records\", rows.hasNext());\n        TestHelpers.assertRowData(schema.asStruct(), flinkSchema, expected.next(), rows.next());\n      }\n      Assert.assertFalse(\"Should not have extra records\", rows.hasNext());\n    }\n\n    File rowDataFile = temp.newFile();\n    Assert.assertTrue(\"Delete should succeed\", rowDataFile.delete());\n\n    \r\n    RowType rowType = FlinkSchemaUtil.convert(schema);\n    try (FileAppender<RowData> writer = ORC.write(Files.localOutput(rowDataFile))\n        .schema(schema)\n        .createWriterFunc((iSchema, typeDesc) -> FlinkOrcWriter.buildWriter(rowType, iSchema))\n        .build()) {\n      writer.addAll(expectedRows);\n    }\n\n    try (CloseableIterable<Record> reader = ORC.read(Files.localInput(rowDataFile))\n        .project(schema)\n        .createReaderFunc(type -> GenericOrcReader.buildReader(schema, type))\n        .build()) {\n      Iterator<RowData> expected = expectedRows.iterator();\n      Iterator<Record> records = reader.iterator();\n      for (int i = 0; i < NUM_RECORDS; i += 1) {\n        Assert.assertTrue(\"Should have expected number of records\", records.hasNext());\n        TestHelpers.assertRowData(schema.asStruct(), flinkSchema, records.next(), expected.next());\n      }\n      Assert.assertFalse(\"Should not have extra records\", records.hasNext());\n    }\n  }\n","realPath":"flink/src/test/java/org/apache/iceberg/flink/data/TestFlinkOrcReaderWriter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":51,"status":"NB"}],"commitId":"06b5f045381d5e4693bfe4a374ce656f6c8ae836","commitMessage":"@@@Flink: Validate RowData with generated Records in Avro tests (#1363)\n\n","date":"2020-08-22 01:04:38","modifiedFileCount":"3","status":"M","submitter":"openinx"},{"authorTime":"2020-09-25 09:05:47","codes":[{"authorDate":"2020-08-22 01:04:38","commitOrder":4,"curCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    RowType flinkSchema = FlinkSchemaUtil.convert(schema);\n    List<Record> expectedRecords = RandomGenericData.generate(schema, NUM_RECORDS, 1991L);\n    List<RowData> expectedRows = Lists.newArrayList(RandomRowData.convert(schema, expectedRecords));\n\n    File recordsFile = temp.newFile();\n    Assert.assertTrue(\"Delete should succeed\", recordsFile.delete());\n\n    \r\n    try (FileAppender<Record> writer = Avro.write(Files.localOutput(recordsFile))\n        .schema(schema)\n        .createWriterFunc(DataWriter::create)\n        .build()) {\n      writer.addAll(expectedRecords);\n    }\n\n    try (CloseableIterable<RowData> reader = Avro.read(Files.localInput(recordsFile))\n        .project(schema)\n        .createReaderFunc(FlinkAvroReader::new)\n        .build()) {\n      Iterator<Record> expected = expectedRecords.iterator();\n      Iterator<RowData> rows = reader.iterator();\n      for (int i = 0; i < NUM_RECORDS; i++) {\n        Assert.assertTrue(\"Should have expected number of records\", rows.hasNext());\n        TestHelpers.assertRowData(schema.asStruct(), flinkSchema, expected.next(), rows.next());\n      }\n      Assert.assertFalse(\"Should not have extra records\", rows.hasNext());\n    }\n\n    File rowDataFile = temp.newFile();\n    Assert.assertTrue(\"Delete should succeed\", rowDataFile.delete());\n\n    \r\n    try (FileAppender<RowData> writer = Avro.write(Files.localOutput(rowDataFile))\n        .schema(schema)\n        .createWriterFunc(ignore -> new FlinkAvroWriter(flinkSchema))\n        .build()) {\n      writer.addAll(expectedRows);\n    }\n\n    try (CloseableIterable<Record> reader = Avro.read(Files.localInput(rowDataFile))\n        .project(schema)\n        .createReaderFunc(DataReader::create)\n        .build()) {\n      Iterator<RowData> expected = expectedRows.iterator();\n      Iterator<Record> records = reader.iterator();\n      for (int i = 0; i < NUM_RECORDS; i += 1) {\n        Assert.assertTrue(\"Should have expected number of records\", records.hasNext());\n        TestHelpers.assertRowData(schema.asStruct(), flinkSchema, records.next(), expected.next());\n      }\n      Assert.assertFalse(\"Should not have extra records\", records.hasNext());\n    }\n  }\n","date":"2020-08-22 01:04:38","endLine":99,"groupId":"102192","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"writeAndValidate","params":"(Schemaschema)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/4d/ef6dbe94656069fc6a0c3eb817e85bf8abeafe.src","preCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    RowType flinkSchema = FlinkSchemaUtil.convert(schema);\n    List<Record> expectedRecords = RandomGenericData.generate(schema, NUM_RECORDS, 1991L);\n    List<RowData> expectedRows = Lists.newArrayList(RandomRowData.convert(schema, expectedRecords));\n\n    File recordsFile = temp.newFile();\n    Assert.assertTrue(\"Delete should succeed\", recordsFile.delete());\n\n    \r\n    try (FileAppender<Record> writer = Avro.write(Files.localOutput(recordsFile))\n        .schema(schema)\n        .createWriterFunc(DataWriter::create)\n        .build()) {\n      writer.addAll(expectedRecords);\n    }\n\n    try (CloseableIterable<RowData> reader = Avro.read(Files.localInput(recordsFile))\n        .project(schema)\n        .createReaderFunc(FlinkAvroReader::new)\n        .build()) {\n      Iterator<Record> expected = expectedRecords.iterator();\n      Iterator<RowData> rows = reader.iterator();\n      for (int i = 0; i < NUM_RECORDS; i++) {\n        Assert.assertTrue(\"Should have expected number of records\", rows.hasNext());\n        TestHelpers.assertRowData(schema.asStruct(), flinkSchema, expected.next(), rows.next());\n      }\n      Assert.assertFalse(\"Should not have extra records\", rows.hasNext());\n    }\n\n    File rowDataFile = temp.newFile();\n    Assert.assertTrue(\"Delete should succeed\", rowDataFile.delete());\n\n    \r\n    try (FileAppender<RowData> writer = Avro.write(Files.localOutput(rowDataFile))\n        .schema(schema)\n        .createWriterFunc(ignore -> new FlinkAvroWriter(flinkSchema))\n        .build()) {\n      writer.addAll(expectedRows);\n    }\n\n    try (CloseableIterable<Record> reader = Avro.read(Files.localInput(rowDataFile))\n        .project(schema)\n        .createReaderFunc(DataReader::create)\n        .build()) {\n      Iterator<RowData> expected = expectedRows.iterator();\n      Iterator<Record> records = reader.iterator();\n      for (int i = 0; i < NUM_RECORDS; i += 1) {\n        Assert.assertTrue(\"Should have expected number of records\", records.hasNext());\n        TestHelpers.assertRowData(schema.asStruct(), flinkSchema, records.next(), expected.next());\n      }\n      Assert.assertFalse(\"Should not have extra records\", records.hasNext());\n    }\n  }\n","realPath":"flink/src/test/java/org/apache/iceberg/flink/data/TestFlinkAvroReaderWriter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":47,"status":"N"},{"authorDate":"2020-09-25 09:05:47","commitOrder":4,"curCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    RowType flinkSchema = FlinkSchemaUtil.convert(schema);\n    List<Record> expectedRecords = RandomGenericData.generate(schema, NUM_RECORDS, 1990L);\n    List<RowData> expectedRows = Lists.newArrayList(RandomRowData.convert(schema, expectedRecords));\n\n    File recordsFile = temp.newFile();\n    Assert.assertTrue(\"Delete should succeed\", recordsFile.delete());\n\n    \r\n    try (FileAppender<Record> writer = ORC.write(Files.localOutput(recordsFile))\n        .schema(schema)\n        .createWriterFunc(GenericOrcWriter::buildWriter)\n        .build()) {\n      writer.addAll(expectedRecords);\n    }\n\n    try (CloseableIterable<RowData> reader = ORC.read(Files.localInput(recordsFile))\n        .project(schema)\n        .createReaderFunc(type -> new FlinkOrcReader(schema, type))\n        .build()) {\n      Iterator<Record> expected = expectedRecords.iterator();\n      Iterator<RowData> rows = reader.iterator();\n      for (int i = 0; i < NUM_RECORDS; i++) {\n        Assert.assertTrue(\"Should have expected number of records\", rows.hasNext());\n        TestHelpers.assertRowData(schema.asStruct(), flinkSchema, expected.next(), rows.next());\n      }\n      Assert.assertFalse(\"Should not have extra records\", rows.hasNext());\n    }\n\n    File rowDataFile = temp.newFile();\n    Assert.assertTrue(\"Delete should succeed\", rowDataFile.delete());\n\n    \r\n    RowType rowType = FlinkSchemaUtil.convert(schema);\n    try (FileAppender<RowData> writer = ORC.write(Files.localOutput(rowDataFile))\n        .schema(schema)\n        .createWriterFunc((iSchema, typeDesc) -> FlinkOrcWriter.buildWriter(rowType, iSchema))\n        .build()) {\n      writer.addAll(expectedRows);\n    }\n\n    try (CloseableIterable<Record> reader = ORC.read(Files.localInput(rowDataFile))\n        .project(schema)\n        .createReaderFunc(type -> GenericOrcReader.buildReader(schema, type))\n        .build()) {\n      Iterator<RowData> expected = expectedRows.iterator();\n      Iterator<Record> records = reader.iterator();\n      for (int i = 0; i < NUM_RECORDS; i += 1) {\n        Assert.assertTrue(\"Should have expected number of records\", records.hasNext());\n        TestHelpers.assertRowData(schema.asStruct(), flinkSchema, records.next(), expected.next());\n      }\n      Assert.assertFalse(\"Should not have extra records\", records.hasNext());\n    }\n  }\n","date":"2020-09-25 09:05:47","endLine":99,"groupId":"102192","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"writeAndValidate","params":"(Schemaschema)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/98/7e070f78b6e1d9fb1e14e694f1e6e183bf960b.src","preCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    RowType flinkSchema = FlinkSchemaUtil.convert(schema);\n    List<Record> expectedRecords = RandomGenericData.generate(schema, NUM_RECORDS, 1990L);\n    List<RowData> expectedRows = Lists.newArrayList(RandomRowData.convert(schema, expectedRecords));\n\n    File recordsFile = temp.newFile();\n    Assert.assertTrue(\"Delete should succeed\", recordsFile.delete());\n\n    \r\n    try (FileAppender<Record> writer = ORC.write(Files.localOutput(recordsFile))\n        .schema(schema)\n        .createWriterFunc(GenericOrcWriter::buildWriter)\n        .build()) {\n      writer.addAll(expectedRecords);\n    }\n\n    try (CloseableIterable<RowData> reader = ORC.read(Files.localInput(recordsFile))\n        .project(schema)\n        .createReaderFunc(type -> FlinkOrcReader.buildReader(schema, type))\n        .build()) {\n      Iterator<Record> expected = expectedRecords.iterator();\n      Iterator<RowData> rows = reader.iterator();\n      for (int i = 0; i < NUM_RECORDS; i++) {\n        Assert.assertTrue(\"Should have expected number of records\", rows.hasNext());\n        TestHelpers.assertRowData(schema.asStruct(), flinkSchema, expected.next(), rows.next());\n      }\n      Assert.assertFalse(\"Should not have extra records\", rows.hasNext());\n    }\n\n    File rowDataFile = temp.newFile();\n    Assert.assertTrue(\"Delete should succeed\", rowDataFile.delete());\n\n    \r\n    RowType rowType = FlinkSchemaUtil.convert(schema);\n    try (FileAppender<RowData> writer = ORC.write(Files.localOutput(rowDataFile))\n        .schema(schema)\n        .createWriterFunc((iSchema, typeDesc) -> FlinkOrcWriter.buildWriter(rowType, iSchema))\n        .build()) {\n      writer.addAll(expectedRows);\n    }\n\n    try (CloseableIterable<Record> reader = ORC.read(Files.localInput(rowDataFile))\n        .project(schema)\n        .createReaderFunc(type -> GenericOrcReader.buildReader(schema, type))\n        .build()) {\n      Iterator<RowData> expected = expectedRows.iterator();\n      Iterator<Record> records = reader.iterator();\n      for (int i = 0; i < NUM_RECORDS; i += 1) {\n        Assert.assertTrue(\"Should have expected number of records\", records.hasNext());\n        TestHelpers.assertRowData(schema.asStruct(), flinkSchema, records.next(), expected.next());\n      }\n      Assert.assertFalse(\"Should not have extra records\", records.hasNext());\n    }\n  }\n","realPath":"flink/src/test/java/org/apache/iceberg/flink/data/TestFlinkOrcReaderWriter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":46,"status":"M"}],"commitId":"1a797cd986c9193f2e422fec295aaf25ce7e1916","commitMessage":"@@@Flink: Introduce Flink InputFormat (#1346)\n\n","date":"2020-09-25 09:05:47","modifiedFileCount":"5","status":"M","submitter":"Jingsong Lee"}]
