[{"authorTime":"2020-06-30 08:56:05","codes":[{"authorDate":"2020-06-30 08:56:05","commitOrder":1,"curCode":"  public void testUnpartitionedIDFilters() {\n    CaseInsensitiveStringMap options = new CaseInsensitiveStringMap(ImmutableMap.of(\n        \"path\", unpartitioned.toString())\n    );\n    SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options);\n\n    for (int i = 0; i < 10; i += 1) {\n      pushFilters(builder, EqualTo.apply(\"id\", i));\n      Batch scan = builder.build().toBatch();\n\n      InputPartition[] partitions = scan.planInputPartitions();\n      Assert.assertEquals(\"Should only create one task for a small file\", 1, partitions.length);\n\n      \r\n      assertEqualsSafe(SCHEMA.asStruct(), expected(i),\n          read(unpartitioned.toString(), \"id = \" + i));\n    }\n  }\n","date":"2020-06-30 08:56:05","endLine":229,"groupId":"4068","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testUnpartitionedIDFilters","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/7d/d308dcbed19b9615af2fe851dd00df86c8c855.src","preCode":"  public void testUnpartitionedIDFilters() {\n    CaseInsensitiveStringMap options = new CaseInsensitiveStringMap(ImmutableMap.of(\n        \"path\", unpartitioned.toString())\n    );\n    SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options);\n\n    for (int i = 0; i < 10; i += 1) {\n      pushFilters(builder, EqualTo.apply(\"id\", i));\n      Batch scan = builder.build().toBatch();\n\n      InputPartition[] partitions = scan.planInputPartitions();\n      Assert.assertEquals(\"Should only create one task for a small file\", 1, partitions.length);\n\n      \r\n      assertEqualsSafe(SCHEMA.asStruct(), expected(i),\n          read(unpartitioned.toString(), \"id = \" + i));\n    }\n  }\n","realPath":"spark3/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":212,"status":"B"},{"authorDate":"2020-06-30 08:56:05","commitOrder":1,"curCode":"  public void testUnpartitionedCaseInsensitiveIDFilters() {\n    CaseInsensitiveStringMap options = new CaseInsensitiveStringMap(ImmutableMap.of(\n        \"path\", unpartitioned.toString())\n    );\n\n    \r\n    String caseSensitivityBeforeTest = TestFilteredScan.spark.conf().get(\"spark.sql.caseSensitive\");\n    TestFilteredScan.spark.conf().set(\"spark.sql.caseSensitive\", \"false\");\n\n    try {\n\n      for (int i = 0; i < 10; i += 1) {\n        SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options)\n            .caseSensitive(false);\n\n        pushFilters(builder, EqualTo.apply(\"ID\", i)); \r\n        Batch scan = builder.build().toBatch();\n\n        InputPartition[] tasks = scan.planInputPartitions();\n        Assert.assertEquals(\"Should only create one task for a small file\", 1, tasks.length);\n\n        \r\n        assertEqualsSafe(SCHEMA.asStruct(), expected(i),\n            read(unpartitioned.toString(), \"id = \" + i));\n      }\n    } finally {\n      \r\n      TestFilteredScan.spark.conf().set(\"spark.sql.caseSensitive\", caseSensitivityBeforeTest);\n    }\n  }\n","date":"2020-06-30 08:56:05","endLine":261,"groupId":"576","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testUnpartitionedCaseInsensitiveIDFilters","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/7d/d308dcbed19b9615af2fe851dd00df86c8c855.src","preCode":"  public void testUnpartitionedCaseInsensitiveIDFilters() {\n    CaseInsensitiveStringMap options = new CaseInsensitiveStringMap(ImmutableMap.of(\n        \"path\", unpartitioned.toString())\n    );\n\n    \r\n    String caseSensitivityBeforeTest = TestFilteredScan.spark.conf().get(\"spark.sql.caseSensitive\");\n    TestFilteredScan.spark.conf().set(\"spark.sql.caseSensitive\", \"false\");\n\n    try {\n\n      for (int i = 0; i < 10; i += 1) {\n        SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options)\n            .caseSensitive(false);\n\n        pushFilters(builder, EqualTo.apply(\"ID\", i)); \r\n        Batch scan = builder.build().toBatch();\n\n        InputPartition[] tasks = scan.planInputPartitions();\n        Assert.assertEquals(\"Should only create one task for a small file\", 1, tasks.length);\n\n        \r\n        assertEqualsSafe(SCHEMA.asStruct(), expected(i),\n            read(unpartitioned.toString(), \"id = \" + i));\n      }\n    } finally {\n      \r\n      TestFilteredScan.spark.conf().set(\"spark.sql.caseSensitive\", caseSensitivityBeforeTest);\n    }\n  }\n","realPath":"spark3/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":232,"status":"B"}],"commitId":"51c930e33867e54b3d7e0159b11d6b9c4bc81f1a","commitMessage":"@@@Spark: Add Spark 3 data source classes (#1124)\n\n","date":"2020-06-30 08:56:05","modifiedFileCount":"7","status":"B","submitter":"Ryan Blue"},{"authorTime":"2020-07-14 05:27:36","codes":[{"authorDate":"2020-07-14 05:27:36","commitOrder":2,"curCode":"  public void testUnpartitionedIDFilters() {\n    CaseInsensitiveStringMap options = new CaseInsensitiveStringMap(ImmutableMap.of(\n        \"path\", unpartitioned.toString())\n    );\n    SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options);\n\n    for (int i = 0; i < 10; i += 1) {\n      pushFilters(builder, EqualTo.apply(\"id\", i));\n      Batch scan = builder.build().toBatch();\n\n      InputPartition[] partitions = scan.planInputPartitions();\n      Assert.assertEquals(\"Should only create one task for a small file\", 1, partitions.length);\n\n      \r\n      assertEqualsSafe(SCHEMA.asStruct(), expected(i),\n          read(unpartitioned.toString(), vectorized, \"id = \" + i));\n    }\n  }\n","date":"2020-07-14 05:27:36","endLine":247,"groupId":"10713","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testUnpartitionedIDFilters","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/9b/e99383873ff072f8b9a1577c9034bf17a6d465.src","preCode":"  public void testUnpartitionedIDFilters() {\n    CaseInsensitiveStringMap options = new CaseInsensitiveStringMap(ImmutableMap.of(\n        \"path\", unpartitioned.toString())\n    );\n    SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options);\n\n    for (int i = 0; i < 10; i += 1) {\n      pushFilters(builder, EqualTo.apply(\"id\", i));\n      Batch scan = builder.build().toBatch();\n\n      InputPartition[] partitions = scan.planInputPartitions();\n      Assert.assertEquals(\"Should only create one task for a small file\", 1, partitions.length);\n\n      \r\n      assertEqualsSafe(SCHEMA.asStruct(), expected(i),\n          read(unpartitioned.toString(), \"id = \" + i));\n    }\n  }\n","realPath":"spark3/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":230,"status":"M"},{"authorDate":"2020-07-14 05:27:36","commitOrder":2,"curCode":"  public void testUnpartitionedCaseInsensitiveIDFilters() {\n    CaseInsensitiveStringMap options = new CaseInsensitiveStringMap(ImmutableMap.of(\n        \"path\", unpartitioned.toString())\n    );\n\n    \r\n    String caseSensitivityBeforeTest = TestFilteredScan.spark.conf().get(\"spark.sql.caseSensitive\");\n    TestFilteredScan.spark.conf().set(\"spark.sql.caseSensitive\", \"false\");\n\n    try {\n\n      for (int i = 0; i < 10; i += 1) {\n        SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options)\n            .caseSensitive(false);\n\n        pushFilters(builder, EqualTo.apply(\"ID\", i)); \r\n        Batch scan = builder.build().toBatch();\n\n        InputPartition[] tasks = scan.planInputPartitions();\n        Assert.assertEquals(\"Should only create one task for a small file\", 1, tasks.length);\n\n        \r\n        assertEqualsSafe(SCHEMA.asStruct(), expected(i),\n            read(unpartitioned.toString(), vectorized, \"id = \" + i));\n      }\n    } finally {\n      \r\n      TestFilteredScan.spark.conf().set(\"spark.sql.caseSensitive\", caseSensitivityBeforeTest);\n    }\n  }\n","date":"2020-07-14 05:27:36","endLine":279,"groupId":"10713","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testUnpartitionedCaseInsensitiveIDFilters","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/9b/e99383873ff072f8b9a1577c9034bf17a6d465.src","preCode":"  public void testUnpartitionedCaseInsensitiveIDFilters() {\n    CaseInsensitiveStringMap options = new CaseInsensitiveStringMap(ImmutableMap.of(\n        \"path\", unpartitioned.toString())\n    );\n\n    \r\n    String caseSensitivityBeforeTest = TestFilteredScan.spark.conf().get(\"spark.sql.caseSensitive\");\n    TestFilteredScan.spark.conf().set(\"spark.sql.caseSensitive\", \"false\");\n\n    try {\n\n      for (int i = 0; i < 10; i += 1) {\n        SparkScanBuilder builder = new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options)\n            .caseSensitive(false);\n\n        pushFilters(builder, EqualTo.apply(\"ID\", i)); \r\n        Batch scan = builder.build().toBatch();\n\n        InputPartition[] tasks = scan.planInputPartitions();\n        Assert.assertEquals(\"Should only create one task for a small file\", 1, tasks.length);\n\n        \r\n        assertEqualsSafe(SCHEMA.asStruct(), expected(i),\n            read(unpartitioned.toString(), \"id = \" + i));\n      }\n    } finally {\n      \r\n      TestFilteredScan.spark.conf().set(\"spark.sql.caseSensitive\", caseSensitivityBeforeTest);\n    }\n  }\n","realPath":"spark3/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":250,"status":"M"}],"commitId":"6fab8f57bdb7e5fe7eadc3ff41558581338e1b69","commitMessage":"@@@Spark: Support ORC vectorized reads (#1189)\n\n","date":"2020-07-14 05:27:36","modifiedFileCount":"25","status":"M","submitter":"Shardul Mahadik"}]
