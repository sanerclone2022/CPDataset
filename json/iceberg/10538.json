[{"authorTime":"2020-05-26 01:15:35","codes":[{"authorDate":"2020-06-16 06:16:19","commitOrder":3,"curCode":"  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n    CloseableIterable<ColumnarBatch> iter;\n    InputFile location = getInputFile(task);\n    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n    if (task.file().format() == FileFormat.PARQUET) {\n      iter = Parquet.read(location)\n          .project(expectedSchema)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n              fileSchema,  NullCheckingForGet.NULL_CHECKING_ENABLED))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          \r\n          \r\n          \r\n          .reuseContainers()\n          .build();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Format: \" + task.file().format() + \" not supported for batched reads\");\n    }\n    return iter.iterator();\n  }\n","date":"2020-06-16 06:16:19","endLine":75,"groupId":"3341","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"open","params":"(FileScanTasktask)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/ee/b3ad559858759a4846d1a015cf2c893218b43e.src","preCode":"  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n    CloseableIterable<ColumnarBatch> iter;\n    InputFile location = getInputFile(task);\n    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n    if (task.file().format() == FileFormat.PARQUET) {\n      iter = Parquet.read(location)\n          .project(expectedSchema)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n              fileSchema,  NullCheckingForGet.NULL_CHECKING_ENABLED))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          \r\n          \r\n          \r\n          .reuseContainers()\n          .build();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Format: \" + task.file().format() + \" not supported for batched reads\");\n    }\n    return iter.iterator();\n  }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":52,"status":"B"},{"authorDate":"2020-05-26 01:15:35","commitOrder":3,"curCode":"  private CloseableIterable<InternalRow> newOrcIterable(\n      InputFile location,\n      FileScanTask task,\n      Schema readSchema,\n      Map<Integer, ?> idToConstant) {\n    return ORC.read(location)\n        .project(readSchema)\n        .split(task.start(), task.length())\n        .createReaderFunc(readOrcSchema -> new SparkOrcReader(readSchema, readOrcSchema, idToConstant))\n        .filter(task.residual())\n        .caseSensitive(caseSensitive)\n        .build();\n  }\n","date":"2020-05-26 01:15:35","endLine":175,"groupId":"3039","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"newOrcIterable","params":"(InputFilelocation@FileScanTasktask@SchemareadSchema@Map<Integer@?>idToConstant)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/9e/0e5df2ba8850d4b2e8e17f86b6aa66948b8f25.src","preCode":"  private CloseableIterable<InternalRow> newOrcIterable(\n      InputFile location,\n      FileScanTask task,\n      Schema readSchema,\n      Map<Integer, ?> idToConstant) {\n    return ORC.read(location)\n        .project(readSchema)\n        .split(task.start(), task.length())\n        .createReaderFunc(readOrcSchema -> new SparkOrcReader(readSchema, readOrcSchema, idToConstant))\n        .filter(task.residual())\n        .caseSensitive(caseSensitive)\n        .build();\n  }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":163,"status":"NB"}],"commitId":"ffdcf09027e09460b7d7505e65aea119107934a3","commitMessage":"@@@Spark: Support vectorized Parquet reads for flat projections (#828)\n\n","date":"2020-06-16 06:16:19","modifiedFileCount":"21","status":"M","submitter":"Samarth Jain"},{"authorTime":"2020-05-26 01:15:35","codes":[{"authorDate":"2020-06-18 00:57:08","commitOrder":4,"curCode":"  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n    CloseableIterable<ColumnarBatch> iter;\n    InputFile location = getInputFile(task);\n    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n    if (task.file().format() == FileFormat.PARQUET) {\n      Parquet.ReadBuilder builder = Parquet.read(location)\n          .project(expectedSchema)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n              fileSchema,  NullCheckingForGet.NULL_CHECKING_ENABLED))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          \r\n          \r\n          \r\n          .reuseContainers();\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Format: \" + task.file().format() + \" not supported for batched reads\");\n    }\n    return iter.iterator();\n  }\n","date":"2020-06-18 00:57:08","endLine":83,"groupId":"3341","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"open","params":"(FileScanTasktask)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/f7/84b638d376af4ff0aef50b0b34bcfd6d15bf19.src","preCode":"  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n    CloseableIterable<ColumnarBatch> iter;\n    InputFile location = getInputFile(task);\n    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n    if (task.file().format() == FileFormat.PARQUET) {\n      iter = Parquet.read(location)\n          .project(expectedSchema)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n              fileSchema,  NullCheckingForGet.NULL_CHECKING_ENABLED))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          \r\n          \r\n          \r\n          .reuseContainers()\n          .build();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Format: \" + task.file().format() + \" not supported for batched reads\");\n    }\n    return iter.iterator();\n  }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":55,"status":"M"},{"authorDate":"2020-05-26 01:15:35","commitOrder":4,"curCode":"  private CloseableIterable<InternalRow> newOrcIterable(\n      InputFile location,\n      FileScanTask task,\n      Schema readSchema,\n      Map<Integer, ?> idToConstant) {\n    return ORC.read(location)\n        .project(readSchema)\n        .split(task.start(), task.length())\n        .createReaderFunc(readOrcSchema -> new SparkOrcReader(readSchema, readOrcSchema, idToConstant))\n        .filter(task.residual())\n        .caseSensitive(caseSensitive)\n        .build();\n  }\n","date":"2020-05-26 01:15:35","endLine":175,"groupId":"3039","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"newOrcIterable","params":"(InputFilelocation@FileScanTasktask@SchemareadSchema@Map<Integer@?>idToConstant)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/9e/0e5df2ba8850d4b2e8e17f86b6aa66948b8f25.src","preCode":"  private CloseableIterable<InternalRow> newOrcIterable(\n      InputFile location,\n      FileScanTask task,\n      Schema readSchema,\n      Map<Integer, ?> idToConstant) {\n    return ORC.read(location)\n        .project(readSchema)\n        .split(task.start(), task.length())\n        .createReaderFunc(readOrcSchema -> new SparkOrcReader(readSchema, readOrcSchema, idToConstant))\n        .filter(task.residual())\n        .caseSensitive(caseSensitive)\n        .build();\n  }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":163,"status":"N"}],"commitId":"5a3cd22e775dfa8bf79deab675390aad48ba79a5","commitMessage":"@@@Parquet: Support name mappings to recover field IDs (#830)\n\n","date":"2020-06-18 00:57:08","modifiedFileCount":"16","status":"M","submitter":"Chen Junjie"},{"authorTime":"2020-07-11 08:05:39","codes":[{"authorDate":"2020-06-18 00:57:08","commitOrder":5,"curCode":"  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n    CloseableIterable<ColumnarBatch> iter;\n    InputFile location = getInputFile(task);\n    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n    if (task.file().format() == FileFormat.PARQUET) {\n      Parquet.ReadBuilder builder = Parquet.read(location)\n          .project(expectedSchema)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n              fileSchema,  NullCheckingForGet.NULL_CHECKING_ENABLED))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          \r\n          \r\n          \r\n          .reuseContainers();\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Format: \" + task.file().format() + \" not supported for batched reads\");\n    }\n    return iter.iterator();\n  }\n","date":"2020-06-18 00:57:08","endLine":83,"groupId":"3341","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"open","params":"(FileScanTasktask)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/f7/84b638d376af4ff0aef50b0b34bcfd6d15bf19.src","preCode":"  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n    CloseableIterable<ColumnarBatch> iter;\n    InputFile location = getInputFile(task);\n    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n    if (task.file().format() == FileFormat.PARQUET) {\n      Parquet.ReadBuilder builder = Parquet.read(location)\n          .project(expectedSchema)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n              fileSchema,  NullCheckingForGet.NULL_CHECKING_ENABLED))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          \r\n          \r\n          \r\n          .reuseContainers();\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Format: \" + task.file().format() + \" not supported for batched reads\");\n    }\n    return iter.iterator();\n  }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":55,"status":"N"},{"authorDate":"2020-07-11 08:05:39","commitOrder":5,"curCode":"  private CloseableIterable<InternalRow> newOrcIterable(\n      InputFile location,\n      FileScanTask task,\n      Schema readSchema,\n      Map<Integer, ?> idToConstant) {\n    Schema readSchemaWithoutConstants = TypeUtil.selectNot(readSchema, idToConstant.keySet());\n    return ORC.read(location)\n        .project(readSchemaWithoutConstants)\n        .split(task.start(), task.length())\n        .createReaderFunc(readOrcSchema -> new SparkOrcReader(readSchema, readOrcSchema, idToConstant))\n        .filter(task.residual())\n        .caseSensitive(caseSensitive)\n        .build();\n  }\n","date":"2020-07-11 08:05:39","endLine":189,"groupId":"3039","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"newOrcIterable","params":"(InputFilelocation@FileScanTasktask@SchemareadSchema@Map<Integer@?>idToConstant)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/16/d5cb9cffcae23a43cf14cc7f01f0655184bf71.src","preCode":"  private CloseableIterable<InternalRow> newOrcIterable(\n      InputFile location,\n      FileScanTask task,\n      Schema readSchema,\n      Map<Integer, ?> idToConstant) {\n    return ORC.read(location)\n        .project(readSchema)\n        .split(task.start(), task.length())\n        .createReaderFunc(readOrcSchema -> new SparkOrcReader(readSchema, readOrcSchema, idToConstant))\n        .filter(task.residual())\n        .caseSensitive(caseSensitive)\n        .build();\n  }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":176,"status":"M"}],"commitId":"a2796f0fd38e517d17462ebc7b0234282e976dd3","commitMessage":"@@@ORC: Use ConstantReader for identity partition columns (#1191)\n\n","date":"2020-07-11 08:05:39","modifiedFileCount":"5","status":"M","submitter":"Shardul Mahadik"},{"authorTime":"2020-07-11 08:05:39","codes":[{"authorDate":"2020-07-14 05:27:36","commitOrder":6,"curCode":"  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n    DataFile file = task.file();\n\n    \r\n    InputFileBlockHolder.set(file.path().toString(), task.start(), task.length());\n\n    \r\n    PartitionSpec spec = task.spec();\n    Set<Integer> idColumns = spec.identitySourceIds();\n    Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n    boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n\n    Map<Integer, ?> idToConstant;\n    if (projectsIdentityPartitionColumns) {\n      idToConstant = PartitionUtil.constantsMap(task, BatchDataReader::convertConstant);\n    } else {\n      idToConstant = ImmutableMap.of();\n    }\n\n    CloseableIterable<ColumnarBatch> iter;\n    InputFile location = getInputFile(task);\n    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n    if (task.file().format() == FileFormat.PARQUET) {\n      Parquet.ReadBuilder builder = Parquet.read(location)\n          .project(expectedSchema)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n              fileSchema,  NullCheckingForGet.NULL_CHECKING_ENABLED))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          \r\n          \r\n          \r\n          .reuseContainers();\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else if (task.file().format() == FileFormat.ORC) {\n      Schema schemaWithoutConstants = TypeUtil.selectNot(expectedSchema, idToConstant.keySet());\n      iter = ORC.read(location)\n          .project(schemaWithoutConstants)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkOrcReaders.buildReader(expectedSchema, fileSchema,\n              idToConstant))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          .build();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Format: \" + task.file().format() + \" not supported for batched reads\");\n    }\n    return iter.iterator();\n  }\n","date":"2020-07-14 05:27:36","endLine":122,"groupId":"3341","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"open","params":"(FileScanTasktask)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/ef/f18ca3100d36bce9d429ba137cfb8e0d1f930c.src","preCode":"  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n    CloseableIterable<ColumnarBatch> iter;\n    InputFile location = getInputFile(task);\n    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n    if (task.file().format() == FileFormat.PARQUET) {\n      Parquet.ReadBuilder builder = Parquet.read(location)\n          .project(expectedSchema)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n              fileSchema,  NullCheckingForGet.NULL_CHECKING_ENABLED))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          \r\n          \r\n          \r\n          .reuseContainers();\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Format: \" + task.file().format() + \" not supported for batched reads\");\n    }\n    return iter.iterator();\n  }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":65,"status":"M"},{"authorDate":"2020-07-11 08:05:39","commitOrder":6,"curCode":"  private CloseableIterable<InternalRow> newOrcIterable(\n      InputFile location,\n      FileScanTask task,\n      Schema readSchema,\n      Map<Integer, ?> idToConstant) {\n    Schema readSchemaWithoutConstants = TypeUtil.selectNot(readSchema, idToConstant.keySet());\n    return ORC.read(location)\n        .project(readSchemaWithoutConstants)\n        .split(task.start(), task.length())\n        .createReaderFunc(readOrcSchema -> new SparkOrcReader(readSchema, readOrcSchema, idToConstant))\n        .filter(task.residual())\n        .caseSensitive(caseSensitive)\n        .build();\n  }\n","date":"2020-07-11 08:05:39","endLine":189,"groupId":"3039","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"newOrcIterable","params":"(InputFilelocation@FileScanTasktask@SchemareadSchema@Map<Integer@?>idToConstant)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/16/d5cb9cffcae23a43cf14cc7f01f0655184bf71.src","preCode":"  private CloseableIterable<InternalRow> newOrcIterable(\n      InputFile location,\n      FileScanTask task,\n      Schema readSchema,\n      Map<Integer, ?> idToConstant) {\n    Schema readSchemaWithoutConstants = TypeUtil.selectNot(readSchema, idToConstant.keySet());\n    return ORC.read(location)\n        .project(readSchemaWithoutConstants)\n        .split(task.start(), task.length())\n        .createReaderFunc(readOrcSchema -> new SparkOrcReader(readSchema, readOrcSchema, idToConstant))\n        .filter(task.residual())\n        .caseSensitive(caseSensitive)\n        .build();\n  }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":176,"status":"N"}],"commitId":"6fab8f57bdb7e5fe7eadc3ff41558581338e1b69","commitMessage":"@@@Spark: Support ORC vectorized reads (#1189)\n\n","date":"2020-07-14 05:27:36","modifiedFileCount":"25","status":"M","submitter":"Shardul Mahadik"},{"authorTime":"2020-07-21 04:43:28","codes":[{"authorDate":"2020-07-14 05:27:36","commitOrder":7,"curCode":"  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n    DataFile file = task.file();\n\n    \r\n    InputFileBlockHolder.set(file.path().toString(), task.start(), task.length());\n\n    \r\n    PartitionSpec spec = task.spec();\n    Set<Integer> idColumns = spec.identitySourceIds();\n    Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n    boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n\n    Map<Integer, ?> idToConstant;\n    if (projectsIdentityPartitionColumns) {\n      idToConstant = PartitionUtil.constantsMap(task, BatchDataReader::convertConstant);\n    } else {\n      idToConstant = ImmutableMap.of();\n    }\n\n    CloseableIterable<ColumnarBatch> iter;\n    InputFile location = getInputFile(task);\n    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n    if (task.file().format() == FileFormat.PARQUET) {\n      Parquet.ReadBuilder builder = Parquet.read(location)\n          .project(expectedSchema)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n              fileSchema,  NullCheckingForGet.NULL_CHECKING_ENABLED))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          \r\n          \r\n          \r\n          .reuseContainers();\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else if (task.file().format() == FileFormat.ORC) {\n      Schema schemaWithoutConstants = TypeUtil.selectNot(expectedSchema, idToConstant.keySet());\n      iter = ORC.read(location)\n          .project(schemaWithoutConstants)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkOrcReaders.buildReader(expectedSchema, fileSchema,\n              idToConstant))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          .build();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Format: \" + task.file().format() + \" not supported for batched reads\");\n    }\n    return iter.iterator();\n  }\n","date":"2020-07-14 05:27:36","endLine":122,"groupId":"3341","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"open","params":"(FileScanTasktask)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/ef/f18ca3100d36bce9d429ba137cfb8e0d1f930c.src","preCode":"  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n    DataFile file = task.file();\n\n    \r\n    InputFileBlockHolder.set(file.path().toString(), task.start(), task.length());\n\n    \r\n    PartitionSpec spec = task.spec();\n    Set<Integer> idColumns = spec.identitySourceIds();\n    Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n    boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n\n    Map<Integer, ?> idToConstant;\n    if (projectsIdentityPartitionColumns) {\n      idToConstant = PartitionUtil.constantsMap(task, BatchDataReader::convertConstant);\n    } else {\n      idToConstant = ImmutableMap.of();\n    }\n\n    CloseableIterable<ColumnarBatch> iter;\n    InputFile location = getInputFile(task);\n    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n    if (task.file().format() == FileFormat.PARQUET) {\n      Parquet.ReadBuilder builder = Parquet.read(location)\n          .project(expectedSchema)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n              fileSchema,  NullCheckingForGet.NULL_CHECKING_ENABLED))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          \r\n          \r\n          \r\n          .reuseContainers();\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else if (task.file().format() == FileFormat.ORC) {\n      Schema schemaWithoutConstants = TypeUtil.selectNot(expectedSchema, idToConstant.keySet());\n      iter = ORC.read(location)\n          .project(schemaWithoutConstants)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkOrcReaders.buildReader(expectedSchema, fileSchema,\n              idToConstant))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          .build();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Format: \" + task.file().format() + \" not supported for batched reads\");\n    }\n    return iter.iterator();\n  }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":65,"status":"N"},{"authorDate":"2020-07-21 04:43:28","commitOrder":7,"curCode":"  private CloseableIterable<InternalRow> newOrcIterable(\n      InputFile location,\n      FileScanTask task,\n      Schema readSchema,\n      Map<Integer, ?> idToConstant) {\n    Schema readSchemaWithoutConstantAndMetadataFields = TypeUtil.selectNot(readSchema,\n        Sets.union(idToConstant.keySet(), MetadataColumns.metadataFieldIds()));\n    return ORC.read(location)\n        .project(readSchemaWithoutConstantAndMetadataFields)\n        .split(task.start(), task.length())\n        .createReaderFunc(readOrcSchema -> new SparkOrcReader(readSchema, readOrcSchema, idToConstant))\n        .filter(task.residual())\n        .caseSensitive(caseSensitive)\n        .build();\n  }\n","date":"2020-07-21 04:43:28","endLine":184,"groupId":"3039","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"newOrcIterable","params":"(InputFilelocation@FileScanTasktask@SchemareadSchema@Map<Integer@?>idToConstant)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a9/b0306c723c05e23affb8b1be711a229ab91f61.src","preCode":"  private CloseableIterable<InternalRow> newOrcIterable(\n      InputFile location,\n      FileScanTask task,\n      Schema readSchema,\n      Map<Integer, ?> idToConstant) {\n    Schema readSchemaWithoutConstants = TypeUtil.selectNot(readSchema, idToConstant.keySet());\n    return ORC.read(location)\n        .project(readSchemaWithoutConstants)\n        .split(task.start(), task.length())\n        .createReaderFunc(readOrcSchema -> new SparkOrcReader(readSchema, readOrcSchema, idToConstant))\n        .filter(task.residual())\n        .caseSensitive(caseSensitive)\n        .build();\n  }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":170,"status":"M"}],"commitId":"247d30622f20fcb63799992cea37733b663da8a5","commitMessage":"@@@ORC: Support row postition as a metadata column (#1207)\n\n","date":"2020-07-21 04:43:28","modifiedFileCount":"14","status":"M","submitter":"Shardul Mahadik"},{"authorTime":"2020-07-21 04:43:28","codes":[{"authorDate":"2020-08-05 09:08:03","commitOrder":8,"curCode":"  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n    DataFile file = task.file();\n\n    \r\n    InputFileBlockHolder.set(file.path().toString(), task.start(), task.length());\n\n    \r\n    PartitionSpec spec = task.spec();\n    Set<Integer> idColumns = spec.identitySourceIds();\n    Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n    boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n\n    Map<Integer, ?> idToConstant;\n    if (projectsIdentityPartitionColumns) {\n      idToConstant = PartitionUtil.constantsMap(task, BatchDataReader::convertConstant);\n    } else {\n      idToConstant = ImmutableMap.of();\n    }\n\n    CloseableIterable<ColumnarBatch> iter;\n    InputFile location = getInputFile(task);\n    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n    if (task.file().format() == FileFormat.PARQUET) {\n      Parquet.ReadBuilder builder = Parquet.read(location)\n          .project(expectedSchema)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n              fileSchema,  NullCheckingForGet.NULL_CHECKING_ENABLED, idToConstant))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          \r\n          \r\n          \r\n          .reuseContainers();\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else if (task.file().format() == FileFormat.ORC) {\n      Schema schemaWithoutConstants = TypeUtil.selectNot(expectedSchema, idToConstant.keySet());\n      iter = ORC.read(location)\n          .project(schemaWithoutConstants)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkOrcReaders.buildReader(expectedSchema, fileSchema,\n              idToConstant))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          .build();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Format: \" + task.file().format() + \" not supported for batched reads\");\n    }\n    return iter.iterator();\n  }\n","date":"2020-08-05 09:08:03","endLine":122,"groupId":"2521","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"open","params":"(FileScanTasktask)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/82/f889ecb4902a76323526758af16a46bcf74ea9.src","preCode":"  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n    DataFile file = task.file();\n\n    \r\n    InputFileBlockHolder.set(file.path().toString(), task.start(), task.length());\n\n    \r\n    PartitionSpec spec = task.spec();\n    Set<Integer> idColumns = spec.identitySourceIds();\n    Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n    boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n\n    Map<Integer, ?> idToConstant;\n    if (projectsIdentityPartitionColumns) {\n      idToConstant = PartitionUtil.constantsMap(task, BatchDataReader::convertConstant);\n    } else {\n      idToConstant = ImmutableMap.of();\n    }\n\n    CloseableIterable<ColumnarBatch> iter;\n    InputFile location = getInputFile(task);\n    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n    if (task.file().format() == FileFormat.PARQUET) {\n      Parquet.ReadBuilder builder = Parquet.read(location)\n          .project(expectedSchema)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n              fileSchema,  NullCheckingForGet.NULL_CHECKING_ENABLED))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          \r\n          \r\n          \r\n          .reuseContainers();\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else if (task.file().format() == FileFormat.ORC) {\n      Schema schemaWithoutConstants = TypeUtil.selectNot(expectedSchema, idToConstant.keySet());\n      iter = ORC.read(location)\n          .project(schemaWithoutConstants)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkOrcReaders.buildReader(expectedSchema, fileSchema,\n              idToConstant))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          .build();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Format: \" + task.file().format() + \" not supported for batched reads\");\n    }\n    return iter.iterator();\n  }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":65,"status":"M"},{"authorDate":"2020-07-21 04:43:28","commitOrder":8,"curCode":"  private CloseableIterable<InternalRow> newOrcIterable(\n      InputFile location,\n      FileScanTask task,\n      Schema readSchema,\n      Map<Integer, ?> idToConstant) {\n    Schema readSchemaWithoutConstantAndMetadataFields = TypeUtil.selectNot(readSchema,\n        Sets.union(idToConstant.keySet(), MetadataColumns.metadataFieldIds()));\n    return ORC.read(location)\n        .project(readSchemaWithoutConstantAndMetadataFields)\n        .split(task.start(), task.length())\n        .createReaderFunc(readOrcSchema -> new SparkOrcReader(readSchema, readOrcSchema, idToConstant))\n        .filter(task.residual())\n        .caseSensitive(caseSensitive)\n        .build();\n  }\n","date":"2020-07-21 04:43:28","endLine":184,"groupId":"3039","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"newOrcIterable","params":"(InputFilelocation@FileScanTasktask@SchemareadSchema@Map<Integer@?>idToConstant)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a9/b0306c723c05e23affb8b1be711a229ab91f61.src","preCode":"  private CloseableIterable<InternalRow> newOrcIterable(\n      InputFile location,\n      FileScanTask task,\n      Schema readSchema,\n      Map<Integer, ?> idToConstant) {\n    Schema readSchemaWithoutConstantAndMetadataFields = TypeUtil.selectNot(readSchema,\n        Sets.union(idToConstant.keySet(), MetadataColumns.metadataFieldIds()));\n    return ORC.read(location)\n        .project(readSchemaWithoutConstantAndMetadataFields)\n        .split(task.start(), task.length())\n        .createReaderFunc(readOrcSchema -> new SparkOrcReader(readSchema, readOrcSchema, idToConstant))\n        .filter(task.residual())\n        .caseSensitive(caseSensitive)\n        .build();\n  }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":170,"status":"N"}],"commitId":"71de51805be7954dd423ec29fd4c7d6b303adc1f","commitMessage":"@@@Parquet: Support vectorized reads with identity partition values (#1287)\n\n","date":"2020-08-05 09:08:03","modifiedFileCount":"7","status":"M","submitter":"Russell Spitzer"},{"authorTime":"2020-08-19 04:36:13","codes":[{"authorDate":"2020-08-19 04:36:13","commitOrder":9,"curCode":"  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n    DataFile file = task.file();\n\n    \r\n    InputFileBlockHolder.set(file.path().toString(), task.start(), task.length());\n\n    \r\n    PartitionSpec spec = task.spec();\n    Set<Integer> idColumns = spec.identitySourceIds();\n    Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n    boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n\n    Map<Integer, ?> idToConstant;\n    if (projectsIdentityPartitionColumns) {\n      idToConstant = PartitionUtil.constantsMap(task, BatchDataReader::convertConstant);\n    } else {\n      idToConstant = ImmutableMap.of();\n    }\n\n    CloseableIterable<ColumnarBatch> iter;\n    InputFile location = getInputFile(task);\n    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n    if (task.file().format() == FileFormat.PARQUET) {\n      Parquet.ReadBuilder builder = Parquet.read(location)\n          .project(expectedSchema)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n              fileSchema,  NullCheckingForGet.NULL_CHECKING_ENABLED, idToConstant))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          \r\n          \r\n          \r\n          .reuseContainers();\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else if (task.file().format() == FileFormat.ORC) {\n      Schema schemaWithoutConstants = TypeUtil.selectNot(expectedSchema, idToConstant.keySet());\n      ORC.ReadBuilder builder = ORC.read(location)\n          .project(schemaWithoutConstants)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkOrcReaders.buildReader(expectedSchema, fileSchema,\n              idToConstant))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive);\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Format: \" + task.file().format() + \" not supported for batched reads\");\n    }\n    return iter.iterator();\n  }\n","date":"2020-08-19 04:36:13","endLine":127,"groupId":"2521","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"open","params":"(FileScanTasktask)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/e9/b75a1da8cffaa0c19f562d09ef00f9fce8dc54.src","preCode":"  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n    DataFile file = task.file();\n\n    \r\n    InputFileBlockHolder.set(file.path().toString(), task.start(), task.length());\n\n    \r\n    PartitionSpec spec = task.spec();\n    Set<Integer> idColumns = spec.identitySourceIds();\n    Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n    boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n\n    Map<Integer, ?> idToConstant;\n    if (projectsIdentityPartitionColumns) {\n      idToConstant = PartitionUtil.constantsMap(task, BatchDataReader::convertConstant);\n    } else {\n      idToConstant = ImmutableMap.of();\n    }\n\n    CloseableIterable<ColumnarBatch> iter;\n    InputFile location = getInputFile(task);\n    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n    if (task.file().format() == FileFormat.PARQUET) {\n      Parquet.ReadBuilder builder = Parquet.read(location)\n          .project(expectedSchema)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n              fileSchema,  NullCheckingForGet.NULL_CHECKING_ENABLED, idToConstant))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          \r\n          \r\n          \r\n          .reuseContainers();\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else if (task.file().format() == FileFormat.ORC) {\n      Schema schemaWithoutConstants = TypeUtil.selectNot(expectedSchema, idToConstant.keySet());\n      iter = ORC.read(location)\n          .project(schemaWithoutConstants)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkOrcReaders.buildReader(expectedSchema, fileSchema,\n              idToConstant))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          .build();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Format: \" + task.file().format() + \" not supported for batched reads\");\n    }\n    return iter.iterator();\n  }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":65,"status":"M"},{"authorDate":"2020-08-19 04:36:13","commitOrder":9,"curCode":"  private CloseableIterable<InternalRow> newOrcIterable(\n      InputFile location,\n      FileScanTask task,\n      Schema readSchema,\n      Map<Integer, ?> idToConstant) {\n    Schema readSchemaWithoutConstantAndMetadataFields = TypeUtil.selectNot(readSchema,\n        Sets.union(idToConstant.keySet(), MetadataColumns.metadataFieldIds()));\n\n    ORC.ReadBuilder builder = ORC.read(location)\n        .project(readSchemaWithoutConstantAndMetadataFields)\n        .split(task.start(), task.length())\n        .createReaderFunc(readOrcSchema -> new SparkOrcReader(readSchema, readOrcSchema, idToConstant))\n        .filter(task.residual())\n        .caseSensitive(caseSensitive);\n\n    if (nameMapping != null) {\n      builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n    }\n\n    return builder.build();\n  }\n","date":"2020-08-19 04:36:13","endLine":190,"groupId":"3039","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"newOrcIterable","params":"(InputFilelocation@FileScanTasktask@SchemareadSchema@Map<Integer@?>idToConstant)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a7/fbe90ee8b5ea1c9eae71ecc197544b9938cf4b.src","preCode":"  private CloseableIterable<InternalRow> newOrcIterable(\n      InputFile location,\n      FileScanTask task,\n      Schema readSchema,\n      Map<Integer, ?> idToConstant) {\n    Schema readSchemaWithoutConstantAndMetadataFields = TypeUtil.selectNot(readSchema,\n        Sets.union(idToConstant.keySet(), MetadataColumns.metadataFieldIds()));\n    return ORC.read(location)\n        .project(readSchemaWithoutConstantAndMetadataFields)\n        .split(task.start(), task.length())\n        .createReaderFunc(readOrcSchema -> new SparkOrcReader(readSchema, readOrcSchema, idToConstant))\n        .filter(task.residual())\n        .caseSensitive(caseSensitive)\n        .build();\n  }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":170,"status":"M"}],"commitId":"131c9c0eebf55d02ab88dc62677e46f2d9501048","commitMessage":"@@@ORC: Add name mapping support (#1208)\n\n","date":"2020-08-19 04:36:13","modifiedFileCount":"10","status":"M","submitter":"Edgar Rodriguez"},{"authorTime":"2020-08-19 04:36:13","codes":[{"authorDate":"2020-12-22 05:19:14","commitOrder":10,"curCode":"  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n    DataFile file = task.file();\n\n    \r\n    InputFileBlockHolder.set(file.path().toString(), task.start(), task.length());\n\n    Map<Integer, ?> idToConstant = PartitionUtil.constantsMap(task, BatchDataReader::convertConstant);\n\n    CloseableIterable<ColumnarBatch> iter;\n    InputFile location = getInputFile(task);\n    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n    if (task.file().format() == FileFormat.PARQUET) {\n      Parquet.ReadBuilder builder = Parquet.read(location)\n          .project(expectedSchema)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n              fileSchema,  NullCheckingForGet.NULL_CHECKING_ENABLED, idToConstant))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          \r\n          \r\n          \r\n          .reuseContainers();\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else if (task.file().format() == FileFormat.ORC) {\n      Schema schemaWithoutConstants = TypeUtil.selectNot(expectedSchema, idToConstant.keySet());\n      ORC.ReadBuilder builder = ORC.read(location)\n          .project(schemaWithoutConstants)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkOrcReaders.buildReader(expectedSchema, fileSchema,\n              idToConstant))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive);\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Format: \" + task.file().format() + \" not supported for batched reads\");\n    }\n    return iter.iterator();\n  }\n","date":"2020-12-22 05:19:14","endLine":113,"groupId":"2521","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"open","params":"(FileScanTasktask)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/6f/495098ffe8de87a2dfec1003b3a4e7c2f4571e.src","preCode":"  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n    DataFile file = task.file();\n\n    \r\n    InputFileBlockHolder.set(file.path().toString(), task.start(), task.length());\n\n    \r\n    PartitionSpec spec = task.spec();\n    Set<Integer> idColumns = spec.identitySourceIds();\n    Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n    boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n\n    Map<Integer, ?> idToConstant;\n    if (projectsIdentityPartitionColumns) {\n      idToConstant = PartitionUtil.constantsMap(task, BatchDataReader::convertConstant);\n    } else {\n      idToConstant = ImmutableMap.of();\n    }\n\n    CloseableIterable<ColumnarBatch> iter;\n    InputFile location = getInputFile(task);\n    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n    if (task.file().format() == FileFormat.PARQUET) {\n      Parquet.ReadBuilder builder = Parquet.read(location)\n          .project(expectedSchema)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n              fileSchema,  NullCheckingForGet.NULL_CHECKING_ENABLED, idToConstant))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          \r\n          \r\n          \r\n          .reuseContainers();\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else if (task.file().format() == FileFormat.ORC) {\n      Schema schemaWithoutConstants = TypeUtil.selectNot(expectedSchema, idToConstant.keySet());\n      ORC.ReadBuilder builder = ORC.read(location)\n          .project(schemaWithoutConstants)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkOrcReaders.buildReader(expectedSchema, fileSchema,\n              idToConstant))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive);\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Format: \" + task.file().format() + \" not supported for batched reads\");\n    }\n    return iter.iterator();\n  }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":62,"status":"M"},{"authorDate":"2020-08-19 04:36:13","commitOrder":10,"curCode":"  private CloseableIterable<InternalRow> newOrcIterable(\n      InputFile location,\n      FileScanTask task,\n      Schema readSchema,\n      Map<Integer, ?> idToConstant) {\n    Schema readSchemaWithoutConstantAndMetadataFields = TypeUtil.selectNot(readSchema,\n        Sets.union(idToConstant.keySet(), MetadataColumns.metadataFieldIds()));\n\n    ORC.ReadBuilder builder = ORC.read(location)\n        .project(readSchemaWithoutConstantAndMetadataFields)\n        .split(task.start(), task.length())\n        .createReaderFunc(readOrcSchema -> new SparkOrcReader(readSchema, readOrcSchema, idToConstant))\n        .filter(task.residual())\n        .caseSensitive(caseSensitive);\n\n    if (nameMapping != null) {\n      builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n    }\n\n    return builder.build();\n  }\n","date":"2020-08-19 04:36:13","endLine":190,"groupId":"3039","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"newOrcIterable","params":"(InputFilelocation@FileScanTasktask@SchemareadSchema@Map<Integer@?>idToConstant)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a7/fbe90ee8b5ea1c9eae71ecc197544b9938cf4b.src","preCode":"  private CloseableIterable<InternalRow> newOrcIterable(\n      InputFile location,\n      FileScanTask task,\n      Schema readSchema,\n      Map<Integer, ?> idToConstant) {\n    Schema readSchemaWithoutConstantAndMetadataFields = TypeUtil.selectNot(readSchema,\n        Sets.union(idToConstant.keySet(), MetadataColumns.metadataFieldIds()));\n\n    ORC.ReadBuilder builder = ORC.read(location)\n        .project(readSchemaWithoutConstantAndMetadataFields)\n        .split(task.start(), task.length())\n        .createReaderFunc(readOrcSchema -> new SparkOrcReader(readSchema, readOrcSchema, idToConstant))\n        .filter(task.residual())\n        .caseSensitive(caseSensitive);\n\n    if (nameMapping != null) {\n      builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n    }\n\n    return builder.build();\n  }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":170,"status":"N"}],"commitId":"bafda6168b533286bb57bea71e8060f660d0f4dc","commitMessage":"@@@Spark: Sort retained rows in DELETE FROM by file and position (#1955)\n\n","date":"2020-12-22 05:19:14","modifiedFileCount":"7","status":"M","submitter":"Ryan Blue"},{"authorTime":"2020-08-19 04:36:13","codes":[{"authorDate":"2021-02-17 02:39:25","commitOrder":11,"curCode":"  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n    DataFile file = task.file();\n\n    \r\n    InputFileBlockHolder.set(file.path().toString(), task.start(), task.length());\n\n    Map<Integer, ?> idToConstant = PartitionUtil.constantsMap(task, BatchDataReader::convertConstant);\n\n    CloseableIterable<ColumnarBatch> iter;\n    InputFile location = getInputFile(task);\n    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n    if (task.file().format() == FileFormat.PARQUET) {\n      Parquet.ReadBuilder builder = Parquet.read(location)\n          .project(expectedSchema)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n              fileSchema,  NullCheckingForGet.NULL_CHECKING_ENABLED, idToConstant))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          \r\n          \r\n          \r\n          .reuseContainers();\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else if (task.file().format() == FileFormat.ORC) {\n      Set<Integer> constantFieldIds = idToConstant.keySet();\n      Set<Integer> metadataFieldIds = MetadataColumns.metadataFieldIds();\n      Sets.SetView<Integer> constantAndMetadataFieldIds = Sets.union(constantFieldIds, metadataFieldIds);\n      Schema schemaWithoutConstantAndMetadataFields = TypeUtil.selectNot(expectedSchema, constantAndMetadataFieldIds);\n      ORC.ReadBuilder builder = ORC.read(location)\n          .project(schemaWithoutConstantAndMetadataFields)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkOrcReaders.buildReader(expectedSchema, fileSchema,\n              idToConstant))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive);\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Format: \" + task.file().format() + \" not supported for batched reads\");\n    }\n    return iter.iterator();\n  }\n","date":"2021-02-17 02:39:25","endLine":119,"groupId":"10538","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"open","params":"(FileScanTasktask)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/d4/8cf2430d694bb13edc226c1b19f5ae4c603b67.src","preCode":"  CloseableIterator<ColumnarBatch> open(FileScanTask task) {\n    DataFile file = task.file();\n\n    \r\n    InputFileBlockHolder.set(file.path().toString(), task.start(), task.length());\n\n    Map<Integer, ?> idToConstant = PartitionUtil.constantsMap(task, BatchDataReader::convertConstant);\n\n    CloseableIterable<ColumnarBatch> iter;\n    InputFile location = getInputFile(task);\n    Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n    if (task.file().format() == FileFormat.PARQUET) {\n      Parquet.ReadBuilder builder = Parquet.read(location)\n          .project(expectedSchema)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkParquetReaders.buildReader(expectedSchema,\n              fileSchema,  NullCheckingForGet.NULL_CHECKING_ENABLED, idToConstant))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          \r\n          \r\n          \r\n          .reuseContainers();\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else if (task.file().format() == FileFormat.ORC) {\n      Schema schemaWithoutConstants = TypeUtil.selectNot(expectedSchema, idToConstant.keySet());\n      ORC.ReadBuilder builder = ORC.read(location)\n          .project(schemaWithoutConstants)\n          .split(task.start(), task.length())\n          .createBatchedReaderFunc(fileSchema -> VectorizedSparkOrcReaders.buildReader(expectedSchema, fileSchema,\n              idToConstant))\n          .recordsPerBatch(batchSize)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive);\n\n      if (nameMapping != null) {\n        builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n      }\n\n      iter = builder.build();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Format: \" + task.file().format() + \" not supported for batched reads\");\n    }\n    return iter.iterator();\n  }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":65,"status":"M"},{"authorDate":"2020-08-19 04:36:13","commitOrder":11,"curCode":"  private CloseableIterable<InternalRow> newOrcIterable(\n      InputFile location,\n      FileScanTask task,\n      Schema readSchema,\n      Map<Integer, ?> idToConstant) {\n    Schema readSchemaWithoutConstantAndMetadataFields = TypeUtil.selectNot(readSchema,\n        Sets.union(idToConstant.keySet(), MetadataColumns.metadataFieldIds()));\n\n    ORC.ReadBuilder builder = ORC.read(location)\n        .project(readSchemaWithoutConstantAndMetadataFields)\n        .split(task.start(), task.length())\n        .createReaderFunc(readOrcSchema -> new SparkOrcReader(readSchema, readOrcSchema, idToConstant))\n        .filter(task.residual())\n        .caseSensitive(caseSensitive);\n\n    if (nameMapping != null) {\n      builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n    }\n\n    return builder.build();\n  }\n","date":"2020-08-19 04:36:13","endLine":190,"groupId":"10538","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"newOrcIterable","params":"(InputFilelocation@FileScanTasktask@SchemareadSchema@Map<Integer@?>idToConstant)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a7/fbe90ee8b5ea1c9eae71ecc197544b9938cf4b.src","preCode":"  private CloseableIterable<InternalRow> newOrcIterable(\n      InputFile location,\n      FileScanTask task,\n      Schema readSchema,\n      Map<Integer, ?> idToConstant) {\n    Schema readSchemaWithoutConstantAndMetadataFields = TypeUtil.selectNot(readSchema,\n        Sets.union(idToConstant.keySet(), MetadataColumns.metadataFieldIds()));\n\n    ORC.ReadBuilder builder = ORC.read(location)\n        .project(readSchemaWithoutConstantAndMetadataFields)\n        .split(task.start(), task.length())\n        .createReaderFunc(readOrcSchema -> new SparkOrcReader(readSchema, readOrcSchema, idToConstant))\n        .filter(task.residual())\n        .caseSensitive(caseSensitive);\n\n    if (nameMapping != null) {\n      builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n    }\n\n    return builder.build();\n  }\n","realPath":"spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":170,"status":"N"}],"commitId":"631efec2f9ce1f0526d6613c81ac8fd0ccb95b5e","commitMessage":"@@@ORC: Fix vectorized reads with metadata columns (#2241)\n\n","date":"2021-02-17 02:39:25","modifiedFileCount":"2","status":"M","submitter":"Anton Okolnychyi"}]
