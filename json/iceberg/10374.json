[{"authorTime":"2019-06-11 01:36:27","codes":[{"authorDate":"2019-06-11 01:36:27","commitOrder":1,"curCode":"  public void testSnapshotSelectionByInvalidSnapshotId() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    tables.create(SCHEMA, spec, tableLocation);\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .option(\"snapshot-id\", -10)\n        .load(tableLocation);\n\n    df.collectAsList();\n  }\n","date":"2019-06-11 01:36:27","endLine":192,"groupId":"2014","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testSnapshotSelectionByInvalidSnapshotId","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/cb/d9b1d310fa2b9edb740ba9e61f880ef8c8de2a.src","preCode":"  public void testSnapshotSelectionByInvalidSnapshotId() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    tables.create(SCHEMA, spec, tableLocation);\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .option(\"snapshot-id\", -10)\n        .load(tableLocation);\n\n    df.collectAsList();\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":179,"status":"B"},{"authorDate":"2019-06-11 01:36:27","commitOrder":1,"curCode":"  public void testSnapshotSelectionByInvalidTimestamp() throws IOException {\n    long timestamp = System.currentTimeMillis();\n\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    tables.create(SCHEMA, spec, tableLocation);\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .option(\"as-of-timestamp\", timestamp)\n        .load(tableLocation);\n\n    df.collectAsList();\n  }\n","date":"2019-06-11 01:36:27","endLine":209,"groupId":"2014","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testSnapshotSelectionByInvalidTimestamp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/cb/d9b1d310fa2b9edb740ba9e61f880ef8c8de2a.src","preCode":"  public void testSnapshotSelectionByInvalidTimestamp() throws IOException {\n    long timestamp = System.currentTimeMillis();\n\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    tables.create(SCHEMA, spec, tableLocation);\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .option(\"as-of-timestamp\", timestamp)\n        .load(tableLocation);\n\n    df.collectAsList();\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":195,"status":"B"}],"commitId":"d687f96beb4e0b523ae86d814a720fdbc0299b4b","commitMessage":"@@@Spark: Add snapshot selection options to reads (#61)\n\n","date":"2019-06-11 01:36:27","modifiedFileCount":"2","status":"B","submitter":"Ryan Blue"},{"authorTime":"2020-12-29 05:55:09","codes":[{"authorDate":"2019-06-11 01:36:27","commitOrder":2,"curCode":"  public void testSnapshotSelectionByInvalidSnapshotId() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    tables.create(SCHEMA, spec, tableLocation);\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .option(\"snapshot-id\", -10)\n        .load(tableLocation);\n\n    df.collectAsList();\n  }\n","date":"2019-06-11 01:36:27","endLine":192,"groupId":"10374","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testSnapshotSelectionByInvalidSnapshotId","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/cb/d9b1d310fa2b9edb740ba9e61f880ef8c8de2a.src","preCode":"  public void testSnapshotSelectionByInvalidSnapshotId() throws IOException {\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    tables.create(SCHEMA, spec, tableLocation);\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .option(\"snapshot-id\", -10)\n        .load(tableLocation);\n\n    df.collectAsList();\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":179,"status":"N"},{"authorDate":"2020-12-29 05:55:09","commitOrder":2,"curCode":"  public void testSnapshotSelectionByInvalidTimestamp() throws IOException {\n    long timestamp = System.currentTimeMillis();\n\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    tables.create(SCHEMA, spec, tableLocation);\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .option(SparkReadOptions.AS_OF_TIMESTAMP, timestamp)\n        .load(tableLocation);\n\n    df.collectAsList();\n  }\n","date":"2020-12-29 05:55:09","endLine":210,"groupId":"10374","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"testSnapshotSelectionByInvalidTimestamp","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/8a/adfb858ca71aa6df1cf7f902674d98382aed05.src","preCode":"  public void testSnapshotSelectionByInvalidTimestamp() throws IOException {\n    long timestamp = System.currentTimeMillis();\n\n    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.unpartitioned();\n    tables.create(SCHEMA, spec, tableLocation);\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .option(\"as-of-timestamp\", timestamp)\n        .load(tableLocation);\n\n    df.collectAsList();\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":196,"status":"M"}],"commitId":"cbb244759ae218b85a7e68a1568fddaf815aab3c","commitMessage":"@@@Spark: Use constants for DF read and write options (#1933)\n\n","date":"2020-12-29 05:55:09","modifiedFileCount":"22","status":"M","submitter":"Karuppayya"}]
