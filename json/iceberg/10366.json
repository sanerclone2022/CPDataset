[{"authorTime":"2020-11-06 09:48:18","codes":[{"authorDate":"2020-10-06 01:02:02","commitOrder":3,"curCode":"  public StructLikeSet rowSet(String name, Table table, String... columns) {\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(TableIdentifier.of(\"default\", name).toString())\n        .selectExpr(columns);\n\n    Types.StructType projection = table.schema().select(columns).asStruct();\n    StructLikeSet set = StructLikeSet.create(projection);\n    df.collectAsList().forEach(row -> {\n      SparkStructLike rowWrapper = new SparkStructLike(projection);\n      set.add(rowWrapper.wrap(row));\n    });\n\n    return set;\n  }\n","date":"2020-10-06 01:02:02","endLine":115,"groupId":"743","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"rowSet","params":"(Stringname@Tabletable@String...columns)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/22/9a361f69242b924cbd7c362780061506092eee.src","preCode":"  public StructLikeSet rowSet(String name, Table table, String... columns) {\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(TableIdentifier.of(\"default\", name).toString())\n        .selectExpr(columns);\n\n    Types.StructType projection = table.schema().select(columns).asStruct();\n    StructLikeSet set = StructLikeSet.create(projection);\n    df.collectAsList().forEach(row -> {\n      SparkStructLike rowWrapper = new SparkStructLike(projection);\n      set.add(rowWrapper.wrap(row));\n    });\n\n    return set;\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":101,"status":"NB"},{"authorDate":"2020-11-06 09:48:18","commitOrder":3,"curCode":"  public void testEqualityDeleteWithFilter() throws IOException {\n    String tableName = \"test_with_filter\";\n    Table table = createTable(tableName, SCHEMA, SPEC);\n    Schema deleteRowSchema = table.schema().select(\"data\");\n    Record dataDelete = GenericRecord.create(deleteRowSchema);\n    List<Record> dataDeletes = Lists.newArrayList(\n        dataDelete.copy(\"data\", \"a\"), \r\n        dataDelete.copy(\"data\", \"d\"), \r\n        dataDelete.copy(\"data\", \"g\") \r\n    );\n\n    DeleteFile eqDeletes = FileHelpers.writeDeleteFile(\n        table, Files.localOutput(temp.newFile()), TestHelpers.Row.of(0), dataDeletes, deleteRowSchema);\n\n    table.newRowDelta()\n        .addDeletes(eqDeletes)\n        .commit();\n\n    Types.StructType projection = table.schema().select(\"*\").asStruct();\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(TableIdentifier.of(\"default\", tableName).toString())\n        .filter(\"data = 'a'\") \r\n        .selectExpr(\"*\");\n\n    StructLikeSet actual = StructLikeSet.create(projection);\n    df.collectAsList().forEach(row -> {\n      SparkStructLike rowWrapper = new SparkStructLike(projection);\n      actual.add(rowWrapper.wrap(row));\n    });\n\n    Assert.assertEquals(\"Table should contain no rows\", 0, actual.size());\n  }\n","date":"2020-11-06 09:48:18","endLine":161,"groupId":"3942","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testEqualityDeleteWithFilter","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/68/787a2288dd91ad6fbcb9ff89c9e935dcf0d11c.src","preCode":"  public void testEqualityDeleteWithFilter() throws IOException {\n    String tableName = \"test_with_filter\";\n    Table table = createTable(tableName, SCHEMA, SPEC);\n    Schema deleteRowSchema = table.schema().select(\"data\");\n    Record dataDelete = GenericRecord.create(deleteRowSchema);\n    List<Record> dataDeletes = Lists.newArrayList(\n        dataDelete.copy(\"data\", \"a\"), \r\n        dataDelete.copy(\"data\", \"d\"), \r\n        dataDelete.copy(\"data\", \"g\") \r\n    );\n\n    DeleteFile eqDeletes = FileHelpers.writeDeleteFile(\n        table, Files.localOutput(temp.newFile()), TestHelpers.Row.of(0), dataDeletes, deleteRowSchema);\n\n    table.newRowDelta()\n        .addDeletes(eqDeletes)\n        .commit();\n\n    Types.StructType projection = table.schema().select(\"*\").asStruct();\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(TableIdentifier.of(\"default\", tableName).toString())\n        .filter(\"data = 'a'\") \r\n        .selectExpr(\"*\");\n\n    StructLikeSet actual = StructLikeSet.create(projection);\n    df.collectAsList().forEach(row -> {\n      SparkStructLike rowWrapper = new SparkStructLike(projection);\n      actual.add(rowWrapper.wrap(row));\n    });\n\n    Assert.assertEquals(\"Table should contain no rows\", 0, actual.size());\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":129,"status":"B"}],"commitId":"d0625c8da5b22f6ef1e22059b3d6d847fbf9b350","commitMessage":"@@@Core: Fix NullPointerException in ManifestReader (#1730)\n\nThis fixes a NullPointerException that is thrown by a ManifestReader for delete files when there is a query filter. The DeleteFileIndex projects all fields of a delete manifest.  so it doesn't call select to select specific columns.  unlike ManifestGroup.  which selects * by default. When select is not called.  methods that check whether to add stats columns fail.  but only if there is a row filter because stats columns are not needed if there is no row filter.\n\nExisting tests either called select to configure the reader.  or didn't pass a row filter and projected all rows. This adds a test that uses DeleteFileIndex and a test for ManifestReader. This also fixes dropStats in addition to requireStatsProjection.\n\nCo-authored-by: ??? <zhongbaoluo@shxgroup.net>\n","date":"2020-11-06 09:48:18","modifiedFileCount":"3","status":"M","submitter":"Ryan Blue"},{"authorTime":"2021-03-12 10:30:30","codes":[{"authorDate":"2020-10-06 01:02:02","commitOrder":4,"curCode":"  public StructLikeSet rowSet(String name, Table table, String... columns) {\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(TableIdentifier.of(\"default\", name).toString())\n        .selectExpr(columns);\n\n    Types.StructType projection = table.schema().select(columns).asStruct();\n    StructLikeSet set = StructLikeSet.create(projection);\n    df.collectAsList().forEach(row -> {\n      SparkStructLike rowWrapper = new SparkStructLike(projection);\n      set.add(rowWrapper.wrap(row));\n    });\n\n    return set;\n  }\n","date":"2020-10-06 01:02:02","endLine":115,"groupId":"10366","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"rowSet","params":"(Stringname@Tabletable@String...columns)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/22/9a361f69242b924cbd7c362780061506092eee.src","preCode":"  public StructLikeSet rowSet(String name, Table table, String... columns) {\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(TableIdentifier.of(\"default\", name).toString())\n        .selectExpr(columns);\n\n    Types.StructType projection = table.schema().select(columns).asStruct();\n    StructLikeSet set = StructLikeSet.create(projection);\n    df.collectAsList().forEach(row -> {\n      SparkStructLike rowWrapper = new SparkStructLike(projection);\n      set.add(rowWrapper.wrap(row));\n    });\n\n    return set;\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":101,"status":"N"},{"authorDate":"2021-03-12 10:30:30","commitOrder":4,"curCode":"  public void testEqualityDeleteWithFilter() throws IOException {\n    String tableName = table.name().substring(table.name().lastIndexOf(\".\") + 1);\n    Schema deleteRowSchema = table.schema().select(\"data\");\n    Record dataDelete = GenericRecord.create(deleteRowSchema);\n    List<Record> dataDeletes = Lists.newArrayList(\n        dataDelete.copy(\"data\", \"a\"), \r\n        dataDelete.copy(\"data\", \"d\"), \r\n        dataDelete.copy(\"data\", \"g\") \r\n    );\n\n    DeleteFile eqDeletes = FileHelpers.writeDeleteFile(\n        table, Files.localOutput(temp.newFile()), TestHelpers.Row.of(0), dataDeletes, deleteRowSchema);\n\n    table.newRowDelta()\n        .addDeletes(eqDeletes)\n        .commit();\n\n    Types.StructType projection = table.schema().select(\"*\").asStruct();\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(TableIdentifier.of(\"default\", tableName).toString())\n        .filter(\"data = 'a'\") \r\n        .selectExpr(\"*\");\n\n    StructLikeSet actual = StructLikeSet.create(projection);\n    df.collectAsList().forEach(row -> {\n      SparkStructLike rowWrapper = new SparkStructLike(projection);\n      actual.add(rowWrapper.wrap(row));\n    });\n\n    Assert.assertEquals(\"Table should contain no rows\", 0, actual.size());\n  }\n","date":"2021-03-12 10:30:30","endLine":160,"groupId":"10366","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testEqualityDeleteWithFilter","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/00/2afd5218546cfe876ed2f98ee06ab061c0d8f9.src","preCode":"  public void testEqualityDeleteWithFilter() throws IOException {\n    String tableName = \"test_with_filter\";\n    Table table = createTable(tableName, SCHEMA, SPEC);\n    Schema deleteRowSchema = table.schema().select(\"data\");\n    Record dataDelete = GenericRecord.create(deleteRowSchema);\n    List<Record> dataDeletes = Lists.newArrayList(\n        dataDelete.copy(\"data\", \"a\"), \r\n        dataDelete.copy(\"data\", \"d\"), \r\n        dataDelete.copy(\"data\", \"g\") \r\n    );\n\n    DeleteFile eqDeletes = FileHelpers.writeDeleteFile(\n        table, Files.localOutput(temp.newFile()), TestHelpers.Row.of(0), dataDeletes, deleteRowSchema);\n\n    table.newRowDelta()\n        .addDeletes(eqDeletes)\n        .commit();\n\n    Types.StructType projection = table.schema().select(\"*\").asStruct();\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(TableIdentifier.of(\"default\", tableName).toString())\n        .filter(\"data = 'a'\") \r\n        .selectExpr(\"*\");\n\n    StructLikeSet actual = StructLikeSet.create(projection);\n    df.collectAsList().forEach(row -> {\n      SparkStructLike rowWrapper = new SparkStructLike(projection);\n      actual.add(rowWrapper.wrap(row));\n    });\n\n    Assert.assertEquals(\"Table should contain no rows\", 0, actual.size());\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":129,"status":"M"}],"commitId":"94c9da5b4722b0acaa25e3877961f9a2cb5afc20","commitMessage":"@@@Spark: Improve the test case testEqualityDeleteWithFilter (#2321)\n\n","date":"2021-03-12 10:30:30","modifiedFileCount":"2","status":"M","submitter":"Chen Junjie"}]
