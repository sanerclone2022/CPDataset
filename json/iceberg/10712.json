[{"authorTime":"2020-06-20 09:02:23","codes":[{"authorDate":"2020-06-30 08:56:05","commitOrder":2,"curCode":"  public void writeUnpartitionedTable() throws IOException {\n    this.parent = temp.newFolder(\"TestFilteredScan\");\n    this.unpartitioned = new File(parent, \"unpartitioned\");\n    File dataFolder = new File(unpartitioned, \"data\");\n    Assert.assertTrue(\"Mkdir should succeed\", dataFolder.mkdirs());\n\n    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());\n    Schema tableSchema = table.schema(); \r\n\n    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n\n    File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));\n\n    \r\n    org.apache.avro.Schema avroSchema = AvroSchemaUtil.convert(tableSchema, \"test\");\n    this.records = testRecords(avroSchema);\n\n    switch (fileFormat) {\n      case AVRO:\n        try (FileAppender<Record> writer = Avro.write(localOutput(testFile))\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n\n      case PARQUET:\n        try (FileAppender<Record> writer = Parquet.write(localOutput(testFile))\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(records.size())\n        .withFileSizeInBytes(testFile.length())\n        .withPath(testFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n  }\n","date":"2020-06-30 08:56:05","endLine":209,"groupId":"1391","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"writeUnpartitionedTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/7d/d308dcbed19b9615af2fe851dd00df86c8c855.src","preCode":"  public void writeUnpartitionedTable() throws IOException {\n    this.parent = temp.newFolder(\"TestFilteredScan\");\n    this.unpartitioned = new File(parent, \"unpartitioned\");\n    File dataFolder = new File(unpartitioned, \"data\");\n    Assert.assertTrue(\"Mkdir should succeed\", dataFolder.mkdirs());\n\n    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());\n    Schema tableSchema = table.schema(); \r\n\n    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n\n    File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));\n\n    \r\n    org.apache.avro.Schema avroSchema = AvroSchemaUtil.convert(tableSchema, \"test\");\n    this.records = testRecords(avroSchema);\n\n    switch (fileFormat) {\n      case AVRO:\n        try (FileAppender<Record> writer = Avro.write(localOutput(testFile))\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n\n      case PARQUET:\n        try (FileAppender<Record> writer = Parquet.write(localOutput(testFile))\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(records.size())\n        .withFileSizeInBytes(testFile.length())\n        .withPath(testFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n  }\n","realPath":"spark3/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":167,"status":"B"},{"authorDate":"2020-06-20 09:02:23","commitOrder":2,"curCode":"  public void writeUnpartitionedTable() throws IOException {\n    this.parent = temp.newFolder(\"TestFilteredScan\");\n    this.unpartitioned = new File(parent, \"unpartitioned\");\n    File dataFolder = new File(unpartitioned, \"data\");\n    Assert.assertTrue(\"Mkdir should succeed\", dataFolder.mkdirs());\n\n    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());\n    Schema tableSchema = table.schema(); \r\n\n    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n\n    File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));\n\n    \r\n    this.records = testRecords(tableSchema);\n\n    switch (fileFormat) {\n      case AVRO:\n        try (FileAppender<Record> writer = Avro.write(localOutput(testFile))\n            .createWriterFunc(DataWriter::create)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n\n      case PARQUET:\n        try (FileAppender<Record> writer = Parquet.write(localOutput(testFile))\n            .createWriterFunc(GenericParquetWriter::buildWriter)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n\n      case ORC:\n        try (FileAppender<Record> writer = ORC.write(localOutput(testFile))\n            .createWriterFunc(GenericOrcWriter::buildWriter)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(records.size())\n        .withFileSizeInBytes(testFile.length())\n        .withPath(testFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n  }\n","date":"2020-06-20 09:02:23","endLine":226,"groupId":"1657","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"writeUnpartitionedTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/c0/d676ec8aa99c9101f89d24d70e070468572cda.src","preCode":"  public void writeUnpartitionedTable() throws IOException {\n    this.parent = temp.newFolder(\"TestFilteredScan\");\n    this.unpartitioned = new File(parent, \"unpartitioned\");\n    File dataFolder = new File(unpartitioned, \"data\");\n    Assert.assertTrue(\"Mkdir should succeed\", dataFolder.mkdirs());\n\n    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());\n    Schema tableSchema = table.schema(); \r\n\n    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n\n    File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));\n\n    \r\n    this.records = testRecords(tableSchema);\n\n    switch (fileFormat) {\n      case AVRO:\n        try (FileAppender<Record> writer = Avro.write(localOutput(testFile))\n            .createWriterFunc(DataWriter::create)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n\n      case PARQUET:\n        try (FileAppender<Record> writer = Parquet.write(localOutput(testFile))\n            .createWriterFunc(GenericParquetWriter::buildWriter)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n\n      case ORC:\n        try (FileAppender<Record> writer = ORC.write(localOutput(testFile))\n            .createWriterFunc(GenericOrcWriter::buildWriter)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(records.size())\n        .withFileSizeInBytes(testFile.length())\n        .withPath(testFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":174,"status":"NB"}],"commitId":"51c930e33867e54b3d7e0159b11d6b9c4bc81f1a","commitMessage":"@@@Spark: Add Spark 3 data source classes (#1124)\n\n","date":"2020-06-30 08:56:05","modifiedFileCount":"7","status":"M","submitter":"Ryan Blue"},{"authorTime":"2020-06-20 09:02:23","codes":[{"authorDate":"2020-07-14 05:27:36","commitOrder":3,"curCode":"  public void writeUnpartitionedTable() throws IOException {\n    this.parent = temp.newFolder(\"TestFilteredScan\");\n    this.unpartitioned = new File(parent, \"unpartitioned\");\n    File dataFolder = new File(unpartitioned, \"data\");\n    Assert.assertTrue(\"Mkdir should succeed\", dataFolder.mkdirs());\n\n    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());\n    Schema tableSchema = table.schema(); \r\n\n    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n\n    File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));\n\n    this.records = testRecords(tableSchema);\n\n    switch (fileFormat) {\n      case AVRO:\n        try (FileAppender<Record> writer = Avro.write(localOutput(testFile))\n            .createWriterFunc(DataWriter::create)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n\n      case PARQUET:\n        try (FileAppender<Record> writer = Parquet.write(localOutput(testFile))\n            .createWriterFunc(GenericParquetWriter::buildWriter)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n\n      case ORC:\n        try (FileAppender<Record> writer = ORC.write(localOutput(testFile))\n            .createWriterFunc(GenericOrcWriter::buildWriter)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(records.size())\n        .withFileSizeInBytes(testFile.length())\n        .withPath(testFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n  }\n","date":"2020-07-14 05:27:36","endLine":227,"groupId":"1657","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"writeUnpartitionedTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/9b/e99383873ff072f8b9a1577c9034bf17a6d465.src","preCode":"  public void writeUnpartitionedTable() throws IOException {\n    this.parent = temp.newFolder(\"TestFilteredScan\");\n    this.unpartitioned = new File(parent, \"unpartitioned\");\n    File dataFolder = new File(unpartitioned, \"data\");\n    Assert.assertTrue(\"Mkdir should succeed\", dataFolder.mkdirs());\n\n    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());\n    Schema tableSchema = table.schema(); \r\n\n    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n\n    File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));\n\n    \r\n    org.apache.avro.Schema avroSchema = AvroSchemaUtil.convert(tableSchema, \"test\");\n    this.records = testRecords(avroSchema);\n\n    switch (fileFormat) {\n      case AVRO:\n        try (FileAppender<Record> writer = Avro.write(localOutput(testFile))\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n\n      case PARQUET:\n        try (FileAppender<Record> writer = Parquet.write(localOutput(testFile))\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(records.size())\n        .withFileSizeInBytes(testFile.length())\n        .withPath(testFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n  }\n","realPath":"spark3/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":176,"status":"M"},{"authorDate":"2020-06-20 09:02:23","commitOrder":3,"curCode":"  public void writeUnpartitionedTable() throws IOException {\n    this.parent = temp.newFolder(\"TestFilteredScan\");\n    this.unpartitioned = new File(parent, \"unpartitioned\");\n    File dataFolder = new File(unpartitioned, \"data\");\n    Assert.assertTrue(\"Mkdir should succeed\", dataFolder.mkdirs());\n\n    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());\n    Schema tableSchema = table.schema(); \r\n\n    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n\n    File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));\n\n    \r\n    this.records = testRecords(tableSchema);\n\n    switch (fileFormat) {\n      case AVRO:\n        try (FileAppender<Record> writer = Avro.write(localOutput(testFile))\n            .createWriterFunc(DataWriter::create)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n\n      case PARQUET:\n        try (FileAppender<Record> writer = Parquet.write(localOutput(testFile))\n            .createWriterFunc(GenericParquetWriter::buildWriter)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n\n      case ORC:\n        try (FileAppender<Record> writer = ORC.write(localOutput(testFile))\n            .createWriterFunc(GenericOrcWriter::buildWriter)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(records.size())\n        .withFileSizeInBytes(testFile.length())\n        .withPath(testFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n  }\n","date":"2020-06-20 09:02:23","endLine":226,"groupId":"1657","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"writeUnpartitionedTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/c0/d676ec8aa99c9101f89d24d70e070468572cda.src","preCode":"  public void writeUnpartitionedTable() throws IOException {\n    this.parent = temp.newFolder(\"TestFilteredScan\");\n    this.unpartitioned = new File(parent, \"unpartitioned\");\n    File dataFolder = new File(unpartitioned, \"data\");\n    Assert.assertTrue(\"Mkdir should succeed\", dataFolder.mkdirs());\n\n    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());\n    Schema tableSchema = table.schema(); \r\n\n    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n\n    File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));\n\n    \r\n    this.records = testRecords(tableSchema);\n\n    switch (fileFormat) {\n      case AVRO:\n        try (FileAppender<Record> writer = Avro.write(localOutput(testFile))\n            .createWriterFunc(DataWriter::create)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n\n      case PARQUET:\n        try (FileAppender<Record> writer = Parquet.write(localOutput(testFile))\n            .createWriterFunc(GenericParquetWriter::buildWriter)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n\n      case ORC:\n        try (FileAppender<Record> writer = ORC.write(localOutput(testFile))\n            .createWriterFunc(GenericOrcWriter::buildWriter)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(records.size())\n        .withFileSizeInBytes(testFile.length())\n        .withPath(testFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":174,"status":"N"}],"commitId":"6fab8f57bdb7e5fe7eadc3ff41558581338e1b69","commitMessage":"@@@Spark: Support ORC vectorized reads (#1189)\n\n","date":"2020-07-14 05:27:36","modifiedFileCount":"25","status":"M","submitter":"Shardul Mahadik"},{"authorTime":"2020-08-20 08:23:59","codes":[{"authorDate":"2020-08-20 08:23:59","commitOrder":4,"curCode":"  public void writeUnpartitionedTable() throws IOException {\n    this.parent = temp.newFolder(\"TestFilteredScan\");\n    this.unpartitioned = new File(parent, \"unpartitioned\");\n    File dataFolder = new File(unpartitioned, \"data\");\n    Assert.assertTrue(\"Mkdir should succeed\", dataFolder.mkdirs());\n\n    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());\n    Schema tableSchema = table.schema(); \r\n\n    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n\n    File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));\n\n    this.records = testRecords(tableSchema);\n\n    try (FileAppender<Record> writer = new GenericAppenderFactory(tableSchema).newAppender(\n        localOutput(testFile), fileFormat)) {\n      writer.addAll(records);\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(records.size())\n        .withFileSizeInBytes(testFile.length())\n        .withPath(testFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n  }\n","date":"2020-08-20 08:23:59","endLine":198,"groupId":"10712","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"writeUnpartitionedTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/11/66f32c08acbf30148d752cebfa1816f366003a.src","preCode":"  public void writeUnpartitionedTable() throws IOException {\n    this.parent = temp.newFolder(\"TestFilteredScan\");\n    this.unpartitioned = new File(parent, \"unpartitioned\");\n    File dataFolder = new File(unpartitioned, \"data\");\n    Assert.assertTrue(\"Mkdir should succeed\", dataFolder.mkdirs());\n\n    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());\n    Schema tableSchema = table.schema(); \r\n\n    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n\n    File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));\n\n    this.records = testRecords(tableSchema);\n\n    switch (fileFormat) {\n      case AVRO:\n        try (FileAppender<Record> writer = Avro.write(localOutput(testFile))\n            .createWriterFunc(DataWriter::create)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n\n      case PARQUET:\n        try (FileAppender<Record> writer = Parquet.write(localOutput(testFile))\n            .createWriterFunc(GenericParquetWriter::buildWriter)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n\n      case ORC:\n        try (FileAppender<Record> writer = ORC.write(localOutput(testFile))\n            .createWriterFunc(GenericOrcWriter::buildWriter)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(records.size())\n        .withFileSizeInBytes(testFile.length())\n        .withPath(testFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n  }\n","realPath":"spark3/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":171,"status":"M"},{"authorDate":"2020-08-20 08:23:59","commitOrder":4,"curCode":"  public void writeUnpartitionedTable() throws IOException {\n    this.parent = temp.newFolder(\"TestFilteredScan\");\n    this.unpartitioned = new File(parent, \"unpartitioned\");\n    File dataFolder = new File(unpartitioned, \"data\");\n    Assert.assertTrue(\"Mkdir should succeed\", dataFolder.mkdirs());\n\n    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());\n    Schema tableSchema = table.schema(); \r\n\n    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n\n    File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));\n\n    \r\n    this.records = testRecords(tableSchema);\n\n    try (FileAppender<Record> writer = new GenericAppenderFactory(tableSchema).newAppender(\n        localOutput(testFile), fileFormat)) {\n      writer.addAll(records);\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(records.size())\n        .withFileSizeInBytes(testFile.length())\n        .withPath(testFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n  }\n","date":"2020-08-20 08:23:59","endLine":201,"groupId":"10712","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"writeUnpartitionedTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/7f/c7a98fd0775c19a41bd1102f88db997b809970.src","preCode":"  public void writeUnpartitionedTable() throws IOException {\n    this.parent = temp.newFolder(\"TestFilteredScan\");\n    this.unpartitioned = new File(parent, \"unpartitioned\");\n    File dataFolder = new File(unpartitioned, \"data\");\n    Assert.assertTrue(\"Mkdir should succeed\", dataFolder.mkdirs());\n\n    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());\n    Schema tableSchema = table.schema(); \r\n\n    FileFormat fileFormat = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n\n    File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));\n\n    \r\n    this.records = testRecords(tableSchema);\n\n    switch (fileFormat) {\n      case AVRO:\n        try (FileAppender<Record> writer = Avro.write(localOutput(testFile))\n            .createWriterFunc(DataWriter::create)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n\n      case PARQUET:\n        try (FileAppender<Record> writer = Parquet.write(localOutput(testFile))\n            .createWriterFunc(GenericParquetWriter::buildWriter)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n\n      case ORC:\n        try (FileAppender<Record> writer = ORC.write(localOutput(testFile))\n            .createWriterFunc(GenericOrcWriter::buildWriter)\n            .schema(tableSchema)\n            .build()) {\n          writer.addAll(records);\n        }\n        break;\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(records.size())\n        .withFileSizeInBytes(testFile.length())\n        .withPath(testFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":173,"status":"M"}],"commitId":"c66d0607011d7a87bb7f349b51fa0af78310beee","commitMessage":"@@@Data: Add GenericAppenderFactory and GenericAppenderHelper (#1340)\n\n","date":"2020-08-20 08:23:59","modifiedFileCount":"6","status":"M","submitter":"Jingsong Lee"}]
