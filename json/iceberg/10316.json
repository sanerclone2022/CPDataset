[{"authorTime":"2019-06-23 07:02:44","codes":[{"authorDate":"2019-06-23 07:02:44","commitOrder":1,"curCode":"  public void testStreamingWriteAppendMode() throws IOException {\n    File parent = temp.newFolder(\"parquet\");\n    File location = new File(parent, \"test-table\");\n    File checkpoint = new File(parent, \"checkpoint\");\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n    Table table = tables.create(SCHEMA, spec, location.toString());\n\n    List<SimpleRecord> expected = Lists.newArrayList(\n        new SimpleRecord(1, \"1\"),\n        new SimpleRecord(2, \"2\"),\n        new SimpleRecord(3, \"3\"),\n        new SimpleRecord(4, \"4\")\n    );\n\n    MemoryStream<Integer> inputStream = new MemoryStream<>(1, spark.sqlContext(), Encoders.INT());\n    DataStreamWriter<Row> streamWriter = inputStream.toDF()\n        .selectExpr(\"value AS id\", \"CAST (value AS STRING) AS data\")\n        .writeStream()\n        .outputMode(\"append\")\n        .format(\"iceberg\")\n        .option(\"checkpointLocation\", checkpoint.toString())\n        .option(\"path\", location.toString());\n\n    try {\n      \r\n      StreamingQuery query = streamWriter.start();\n      List<Integer> batch1 = Lists.newArrayList(1, 2);\n      inputStream.addData(JavaConversions.asScalaBuffer(batch1));\n      query.processAllAvailable();\n      List<Integer> batch2 = Lists.newArrayList(3, 4);\n      inputStream.addData(JavaConversions.asScalaBuffer(batch2));\n      query.processAllAvailable();\n      query.stop();\n\n      \r\n      File lastCommitFile = new File(checkpoint.toString() + \"/commits/1\");\n      Assert.assertTrue(\"The commit file must be deleted\", lastCommitFile.delete());\n\n      \r\n      StreamingQuery restartedQuery = streamWriter.start();\n      restartedQuery.processAllAvailable();\n\n      \r\n      Dataset<Row> result = spark.read()\n          .format(\"iceberg\")\n          .load(location.toString());\n      List<SimpleRecord> actual = result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n      Assert.assertEquals(\"Result rows should match\", expected, actual);\n      Assert.assertEquals(\"Number of snapshots should match\", 2, Iterables.size(table.snapshots()));\n    } finally {\n      for (StreamingQuery query : spark.streams().active()) {\n        query.stop();\n      }\n    }\n  }\n","date":"2019-06-23 07:02:44","endLine":139,"groupId":"3180","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testStreamingWriteAppendMode","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/cc/d5904856ae8445f08940b6fe1c22dbb988d34f.src","preCode":"  public void testStreamingWriteAppendMode() throws IOException {\n    File parent = temp.newFolder(\"parquet\");\n    File location = new File(parent, \"test-table\");\n    File checkpoint = new File(parent, \"checkpoint\");\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n    Table table = tables.create(SCHEMA, spec, location.toString());\n\n    List<SimpleRecord> expected = Lists.newArrayList(\n        new SimpleRecord(1, \"1\"),\n        new SimpleRecord(2, \"2\"),\n        new SimpleRecord(3, \"3\"),\n        new SimpleRecord(4, \"4\")\n    );\n\n    MemoryStream<Integer> inputStream = new MemoryStream<>(1, spark.sqlContext(), Encoders.INT());\n    DataStreamWriter<Row> streamWriter = inputStream.toDF()\n        .selectExpr(\"value AS id\", \"CAST (value AS STRING) AS data\")\n        .writeStream()\n        .outputMode(\"append\")\n        .format(\"iceberg\")\n        .option(\"checkpointLocation\", checkpoint.toString())\n        .option(\"path\", location.toString());\n\n    try {\n      \r\n      StreamingQuery query = streamWriter.start();\n      List<Integer> batch1 = Lists.newArrayList(1, 2);\n      inputStream.addData(JavaConversions.asScalaBuffer(batch1));\n      query.processAllAvailable();\n      List<Integer> batch2 = Lists.newArrayList(3, 4);\n      inputStream.addData(JavaConversions.asScalaBuffer(batch2));\n      query.processAllAvailable();\n      query.stop();\n\n      \r\n      File lastCommitFile = new File(checkpoint.toString() + \"/commits/1\");\n      Assert.assertTrue(\"The commit file must be deleted\", lastCommitFile.delete());\n\n      \r\n      StreamingQuery restartedQuery = streamWriter.start();\n      restartedQuery.processAllAvailable();\n\n      \r\n      Dataset<Row> result = spark.read()\n          .format(\"iceberg\")\n          .load(location.toString());\n      List<SimpleRecord> actual = result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n      Assert.assertEquals(\"Result rows should match\", expected, actual);\n      Assert.assertEquals(\"Number of snapshots should match\", 2, Iterables.size(table.snapshots()));\n    } finally {\n      for (StreamingQuery query : spark.streams().active()) {\n        query.stop();\n      }\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":82,"status":"B"},{"authorDate":"2019-06-23 07:02:44","commitOrder":1,"curCode":"  public void testStreamingWriteUpdateMode() throws IOException {\n    exceptionRule.expect(StreamingQueryException.class);\n    exceptionRule.expectMessage(\"Output mode Update is not supported\");\n\n    File parent = temp.newFolder(\"parquet\");\n    File location = new File(parent, \"test-table\");\n    File checkpoint = new File(parent, \"checkpoint\");\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n    tables.create(SCHEMA, spec, location.toString());\n\n    MemoryStream<Integer> inputStream = new MemoryStream<>(1, spark.sqlContext(), Encoders.INT());\n    DataStreamWriter<Row> streamWriter = inputStream.toDF()\n        .selectExpr(\"value AS id\", \"CAST (value AS STRING) AS data\")\n        .writeStream()\n        .outputMode(\"update\")\n        .format(\"iceberg\")\n        .option(\"checkpointLocation\", checkpoint.toString())\n        .option(\"path\", location.toString());\n\n    try {\n      StreamingQuery query = streamWriter.start();\n      List<Integer> batch1 = Lists.newArrayList(1, 2);\n      inputStream.addData(JavaConversions.asScalaBuffer(batch1));\n      query.processAllAvailable();\n    } finally {\n      for (StreamingQuery query : spark.streams().active()) {\n        query.stop();\n      }\n    }\n  }\n","date":"2019-06-23 07:02:44","endLine":234,"groupId":"3184","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testStreamingWriteUpdateMode","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/cc/d5904856ae8445f08940b6fe1c22dbb988d34f.src","preCode":"  public void testStreamingWriteUpdateMode() throws IOException {\n    exceptionRule.expect(StreamingQueryException.class);\n    exceptionRule.expectMessage(\"Output mode Update is not supported\");\n\n    File parent = temp.newFolder(\"parquet\");\n    File location = new File(parent, \"test-table\");\n    File checkpoint = new File(parent, \"checkpoint\");\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n    tables.create(SCHEMA, spec, location.toString());\n\n    MemoryStream<Integer> inputStream = new MemoryStream<>(1, spark.sqlContext(), Encoders.INT());\n    DataStreamWriter<Row> streamWriter = inputStream.toDF()\n        .selectExpr(\"value AS id\", \"CAST (value AS STRING) AS data\")\n        .writeStream()\n        .outputMode(\"update\")\n        .format(\"iceberg\")\n        .option(\"checkpointLocation\", checkpoint.toString())\n        .option(\"path\", location.toString());\n\n    try {\n      StreamingQuery query = streamWriter.start();\n      List<Integer> batch1 = Lists.newArrayList(1, 2);\n      inputStream.addData(JavaConversions.asScalaBuffer(batch1));\n      query.processAllAvailable();\n    } finally {\n      for (StreamingQuery query : spark.streams().active()) {\n        query.stop();\n      }\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":203,"status":"B"}],"commitId":"57e9bd09b0d3ff0d67d320f232946d150b877a41","commitMessage":"@@@Spark: Support structured streaming writes (#228)\n\n","date":"2019-06-23 07:02:44","modifiedFileCount":"2","status":"B","submitter":"Anton Okolnychyi"},{"authorTime":"2020-06-30 08:56:05","codes":[{"authorDate":"2020-06-30 08:56:05","commitOrder":2,"curCode":"  public void testStreamingWriteAppendMode() throws Exception {\n    File parent = temp.newFolder(\"parquet\");\n    File location = new File(parent, \"test-table\");\n    File checkpoint = new File(parent, \"checkpoint\");\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n    Table table = tables.create(SCHEMA, spec, location.toString());\n\n    List<SimpleRecord> expected = Lists.newArrayList(\n        new SimpleRecord(1, \"1\"),\n        new SimpleRecord(2, \"2\"),\n        new SimpleRecord(3, \"3\"),\n        new SimpleRecord(4, \"4\")\n    );\n\n    MemoryStream<Integer> inputStream = newMemoryStream(1, spark.sqlContext(), Encoders.INT());\n    DataStreamWriter<Row> streamWriter = inputStream.toDF()\n        .selectExpr(\"value AS id\", \"CAST (value AS STRING) AS data\")\n        .writeStream()\n        .outputMode(\"append\")\n        .format(\"iceberg\")\n        .option(\"checkpointLocation\", checkpoint.toString())\n        .option(\"path\", location.toString());\n\n    try {\n      \r\n      StreamingQuery query = streamWriter.start();\n      List<Integer> batch1 = Lists.newArrayList(1, 2);\n      send(batch1, inputStream);\n      query.processAllAvailable();\n      List<Integer> batch2 = Lists.newArrayList(3, 4);\n      send(batch2, inputStream);\n      query.processAllAvailable();\n      query.stop();\n\n      \r\n      File lastCommitFile = new File(checkpoint.toString() + \"/commits/1\");\n      Assert.assertTrue(\"The commit file must be deleted\", lastCommitFile.delete());\n\n      \r\n      StreamingQuery restartedQuery = streamWriter.start();\n      restartedQuery.processAllAvailable();\n\n      \r\n      Dataset<Row> result = spark.read()\n          .format(\"iceberg\")\n          .load(location.toString());\n      List<SimpleRecord> actual = result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n      Assert.assertEquals(\"Result rows should match\", expected, actual);\n      Assert.assertEquals(\"Number of snapshots should match\", 2, Iterables.size(table.snapshots()));\n    } finally {\n      for (StreamingQuery query : spark.streams().active()) {\n        query.stop();\n      }\n    }\n  }\n","date":"2020-06-30 08:56:05","endLine":143,"groupId":"10316","id":3,"instanceNumber":1,"isCurCommit":1,"methodName":"testStreamingWriteAppendMode","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a6/4145d78f875b4ee3c69330059a96e2033b6101.src","preCode":"  public void testStreamingWriteAppendMode() throws IOException {\n    File parent = temp.newFolder(\"parquet\");\n    File location = new File(parent, \"test-table\");\n    File checkpoint = new File(parent, \"checkpoint\");\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n    Table table = tables.create(SCHEMA, spec, location.toString());\n\n    List<SimpleRecord> expected = Lists.newArrayList(\n        new SimpleRecord(1, \"1\"),\n        new SimpleRecord(2, \"2\"),\n        new SimpleRecord(3, \"3\"),\n        new SimpleRecord(4, \"4\")\n    );\n\n    MemoryStream<Integer> inputStream = new MemoryStream<>(1, spark.sqlContext(), Encoders.INT());\n    DataStreamWriter<Row> streamWriter = inputStream.toDF()\n        .selectExpr(\"value AS id\", \"CAST (value AS STRING) AS data\")\n        .writeStream()\n        .outputMode(\"append\")\n        .format(\"iceberg\")\n        .option(\"checkpointLocation\", checkpoint.toString())\n        .option(\"path\", location.toString());\n\n    try {\n      \r\n      StreamingQuery query = streamWriter.start();\n      List<Integer> batch1 = Lists.newArrayList(1, 2);\n      inputStream.addData(JavaConversions.asScalaBuffer(batch1));\n      query.processAllAvailable();\n      List<Integer> batch2 = Lists.newArrayList(3, 4);\n      inputStream.addData(JavaConversions.asScalaBuffer(batch2));\n      query.processAllAvailable();\n      query.stop();\n\n      \r\n      File lastCommitFile = new File(checkpoint.toString() + \"/commits/1\");\n      Assert.assertTrue(\"The commit file must be deleted\", lastCommitFile.delete());\n\n      \r\n      StreamingQuery restartedQuery = streamWriter.start();\n      restartedQuery.processAllAvailable();\n\n      \r\n      Dataset<Row> result = spark.read()\n          .format(\"iceberg\")\n          .load(location.toString());\n      List<SimpleRecord> actual = result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n      Assert.assertEquals(\"Result rows should match\", expected, actual);\n      Assert.assertEquals(\"Number of snapshots should match\", 2, Iterables.size(table.snapshots()));\n    } finally {\n      for (StreamingQuery query : spark.streams().active()) {\n        query.stop();\n      }\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"M"},{"authorDate":"2020-06-30 08:56:05","commitOrder":2,"curCode":"  public void testStreamingWriteUpdateMode() throws Exception {\n    exceptionRule.expect(StreamingQueryException.class);\n\n    File parent = temp.newFolder(\"parquet\");\n    File location = new File(parent, \"test-table\");\n    File checkpoint = new File(parent, \"checkpoint\");\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n    tables.create(SCHEMA, spec, location.toString());\n\n    MemoryStream<Integer> inputStream = newMemoryStream(1, spark.sqlContext(), Encoders.INT());\n    DataStreamWriter<Row> streamWriter = inputStream.toDF()\n        .selectExpr(\"value AS id\", \"CAST (value AS STRING) AS data\")\n        .writeStream()\n        .outputMode(\"update\")\n        .format(\"iceberg\")\n        .option(\"checkpointLocation\", checkpoint.toString())\n        .option(\"path\", location.toString());\n\n    try {\n      StreamingQuery query = streamWriter.start();\n      List<Integer> batch1 = Lists.newArrayList(1, 2);\n      send(batch1, inputStream);\n      query.processAllAvailable();\n    } finally {\n      for (StreamingQuery query : spark.streams().active()) {\n        query.stop();\n      }\n    }\n  }\n","date":"2020-06-30 08:56:05","endLine":298,"groupId":"10316","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"testStreamingWriteUpdateMode","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a6/4145d78f875b4ee3c69330059a96e2033b6101.src","preCode":"  public void testStreamingWriteUpdateMode() throws IOException {\n    exceptionRule.expect(StreamingQueryException.class);\n    exceptionRule.expectMessage(\"Output mode Update is not supported\");\n\n    File parent = temp.newFolder(\"parquet\");\n    File location = new File(parent, \"test-table\");\n    File checkpoint = new File(parent, \"checkpoint\");\n\n    HadoopTables tables = new HadoopTables(CONF);\n    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n    tables.create(SCHEMA, spec, location.toString());\n\n    MemoryStream<Integer> inputStream = new MemoryStream<>(1, spark.sqlContext(), Encoders.INT());\n    DataStreamWriter<Row> streamWriter = inputStream.toDF()\n        .selectExpr(\"value AS id\", \"CAST (value AS STRING) AS data\")\n        .writeStream()\n        .outputMode(\"update\")\n        .format(\"iceberg\")\n        .option(\"checkpointLocation\", checkpoint.toString())\n        .option(\"path\", location.toString());\n\n    try {\n      StreamingQuery query = streamWriter.start();\n      List<Integer> batch1 = Lists.newArrayList(1, 2);\n      inputStream.addData(JavaConversions.asScalaBuffer(batch1));\n      query.processAllAvailable();\n    } finally {\n      for (StreamingQuery query : spark.streams().active()) {\n        query.stop();\n      }\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":268,"status":"M"}],"commitId":"51c930e33867e54b3d7e0159b11d6b9c4bc81f1a","commitMessage":"@@@Spark: Add Spark 3 data source classes (#1124)\n\n","date":"2020-06-30 08:56:05","modifiedFileCount":"7","status":"M","submitter":"Ryan Blue"}]
