[{"authorTime":"2020-12-04 18:09:09","codes":[{"authorDate":"2020-10-06 01:02:02","commitOrder":3,"curCode":"  public static void startMetastoreAndSpark() {\n    metastore = new TestHiveMetastore();\n    metastore.start();\n    HiveConf hiveConf = metastore.hiveConf();\n\n    spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n\n    try {\n      catalog.createNamespace(Namespace.of(\"default\"));\n    } catch (AlreadyExistsException ignored) {\n      \r\n    }\n  }\n","date":"2020-10-06 01:02:02","endLine":73,"groupId":"742","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"startMetastoreAndSpark","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/22/9a361f69242b924cbd7c362780061506092eee.src","preCode":"  public static void startMetastoreAndSpark() {\n    metastore = new TestHiveMetastore();\n    metastore.start();\n    HiveConf hiveConf = metastore.hiveConf();\n\n    spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n\n    try {\n      catalog.createNamespace(Namespace.of(\"default\"));\n    } catch (AlreadyExistsException ignored) {\n      \r\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":54,"status":"NB"},{"authorDate":"2020-12-04 18:09:09","commitOrder":3,"curCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(\"spark.testing\", \"true\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n  }\n","date":"2020-12-04 18:09:09","endLine":55,"groupId":"3520","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"startMetastoreAndSpark","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/34/2f95127906d73f8b5c4fdf4600cb418c02f498.src","preCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(\"spark.testing\", \"true\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n  }\n","realPath":"spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkExtensionsTestBase.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"MB"}],"commitId":"bfddabbf876660ab4369495c5c321e4cfd19faac","commitMessage":"@@@Spark: Add RemoveOrphanFilesProcedure (#1869)\n\nFixes #1599.\n\nLead-authored-by: Kun Liu <liukun@apache.org>\nCo-authored-by: Anton Okolnychyi <aokolnychyi@apple.com>","date":"2020-12-04 18:09:09","modifiedFileCount":"2","status":"M","submitter":"Anton Okolnychyi"},{"authorTime":"2020-12-10 21:40:06","codes":[{"authorDate":"2020-10-06 01:02:02","commitOrder":4,"curCode":"  public static void startMetastoreAndSpark() {\n    metastore = new TestHiveMetastore();\n    metastore.start();\n    HiveConf hiveConf = metastore.hiveConf();\n\n    spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n\n    try {\n      catalog.createNamespace(Namespace.of(\"default\"));\n    } catch (AlreadyExistsException ignored) {\n      \r\n    }\n  }\n","date":"2020-10-06 01:02:02","endLine":73,"groupId":"742","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"startMetastoreAndSpark","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/22/9a361f69242b924cbd7c362780061506092eee.src","preCode":"  public static void startMetastoreAndSpark() {\n    metastore = new TestHiveMetastore();\n    metastore.start();\n    HiveConf hiveConf = metastore.hiveConf();\n\n    spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n\n    try {\n      catalog.createNamespace(Namespace.of(\"default\"));\n    } catch (AlreadyExistsException ignored) {\n      \r\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":54,"status":"N"},{"authorDate":"2020-12-10 21:40:06","commitOrder":4,"curCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(\"spark.testing\", \"true\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .config(\"spark.sql.shuffle.partitions\", \"4\")\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n  }\n","date":"2020-12-10 21:40:06","endLine":56,"groupId":"3520","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"startMetastoreAndSpark","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/83/c104c2281debce35815c3004d5baf568513b09.src","preCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(\"spark.testing\", \"true\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n  }\n","realPath":"spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkExtensionsTestBase.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"M"}],"commitId":"38a48a52dd6f4affd26c173aafedab8d3959ad5c","commitMessage":"@@@Spark: Speed up tests for extensions (#1903)\n\n","date":"2020-12-10 21:40:06","modifiedFileCount":"1","status":"M","submitter":"Anton Okolnychyi"},{"authorTime":"2021-03-12 13:53:17","codes":[{"authorDate":"2021-03-12 13:53:17","commitOrder":5,"curCode":"  public static void startMetastoreAndSpark() {\n    metastore = new TestHiveMetastore();\n    metastore.start();\n    HiveConf hiveConf = metastore.hiveConf();\n\n    spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    catalog = (HiveCatalog)\n        CatalogUtil.loadCatalog(HiveCatalog.class.getName(), \"hive\", ImmutableMap.of(), hiveConf);\n\n    try {\n      catalog.createNamespace(Namespace.of(\"default\"));\n    } catch (AlreadyExistsException ignored) {\n      \r\n    }\n  }\n","date":"2021-03-12 13:53:17","endLine":87,"groupId":"742","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"startMetastoreAndSpark","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/34/070e1d9fcb78924c8db41ddb9e0ccba65dbedc.src","preCode":"  public static void startMetastoreAndSpark() {\n    metastore = new TestHiveMetastore();\n    metastore.start();\n    HiveConf hiveConf = metastore.hiveConf();\n\n    spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n\n    try {\n      catalog.createNamespace(Namespace.of(\"default\"));\n    } catch (AlreadyExistsException ignored) {\n      \r\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":67,"status":"M"},{"authorDate":"2021-03-12 13:53:17","commitOrder":5,"curCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(\"spark.testing\", \"true\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .config(\"spark.sql.shuffle.partitions\", \"4\")\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = (HiveCatalog)\n        CatalogUtil.loadCatalog(HiveCatalog.class.getName(), \"hive\", ImmutableMap.of(), hiveConf);\n  }\n","date":"2021-03-12 13:53:17","endLine":59,"groupId":"3520","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"startMetastoreAndSpark","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/d5/6c1dd60c08b4afc09d89ac194561b891755289.src","preCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(\"spark.testing\", \"true\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .config(\"spark.sql.shuffle.partitions\", \"4\")\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = new HiveCatalog(spark.sessionState().newHadoopConf());\n  }\n","realPath":"spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkExtensionsTestBase.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"M"}],"commitId":"c8b74c19b831ecbbf33f78094b5c8098c46eddec","commitMessage":"@@@Hive: Refactor HiveCatalog and HiveClientPool constructors (#2203)\n\n","date":"2021-03-12 13:53:17","modifiedFileCount":"11","status":"M","submitter":"Ryan Murray"},{"authorTime":"2021-09-10 06:01:28","codes":[{"authorDate":"2021-03-12 13:53:17","commitOrder":6,"curCode":"  public static void startMetastoreAndSpark() {\n    metastore = new TestHiveMetastore();\n    metastore.start();\n    HiveConf hiveConf = metastore.hiveConf();\n\n    spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    catalog = (HiveCatalog)\n        CatalogUtil.loadCatalog(HiveCatalog.class.getName(), \"hive\", ImmutableMap.of(), hiveConf);\n\n    try {\n      catalog.createNamespace(Namespace.of(\"default\"));\n    } catch (AlreadyExistsException ignored) {\n      \r\n    }\n  }\n","date":"2021-03-12 13:53:17","endLine":87,"groupId":"10360","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"startMetastoreAndSpark","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/34/070e1d9fcb78924c8db41ddb9e0ccba65dbedc.src","preCode":"  public static void startMetastoreAndSpark() {\n    metastore = new TestHiveMetastore();\n    metastore.start();\n    HiveConf hiveConf = metastore.hiveConf();\n\n    spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .enableHiveSupport()\n        .getOrCreate();\n\n    catalog = (HiveCatalog)\n        CatalogUtil.loadCatalog(HiveCatalog.class.getName(), \"hive\", ImmutableMap.of(), hiveConf);\n\n    try {\n      catalog.createNamespace(Namespace.of(\"default\"));\n    } catch (AlreadyExistsException ignored) {\n      \r\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":67,"status":"N"},{"authorDate":"2021-09-10 06:01:28","commitOrder":6,"curCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(\"spark.testing\", \"true\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .config(\"spark.sql.shuffle.partitions\", \"4\")\n        .config(\"spark.sql.hive.metastorePartitionPruningFallbackOnException\", \"true\")\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = (HiveCatalog)\n        CatalogUtil.loadCatalog(HiveCatalog.class.getName(), \"hive\", ImmutableMap.of(), hiveConf);\n  }\n","date":"2021-09-10 06:01:28","endLine":60,"groupId":"10360","id":8,"instanceNumber":2,"isCurCommit":1,"methodName":"startMetastoreAndSpark","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/36/ca608ccd3bc4a6e8eaf377c9125348d9d71770.src","preCode":"  public static void startMetastoreAndSpark() {\n    SparkTestBase.metastore = new TestHiveMetastore();\n    metastore.start();\n    SparkTestBase.hiveConf = metastore.hiveConf();\n\n    SparkTestBase.spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .config(\"spark.testing\", \"true\")\n        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n        .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n        .config(\"spark.sql.shuffle.partitions\", \"4\")\n        .enableHiveSupport()\n        .getOrCreate();\n\n    SparkTestBase.catalog = (HiveCatalog)\n        CatalogUtil.loadCatalog(HiveCatalog.class.getName(), \"hive\", ImmutableMap.of(), hiveConf);\n  }\n","realPath":"spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkExtensionsTestBase.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"M"}],"commitId":"af4bde667ff2ed9c2a32a313ed00f51de580dfc1","commitMessage":"@@@Spark: Add config needed in tests after SPARK-36128 (#3090)\n\n","date":"2021-09-10 06:01:28","modifiedFileCount":"1","status":"M","submitter":"Kyle Bendickson"}]
