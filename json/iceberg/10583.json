[{"authorTime":"2020-05-26 01:15:35","codes":[{"authorDate":"2020-05-26 01:15:35","commitOrder":1,"curCode":"  private void createParquetInputFile() throws IOException {\n    if (parquetFile.exists()) {\n      Assert.assertTrue(parquetFile.delete());\n    }\n\n    \r\n    org.apache.avro.Schema structSchema = AvroSchemaUtil.convert(_structFieldType);\n\n    OutputFile outFile = Files.localOutput(parquetFile);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .build()) {\n      GenericRecordBuilder builder = new GenericRecordBuilder(convert(FILE_SCHEMA, \"table\"));\n      \r\n      for (int i = 0; i < INT_MAX_VALUE - INT_MIN_VALUE + 1; i += 1) {\n        builder.set(\"_id\", INT_MIN_VALUE + i); \r\n        builder.set(\"_no_stats_parquet\", TOO_LONG_FOR_STATS_PARQUET); \r\n                                                                      \r\n        builder.set(\"_required\", \"req\"); \r\n        builder.set(\"_all_nulls\", null); \r\n        builder.set(\"_some_nulls\", (i % 10 == 0) ? null : \"some\"); \r\n        builder.set(\"_no_nulls\", \"\"); \r\n        builder.set(\"_str\", i + \"str\" + i);\n\n        Record structNotNull = new Record(structSchema);\n        structNotNull.put(\"_int_field\", INT_MIN_VALUE + i);\n        builder.set(\"_struct_not_null\", structNotNull); \r\n\n        appender.add(builder.build());\n      }\n    }\n\n    InputFile inFile = Files.localInput(parquetFile);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    parquetFile.deleteOnExit();\n  }\n","date":"2020-05-26 01:15:35","endLine":249,"groupId":"706","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"createParquetInputFile","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/8f/a4caee610f5c2e6dd4a158f7b405c730067ff0.src","preCode":"  private void createParquetInputFile() throws IOException {\n    if (parquetFile.exists()) {\n      Assert.assertTrue(parquetFile.delete());\n    }\n\n    \r\n    org.apache.avro.Schema structSchema = AvroSchemaUtil.convert(_structFieldType);\n\n    OutputFile outFile = Files.localOutput(parquetFile);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .build()) {\n      GenericRecordBuilder builder = new GenericRecordBuilder(convert(FILE_SCHEMA, \"table\"));\n      \r\n      for (int i = 0; i < INT_MAX_VALUE - INT_MIN_VALUE + 1; i += 1) {\n        builder.set(\"_id\", INT_MIN_VALUE + i); \r\n        builder.set(\"_no_stats_parquet\", TOO_LONG_FOR_STATS_PARQUET); \r\n                                                                      \r\n        builder.set(\"_required\", \"req\"); \r\n        builder.set(\"_all_nulls\", null); \r\n        builder.set(\"_some_nulls\", (i % 10 == 0) ? null : \"some\"); \r\n        builder.set(\"_no_nulls\", \"\"); \r\n        builder.set(\"_str\", i + \"str\" + i);\n\n        Record structNotNull = new Record(structSchema);\n        structNotNull.put(\"_int_field\", INT_MIN_VALUE + i);\n        builder.set(\"_struct_not_null\", structNotNull); \r\n\n        appender.add(builder.build());\n      }\n    }\n\n    InputFile inFile = Files.localInput(parquetFile);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    parquetFile.deleteOnExit();\n  }\n","realPath":"data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":209,"status":"B"},{"authorDate":"2020-05-26 01:15:35","commitOrder":1,"curCode":"  public void createParquetInputFile(List<Record> records) throws IOException {\n    if (PARQUET_FILE.exists()) {\n      Assert.assertTrue(PARQUET_FILE.delete());\n    }\n\n    OutputFile outFile = Files.localOutput(PARQUET_FILE);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .createWriterFunc(GenericParquetWriter::buildWriter)\n        .build()) {\n      appender.addAll(records);\n    }\n\n    InputFile inFile = Files.localInput(PARQUET_FILE);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    PARQUET_FILE.deleteOnExit();\n  }\n","date":"2020-05-26 01:15:35","endLine":218,"groupId":"3302","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"createParquetInputFile","params":"(List<Record>records)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/86/47a9c7dee02b6ce46ecee97a74de7421cd16f4.src","preCode":"  public void createParquetInputFile(List<Record> records) throws IOException {\n    if (PARQUET_FILE.exists()) {\n      Assert.assertTrue(PARQUET_FILE.delete());\n    }\n\n    OutputFile outFile = Files.localOutput(PARQUET_FILE);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .createWriterFunc(GenericParquetWriter::buildWriter)\n        .build()) {\n      appender.addAll(records);\n    }\n\n    InputFile inFile = Files.localInput(PARQUET_FILE);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    PARQUET_FILE.deleteOnExit();\n  }\n","realPath":"data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":197,"status":"B"}],"commitId":"e8f7379ffe253623e9dd27a1ada7d4421af8b937","commitMessage":"@@@ORC: Push down Iceberg filters (#973)\n\n","date":"2020-05-26 01:15:35","modifiedFileCount":"6","status":"B","submitter":"Shardul Mahadik"},{"authorTime":"2020-05-26 01:15:35","codes":[{"authorDate":"2020-10-09 23:46:12","commitOrder":2,"curCode":"  private void createParquetInputFile() throws IOException {\n    File parquetFile = temp.newFile();\n    Assert.assertTrue(parquetFile.delete());\n\n    \r\n    org.apache.avro.Schema structSchema = AvroSchemaUtil.convert(_structFieldType);\n\n    OutputFile outFile = Files.localOutput(parquetFile);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .build()) {\n      GenericRecordBuilder builder = new GenericRecordBuilder(convert(FILE_SCHEMA, \"table\"));\n      \r\n      for (int i = 0; i < INT_MAX_VALUE - INT_MIN_VALUE + 1; i += 1) {\n        builder.set(\"_id\", INT_MIN_VALUE + i); \r\n        builder.set(\"_no_stats_parquet\", TOO_LONG_FOR_STATS_PARQUET); \r\n                                                                      \r\n        builder.set(\"_required\", \"req\"); \r\n        builder.set(\"_all_nulls\", null); \r\n        builder.set(\"_some_nulls\", (i % 10 == 0) ? null : \"some\"); \r\n        builder.set(\"_no_nulls\", \"\"); \r\n        builder.set(\"_str\", i + \"str\" + i);\n\n        Record structNotNull = new Record(structSchema);\n        structNotNull.put(\"_int_field\", INT_MIN_VALUE + i);\n        builder.set(\"_struct_not_null\", structNotNull); \r\n\n        appender.add(builder.build());\n      }\n    }\n\n    InputFile inFile = Files.localInput(parquetFile);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    parquetFile.deleteOnExit();\n  }\n","date":"2020-10-09 23:46:12","endLine":243,"groupId":"706","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"createParquetInputFile","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/d4/13f476504abbe1ede3d3771a911de415ceaa6c.src","preCode":"  private void createParquetInputFile() throws IOException {\n    if (parquetFile.exists()) {\n      Assert.assertTrue(parquetFile.delete());\n    }\n\n    \r\n    org.apache.avro.Schema structSchema = AvroSchemaUtil.convert(_structFieldType);\n\n    OutputFile outFile = Files.localOutput(parquetFile);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .build()) {\n      GenericRecordBuilder builder = new GenericRecordBuilder(convert(FILE_SCHEMA, \"table\"));\n      \r\n      for (int i = 0; i < INT_MAX_VALUE - INT_MIN_VALUE + 1; i += 1) {\n        builder.set(\"_id\", INT_MIN_VALUE + i); \r\n        builder.set(\"_no_stats_parquet\", TOO_LONG_FOR_STATS_PARQUET); \r\n                                                                      \r\n        builder.set(\"_required\", \"req\"); \r\n        builder.set(\"_all_nulls\", null); \r\n        builder.set(\"_some_nulls\", (i % 10 == 0) ? null : \"some\"); \r\n        builder.set(\"_no_nulls\", \"\"); \r\n        builder.set(\"_str\", i + \"str\" + i);\n\n        Record structNotNull = new Record(structSchema);\n        structNotNull.put(\"_int_field\", INT_MIN_VALUE + i);\n        builder.set(\"_struct_not_null\", structNotNull); \r\n\n        appender.add(builder.build());\n      }\n    }\n\n    InputFile inFile = Files.localInput(parquetFile);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    parquetFile.deleteOnExit();\n  }\n","realPath":"data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":204,"status":"M"},{"authorDate":"2020-05-26 01:15:35","commitOrder":2,"curCode":"  public void createParquetInputFile(List<Record> records) throws IOException {\n    if (PARQUET_FILE.exists()) {\n      Assert.assertTrue(PARQUET_FILE.delete());\n    }\n\n    OutputFile outFile = Files.localOutput(PARQUET_FILE);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .createWriterFunc(GenericParquetWriter::buildWriter)\n        .build()) {\n      appender.addAll(records);\n    }\n\n    InputFile inFile = Files.localInput(PARQUET_FILE);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    PARQUET_FILE.deleteOnExit();\n  }\n","date":"2020-05-26 01:15:35","endLine":218,"groupId":"3302","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"createParquetInputFile","params":"(List<Record>records)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/86/47a9c7dee02b6ce46ecee97a74de7421cd16f4.src","preCode":"  public void createParquetInputFile(List<Record> records) throws IOException {\n    if (PARQUET_FILE.exists()) {\n      Assert.assertTrue(PARQUET_FILE.delete());\n    }\n\n    OutputFile outFile = Files.localOutput(PARQUET_FILE);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .createWriterFunc(GenericParquetWriter::buildWriter)\n        .build()) {\n      appender.addAll(records);\n    }\n\n    InputFile inFile = Files.localInput(PARQUET_FILE);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    PARQUET_FILE.deleteOnExit();\n  }\n","realPath":"data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":197,"status":"N"}],"commitId":"29c245471b9b9244e944953a2d6e556a347573ad","commitMessage":"@@@Parquet: Remove hard-coded file paths from tests (#1562)\n\n* Remove hard-coded file paths from tests.\n\n* Fix checkstyle in tests.","date":"2020-10-09 23:46:12","modifiedFileCount":"2","status":"M","submitter":"Ryan Blue"},{"authorTime":"2020-05-26 01:15:35","codes":[{"authorDate":"2020-12-06 09:14:27","commitOrder":3,"curCode":"  private void createParquetInputFile() throws IOException {\n    File parquetFile = temp.newFile();\n    Assert.assertTrue(parquetFile.delete());\n\n    \r\n    org.apache.avro.Schema structSchema = AvroSchemaUtil.convert(_structFieldType);\n\n    OutputFile outFile = Files.localOutput(parquetFile);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .build()) {\n      GenericRecordBuilder builder = new GenericRecordBuilder(convert(FILE_SCHEMA, \"table\"));\n      \r\n      for (int i = 0; i < INT_MAX_VALUE - INT_MIN_VALUE + 1; i += 1) {\n        builder.set(\"_id\", INT_MIN_VALUE + i); \r\n        builder.set(\"_no_stats_parquet\", TOO_LONG_FOR_STATS_PARQUET); \r\n                                                                      \r\n        builder.set(\"_required\", \"req\"); \r\n        builder.set(\"_all_nulls\", null); \r\n        builder.set(\"_some_nulls\", (i % 10 == 0) ? null : \"some\"); \r\n        builder.set(\"_no_nulls\", \"\"); \r\n        builder.set(\"_all_nans\", Double.NaN); \r\n        builder.set(\"_some_nans\", (i % 10 == 0) ? Float.NaN : 2F); \r\n        builder.set(\"_no_nans\", 3D); \r\n        builder.set(\"_str\", i + \"str\" + i);\n\n        Record structNotNull = new Record(structSchema);\n        structNotNull.put(\"_int_field\", INT_MIN_VALUE + i);\n        builder.set(\"_struct_not_null\", structNotNull); \r\n\n        appender.add(builder.build());\n      }\n    }\n\n    InputFile inFile = Files.localInput(parquetFile);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    parquetFile.deleteOnExit();\n  }\n","date":"2020-12-06 09:14:27","endLine":258,"groupId":"10583","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"createParquetInputFile","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/b8/2d6eb663d5a242a3e36e0413c2888e180e76d6.src","preCode":"  private void createParquetInputFile() throws IOException {\n    File parquetFile = temp.newFile();\n    Assert.assertTrue(parquetFile.delete());\n\n    \r\n    org.apache.avro.Schema structSchema = AvroSchemaUtil.convert(_structFieldType);\n\n    OutputFile outFile = Files.localOutput(parquetFile);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .build()) {\n      GenericRecordBuilder builder = new GenericRecordBuilder(convert(FILE_SCHEMA, \"table\"));\n      \r\n      for (int i = 0; i < INT_MAX_VALUE - INT_MIN_VALUE + 1; i += 1) {\n        builder.set(\"_id\", INT_MIN_VALUE + i); \r\n        builder.set(\"_no_stats_parquet\", TOO_LONG_FOR_STATS_PARQUET); \r\n                                                                      \r\n        builder.set(\"_required\", \"req\"); \r\n        builder.set(\"_all_nulls\", null); \r\n        builder.set(\"_some_nulls\", (i % 10 == 0) ? null : \"some\"); \r\n        builder.set(\"_no_nulls\", \"\"); \r\n        builder.set(\"_str\", i + \"str\" + i);\n\n        Record structNotNull = new Record(structSchema);\n        structNotNull.put(\"_int_field\", INT_MIN_VALUE + i);\n        builder.set(\"_struct_not_null\", structNotNull); \r\n\n        appender.add(builder.build());\n      }\n    }\n\n    InputFile inFile = Files.localInput(parquetFile);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    parquetFile.deleteOnExit();\n  }\n","realPath":"data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":216,"status":"M"},{"authorDate":"2020-05-26 01:15:35","commitOrder":3,"curCode":"  public void createParquetInputFile(List<Record> records) throws IOException {\n    if (PARQUET_FILE.exists()) {\n      Assert.assertTrue(PARQUET_FILE.delete());\n    }\n\n    OutputFile outFile = Files.localOutput(PARQUET_FILE);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .createWriterFunc(GenericParquetWriter::buildWriter)\n        .build()) {\n      appender.addAll(records);\n    }\n\n    InputFile inFile = Files.localInput(PARQUET_FILE);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    PARQUET_FILE.deleteOnExit();\n  }\n","date":"2020-05-26 01:15:35","endLine":218,"groupId":"10583","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"createParquetInputFile","params":"(List<Record>records)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/86/47a9c7dee02b6ce46ecee97a74de7421cd16f4.src","preCode":"  public void createParquetInputFile(List<Record> records) throws IOException {\n    if (PARQUET_FILE.exists()) {\n      Assert.assertTrue(PARQUET_FILE.delete());\n    }\n\n    OutputFile outFile = Files.localOutput(PARQUET_FILE);\n    try (FileAppender<Record> appender = Parquet.write(outFile)\n        .schema(FILE_SCHEMA)\n        .createWriterFunc(GenericParquetWriter::buildWriter)\n        .build()) {\n      appender.addAll(records);\n    }\n\n    InputFile inFile = Files.localInput(PARQUET_FILE);\n    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {\n      Assert.assertEquals(\"Should create only one row group\", 1, reader.getRowGroups().size());\n      rowGroupMetadata = reader.getRowGroups().get(0);\n      parquetSchema = reader.getFileMetaData().getSchema();\n    }\n\n    PARQUET_FILE.deleteOnExit();\n  }\n","realPath":"data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":197,"status":"N"}],"commitId":"fab4a5f2db140fdb132205e78934a145e646758b","commitMessage":"@@@API: add isNaN and notNaN predicates (#1747)\n\n","date":"2020-12-06 09:14:27","modifiedFileCount":"27","status":"M","submitter":"yyanyy"}]
