[{"authorTime":"2020-08-21 00:17:18","codes":[{"authorDate":"2020-07-30 04:58:48","commitOrder":2,"curCode":"    public void nonNullWrite(int rowId, List<T> value, ColumnVector output) {\n      ListColumnVector cv = (ListColumnVector) output;\n      \r\n      cv.lengths[rowId] = value.size();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount += cv.lengths[rowId];\n      \r\n      cv.child.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        element.write((int) (e + cv.offsets[rowId]), value.get(e), cv.child);\n      }\n    }\n","date":"2020-07-30 04:58:48","endLine":373,"groupId":"747","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"nonNullWrite","params":"(introwId@List<T>value@ColumnVectoroutput)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/61/03c1e3e8b77dbe8a71a81070211d5cade27150.src","preCode":"    public void nonNullWrite(int rowId, List<T> value, ColumnVector output) {\n      ListColumnVector cv = (ListColumnVector) output;\n      \r\n      cv.lengths[rowId] = value.size();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount += cv.lengths[rowId];\n      \r\n      cv.child.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        element.write((int) (e + cv.offsets[rowId]), value.get(e), cv.child);\n      }\n    }\n","realPath":"data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":361,"status":"NB"},{"authorDate":"2020-08-21 00:17:18","commitOrder":2,"curCode":"    public void nonNullWrite(int rowId, MapData data, ColumnVector output) {\n      MapColumnVector cv = (MapColumnVector) output;\n      ArrayData keyArray = data.keyArray();\n      ArrayData valArray = data.valueArray();\n\n      \r\n      cv.lengths[rowId] = data.size();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount += cv.lengths[rowId];\n      \r\n      cv.keys.ensureSize(cv.childCount, true);\n      cv.values.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        int pos = (int) (e + cv.offsets[rowId]);\n        keyWriter.write(pos, (K) keyGetter.getElementOrNull(keyArray, e), cv.keys);\n        valueWriter.write(pos, (V) valueGetter.getElementOrNull(valArray, e), cv.values);\n      }\n    }\n","date":"2020-08-21 00:17:18","endLine":298,"groupId":"4030","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"nonNullWrite","params":"(introwId@MapDatadata@ColumnVectoroutput)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a3/919c988c21921fea052ca187d10c2843213058.src","preCode":"    public void nonNullWrite(int rowId, MapData data, ColumnVector output) {\n      MapColumnVector cv = (MapColumnVector) output;\n      ArrayData keyArray = data.keyArray();\n      ArrayData valArray = data.valueArray();\n\n      \r\n      cv.lengths[rowId] = data.size();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount += cv.lengths[rowId];\n      \r\n      cv.keys.ensureSize(cv.childCount, true);\n      cv.values.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        int pos = (int) (e + cv.offsets[rowId]);\n        keyWriter.write(pos, (K) keyGetter.getElementOrNull(keyArray, e), cv.keys);\n        valueWriter.write(pos, (V) valueGetter.getElementOrNull(valArray, e), cv.values);\n      }\n    }\n","realPath":"flink/src/main/java/org/apache/iceberg/flink/data/FlinkOrcWriters.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":280,"status":"B"}],"commitId":"311f2a107fb2fac0103d9af871493f1cc1b9828b","commitMessage":"@@@Flink: Add Orc reader.  writer implementations (#1255)\n\n","date":"2020-08-21 00:17:18","modifiedFileCount":"6","status":"M","submitter":"openinx"},{"authorTime":"2021-01-23 02:17:49","codes":[{"authorDate":"2021-01-23 02:17:49","commitOrder":3,"curCode":"    public void nonNullWrite(int rowId, List<T> value, ColumnVector output) {\n      ListColumnVector cv = (ListColumnVector) output;\n      \r\n      cv.lengths[rowId] = value.size();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount = (int) (cv.childCount + cv.lengths[rowId]);\n      \r\n      cv.child.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        element.write((int) (e + cv.offsets[rowId]), value.get(e), cv.child);\n      }\n    }\n","date":"2021-01-23 02:17:49","endLine":438,"groupId":"747","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"nonNullWrite","params":"(introwId@List<T>value@ColumnVectoroutput)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/00/e9121ac58b4e683fae8e722c807051bd8b7990.src","preCode":"    public void nonNullWrite(int rowId, List<T> value, ColumnVector output) {\n      ListColumnVector cv = (ListColumnVector) output;\n      \r\n      cv.lengths[rowId] = value.size();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount += cv.lengths[rowId];\n      \r\n      cv.child.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        element.write((int) (e + cv.offsets[rowId]), value.get(e), cv.child);\n      }\n    }\n","realPath":"data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":426,"status":"M"},{"authorDate":"2021-01-23 02:17:49","commitOrder":3,"curCode":"    public void nonNullWrite(int rowId, MapData data, ColumnVector output) {\n      MapColumnVector cv = (MapColumnVector) output;\n      ArrayData keyArray = data.keyArray();\n      ArrayData valArray = data.valueArray();\n\n      \r\n      cv.lengths[rowId] = data.size();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount = (int) (cv.childCount + cv.lengths[rowId]);\n      \r\n      cv.keys.ensureSize(cv.childCount, true);\n      cv.values.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        int pos = (int) (e + cv.offsets[rowId]);\n        keyWriter.write(pos, (K) keyGetter.getElementOrNull(keyArray, e), cv.keys);\n        valueWriter.write(pos, (V) valueGetter.getElementOrNull(valArray, e), cv.values);\n      }\n    }\n","date":"2021-01-23 02:17:49","endLine":298,"groupId":"4030","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"nonNullWrite","params":"(introwId@MapDatadata@ColumnVectoroutput)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/80/0ad2071274f21f1134edb9535c53d8d7049fd8.src","preCode":"    public void nonNullWrite(int rowId, MapData data, ColumnVector output) {\n      MapColumnVector cv = (MapColumnVector) output;\n      ArrayData keyArray = data.keyArray();\n      ArrayData valArray = data.valueArray();\n\n      \r\n      cv.lengths[rowId] = data.size();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount += cv.lengths[rowId];\n      \r\n      cv.keys.ensureSize(cv.childCount, true);\n      cv.values.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        int pos = (int) (e + cv.offsets[rowId]);\n        keyWriter.write(pos, (K) keyGetter.getElementOrNull(keyArray, e), cv.keys);\n        valueWriter.write(pos, (V) valueGetter.getElementOrNull(valArray, e), cv.values);\n      }\n    }\n","realPath":"flink/src/main/java/org/apache/iceberg/flink/data/FlinkOrcWriters.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":280,"status":"M"}],"commitId":"503c0eff8ba483a370f2c904fc696175102e8603","commitMessage":"@@@ORC: Fix NarrowingCompoundAssignment warnings (#2103)\n\n","date":"2021-01-23 02:17:49","modifiedFileCount":"3","status":"M","submitter":"Kyle Bendickson"},{"authorTime":"2021-02-06 10:04:47","codes":[{"authorDate":"2021-02-06 10:04:47","commitOrder":4,"curCode":"    public void nonNullWrite(int rowId, List<T> value, ColumnVector output) {\n      ListColumnVector cv = (ListColumnVector) output;\n      \r\n      cv.lengths[rowId] = value.size();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount = (int) (cv.childCount + cv.lengths[rowId]);\n      \r\n      growColumnVector(cv.child, cv.childCount);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        element.write((int) (e + cv.offsets[rowId]), value.get(e), cv.child);\n      }\n    }\n","date":"2021-02-06 10:04:47","endLine":469,"groupId":"12696","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"nonNullWrite","params":"(introwId@List<T>value@ColumnVectoroutput)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/65/894240089606ce4150889c5d9b2c18785abd54.src","preCode":"    public void nonNullWrite(int rowId, List<T> value, ColumnVector output) {\n      ListColumnVector cv = (ListColumnVector) output;\n      \r\n      cv.lengths[rowId] = value.size();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount = (int) (cv.childCount + cv.lengths[rowId]);\n      \r\n      cv.child.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        element.write((int) (e + cv.offsets[rowId]), value.get(e), cv.child);\n      }\n    }\n","realPath":"data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":457,"status":"M"},{"authorDate":"2021-02-06 10:04:47","commitOrder":4,"curCode":"    public void nonNullWrite(int rowId, MapData data, ColumnVector output) {\n      MapColumnVector cv = (MapColumnVector) output;\n      ArrayData keyArray = data.keyArray();\n      ArrayData valArray = data.valueArray();\n\n      \r\n      cv.lengths[rowId] = data.size();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount = (int) (cv.childCount + cv.lengths[rowId]);\n      \r\n      growColumnVector(cv.keys, cv.childCount);\n      growColumnVector(cv.values, cv.childCount);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        int pos = (int) (e + cv.offsets[rowId]);\n        keyWriter.write(pos, (K) keyGetter.getElementOrNull(keyArray, e), cv.keys);\n        valueWriter.write(pos, (V) valueGetter.getElementOrNull(valArray, e), cv.values);\n      }\n    }\n","date":"2021-02-06 10:04:47","endLine":306,"groupId":"12696","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"nonNullWrite","params":"(introwId@MapDatadata@ColumnVectoroutput)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/75/8d73d87e9b2f629236ff2e9c5d2aab6e0854e3.src","preCode":"    public void nonNullWrite(int rowId, MapData data, ColumnVector output) {\n      MapColumnVector cv = (MapColumnVector) output;\n      ArrayData keyArray = data.keyArray();\n      ArrayData valArray = data.valueArray();\n\n      \r\n      cv.lengths[rowId] = data.size();\n      cv.offsets[rowId] = cv.childCount;\n      cv.childCount = (int) (cv.childCount + cv.lengths[rowId]);\n      \r\n      cv.keys.ensureSize(cv.childCount, true);\n      cv.values.ensureSize(cv.childCount, true);\n      \r\n      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n        int pos = (int) (e + cv.offsets[rowId]);\n        keyWriter.write(pos, (K) keyGetter.getElementOrNull(keyArray, e), cv.keys);\n        valueWriter.write(pos, (V) valueGetter.getElementOrNull(valArray, e), cv.values);\n      }\n    }\n","realPath":"flink/src/main/java/org/apache/iceberg/flink/data/FlinkOrcWriters.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":288,"status":"M"}],"commitId":"592e97f5daf0716ec6f57b4f049dd191d8094fad","commitMessage":"@@@ORC: Grow list and map child vectors with a growth factor of 3 (#2218)\n\n","date":"2021-02-06 10:04:47","modifiedFileCount":"3","status":"M","submitter":"Shardul Mahadik"}]
