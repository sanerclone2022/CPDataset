[{"authorTime":"2019-03-21 07:25:05","codes":[{"authorDate":"2019-03-21 07:25:05","commitOrder":1,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName()))-1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","date":"2019-03-21 07:25:05","endLine":122,"groupId":"224","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/01/d2311ea20a40ee5a9bfb2d18cdea423550cdd9.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName()))-1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","realPath":"parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroValueReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":88,"status":"B"},{"authorDate":"2019-03-21 07:25:05","commitOrder":1,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct, List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new TupleReader(types, reorderedFields, partitionValues);\n    }\n","date":"2019-03-21 07:25:05","endLine":165,"groupId":"224","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/fe/2e438f6adcf5ba153502dcf79411b5c1ffb1ca.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct, List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new TupleReader(types, reorderedFields, partitionValues);\n    }\n","realPath":"pig/src/main/java/org/apache/iceberg/pig/PigParquetReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":134,"status":"B"}],"commitId":"c20927801a369104e5ea510470e1cf7c8e28b808","commitMessage":"@@@Rename packages to org.apache.iceberg (#138)\n\n* Move all packages by directory (but don't change references)\n* Rename all references from com.netflix.iceberg to org.apache.iceberg\n* Reorganize all imports due to new package name.\n  Previous commit only did a string find-replace.  which made all the imports out of order. Use an IDE to auto-sort all imports.\n\n","date":"2019-03-21 07:25:05","modifiedFileCount":"0","status":"B","submitter":"mccheah"},{"authorTime":"2019-09-05 01:51:12","codes":[{"authorDate":"2019-03-21 07:25:05","commitOrder":2,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName()))-1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","date":"2019-03-21 07:25:05","endLine":122,"groupId":"224","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/01/d2311ea20a40ee5a9bfb2d18cdea423550cdd9.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName()))-1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","realPath":"parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroValueReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":88,"status":"N"},{"authorDate":"2019-09-05 01:51:12","commitOrder":2,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct, List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (partitionValues.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(partitionValues.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new TupleReader(types, reorderedFields);\n    }\n","date":"2019-09-05 01:51:12","endLine":167,"groupId":"224","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/de/b80d20091e507eb7dbe04cd9d50f8e9ac75fe2.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct, List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new TupleReader(types, reorderedFields, partitionValues);\n    }\n","realPath":"pig/src/main/java/org/apache/iceberg/pig/PigParquetReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":130,"status":"M"}],"commitId":"d158818cb90beed3298be48a0a9de2366ba3cdb5","commitMessage":"@@@Use constant readers for Pig partition values. (#444)\n\n","date":"2019-09-05 01:51:12","modifiedFileCount":"3","status":"M","submitter":"Ryan Blue"},{"authorTime":"2019-09-05 01:51:12","codes":[{"authorDate":"2019-10-23 02:17:28","commitOrder":3,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","date":"2019-10-23 02:17:28","endLine":120,"groupId":"224","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a2/6fe92a12b858edf1b08dd88c1c0dff344a62f4.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName()))-1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","realPath":"parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroValueReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"M"},{"authorDate":"2019-09-05 01:51:12","commitOrder":3,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct, List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (partitionValues.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(partitionValues.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new TupleReader(types, reorderedFields);\n    }\n","date":"2019-09-05 01:51:12","endLine":167,"groupId":"224","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/de/b80d20091e507eb7dbe04cd9d50f8e9ac75fe2.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct, List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (partitionValues.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(partitionValues.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new TupleReader(types, reorderedFields);\n    }\n","realPath":"pig/src/main/java/org/apache/iceberg/pig/PigParquetReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":130,"status":"N"}],"commitId":"336174b0a4438ed68cdb8e208833e380adaa15fc","commitMessage":"@@@Baseline: Add Baseline to iceberg-parquet (#526)\n\n","date":"2019-10-23 02:17:28","modifiedFileCount":"29","status":"M","submitter":"Fokko Driesprong"},{"authorTime":"2019-10-28 05:06:22","codes":[{"authorDate":"2019-10-23 02:17:28","commitOrder":4,"curCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","date":"2019-10-23 02:17:28","endLine":120,"groupId":"12434","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a2/6fe92a12b858edf1b08dd88c1c0dff344a62f4.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n                                        List<ParquetValueReader<?>> fieldReaders) {\n      Schema avroSchema = avroSchemas.get(expected);\n\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        ParquetValueReader<?> reader = readersById.get(id);\n        if (reader != null) {\n          reorderedFields.add(reader);\n          types.add(typesById.get(id));\n        } else {\n          reorderedFields.add(ParquetValueReaders.nulls());\n          types.add(null);\n        }\n      }\n\n      return new RecordReader(types, reorderedFields, avroSchema);\n    }\n","realPath":"parquet/src/main/java/org/apache/iceberg/parquet/ParquetAvroValueReaders.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":86,"status":"N"},{"authorDate":"2019-10-28 05:06:22","commitOrder":4,"curCode":"    public ParquetValueReader<?> struct(\n        Types.StructType expected, GroupType struct, List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (partitionValues.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(partitionValues.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new TupleReader(types, reorderedFields);\n    }\n","date":"2019-10-28 05:06:22","endLine":173,"groupId":"12434","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"struct","params":"(Types.StructTypeexpected@GroupTypestruct@List<ParquetValueReader<?>>fieldReaders)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/aa/3f1dc7d6a423647980e090c67f868e9174860f.src","preCode":"    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct, List<ParquetValueReader<?>> fieldReaders) {\n      \r\n      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n      Map<Integer, Type> typesById = Maps.newHashMap();\n      List<Type> fields = struct.getFields();\n      for (int i = 0; i < fields.size(); i += 1) {\n        Type fieldType = fields.get(i);\n        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n        int id = fieldType.getId().intValue();\n        readersById.put(id, option(fieldType, fieldD, fieldReaders.get(i)));\n        typesById.put(id, fieldType);\n      }\n\n      List<Types.NestedField> expectedFields = expected != null ?\n          expected.fields() : ImmutableList.of();\n      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n          expectedFields.size());\n      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n      for (Types.NestedField field : expectedFields) {\n        int id = field.fieldId();\n        if (partitionValues.containsKey(id)) {\n          \r\n          reorderedFields.add(ParquetValueReaders.constant(partitionValues.get(id)));\n          types.add(null);\n        } else {\n          ParquetValueReader<?> reader = readersById.get(id);\n          if (reader != null) {\n            reorderedFields.add(reader);\n            types.add(typesById.get(id));\n          } else {\n            reorderedFields.add(ParquetValueReaders.nulls());\n            types.add(null);\n          }\n        }\n      }\n\n      return new TupleReader(types, reorderedFields);\n    }\n","realPath":"pig/src/main/java/org/apache/iceberg/pig/PigParquetReader.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":135,"status":"M"}],"commitId":"040b272f360f796196f689c7a3f654413e0c4f66","commitMessage":"@@@Apply Baseline to iceberg-pig (#525)\n\n","date":"2019-10-28 05:06:22","modifiedFileCount":"5","status":"M","submitter":"Fokko Driesprong"}]
