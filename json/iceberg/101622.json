[{"authorTime":"2020-04-07 07:02:56","codes":[{"authorDate":"2020-04-07 07:02:56","commitOrder":1,"curCode":"    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n          .project(readSchema)\n          .split(task.start(), task.length());\n      if (reuseContainers) {\n        avroReadBuilder.reuseContainers();\n      }\n\n      switch (inMemoryDataModel) {\n        case PIG:\n        case HIVE:\n          \r\n          throw new UnsupportedOperationException(\"Avro support not yet supported for Pig and Hive\");\n        case GENERIC:\n          avroReadBuilder.createReaderFunc(DataReader::create);\n      }\n      return avroReadBuilder.build();\n    }\n","date":"2020-04-07 07:02:56","endLine":461,"groupId":"1112","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"newAvroIterable","params":"(InputFileinputFile@FileScanTasktask@SchemareadSchema)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/4f/31abca95d84d6bab96a7d2a4c0995a1b68250e.src","preCode":"    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n          .project(readSchema)\n          .split(task.start(), task.length());\n      if (reuseContainers) {\n        avroReadBuilder.reuseContainers();\n      }\n\n      switch (inMemoryDataModel) {\n        case PIG:\n        case HIVE:\n          \r\n          throw new UnsupportedOperationException(\"Avro support not yet supported for Pig and Hive\");\n        case GENERIC:\n          avroReadBuilder.createReaderFunc(DataReader::create);\n      }\n      return avroReadBuilder.build();\n    }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":444,"status":"B"},{"authorDate":"2020-04-07 07:02:56","commitOrder":1,"curCode":"    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)\n          .project(readSchema)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          .split(task.start(), task.length());\n      if (reuseContainers) {\n        parquetReadBuilder.reuseContainers();\n      }\n\n      switch (inMemoryDataModel) {\n        case PIG:\n        case HIVE:\n          \r\n          throw new UnsupportedOperationException(\"Parquet support not yet supported for Pig and Hive\");\n        case GENERIC:\n          parquetReadBuilder.createReaderFunc(\n              fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));\n      }\n      return parquetReadBuilder.build();\n    }\n","date":"2020-04-07 07:02:56","endLine":483,"groupId":"4884","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"newParquetIterable","params":"(InputFileinputFile@FileScanTasktask@SchemareadSchema)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/4f/31abca95d84d6bab96a7d2a4c0995a1b68250e.src","preCode":"    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)\n          .project(readSchema)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          .split(task.start(), task.length());\n      if (reuseContainers) {\n        parquetReadBuilder.reuseContainers();\n      }\n\n      switch (inMemoryDataModel) {\n        case PIG:\n        case HIVE:\n          \r\n          throw new UnsupportedOperationException(\"Parquet support not yet supported for Pig and Hive\");\n        case GENERIC:\n          parquetReadBuilder.createReaderFunc(\n              fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));\n      }\n      return parquetReadBuilder.build();\n    }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":463,"status":"B"}],"commitId":"71a7ab986a003fab614d076cffb5a4580062806e","commitMessage":"@@@MR: Add InputFormat (#843)\n\n","date":"2020-04-07 07:02:56","modifiedFileCount":"2","status":"B","submitter":"Ratandeep Ratti"},{"authorTime":"2020-04-21 02:33:04","codes":[{"authorDate":"2020-04-21 02:33:04","commitOrder":2,"curCode":"    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n          .project(readSchema)\n          .split(task.start(), task.length());\n      if (reuseContainers) {\n        avroReadBuilder.reuseContainers();\n      }\n\n      switch (inMemoryDataModel) {\n        case PIG:\n        case HIVE:\n          \r\n          throw new UnsupportedOperationException(\"Avro support not yet supported for Pig and Hive\");\n        case GENERIC:\n          avroReadBuilder.createReaderFunc(DataReader::create);\n      }\n      return applyResidualFiltering(avroReadBuilder.build(), task.residual(), readSchema);\n    }\n","date":"2020-04-21 02:33:04","endLine":475,"groupId":"1112","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"newAvroIterable","params":"(InputFileinputFile@FileScanTasktask@SchemareadSchema)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/0d/35644ab89fff4128496fede20cb2d539e52870.src","preCode":"    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n          .project(readSchema)\n          .split(task.start(), task.length());\n      if (reuseContainers) {\n        avroReadBuilder.reuseContainers();\n      }\n\n      switch (inMemoryDataModel) {\n        case PIG:\n        case HIVE:\n          \r\n          throw new UnsupportedOperationException(\"Avro support not yet supported for Pig and Hive\");\n        case GENERIC:\n          avroReadBuilder.createReaderFunc(DataReader::create);\n      }\n      return avroReadBuilder.build();\n    }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":458,"status":"M"},{"authorDate":"2020-04-21 02:33:04","commitOrder":2,"curCode":"    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)\n          .project(readSchema)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          .split(task.start(), task.length());\n      if (reuseContainers) {\n        parquetReadBuilder.reuseContainers();\n      }\n\n      switch (inMemoryDataModel) {\n        case PIG:\n        case HIVE:\n          \r\n          throw new UnsupportedOperationException(\"Parquet support not yet supported for Pig and Hive\");\n        case GENERIC:\n          parquetReadBuilder.createReaderFunc(\n              fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));\n      }\n      return applyResidualFiltering(parquetReadBuilder.build(), task.residual(), readSchema);\n    }\n","date":"2020-04-21 02:33:04","endLine":497,"groupId":"4884","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"newParquetIterable","params":"(InputFileinputFile@FileScanTasktask@SchemareadSchema)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/0d/35644ab89fff4128496fede20cb2d539e52870.src","preCode":"    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)\n          .project(readSchema)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          .split(task.start(), task.length());\n      if (reuseContainers) {\n        parquetReadBuilder.reuseContainers();\n      }\n\n      switch (inMemoryDataModel) {\n        case PIG:\n        case HIVE:\n          \r\n          throw new UnsupportedOperationException(\"Parquet support not yet supported for Pig and Hive\");\n        case GENERIC:\n          parquetReadBuilder.createReaderFunc(\n              fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));\n      }\n      return parquetReadBuilder.build();\n    }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":477,"status":"M"}],"commitId":"af25cbedac34c09633a66a2c73d68f3f4822d64a","commitMessage":"@@@Add residual evaluation for MR reader (#931)\n\n","date":"2020-04-21 02:33:04","modifiedFileCount":"2","status":"M","submitter":"Chen Junjie"},{"authorTime":"2020-06-23 05:52:19","codes":[{"authorDate":"2020-06-23 05:52:19","commitOrder":3,"curCode":"    private CloseableIterable<T> newAvroIterable(\n        InputFile inputFile, FileScanTask task, Schema readSchema) {\n      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n          .project(readSchema)\n          .split(task.start(), task.length());\n      if (reuseContainers) {\n        avroReadBuilder.reuseContainers();\n      }\n\n      switch (inMemoryDataModel) {\n        case PIG:\n        case HIVE:\n          \r\n          throw new UnsupportedOperationException(\"Avro support not yet supported for Pig and Hive\");\n        case GENERIC:\n          avroReadBuilder.createReaderFunc(\n              (expIcebergSchema, expAvroSchema) ->\n                  DataReader.create(expIcebergSchema, expAvroSchema,\n                      constantsMap(task, IdentityPartitionConverters::convertConstant)));\n      }\n      return applyResidualFiltering(avroReadBuilder.build(), task.residual(), readSchema);\n    }\n","date":"2020-06-23 05:52:19","endLine":404,"groupId":"101622","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"newAvroIterable","params":"(InputFileinputFile@FileScanTasktask@SchemareadSchema)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/b2/09a80e4feea2e3d828922ea44cd5477054d696.src","preCode":"    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n          .project(readSchema)\n          .split(task.start(), task.length());\n      if (reuseContainers) {\n        avroReadBuilder.reuseContainers();\n      }\n\n      switch (inMemoryDataModel) {\n        case PIG:\n        case HIVE:\n          \r\n          throw new UnsupportedOperationException(\"Avro support not yet supported for Pig and Hive\");\n        case GENERIC:\n          avroReadBuilder.createReaderFunc(DataReader::create);\n      }\n      return applyResidualFiltering(avroReadBuilder.build(), task.residual(), readSchema);\n    }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":383,"status":"M"},{"authorDate":"2020-06-23 05:52:19","commitOrder":3,"curCode":"    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)\n          .project(readSchema)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          .split(task.start(), task.length());\n      if (reuseContainers) {\n        parquetReadBuilder.reuseContainers();\n      }\n\n      switch (inMemoryDataModel) {\n        case PIG:\n        case HIVE:\n          \r\n          throw new UnsupportedOperationException(\"Parquet support not yet supported for Pig and Hive\");\n        case GENERIC:\n          parquetReadBuilder.createReaderFunc(\n              fileSchema -> GenericParquetReaders.buildReader(\n                  readSchema, fileSchema, constantsMap(task, IdentityPartitionConverters::convertConstant)));\n      }\n      return applyResidualFiltering(parquetReadBuilder.build(), task.residual(), readSchema);\n    }\n","date":"2020-06-23 05:52:19","endLine":427,"groupId":"101622","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"newParquetIterable","params":"(InputFileinputFile@FileScanTasktask@SchemareadSchema)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/b2/09a80e4feea2e3d828922ea44cd5477054d696.src","preCode":"    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)\n          .project(readSchema)\n          .filter(task.residual())\n          .caseSensitive(caseSensitive)\n          .split(task.start(), task.length());\n      if (reuseContainers) {\n        parquetReadBuilder.reuseContainers();\n      }\n\n      switch (inMemoryDataModel) {\n        case PIG:\n        case HIVE:\n          \r\n          throw new UnsupportedOperationException(\"Parquet support not yet supported for Pig and Hive\");\n        case GENERIC:\n          parquetReadBuilder.createReaderFunc(\n              fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));\n      }\n      return applyResidualFiltering(parquetReadBuilder.build(), task.residual(), readSchema);\n    }\n","realPath":"mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":406,"status":"M"}],"commitId":"afb6a12027b1eaffafd667a77ade81f7bd0180d3","commitMessage":"@@@MR: Pass identity values via constants map in InputFormat (#1130)\n\n","date":"2020-06-23 05:52:19","modifiedFileCount":"2","status":"M","submitter":"Ratandeep Ratti"}]
