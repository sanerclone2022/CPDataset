[{"authorTime":"2020-07-23 04:11:52","codes":[{"authorDate":"2020-07-23 04:11:52","commitOrder":1,"curCode":"  public void testImportPartitionedWithWhitespace() throws Exception {\n    String partitionCol = \"dAtA sPaced\";\n    String spacedTableName = \"whitespacetable\";\n    String whiteSpaceKey = \"some key value\";\n\n    List<SimpleRecord> spacedRecords = Lists.newArrayList(new SimpleRecord(1, whiteSpaceKey));\n\n    File icebergLocation = temp.newFolder(\"partitioned_table\");\n\n    spark.createDataFrame(spacedRecords, SimpleRecord.class)\n        .withColumnRenamed(\"data\", partitionCol)\n        .write().mode(\"overwrite\").partitionBy(partitionCol).format(\"parquet\")\n        .saveAsTable(spacedTableName);\n\n    TableIdentifier source = spark.sessionState().sqlParser()\n        .parseTableIdentifier(spacedTableName);\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Table table = tables.create(SparkSchemaUtil.schemaForTable(spark, spacedTableName),\n        SparkSchemaUtil.specForTable(spark, spacedTableName),\n        ImmutableMap.of(),\n        icebergLocation.getCanonicalPath());\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n    List<SimpleRecord> results = spark.read().format(\"iceberg\").load(icebergLocation.toString())\n        .withColumnRenamed(partitionCol, \"data\")\n        .as(Encoders.bean(SimpleRecord.class))\n        .collectAsList();\n\n    Assert.assertEquals(\"Data should match\", spacedRecords, results);\n  }\n","date":"2020-07-23 04:11:52","endLine":332,"groupId":"1985","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testImportPartitionedWithWhitespace","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/1c/c489af8608a7cf59d35faafa08ce890510291c.src","preCode":"  public void testImportPartitionedWithWhitespace() throws Exception {\n    String partitionCol = \"dAtA sPaced\";\n    String spacedTableName = \"whitespacetable\";\n    String whiteSpaceKey = \"some key value\";\n\n    List<SimpleRecord> spacedRecords = Lists.newArrayList(new SimpleRecord(1, whiteSpaceKey));\n\n    File icebergLocation = temp.newFolder(\"partitioned_table\");\n\n    spark.createDataFrame(spacedRecords, SimpleRecord.class)\n        .withColumnRenamed(\"data\", partitionCol)\n        .write().mode(\"overwrite\").partitionBy(partitionCol).format(\"parquet\")\n        .saveAsTable(spacedTableName);\n\n    TableIdentifier source = spark.sessionState().sqlParser()\n        .parseTableIdentifier(spacedTableName);\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Table table = tables.create(SparkSchemaUtil.schemaForTable(spark, spacedTableName),\n        SparkSchemaUtil.specForTable(spark, spacedTableName),\n        ImmutableMap.of(),\n        icebergLocation.getCanonicalPath());\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n    List<SimpleRecord> results = spark.read().format(\"iceberg\").load(icebergLocation.toString())\n        .withColumnRenamed(partitionCol, \"data\")\n        .as(Encoders.bean(SimpleRecord.class))\n        .collectAsList();\n\n    Assert.assertEquals(\"Data should match\", spacedRecords, results);\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":303,"status":"B"},{"authorDate":"2020-07-23 04:11:52","commitOrder":1,"curCode":"  public void testImportUnpartitionedWithWhitespace() throws Exception {\n    String spacedTableName = \"whitespacetable\";\n    String whiteSpaceKey = \"some key value\";\n\n    List<SimpleRecord> spacedRecords = Lists.newArrayList(new SimpleRecord(1, whiteSpaceKey));\n\n    File whiteSpaceOldLocation = temp.newFolder(\"white space location\");\n    File icebergLocation = temp.newFolder(\"partitioned_table\");\n\n    spark.createDataFrame(spacedRecords, SimpleRecord.class)\n        .write().mode(\"overwrite\").parquet(whiteSpaceOldLocation.getPath());\n\n    spark.catalog().createExternalTable(spacedTableName, whiteSpaceOldLocation.getPath());\n\n    TableIdentifier source = spark.sessionState().sqlParser()\n        .parseTableIdentifier(spacedTableName);\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Table table = tables.create(SparkSchemaUtil.schemaForTable(spark, spacedTableName),\n        SparkSchemaUtil.specForTable(spark, spacedTableName),\n        ImmutableMap.of(),\n        icebergLocation.getCanonicalPath());\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n    List<SimpleRecord> results = spark.read().format(\"iceberg\").load(icebergLocation.toString())\n        .as(Encoders.bean(SimpleRecord.class)).collectAsList();\n\n    Assert.assertEquals(\"Data should match\", spacedRecords, results);\n  }\n","date":"2020-07-23 04:11:52","endLine":362,"groupId":"2020","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testImportUnpartitionedWithWhitespace","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/1c/c489af8608a7cf59d35faafa08ce890510291c.src","preCode":"  public void testImportUnpartitionedWithWhitespace() throws Exception {\n    String spacedTableName = \"whitespacetable\";\n    String whiteSpaceKey = \"some key value\";\n\n    List<SimpleRecord> spacedRecords = Lists.newArrayList(new SimpleRecord(1, whiteSpaceKey));\n\n    File whiteSpaceOldLocation = temp.newFolder(\"white space location\");\n    File icebergLocation = temp.newFolder(\"partitioned_table\");\n\n    spark.createDataFrame(spacedRecords, SimpleRecord.class)\n        .write().mode(\"overwrite\").parquet(whiteSpaceOldLocation.getPath());\n\n    spark.catalog().createExternalTable(spacedTableName, whiteSpaceOldLocation.getPath());\n\n    TableIdentifier source = spark.sessionState().sqlParser()\n        .parseTableIdentifier(spacedTableName);\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Table table = tables.create(SparkSchemaUtil.schemaForTable(spark, spacedTableName),\n        SparkSchemaUtil.specForTable(spark, spacedTableName),\n        ImmutableMap.of(),\n        icebergLocation.getCanonicalPath());\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n    List<SimpleRecord> results = spark.read().format(\"iceberg\").load(icebergLocation.toString())\n        .as(Encoders.bean(SimpleRecord.class)).collectAsList();\n\n    Assert.assertEquals(\"Data should match\", spacedRecords, results);\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":335,"status":"B"}],"commitId":"dfefa5d2f69dd951a0301b3aba282ffa974364bf","commitMessage":"@@@Spark: Fix import for paths that include spaces (#1228)\n\nIn order to avoid changing the API and SparkSQL compatible types we will fix the whitespace issue by\nreplacing the encoded string representation with a decoded string representation. We use a\nmethod identical to Apache Spark.  taking the Hadoop Path representation of the URI and getting the\nstring representation from that.","date":"2020-07-23 04:11:52","modifiedFileCount":"3","status":"B","submitter":"Russell Spitzer"},{"authorTime":"2020-09-26 06:55:33","codes":[{"authorDate":"2020-09-26 06:55:33","commitOrder":2,"curCode":"    public void testImportPartitionedWithWhitespace() throws Exception {\n      String partitionCol = \"dAtA sPaced\";\n      String spacedTableName = \"whitespacetable\";\n      String whiteSpaceKey = \"some key value\";\n\n      List<SimpleRecord> spacedRecords = Lists.newArrayList(new SimpleRecord(1, whiteSpaceKey));\n\n      File icebergLocation = temp.newFolder(\"partitioned_table\");\n\n      spark.createDataFrame(spacedRecords, SimpleRecord.class)\n          .withColumnRenamed(\"data\", partitionCol)\n          .write().mode(\"overwrite\").partitionBy(partitionCol).format(format.toString())\n          .saveAsTable(spacedTableName);\n\n      TableIdentifier source = spark.sessionState().sqlParser()\n          .parseTableIdentifier(spacedTableName);\n      HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n      Table table = tables.create(SparkSchemaUtil.schemaForTable(spark, spacedTableName),\n          SparkSchemaUtil.specForTable(spark, spacedTableName),\n          ImmutableMap.of(),\n          icebergLocation.getCanonicalPath());\n      File stagingDir = temp.newFolder(\"staging-dir\");\n      SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n      List<SimpleRecord> results = spark.read().format(\"iceberg\").load(icebergLocation.toString())\n          .withColumnRenamed(partitionCol, \"data\")\n          .as(Encoders.bean(SimpleRecord.class))\n          .collectAsList();\n\n      Assert.assertEquals(\"Data should match\", spacedRecords, results);\n    }\n","date":"2020-09-26 06:55:33","endLine":342,"groupId":"10952","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testImportPartitionedWithWhitespace","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/fe/948a4ddaac357c6c86c8dc1f2ebd8ac85b57f8.src","preCode":"  public void testImportPartitionedWithWhitespace() throws Exception {\n    String partitionCol = \"dAtA sPaced\";\n    String spacedTableName = \"whitespacetable\";\n    String whiteSpaceKey = \"some key value\";\n\n    List<SimpleRecord> spacedRecords = Lists.newArrayList(new SimpleRecord(1, whiteSpaceKey));\n\n    File icebergLocation = temp.newFolder(\"partitioned_table\");\n\n    spark.createDataFrame(spacedRecords, SimpleRecord.class)\n        .withColumnRenamed(\"data\", partitionCol)\n        .write().mode(\"overwrite\").partitionBy(partitionCol).format(\"parquet\")\n        .saveAsTable(spacedTableName);\n\n    TableIdentifier source = spark.sessionState().sqlParser()\n        .parseTableIdentifier(spacedTableName);\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Table table = tables.create(SparkSchemaUtil.schemaForTable(spark, spacedTableName),\n        SparkSchemaUtil.specForTable(spark, spacedTableName),\n        ImmutableMap.of(),\n        icebergLocation.getCanonicalPath());\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n    List<SimpleRecord> results = spark.read().format(\"iceberg\").load(icebergLocation.toString())\n        .withColumnRenamed(partitionCol, \"data\")\n        .as(Encoders.bean(SimpleRecord.class))\n        .collectAsList();\n\n    Assert.assertEquals(\"Data should match\", spacedRecords, results);\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":313,"status":"M"},{"authorDate":"2020-09-26 06:55:33","commitOrder":2,"curCode":"    public void testImportUnpartitionedWithWhitespace() throws Exception {\n      String spacedTableName = \"whitespacetable_\" + format;\n      String whiteSpaceKey = \"some key value\";\n\n      List<SimpleRecord> spacedRecords = Lists.newArrayList(new SimpleRecord(1, whiteSpaceKey));\n\n      File whiteSpaceOldLocation = temp.newFolder(\"white space location\");\n      File icebergLocation = temp.newFolder(\"partitioned_table\");\n\n      spark.createDataFrame(spacedRecords, SimpleRecord.class)\n          .write().mode(\"overwrite\").format(format.toString()).save(whiteSpaceOldLocation.getPath());\n\n      spark.catalog().createExternalTable(spacedTableName, whiteSpaceOldLocation.getPath(), format.toString());\n\n      TableIdentifier source = spark.sessionState().sqlParser()\n          .parseTableIdentifier(spacedTableName);\n      HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n      Table table = tables.create(SparkSchemaUtil.schemaForTable(spark, spacedTableName),\n          SparkSchemaUtil.specForTable(spark, spacedTableName),\n          ImmutableMap.of(),\n          icebergLocation.getCanonicalPath());\n      File stagingDir = temp.newFolder(\"staging-dir\");\n      SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n      List<SimpleRecord> results = spark.read().format(\"iceberg\").load(icebergLocation.toString())\n          .as(Encoders.bean(SimpleRecord.class)).collectAsList();\n\n      Assert.assertEquals(\"Data should match\", spacedRecords, results);\n    }\n","date":"2020-09-26 06:55:33","endLine":372,"groupId":"10952","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testImportUnpartitionedWithWhitespace","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/fe/948a4ddaac357c6c86c8dc1f2ebd8ac85b57f8.src","preCode":"  public void testImportUnpartitionedWithWhitespace() throws Exception {\n    String spacedTableName = \"whitespacetable\";\n    String whiteSpaceKey = \"some key value\";\n\n    List<SimpleRecord> spacedRecords = Lists.newArrayList(new SimpleRecord(1, whiteSpaceKey));\n\n    File whiteSpaceOldLocation = temp.newFolder(\"white space location\");\n    File icebergLocation = temp.newFolder(\"partitioned_table\");\n\n    spark.createDataFrame(spacedRecords, SimpleRecord.class)\n        .write().mode(\"overwrite\").parquet(whiteSpaceOldLocation.getPath());\n\n    spark.catalog().createExternalTable(spacedTableName, whiteSpaceOldLocation.getPath());\n\n    TableIdentifier source = spark.sessionState().sqlParser()\n        .parseTableIdentifier(spacedTableName);\n    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n    Table table = tables.create(SparkSchemaUtil.schemaForTable(spark, spacedTableName),\n        SparkSchemaUtil.specForTable(spark, spacedTableName),\n        ImmutableMap.of(),\n        icebergLocation.getCanonicalPath());\n    File stagingDir = temp.newFolder(\"staging-dir\");\n    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());\n    List<SimpleRecord> results = spark.read().format(\"iceberg\").load(icebergLocation.toString())\n        .as(Encoders.bean(SimpleRecord.class)).collectAsList();\n\n    Assert.assertEquals(\"Data should match\", spacedRecords, results);\n  }\n","realPath":"spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":345,"status":"M"}],"commitId":"c07b23b313e9992d16b8ea8a4eb89ed5a6b12985","commitMessage":"@@@Spark: Follow name mapping when importing ORC tables (#1399)\n\n","date":"2020-09-26 06:55:33","modifiedFileCount":"3","status":"M","submitter":"Edgar Rodriguez"}]
