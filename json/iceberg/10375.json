[{"authorTime":"2019-03-21 07:25:05","codes":[{"authorDate":"2019-03-21 07:25:05","commitOrder":1,"curCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    Assume.assumeTrue(\"Cannot handle non-string map keys in parquet-avro\",\n        null == TypeUtil.find(\n            schema,\n            type -> type.isMapType() && type.asMapType().keyType() != Types.StringType.get()));\n\n    File parent = temp.newFolder(\"parquet\");\n    File location = new File(parent, \"test\");\n    File dataFolder = new File(location, \"data\");\n    dataFolder.mkdirs();\n\n    File parquetFile = new File(dataFolder,\n        FileFormat.PARQUET.addExtension(UUID.randomUUID().toString()));\n\n    HadoopTables tables = new HadoopTables(CONF);\n    Table table = tables.create(schema, PartitionSpec.unpartitioned(), location.toString());\n\n    \r\n    \r\n    Schema tableSchema = table.schema();\n\n    List<GenericData.Record> expected = RandomData.generateList(tableSchema, 100, 1L);\n\n    try (FileAppender<GenericData.Record> writer = Parquet.write(localOutput(parquetFile))\n        .schema(tableSchema)\n        .build()) {\n      writer.addAll(expected);\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withFileSizeInBytes(parquetFile.length())\n        .withPath(parquetFile.toString())\n        .withMetrics(fromInputFile(localInput(parquetFile)))\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(location.toString());\n\n    List<Row> rows = df.collectAsList();\n    Assert.assertEquals(\"Should contain 100 rows\", 100, rows.size());\n\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(tableSchema.asStruct(), expected.get(i), rows.get(i));\n    }\n  }\n","date":"2019-03-21 07:25:05","endLine":124,"groupId":"1813","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"writeAndValidate","params":"(Schemaschema)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/04/75f4082cd25b7237f4bf7c69a451d665a257aa.src","preCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    Assume.assumeTrue(\"Cannot handle non-string map keys in parquet-avro\",\n        null == TypeUtil.find(\n            schema,\n            type -> type.isMapType() && type.asMapType().keyType() != Types.StringType.get()));\n\n    File parent = temp.newFolder(\"parquet\");\n    File location = new File(parent, \"test\");\n    File dataFolder = new File(location, \"data\");\n    dataFolder.mkdirs();\n\n    File parquetFile = new File(dataFolder,\n        FileFormat.PARQUET.addExtension(UUID.randomUUID().toString()));\n\n    HadoopTables tables = new HadoopTables(CONF);\n    Table table = tables.create(schema, PartitionSpec.unpartitioned(), location.toString());\n\n    \r\n    \r\n    Schema tableSchema = table.schema();\n\n    List<GenericData.Record> expected = RandomData.generateList(tableSchema, 100, 1L);\n\n    try (FileAppender<GenericData.Record> writer = Parquet.write(localOutput(parquetFile))\n        .schema(tableSchema)\n        .build()) {\n      writer.addAll(expected);\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withFileSizeInBytes(parquetFile.length())\n        .withPath(parquetFile.toString())\n        .withMetrics(fromInputFile(localInput(parquetFile)))\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(location.toString());\n\n    List<Row> rows = df.collectAsList();\n    Assert.assertEquals(\"Should contain 100 rows\", 100, rows.size());\n\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(tableSchema.asStruct(), expected.get(i), rows.get(i));\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestParquetScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":77,"status":"B"},{"authorDate":"2019-03-21 07:25:05","commitOrder":1,"curCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    File parent = temp.newFolder(\"avro\");\n    File location = new File(parent, \"test\");\n    File dataFolder = new File(location, \"data\");\n    dataFolder.mkdirs();\n\n    File avroFile = new File(dataFolder,\n        FileFormat.AVRO.addExtension(UUID.randomUUID().toString()));\n\n    HadoopTables tables = new HadoopTables(CONF);\n    Table table = tables.create(schema, PartitionSpec.unpartitioned(), location.toString());\n\n    \r\n    \r\n    Schema tableSchema = table.schema();\n\n    List<Record> expected = RandomData.generateList(tableSchema, 100, 1L);\n\n    try (FileAppender<Record> writer = Avro.write(localOutput(avroFile))\n        .schema(tableSchema)\n        .build()) {\n      writer.addAll(expected);\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(100)\n        .withFileSizeInBytes(avroFile.length())\n        .withPath(avroFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(location.toString());\n\n    List<Row> rows = df.collectAsList();\n    Assert.assertEquals(\"Should contain 100 rows\", 100, rows.size());\n\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(tableSchema.asStruct(), expected.get(i), rows.get(i));\n    }\n  }\n","date":"2019-03-21 07:25:05","endLine":113,"groupId":"4144","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"writeAndValidate","params":"(Schemaschema)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/81/47fa10ef4da7e59c973678dfb6a47b25a0f2fc.src","preCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    File parent = temp.newFolder(\"avro\");\n    File location = new File(parent, \"test\");\n    File dataFolder = new File(location, \"data\");\n    dataFolder.mkdirs();\n\n    File avroFile = new File(dataFolder,\n        FileFormat.AVRO.addExtension(UUID.randomUUID().toString()));\n\n    HadoopTables tables = new HadoopTables(CONF);\n    Table table = tables.create(schema, PartitionSpec.unpartitioned(), location.toString());\n\n    \r\n    \r\n    Schema tableSchema = table.schema();\n\n    List<Record> expected = RandomData.generateList(tableSchema, 100, 1L);\n\n    try (FileAppender<Record> writer = Avro.write(localOutput(avroFile))\n        .schema(tableSchema)\n        .build()) {\n      writer.addAll(expected);\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(100)\n        .withFileSizeInBytes(avroFile.length())\n        .withPath(avroFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(location.toString());\n\n    List<Row> rows = df.collectAsList();\n    Assert.assertEquals(\"Should contain 100 rows\", 100, rows.size());\n\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(tableSchema.asStruct(), expected.get(i), rows.get(i));\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestAvroScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":71,"status":"B"}],"commitId":"c20927801a369104e5ea510470e1cf7c8e28b808","commitMessage":"@@@Rename packages to org.apache.iceberg (#138)\n\n* Move all packages by directory (but don't change references)\n* Rename all references from com.netflix.iceberg to org.apache.iceberg\n* Reorganize all imports due to new package name.\n  Previous commit only did a string find-replace.  which made all the imports out of order. Use an IDE to auto-sort all imports.\n\n","date":"2019-03-21 07:25:05","modifiedFileCount":"0","status":"B","submitter":"mccheah"},{"authorTime":"2019-03-21 07:25:05","codes":[{"authorDate":"2019-05-17 05:39:07","commitOrder":2,"curCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    Assume.assumeTrue(\"Cannot handle non-string map keys in parquet-avro\",\n        null == TypeUtil.find(\n            schema,\n            type -> type.isMapType() && type.asMapType().keyType() != Types.StringType.get()));\n\n    File parent = temp.newFolder(\"parquet\");\n    File location = new File(parent, \"test\");\n    File dataFolder = new File(location, \"data\");\n    dataFolder.mkdirs();\n\n    File parquetFile = new File(dataFolder,\n        FileFormat.PARQUET.addExtension(UUID.randomUUID().toString()));\n\n    HadoopTables tables = new HadoopTables(CONF);\n    Table table = tables.create(schema, PartitionSpec.unpartitioned(), location.toString());\n\n    \r\n    \r\n    Schema tableSchema = table.schema();\n\n    List<GenericData.Record> expected = RandomData.generateList(tableSchema, 100, 1L);\n\n    try (FileAppender<GenericData.Record> writer = Parquet.write(localOutput(parquetFile))\n        .schema(tableSchema)\n        .build()) {\n      writer.addAll(expected);\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withFileSizeInBytes(parquetFile.length())\n        .withPath(parquetFile.toString())\n        .withMetrics(fileMetrics(localInput(parquetFile)))\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(location.toString());\n\n    List<Row> rows = df.collectAsList();\n    Assert.assertEquals(\"Should contain 100 rows\", 100, rows.size());\n\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(tableSchema.asStruct(), expected.get(i), rows.get(i));\n    }\n  }\n","date":"2019-05-17 05:39:07","endLine":124,"groupId":"3159","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"writeAndValidate","params":"(Schemaschema)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/ce/d85d40a36f37343859a192f9c8513cdddc1d99.src","preCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    Assume.assumeTrue(\"Cannot handle non-string map keys in parquet-avro\",\n        null == TypeUtil.find(\n            schema,\n            type -> type.isMapType() && type.asMapType().keyType() != Types.StringType.get()));\n\n    File parent = temp.newFolder(\"parquet\");\n    File location = new File(parent, \"test\");\n    File dataFolder = new File(location, \"data\");\n    dataFolder.mkdirs();\n\n    File parquetFile = new File(dataFolder,\n        FileFormat.PARQUET.addExtension(UUID.randomUUID().toString()));\n\n    HadoopTables tables = new HadoopTables(CONF);\n    Table table = tables.create(schema, PartitionSpec.unpartitioned(), location.toString());\n\n    \r\n    \r\n    Schema tableSchema = table.schema();\n\n    List<GenericData.Record> expected = RandomData.generateList(tableSchema, 100, 1L);\n\n    try (FileAppender<GenericData.Record> writer = Parquet.write(localOutput(parquetFile))\n        .schema(tableSchema)\n        .build()) {\n      writer.addAll(expected);\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withFileSizeInBytes(parquetFile.length())\n        .withPath(parquetFile.toString())\n        .withMetrics(fromInputFile(localInput(parquetFile)))\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(location.toString());\n\n    List<Row> rows = df.collectAsList();\n    Assert.assertEquals(\"Should contain 100 rows\", 100, rows.size());\n\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(tableSchema.asStruct(), expected.get(i), rows.get(i));\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestParquetScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":77,"status":"M"},{"authorDate":"2019-03-21 07:25:05","commitOrder":2,"curCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    File parent = temp.newFolder(\"avro\");\n    File location = new File(parent, \"test\");\n    File dataFolder = new File(location, \"data\");\n    dataFolder.mkdirs();\n\n    File avroFile = new File(dataFolder,\n        FileFormat.AVRO.addExtension(UUID.randomUUID().toString()));\n\n    HadoopTables tables = new HadoopTables(CONF);\n    Table table = tables.create(schema, PartitionSpec.unpartitioned(), location.toString());\n\n    \r\n    \r\n    Schema tableSchema = table.schema();\n\n    List<Record> expected = RandomData.generateList(tableSchema, 100, 1L);\n\n    try (FileAppender<Record> writer = Avro.write(localOutput(avroFile))\n        .schema(tableSchema)\n        .build()) {\n      writer.addAll(expected);\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(100)\n        .withFileSizeInBytes(avroFile.length())\n        .withPath(avroFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(location.toString());\n\n    List<Row> rows = df.collectAsList();\n    Assert.assertEquals(\"Should contain 100 rows\", 100, rows.size());\n\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(tableSchema.asStruct(), expected.get(i), rows.get(i));\n    }\n  }\n","date":"2019-03-21 07:25:05","endLine":113,"groupId":"4144","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"writeAndValidate","params":"(Schemaschema)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/81/47fa10ef4da7e59c973678dfb6a47b25a0f2fc.src","preCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    File parent = temp.newFolder(\"avro\");\n    File location = new File(parent, \"test\");\n    File dataFolder = new File(location, \"data\");\n    dataFolder.mkdirs();\n\n    File avroFile = new File(dataFolder,\n        FileFormat.AVRO.addExtension(UUID.randomUUID().toString()));\n\n    HadoopTables tables = new HadoopTables(CONF);\n    Table table = tables.create(schema, PartitionSpec.unpartitioned(), location.toString());\n\n    \r\n    \r\n    Schema tableSchema = table.schema();\n\n    List<Record> expected = RandomData.generateList(tableSchema, 100, 1L);\n\n    try (FileAppender<Record> writer = Avro.write(localOutput(avroFile))\n        .schema(tableSchema)\n        .build()) {\n      writer.addAll(expected);\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(100)\n        .withFileSizeInBytes(avroFile.length())\n        .withPath(avroFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(location.toString());\n\n    List<Row> rows = df.collectAsList();\n    Assert.assertEquals(\"Should contain 100 rows\", 100, rows.size());\n\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(tableSchema.asStruct(), expected.get(i), rows.get(i));\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestAvroScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":71,"status":"N"}],"commitId":"549da809490976b53a13b14596dd240ed74bce5e","commitMessage":"@@@Store split offsets for Parquet files (#186)\n\n","date":"2019-05-17 05:39:07","modifiedFileCount":"10","status":"M","submitter":"Samarth Jain"},{"authorTime":"2019-03-21 07:25:05","codes":[{"authorDate":"2019-07-07 03:01:03","commitOrder":3,"curCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    Assume.assumeTrue(\"Cannot handle non-string map keys in parquet-avro\",\n        null == TypeUtil.find(\n            schema,\n            type -> type.isMapType() && type.asMapType().keyType() != Types.StringType.get()));\n\n    File parent = temp.newFolder(\"parquet\");\n    File location = new File(parent, \"test\");\n    File dataFolder = new File(location, \"data\");\n    dataFolder.mkdirs();\n\n    File parquetFile = new File(dataFolder,\n        FileFormat.PARQUET.addExtension(UUID.randomUUID().toString()));\n\n    HadoopTables tables = new HadoopTables(CONF);\n    Table table = tables.create(schema, PartitionSpec.unpartitioned(), location.toString());\n\n    \r\n    \r\n    Schema tableSchema = table.schema();\n\n    List<GenericData.Record> expected = RandomData.generateList(tableSchema, 100, 1L);\n\n    try (FileAppender<GenericData.Record> writer = Parquet.write(localOutput(parquetFile))\n        .schema(tableSchema)\n        .build()) {\n      writer.addAll(expected);\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withFileSizeInBytes(parquetFile.length())\n        .withPath(parquetFile.toString())\n        .withRecordCount(100)\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(location.toString());\n\n    List<Row> rows = df.collectAsList();\n    Assert.assertEquals(\"Should contain 100 rows\", 100, rows.size());\n\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(tableSchema.asStruct(), expected.get(i), rows.get(i));\n    }\n  }\n","date":"2019-07-07 03:01:03","endLine":122,"groupId":"1813","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"writeAndValidate","params":"(Schemaschema)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/fc/ecb17c5b4dd98584533bd92d8d46c39590de9b.src","preCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    Assume.assumeTrue(\"Cannot handle non-string map keys in parquet-avro\",\n        null == TypeUtil.find(\n            schema,\n            type -> type.isMapType() && type.asMapType().keyType() != Types.StringType.get()));\n\n    File parent = temp.newFolder(\"parquet\");\n    File location = new File(parent, \"test\");\n    File dataFolder = new File(location, \"data\");\n    dataFolder.mkdirs();\n\n    File parquetFile = new File(dataFolder,\n        FileFormat.PARQUET.addExtension(UUID.randomUUID().toString()));\n\n    HadoopTables tables = new HadoopTables(CONF);\n    Table table = tables.create(schema, PartitionSpec.unpartitioned(), location.toString());\n\n    \r\n    \r\n    Schema tableSchema = table.schema();\n\n    List<GenericData.Record> expected = RandomData.generateList(tableSchema, 100, 1L);\n\n    try (FileAppender<GenericData.Record> writer = Parquet.write(localOutput(parquetFile))\n        .schema(tableSchema)\n        .build()) {\n      writer.addAll(expected);\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withFileSizeInBytes(parquetFile.length())\n        .withPath(parquetFile.toString())\n        .withMetrics(fileMetrics(localInput(parquetFile)))\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(location.toString());\n\n    List<Row> rows = df.collectAsList();\n    Assert.assertEquals(\"Should contain 100 rows\", 100, rows.size());\n\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(tableSchema.asStruct(), expected.get(i), rows.get(i));\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestParquetScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":75,"status":"M"},{"authorDate":"2019-03-21 07:25:05","commitOrder":3,"curCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    File parent = temp.newFolder(\"avro\");\n    File location = new File(parent, \"test\");\n    File dataFolder = new File(location, \"data\");\n    dataFolder.mkdirs();\n\n    File avroFile = new File(dataFolder,\n        FileFormat.AVRO.addExtension(UUID.randomUUID().toString()));\n\n    HadoopTables tables = new HadoopTables(CONF);\n    Table table = tables.create(schema, PartitionSpec.unpartitioned(), location.toString());\n\n    \r\n    \r\n    Schema tableSchema = table.schema();\n\n    List<Record> expected = RandomData.generateList(tableSchema, 100, 1L);\n\n    try (FileAppender<Record> writer = Avro.write(localOutput(avroFile))\n        .schema(tableSchema)\n        .build()) {\n      writer.addAll(expected);\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(100)\n        .withFileSizeInBytes(avroFile.length())\n        .withPath(avroFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(location.toString());\n\n    List<Row> rows = df.collectAsList();\n    Assert.assertEquals(\"Should contain 100 rows\", 100, rows.size());\n\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(tableSchema.asStruct(), expected.get(i), rows.get(i));\n    }\n  }\n","date":"2019-03-21 07:25:05","endLine":113,"groupId":"4144","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"writeAndValidate","params":"(Schemaschema)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/81/47fa10ef4da7e59c973678dfb6a47b25a0f2fc.src","preCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    File parent = temp.newFolder(\"avro\");\n    File location = new File(parent, \"test\");\n    File dataFolder = new File(location, \"data\");\n    dataFolder.mkdirs();\n\n    File avroFile = new File(dataFolder,\n        FileFormat.AVRO.addExtension(UUID.randomUUID().toString()));\n\n    HadoopTables tables = new HadoopTables(CONF);\n    Table table = tables.create(schema, PartitionSpec.unpartitioned(), location.toString());\n\n    \r\n    \r\n    Schema tableSchema = table.schema();\n\n    List<Record> expected = RandomData.generateList(tableSchema, 100, 1L);\n\n    try (FileAppender<Record> writer = Avro.write(localOutput(avroFile))\n        .schema(tableSchema)\n        .build()) {\n      writer.addAll(expected);\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(100)\n        .withFileSizeInBytes(avroFile.length())\n        .withPath(avroFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(location.toString());\n\n    List<Row> rows = df.collectAsList();\n    Assert.assertEquals(\"Should contain 100 rows\", 100, rows.size());\n\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(tableSchema.asStruct(), expected.get(i), rows.get(i));\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestAvroScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":71,"status":"N"}],"commitId":"3287fee28359d303df28fef6fc9ea7b95cfa348f","commitMessage":"@@@Truncate string and binary stats from Parquet files (#254)\n\nThe default truncate length is 16 bytes for binary and fixed.  and 16 characters for strings.","date":"2019-07-07 03:01:03","modifiedFileCount":"7","status":"M","submitter":"Vinitha Reddy Gankidi"},{"authorTime":"2019-03-21 07:25:05","codes":[{"authorDate":"2020-06-16 06:16:19","commitOrder":4,"curCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    Assume.assumeTrue(\"Cannot handle non-string map keys in parquet-avro\",\n        null == TypeUtil.find(\n            schema,\n            type -> type.isMapType() && type.asMapType().keyType() != Types.StringType.get()));\n\n    File parent = temp.newFolder(\"parquet\");\n    File location = new File(parent, \"test\");\n    File dataFolder = new File(location, \"data\");\n    dataFolder.mkdirs();\n\n    File parquetFile = new File(dataFolder,\n        FileFormat.PARQUET.addExtension(UUID.randomUUID().toString()));\n\n    HadoopTables tables = new HadoopTables(CONF);\n    Table table = tables.create(schema, PartitionSpec.unpartitioned(), location.toString());\n\n    \r\n    \r\n    Schema tableSchema = table.schema();\n\n    List<GenericData.Record> expected = RandomData.generateList(tableSchema, 100, 1L);\n\n    try (FileAppender<GenericData.Record> writer = Parquet.write(localOutput(parquetFile))\n        .schema(tableSchema)\n        .build()) {\n      writer.addAll(expected);\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withFileSizeInBytes(parquetFile.length())\n        .withPath(parquetFile.toString())\n        .withRecordCount(100)\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n    table.updateProperties().set(TableProperties.PARQUET_VECTORIZATION_ENABLED, String.valueOf(vectorized)).commit();\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(location.toString());\n\n    List<Row> rows = df.collectAsList();\n    Assert.assertEquals(\"Should contain 100 rows\", 100, rows.size());\n\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(tableSchema.asStruct(), expected.get(i), rows.get(i));\n    }\n  }\n","date":"2020-06-16 06:16:19","endLine":141,"groupId":"10375","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"writeAndValidate","params":"(Schemaschema)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/f1/71fdb5766ceff3a327132b82dce99a713d4e20.src","preCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    Assume.assumeTrue(\"Cannot handle non-string map keys in parquet-avro\",\n        null == TypeUtil.find(\n            schema,\n            type -> type.isMapType() && type.asMapType().keyType() != Types.StringType.get()));\n\n    File parent = temp.newFolder(\"parquet\");\n    File location = new File(parent, \"test\");\n    File dataFolder = new File(location, \"data\");\n    dataFolder.mkdirs();\n\n    File parquetFile = new File(dataFolder,\n        FileFormat.PARQUET.addExtension(UUID.randomUUID().toString()));\n\n    HadoopTables tables = new HadoopTables(CONF);\n    Table table = tables.create(schema, PartitionSpec.unpartitioned(), location.toString());\n\n    \r\n    \r\n    Schema tableSchema = table.schema();\n\n    List<GenericData.Record> expected = RandomData.generateList(tableSchema, 100, 1L);\n\n    try (FileAppender<GenericData.Record> writer = Parquet.write(localOutput(parquetFile))\n        .schema(tableSchema)\n        .build()) {\n      writer.addAll(expected);\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withFileSizeInBytes(parquetFile.length())\n        .withPath(parquetFile.toString())\n        .withRecordCount(100)\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(location.toString());\n\n    List<Row> rows = df.collectAsList();\n    Assert.assertEquals(\"Should contain 100 rows\", 100, rows.size());\n\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(tableSchema.asStruct(), expected.get(i), rows.get(i));\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestParquetScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":93,"status":"M"},{"authorDate":"2019-03-21 07:25:05","commitOrder":4,"curCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    File parent = temp.newFolder(\"avro\");\n    File location = new File(parent, \"test\");\n    File dataFolder = new File(location, \"data\");\n    dataFolder.mkdirs();\n\n    File avroFile = new File(dataFolder,\n        FileFormat.AVRO.addExtension(UUID.randomUUID().toString()));\n\n    HadoopTables tables = new HadoopTables(CONF);\n    Table table = tables.create(schema, PartitionSpec.unpartitioned(), location.toString());\n\n    \r\n    \r\n    Schema tableSchema = table.schema();\n\n    List<Record> expected = RandomData.generateList(tableSchema, 100, 1L);\n\n    try (FileAppender<Record> writer = Avro.write(localOutput(avroFile))\n        .schema(tableSchema)\n        .build()) {\n      writer.addAll(expected);\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(100)\n        .withFileSizeInBytes(avroFile.length())\n        .withPath(avroFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(location.toString());\n\n    List<Row> rows = df.collectAsList();\n    Assert.assertEquals(\"Should contain 100 rows\", 100, rows.size());\n\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(tableSchema.asStruct(), expected.get(i), rows.get(i));\n    }\n  }\n","date":"2019-03-21 07:25:05","endLine":113,"groupId":"10375","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"writeAndValidate","params":"(Schemaschema)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/81/47fa10ef4da7e59c973678dfb6a47b25a0f2fc.src","preCode":"  protected void writeAndValidate(Schema schema) throws IOException {\n    File parent = temp.newFolder(\"avro\");\n    File location = new File(parent, \"test\");\n    File dataFolder = new File(location, \"data\");\n    dataFolder.mkdirs();\n\n    File avroFile = new File(dataFolder,\n        FileFormat.AVRO.addExtension(UUID.randomUUID().toString()));\n\n    HadoopTables tables = new HadoopTables(CONF);\n    Table table = tables.create(schema, PartitionSpec.unpartitioned(), location.toString());\n\n    \r\n    \r\n    Schema tableSchema = table.schema();\n\n    List<Record> expected = RandomData.generateList(tableSchema, 100, 1L);\n\n    try (FileAppender<Record> writer = Avro.write(localOutput(avroFile))\n        .schema(tableSchema)\n        .build()) {\n      writer.addAll(expected);\n    }\n\n    DataFile file = DataFiles.builder(PartitionSpec.unpartitioned())\n        .withRecordCount(100)\n        .withFileSizeInBytes(avroFile.length())\n        .withPath(avroFile.toString())\n        .build();\n\n    table.newAppend().appendFile(file).commit();\n\n    Dataset<Row> df = spark.read()\n        .format(\"iceberg\")\n        .load(location.toString());\n\n    List<Row> rows = df.collectAsList();\n    Assert.assertEquals(\"Should contain 100 rows\", 100, rows.size());\n\n    for (int i = 0; i < expected.size(); i += 1) {\n      TestHelpers.assertEqualsSafe(tableSchema.asStruct(), expected.get(i), rows.get(i));\n    }\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestAvroScan.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":71,"status":"N"}],"commitId":"ffdcf09027e09460b7d7505e65aea119107934a3","commitMessage":"@@@Spark: Support vectorized Parquet reads for flat projections (#828)\n\n","date":"2020-06-16 06:16:19","modifiedFileCount":"21","status":"M","submitter":"Samarth Jain"}]
