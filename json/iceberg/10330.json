[{"authorTime":"2019-11-13 09:39:48","codes":[{"authorDate":"2019-11-13 09:39:48","commitOrder":1,"curCode":"  public void testNullableWithWriteOption() throws IOException {\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_NEW_DATA_LOCATION, targetPath);\n\n    spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .getOrCreate();\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    \r\n    new HadoopTables(spark.sparkContext().hadoopConfiguration())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    spark\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").option(\"check-nullability\", false).mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = spark.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n\n  }\n","date":"2019-11-13 09:39:48","endLine":265,"groupId":"3122","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testNullableWithWriteOption","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/4f/4843ece557eb2a1308939ae88d0f47622eb648.src","preCode":"  public void testNullableWithWriteOption() throws IOException {\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_NEW_DATA_LOCATION, targetPath);\n\n    spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .getOrCreate();\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    \r\n    new HadoopTables(spark.sparkContext().hadoopConfiguration())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    spark\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").option(\"check-nullability\", false).mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = spark.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWrites.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":224,"status":"B"},{"authorDate":"2019-11-13 09:39:48","commitOrder":1,"curCode":"  public void testNullableWithSparkSqlOption() throws IOException {\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_NEW_DATA_LOCATION, targetPath);\n\n    spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .getOrCreate();\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    SparkSession newSparkSession = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .config(\"spark.sql.iceberg.check-nullability\", false)\n        .getOrCreate();\n\n    \r\n    new HadoopTables(newSparkSession.sparkContext().hadoopConfiguration())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = newSparkSession.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n\n  }\n","date":"2019-11-13 09:39:48","endLine":315,"groupId":"2574","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testNullableWithSparkSqlOption","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/4f/4843ece557eb2a1308939ae88d0f47622eb648.src","preCode":"  public void testNullableWithSparkSqlOption() throws IOException {\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_NEW_DATA_LOCATION, targetPath);\n\n    spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .getOrCreate();\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    SparkSession newSparkSession = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .config(\"spark.sql.iceberg.check-nullability\", false)\n        .getOrCreate();\n\n    \r\n    new HadoopTables(newSparkSession.sparkContext().hadoopConfiguration())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = newSparkSession.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWrites.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":268,"status":"B"}],"commitId":"cd7cd7b7169c04c15c71f147a37fbe8b23ff95a2","commitMessage":"@@@Spark: Add option to allow writing optional to required fields (#514)\n\n","date":"2019-11-13 09:39:48","modifiedFileCount":"4","status":"B","submitter":"Andrei Ionescu"},{"authorTime":"2019-11-14 04:59:37","codes":[{"authorDate":"2019-11-14 04:59:37","commitOrder":2,"curCode":"  public void testNullableWithWriteOption() throws IOException {\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_NEW_DATA_LOCATION, targetPath);\n\n    spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .getOrCreate();\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    \r\n    new HadoopTables(spark.sessionState().newHadoopConf())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    spark\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").option(\"check-nullability\", false).mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = spark.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n\n  }\n","date":"2019-11-14 04:59:37","endLine":265,"groupId":"3122","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testNullableWithWriteOption","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/e1/89ab3a7da8eae1bbb131715e8b46a94c5457fa.src","preCode":"  public void testNullableWithWriteOption() throws IOException {\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_NEW_DATA_LOCATION, targetPath);\n\n    spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .getOrCreate();\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    \r\n    new HadoopTables(spark.sparkContext().hadoopConfiguration())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    spark\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").option(\"check-nullability\", false).mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = spark.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWrites.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":224,"status":"M"},{"authorDate":"2019-11-14 04:59:37","commitOrder":2,"curCode":"  public void testNullableWithSparkSqlOption() throws IOException {\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_NEW_DATA_LOCATION, targetPath);\n\n    spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .getOrCreate();\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    SparkSession newSparkSession = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .config(\"spark.sql.iceberg.check-nullability\", false)\n        .getOrCreate();\n\n    \r\n    new HadoopTables(newSparkSession.sessionState().newHadoopConf())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = newSparkSession.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n\n  }\n","date":"2019-11-14 04:59:37","endLine":315,"groupId":"2574","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testNullableWithSparkSqlOption","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/e1/89ab3a7da8eae1bbb131715e8b46a94c5457fa.src","preCode":"  public void testNullableWithSparkSqlOption() throws IOException {\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_NEW_DATA_LOCATION, targetPath);\n\n    spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .getOrCreate();\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    SparkSession newSparkSession = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .config(\"spark.sql.iceberg.check-nullability\", false)\n        .getOrCreate();\n\n    \r\n    new HadoopTables(newSparkSession.sparkContext().hadoopConfiguration())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = newSparkSession.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWrites.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":268,"status":"M"}],"commitId":"6f28abfa62838d531be4faa93273965665af933d","commitMessage":"@@@Use SessionState to load Hadoop conf (#642)\n\n","date":"2019-11-14 04:59:37","modifiedFileCount":"6","status":"M","submitter":"Anton Okolnychyi"},{"authorTime":"2020-06-30 08:56:05","codes":[{"authorDate":"2020-06-30 08:56:05","commitOrder":3,"curCode":"  public void testNullableWithWriteOption() throws IOException {\n    Assume.assumeTrue(\"Spark 3.0 rejects writing nulls to a required column\", spark.version().startsWith(\"2\"));\n\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_NEW_DATA_LOCATION, targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    \r\n    new HadoopTables(spark.sessionState().newHadoopConf())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    spark\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").option(\"check-nullability\", false).mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = spark.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n  }\n","date":"2020-06-30 08:56:05","endLine":273,"groupId":"3122","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testNullableWithWriteOption","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/cd/80e07d0a9e8d918d41f2c266aef83bea8788a7.src","preCode":"  public void testNullableWithWriteOption() throws IOException {\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_NEW_DATA_LOCATION, targetPath);\n\n    spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .getOrCreate();\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    \r\n    new HadoopTables(spark.sessionState().newHadoopConf())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    spark\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").option(\"check-nullability\", false).mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = spark.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWrites.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":236,"status":"M"},{"authorDate":"2020-06-30 08:56:05","commitOrder":3,"curCode":"  public void testNullableWithSparkSqlOption() throws IOException {\n    Assume.assumeTrue(\"Spark 3.0 rejects writing nulls to a required column\", spark.version().startsWith(\"2\"));\n\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_NEW_DATA_LOCATION, targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    SparkSession newSparkSession = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .config(\"spark.sql.iceberg.check-nullability\", false)\n        .getOrCreate();\n\n    \r\n    new HadoopTables(newSparkSession.sessionState().newHadoopConf())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = newSparkSession.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n\n  }\n","date":"2020-06-30 08:56:05","endLine":320,"groupId":"2574","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testNullableWithSparkSqlOption","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/cd/80e07d0a9e8d918d41f2c266aef83bea8788a7.src","preCode":"  public void testNullableWithSparkSqlOption() throws IOException {\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_NEW_DATA_LOCATION, targetPath);\n\n    spark = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .getOrCreate();\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    SparkSession newSparkSession = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .config(\"spark.sql.iceberg.check-nullability\", false)\n        .getOrCreate();\n\n    \r\n    new HadoopTables(newSparkSession.sessionState().newHadoopConf())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = newSparkSession.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWrites.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":276,"status":"M"}],"commitId":"51c930e33867e54b3d7e0159b11d6b9c4bc81f1a","commitMessage":"@@@Spark: Add Spark 3 data source classes (#1124)\n\n","date":"2020-06-30 08:56:05","modifiedFileCount":"7","status":"M","submitter":"Ryan Blue"},{"authorTime":"2020-06-30 08:56:05","codes":[{"authorDate":"2020-12-29 05:55:09","commitOrder":4,"curCode":"  public void testNullableWithWriteOption() throws IOException {\n    Assume.assumeTrue(\"Spark 3.0 rejects writing nulls to a required column\", spark.version().startsWith(\"2\"));\n\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_NEW_DATA_LOCATION, targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    \r\n    new HadoopTables(spark.sessionState().newHadoopConf())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    spark\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").option(SparkWriteOptions.CHECK_NULLABILITY, false)\n        .mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = spark.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n  }\n","date":"2020-12-29 05:55:09","endLine":305,"groupId":"3122","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testNullableWithWriteOption","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/01/5b819fa608bd8f55d1cdef6e5fe95282ba51a6.src","preCode":"  public void testNullableWithWriteOption() throws IOException {\n    Assume.assumeTrue(\"Spark 3.0 rejects writing nulls to a required column\", spark.version().startsWith(\"2\"));\n\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_NEW_DATA_LOCATION, targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    \r\n    new HadoopTables(spark.sessionState().newHadoopConf())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    spark\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").option(\"check-nullability\", false).mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = spark.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWrites.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":267,"status":"M"},{"authorDate":"2020-06-30 08:56:05","commitOrder":4,"curCode":"  public void testNullableWithSparkSqlOption() throws IOException {\n    Assume.assumeTrue(\"Spark 3.0 rejects writing nulls to a required column\", spark.version().startsWith(\"2\"));\n\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_NEW_DATA_LOCATION, targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    SparkSession newSparkSession = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .config(\"spark.sql.iceberg.check-nullability\", false)\n        .getOrCreate();\n\n    \r\n    new HadoopTables(newSparkSession.sessionState().newHadoopConf())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = newSparkSession.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n\n  }\n","date":"2020-06-30 08:56:05","endLine":320,"groupId":"2574","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testNullableWithSparkSqlOption","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/cd/80e07d0a9e8d918d41f2c266aef83bea8788a7.src","preCode":"  public void testNullableWithSparkSqlOption() throws IOException {\n    Assume.assumeTrue(\"Spark 3.0 rejects writing nulls to a required column\", spark.version().startsWith(\"2\"));\n\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_NEW_DATA_LOCATION, targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    SparkSession newSparkSession = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .config(\"spark.sql.iceberg.check-nullability\", false)\n        .getOrCreate();\n\n    \r\n    new HadoopTables(newSparkSession.sessionState().newHadoopConf())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = newSparkSession.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWrites.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":276,"status":"N"}],"commitId":"cbb244759ae218b85a7e68a1568fddaf815aab3c","commitMessage":"@@@Spark: Use constants for DF read and write options (#1933)\n\n","date":"2020-12-29 05:55:09","modifiedFileCount":"22","status":"M","submitter":"Karuppayya"},{"authorTime":"2021-08-24 00:27:42","codes":[{"authorDate":"2021-08-24 00:27:42","commitOrder":5,"curCode":"  public void testNullableWithWriteOption() throws IOException {\n    Assume.assumeTrue(\"Spark 3.0 rejects writing nulls to a required column\", spark.version().startsWith(\"2\"));\n\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_FOLDER_STORAGE_LOCATION, targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    \r\n    new HadoopTables(spark.sessionState().newHadoopConf())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    spark\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").option(SparkWriteOptions.CHECK_NULLABILITY, false)\n        .mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = spark.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n  }\n","date":"2021-08-24 00:27:42","endLine":305,"groupId":"3122","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testNullableWithWriteOption","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/8a/d8e2e2b8025ab8d69239195ffeca1fd25e4292.src","preCode":"  public void testNullableWithWriteOption() throws IOException {\n    Assume.assumeTrue(\"Spark 3.0 rejects writing nulls to a required column\", spark.version().startsWith(\"2\"));\n\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_NEW_DATA_LOCATION, targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    \r\n    new HadoopTables(spark.sessionState().newHadoopConf())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    spark\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").option(SparkWriteOptions.CHECK_NULLABILITY, false)\n        .mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = spark.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWrites.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":267,"status":"M"},{"authorDate":"2021-08-24 00:27:42","commitOrder":5,"curCode":"  public void testNullableWithSparkSqlOption() throws IOException {\n    Assume.assumeTrue(\"Spark 3.0 rejects writing nulls to a required column\", spark.version().startsWith(\"2\"));\n\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_FOLDER_STORAGE_LOCATION, targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    SparkSession newSparkSession = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .config(\"spark.sql.iceberg.check-nullability\", false)\n        .getOrCreate();\n\n    \r\n    new HadoopTables(newSparkSession.sessionState().newHadoopConf())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = newSparkSession.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n\n  }\n","date":"2021-08-24 00:27:42","endLine":352,"groupId":"2574","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testNullableWithSparkSqlOption","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/8a/d8e2e2b8025ab8d69239195ffeca1fd25e4292.src","preCode":"  public void testNullableWithSparkSqlOption() throws IOException {\n    Assume.assumeTrue(\"Spark 3.0 rejects writing nulls to a required column\", spark.version().startsWith(\"2\"));\n\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_NEW_DATA_LOCATION, targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    SparkSession newSparkSession = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .config(\"spark.sql.iceberg.check-nullability\", false)\n        .getOrCreate();\n\n    \r\n    new HadoopTables(newSparkSession.sessionState().newHadoopConf())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = newSparkSession.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWrites.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":308,"status":"M"}],"commitId":"aef12c0e702ef8c306826581893963e3dc5f6ba3","commitMessage":"@@@Core: Rename WRITE_NEW_DATA_LOCATION to WRITE_FOLDER_STORAGE_LOCATION (#2965)\n\n","date":"2021-08-24 00:27:42","modifiedFileCount":"7","status":"M","submitter":"Jack Ye"},{"authorTime":"2021-09-23 02:18:43","codes":[{"authorDate":"2021-09-23 02:18:43","commitOrder":6,"curCode":"  public void testNullableWithWriteOption() throws IOException {\n    Assume.assumeTrue(\"Spark 3.0 rejects writing nulls to a required column\", spark.version().startsWith(\"2\"));\n\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_DATA_LOCATION, targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    \r\n    new HadoopTables(spark.sessionState().newHadoopConf())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    spark\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").option(SparkWriteOptions.CHECK_NULLABILITY, false)\n        .mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = spark.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n  }\n","date":"2021-09-23 02:18:43","endLine":305,"groupId":"10330","id":11,"instanceNumber":1,"isCurCommit":1,"methodName":"testNullableWithWriteOption","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/36/c0541f46c5990676122784fe4ee294d37e3ade.src","preCode":"  public void testNullableWithWriteOption() throws IOException {\n    Assume.assumeTrue(\"Spark 3.0 rejects writing nulls to a required column\", spark.version().startsWith(\"2\"));\n\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_FOLDER_STORAGE_LOCATION, targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    \r\n    new HadoopTables(spark.sessionState().newHadoopConf())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    spark\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").option(SparkWriteOptions.CHECK_NULLABILITY, false)\n        .mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = spark.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWrites.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":267,"status":"M"},{"authorDate":"2021-09-23 02:18:43","commitOrder":6,"curCode":"  public void testNullableWithSparkSqlOption() throws IOException {\n    Assume.assumeTrue(\"Spark 3.0 rejects writing nulls to a required column\", spark.version().startsWith(\"2\"));\n\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_DATA_LOCATION, targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    SparkSession newSparkSession = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .config(\"spark.sql.iceberg.check-nullability\", false)\n        .getOrCreate();\n\n    \r\n    new HadoopTables(newSparkSession.sessionState().newHadoopConf())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = newSparkSession.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n\n  }\n","date":"2021-09-23 02:18:43","endLine":352,"groupId":"10330","id":12,"instanceNumber":2,"isCurCommit":1,"methodName":"testNullableWithSparkSqlOption","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/36/c0541f46c5990676122784fe4ee294d37e3ade.src","preCode":"  public void testNullableWithSparkSqlOption() throws IOException {\n    Assume.assumeTrue(\"Spark 3.0 rejects writing nulls to a required column\", spark.version().startsWith(\"2\"));\n\n    File location = new File(temp.newFolder(\"parquet\"), \"test\");\n    String sourcePath = String.format(\"%s/nullable_poc/sourceFolder/\", location.toString());\n    String targetPath = String.format(\"%s/nullable_poc/targetFolder/\", location.toString());\n\n    tableProperties = ImmutableMap.of(TableProperties.WRITE_FOLDER_STORAGE_LOCATION, targetPath);\n\n    \r\n    spark\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data1))\n        .write().parquet(sourcePath);\n\n    SparkSession newSparkSession = SparkSession.builder()\n        .master(\"local[2]\")\n        .appName(\"NullableTest\")\n        .config(\"spark.sql.iceberg.check-nullability\", false)\n        .getOrCreate();\n\n    \r\n    new HadoopTables(newSparkSession.sessionState().newHadoopConf())\n        .create(\n            icebergSchema,\n            PartitionSpec.builderFor(icebergSchema).identity(\"requiredField\").build(),\n            tableProperties,\n            targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(sparkSchema).json(\n        JavaSparkContext.fromSparkContext(spark.sparkContext()).parallelize(data0))\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    newSparkSession\n        .read().schema(SparkSchemaUtil.convert(icebergSchema)).parquet(sourcePath)\n        .write().format(\"iceberg\").mode(SaveMode.Append).save(targetPath);\n\n    \r\n    List<Row> rows = newSparkSession.read().format(\"iceberg\").load(targetPath).collectAsList();\n    Assert.assertEquals(\"Should contain 6 rows\", 6, rows.size());\n\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWrites.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":308,"status":"M"}],"commitId":"12e30ebaeb6dc0ff7e6bb01c0d9f8ae8a5f06ddf","commitMessage":"@@@Core: Prefer write.data.path to write.folder-storage.path or write.object-storage.path (#3094)\n\n","date":"2021-09-23 02:18:43","modifiedFileCount":"7","status":"M","submitter":"Yufei Gu"}]
