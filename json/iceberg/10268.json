[{"authorTime":"2019-03-21 07:25:05","codes":[{"authorDate":"2019-03-21 07:25:05","commitOrder":1,"curCode":"  public static List<Record> generateList(Schema schema, int numRecords, long seed) {\n    RandomDataGenerator generator = new RandomDataGenerator(schema, seed);\n    List<Record> records = Lists.newArrayListWithExpectedSize(numRecords);\n    for (int i = 0; i < numRecords; i += 1) {\n      records.add((Record) TypeUtil.visit(schema, generator));\n    }\n\n    return records;\n  }\n","date":"2019-03-21 07:25:05","endLine":59,"groupId":"2465","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"generateList","params":"(Schemaschema@intnumRecords@longseed)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/65/158a62212d96320fd84c6b3bec6a77b961725c.src","preCode":"  public static List<Record> generateList(Schema schema, int numRecords, long seed) {\n    RandomDataGenerator generator = new RandomDataGenerator(schema, seed);\n    List<Record> records = Lists.newArrayListWithExpectedSize(numRecords);\n    for (int i = 0; i < numRecords; i += 1) {\n      records.add((Record) TypeUtil.visit(schema, generator));\n    }\n\n    return records;\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":51,"status":"B"},{"authorDate":"2019-03-21 07:25:05","commitOrder":1,"curCode":"  public static List<Record> generate(Schema schema, int numRecords, long seed) {\n    RandomDataGenerator generator = new RandomDataGenerator(schema, seed);\n    List<Record> records = Lists.newArrayListWithExpectedSize(numRecords);\n    for (int i = 0; i < numRecords; i += 1) {\n      records.add((Record) TypeUtil.visit(schema, generator));\n    }\n\n    return records;\n  }\n","date":"2019-03-21 07:25:05","endLine":51,"groupId":"2465","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"generate","params":"(Schemaschema@intnumRecords@longseed)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a6/86cfa5e323a2610fe21f1bfde498975159e7a1.src","preCode":"  public static List<Record> generate(Schema schema, int numRecords, long seed) {\n    RandomDataGenerator generator = new RandomDataGenerator(schema, seed);\n    List<Record> records = Lists.newArrayListWithExpectedSize(numRecords);\n    for (int i = 0; i < numRecords; i += 1) {\n      records.add((Record) TypeUtil.visit(schema, generator));\n    }\n\n    return records;\n  }\n","realPath":"core/src/test/java/org/apache/iceberg/avro/RandomAvroData.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":43,"status":"B"}],"commitId":"c20927801a369104e5ea510470e1cf7c8e28b808","commitMessage":"@@@Rename packages to org.apache.iceberg (#138)\n\n* Move all packages by directory (but don't change references)\n* Rename all references from com.netflix.iceberg to org.apache.iceberg\n* Reorganize all imports due to new package name.\n  Previous commit only did a string find-replace.  which made all the imports out of order. Use an IDE to auto-sort all imports.\n\n","date":"2019-03-21 07:25:05","modifiedFileCount":"0","status":"B","submitter":"mccheah"},{"authorTime":"2019-03-21 07:25:05","codes":[{"authorDate":"2020-06-16 06:16:19","commitOrder":2,"curCode":"  public static List<Record> generateList(Schema schema, int numRecords, long seed) {\n    RandomDataGenerator generator = new RandomDataGenerator(schema, seed, DEFAULT_NULL_PERCENTAGE);\n    List<Record> records = Lists.newArrayListWithExpectedSize(numRecords);\n    for (int i = 0; i < numRecords; i += 1) {\n      records.add((Record) TypeUtil.visit(schema, generator));\n    }\n\n    return records;\n  }\n","date":"2020-06-16 06:16:19","endLine":68,"groupId":"10268","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"generateList","params":"(Schemaschema@intnumRecords@longseed)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/f9/9c0fccb89c4208d661ef47bcec51a05dbcb534.src","preCode":"  public static List<Record> generateList(Schema schema, int numRecords, long seed) {\n    RandomDataGenerator generator = new RandomDataGenerator(schema, seed);\n    List<Record> records = Lists.newArrayListWithExpectedSize(numRecords);\n    for (int i = 0; i < numRecords; i += 1) {\n      records.add((Record) TypeUtil.visit(schema, generator));\n    }\n\n    return records;\n  }\n","realPath":"spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":60,"status":"M"},{"authorDate":"2019-03-21 07:25:05","commitOrder":2,"curCode":"  public static List<Record> generate(Schema schema, int numRecords, long seed) {\n    RandomDataGenerator generator = new RandomDataGenerator(schema, seed);\n    List<Record> records = Lists.newArrayListWithExpectedSize(numRecords);\n    for (int i = 0; i < numRecords; i += 1) {\n      records.add((Record) TypeUtil.visit(schema, generator));\n    }\n\n    return records;\n  }\n","date":"2019-03-21 07:25:05","endLine":51,"groupId":"10268","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"generate","params":"(Schemaschema@intnumRecords@longseed)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/a6/86cfa5e323a2610fe21f1bfde498975159e7a1.src","preCode":"  public static List<Record> generate(Schema schema, int numRecords, long seed) {\n    RandomDataGenerator generator = new RandomDataGenerator(schema, seed);\n    List<Record> records = Lists.newArrayListWithExpectedSize(numRecords);\n    for (int i = 0; i < numRecords; i += 1) {\n      records.add((Record) TypeUtil.visit(schema, generator));\n    }\n\n    return records;\n  }\n","realPath":"core/src/test/java/org/apache/iceberg/avro/RandomAvroData.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":43,"status":"N"}],"commitId":"ffdcf09027e09460b7d7505e65aea119107934a3","commitMessage":"@@@Spark: Support vectorized Parquet reads for flat projections (#828)\n\n","date":"2020-06-16 06:16:19","modifiedFileCount":"21","status":"M","submitter":"Samarth Jain"}]
