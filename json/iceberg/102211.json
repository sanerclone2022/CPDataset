[{"authorTime":"2020-08-29 01:19:33","codes":[{"authorDate":"2020-08-29 01:19:33","commitOrder":1,"curCode":"  public void testCommitTxnWithoutDataFiles() throws Exception {\n    long checkpointId = 0;\n    long timestamp = 0;\n    JobID jobId = new JobID();\n    try (OneInputStreamOperatorTestHarness<DataFile, Void> harness = createStreamSink(jobId)) {\n      harness.setup();\n      harness.open();\n\n      SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList());\n      assertSnapshotSize(0);\n      assertMaxCommittedCheckpointId(jobId, -1L);\n\n      \r\n      \r\n      for (int i = 1; i <= 3; i++) {\n        harness.snapshot(++checkpointId, ++timestamp);\n        harness.notifyOfCompletedCheckpoint(checkpointId);\n        assertSnapshotSize(i);\n        assertMaxCommittedCheckpointId(jobId, checkpointId);\n      }\n    }\n  }\n","date":"2020-08-29 01:19:33","endLine":119,"groupId":"4","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testCommitTxnWithoutDataFiles","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/e5/a9a8d4766bfb64ab0af5a242b3dc34f9f377ce.src","preCode":"  public void testCommitTxnWithoutDataFiles() throws Exception {\n    long checkpointId = 0;\n    long timestamp = 0;\n    JobID jobId = new JobID();\n    try (OneInputStreamOperatorTestHarness<DataFile, Void> harness = createStreamSink(jobId)) {\n      harness.setup();\n      harness.open();\n\n      SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList());\n      assertSnapshotSize(0);\n      assertMaxCommittedCheckpointId(jobId, -1L);\n\n      \r\n      \r\n      for (int i = 1; i <= 3; i++) {\n        harness.snapshot(++checkpointId, ++timestamp);\n        harness.notifyOfCompletedCheckpoint(checkpointId);\n        assertSnapshotSize(i);\n        assertMaxCommittedCheckpointId(jobId, checkpointId);\n      }\n    }\n  }\n","realPath":"flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergFilesCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":98,"status":"B"},{"authorDate":"2020-08-29 01:19:33","commitOrder":1,"curCode":"  public void testStartAnotherJobToWriteSameTable() throws Exception {\n    long checkpointId = 0;\n    long timestamp = 0;\n    List<RowData> rows = Lists.newArrayList();\n    List<RowData> tableRows = Lists.newArrayList();\n\n    JobID oldJobId = new JobID();\n    try (OneInputStreamOperatorTestHarness<DataFile, Void> harness = createStreamSink(oldJobId)) {\n      harness.setup();\n      harness.open();\n\n      assertSnapshotSize(0);\n      assertMaxCommittedCheckpointId(oldJobId, -1L);\n\n      for (int i = 1; i <= 3; i++) {\n        rows.add(SimpleDataUtil.createRowData(i, \"hello\" + i));\n        tableRows.addAll(rows);\n\n        DataFile dataFile = writeDataFile(String.format(\"data-%d\", i), rows);\n        harness.processElement(dataFile, ++timestamp);\n        harness.snapshot(++checkpointId, ++timestamp);\n\n        harness.notifyOfCompletedCheckpoint(checkpointId);\n        SimpleDataUtil.assertTableRows(tablePath, tableRows);\n        assertSnapshotSize(i);\n        assertMaxCommittedCheckpointId(oldJobId, checkpointId);\n      }\n    }\n\n    \r\n    checkpointId = 0;\n    timestamp = 0;\n    JobID newJobId = new JobID();\n    try (OneInputStreamOperatorTestHarness<DataFile, Void> harness = createStreamSink(newJobId)) {\n      harness.setup();\n      harness.open();\n\n      assertSnapshotSize(3);\n      assertMaxCommittedCheckpointId(oldJobId, 3);\n      assertMaxCommittedCheckpointId(newJobId, -1);\n\n      rows.add(SimpleDataUtil.createRowData(2, \"world\"));\n      tableRows.addAll(rows);\n\n      DataFile dataFile = writeDataFile(\"data-new-1\", rows);\n      harness.processElement(dataFile, ++timestamp);\n      harness.snapshot(++checkpointId, ++timestamp);\n\n      harness.notifyOfCompletedCheckpoint(checkpointId);\n      SimpleDataUtil.assertTableRows(tablePath, tableRows);\n      assertSnapshotSize(4);\n      assertMaxCommittedCheckpointId(newJobId, checkpointId);\n    }\n  }\n","date":"2020-08-29 01:19:33","endLine":390,"groupId":"10","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testStartAnotherJobToWriteSameTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/e5/a9a8d4766bfb64ab0af5a242b3dc34f9f377ce.src","preCode":"  public void testStartAnotherJobToWriteSameTable() throws Exception {\n    long checkpointId = 0;\n    long timestamp = 0;\n    List<RowData> rows = Lists.newArrayList();\n    List<RowData> tableRows = Lists.newArrayList();\n\n    JobID oldJobId = new JobID();\n    try (OneInputStreamOperatorTestHarness<DataFile, Void> harness = createStreamSink(oldJobId)) {\n      harness.setup();\n      harness.open();\n\n      assertSnapshotSize(0);\n      assertMaxCommittedCheckpointId(oldJobId, -1L);\n\n      for (int i = 1; i <= 3; i++) {\n        rows.add(SimpleDataUtil.createRowData(i, \"hello\" + i));\n        tableRows.addAll(rows);\n\n        DataFile dataFile = writeDataFile(String.format(\"data-%d\", i), rows);\n        harness.processElement(dataFile, ++timestamp);\n        harness.snapshot(++checkpointId, ++timestamp);\n\n        harness.notifyOfCompletedCheckpoint(checkpointId);\n        SimpleDataUtil.assertTableRows(tablePath, tableRows);\n        assertSnapshotSize(i);\n        assertMaxCommittedCheckpointId(oldJobId, checkpointId);\n      }\n    }\n\n    \r\n    checkpointId = 0;\n    timestamp = 0;\n    JobID newJobId = new JobID();\n    try (OneInputStreamOperatorTestHarness<DataFile, Void> harness = createStreamSink(newJobId)) {\n      harness.setup();\n      harness.open();\n\n      assertSnapshotSize(3);\n      assertMaxCommittedCheckpointId(oldJobId, 3);\n      assertMaxCommittedCheckpointId(newJobId, -1);\n\n      rows.add(SimpleDataUtil.createRowData(2, \"world\"));\n      tableRows.addAll(rows);\n\n      DataFile dataFile = writeDataFile(\"data-new-1\", rows);\n      harness.processElement(dataFile, ++timestamp);\n      harness.snapshot(++checkpointId, ++timestamp);\n\n      harness.notifyOfCompletedCheckpoint(checkpointId);\n      SimpleDataUtil.assertTableRows(tablePath, tableRows);\n      assertSnapshotSize(4);\n      assertMaxCommittedCheckpointId(newJobId, checkpointId);\n    }\n  }\n","realPath":"flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergFilesCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":337,"status":"B"}],"commitId":"f950a3e63c98e88b1f8905faacf33a1d5d31e4c0","commitMessage":"@@@Flink: Add operator to collect data files and append to a table (#1185)\n\n","date":"2020-08-29 01:19:33","modifiedFileCount":"2","status":"B","submitter":"openinx"},{"authorTime":"2020-10-28 12:06:53","codes":[{"authorDate":"2020-10-28 12:06:53","commitOrder":2,"curCode":"  public void testCommitTxnWithoutDataFiles() throws Exception {\n    long checkpointId = 0;\n    long timestamp = 0;\n    JobID jobId = new JobID();\n    try (OneInputStreamOperatorTestHarness<DataFile, Void> harness = createStreamSink(jobId)) {\n      harness.setup();\n      harness.open();\n\n      SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList());\n      assertSnapshotSize(0);\n      assertMaxCommittedCheckpointId(jobId, -1L);\n\n      \r\n      \r\n      for (int i = 1; i <= 3; i++) {\n        harness.snapshot(++checkpointId, ++timestamp);\n        assertFlinkManifests(0);\n\n        harness.notifyOfCompletedCheckpoint(checkpointId);\n        assertFlinkManifests(0);\n\n        assertSnapshotSize(i);\n        assertMaxCommittedCheckpointId(jobId, checkpointId);\n      }\n    }\n  }\n","date":"2020-10-28 12:06:53","endLine":137,"groupId":"4","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testCommitTxnWithoutDataFiles","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/58/3b6f18e336b3e4c2ffb4518a11ccf655d0ef76.src","preCode":"  public void testCommitTxnWithoutDataFiles() throws Exception {\n    long checkpointId = 0;\n    long timestamp = 0;\n    JobID jobId = new JobID();\n    try (OneInputStreamOperatorTestHarness<DataFile, Void> harness = createStreamSink(jobId)) {\n      harness.setup();\n      harness.open();\n\n      SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList());\n      assertSnapshotSize(0);\n      assertMaxCommittedCheckpointId(jobId, -1L);\n\n      \r\n      \r\n      for (int i = 1; i <= 3; i++) {\n        harness.snapshot(++checkpointId, ++timestamp);\n        harness.notifyOfCompletedCheckpoint(checkpointId);\n        assertSnapshotSize(i);\n        assertMaxCommittedCheckpointId(jobId, checkpointId);\n      }\n    }\n  }\n","realPath":"flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergFilesCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":112,"status":"M"},{"authorDate":"2020-10-28 12:06:53","commitOrder":2,"curCode":"  public void testStartAnotherJobToWriteSameTable() throws Exception {\n    long checkpointId = 0;\n    long timestamp = 0;\n    List<RowData> rows = Lists.newArrayList();\n    List<RowData> tableRows = Lists.newArrayList();\n\n    JobID oldJobId = new JobID();\n    try (OneInputStreamOperatorTestHarness<DataFile, Void> harness = createStreamSink(oldJobId)) {\n      harness.setup();\n      harness.open();\n\n      assertSnapshotSize(0);\n      assertMaxCommittedCheckpointId(oldJobId, -1L);\n\n      for (int i = 1; i <= 3; i++) {\n        rows.add(SimpleDataUtil.createRowData(i, \"hello\" + i));\n        tableRows.addAll(rows);\n\n        DataFile dataFile = writeDataFile(String.format(\"data-%d\", i), rows);\n        harness.processElement(dataFile, ++timestamp);\n        harness.snapshot(++checkpointId, ++timestamp);\n        assertFlinkManifests(1);\n\n        harness.notifyOfCompletedCheckpoint(checkpointId);\n        assertFlinkManifests(0);\n\n        SimpleDataUtil.assertTableRows(tablePath, tableRows);\n        assertSnapshotSize(i);\n        assertMaxCommittedCheckpointId(oldJobId, checkpointId);\n      }\n    }\n\n    \r\n    checkpointId = 0;\n    timestamp = 0;\n    JobID newJobId = new JobID();\n    try (OneInputStreamOperatorTestHarness<DataFile, Void> harness = createStreamSink(newJobId)) {\n      harness.setup();\n      harness.open();\n\n      assertSnapshotSize(3);\n      assertMaxCommittedCheckpointId(oldJobId, 3);\n      assertMaxCommittedCheckpointId(newJobId, -1);\n\n      rows.add(SimpleDataUtil.createRowData(2, \"world\"));\n      tableRows.addAll(rows);\n\n      DataFile dataFile = writeDataFile(\"data-new-1\", rows);\n      harness.processElement(dataFile, ++timestamp);\n      harness.snapshot(++checkpointId, ++timestamp);\n      assertFlinkManifests(1);\n\n      harness.notifyOfCompletedCheckpoint(checkpointId);\n      assertFlinkManifests(0);\n      SimpleDataUtil.assertTableRows(tablePath, tableRows);\n      assertSnapshotSize(4);\n      assertMaxCommittedCheckpointId(newJobId, checkpointId);\n    }\n  }\n","date":"2020-10-28 12:06:53","endLine":485,"groupId":"10","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testStartAnotherJobToWriteSameTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/58/3b6f18e336b3e4c2ffb4518a11ccf655d0ef76.src","preCode":"  public void testStartAnotherJobToWriteSameTable() throws Exception {\n    long checkpointId = 0;\n    long timestamp = 0;\n    List<RowData> rows = Lists.newArrayList();\n    List<RowData> tableRows = Lists.newArrayList();\n\n    JobID oldJobId = new JobID();\n    try (OneInputStreamOperatorTestHarness<DataFile, Void> harness = createStreamSink(oldJobId)) {\n      harness.setup();\n      harness.open();\n\n      assertSnapshotSize(0);\n      assertMaxCommittedCheckpointId(oldJobId, -1L);\n\n      for (int i = 1; i <= 3; i++) {\n        rows.add(SimpleDataUtil.createRowData(i, \"hello\" + i));\n        tableRows.addAll(rows);\n\n        DataFile dataFile = writeDataFile(String.format(\"data-%d\", i), rows);\n        harness.processElement(dataFile, ++timestamp);\n        harness.snapshot(++checkpointId, ++timestamp);\n\n        harness.notifyOfCompletedCheckpoint(checkpointId);\n        SimpleDataUtil.assertTableRows(tablePath, tableRows);\n        assertSnapshotSize(i);\n        assertMaxCommittedCheckpointId(oldJobId, checkpointId);\n      }\n    }\n\n    \r\n    checkpointId = 0;\n    timestamp = 0;\n    JobID newJobId = new JobID();\n    try (OneInputStreamOperatorTestHarness<DataFile, Void> harness = createStreamSink(newJobId)) {\n      harness.setup();\n      harness.open();\n\n      assertSnapshotSize(3);\n      assertMaxCommittedCheckpointId(oldJobId, 3);\n      assertMaxCommittedCheckpointId(newJobId, -1);\n\n      rows.add(SimpleDataUtil.createRowData(2, \"world\"));\n      tableRows.addAll(rows);\n\n      DataFile dataFile = writeDataFile(\"data-new-1\", rows);\n      harness.processElement(dataFile, ++timestamp);\n      harness.snapshot(++checkpointId, ++timestamp);\n\n      harness.notifyOfCompletedCheckpoint(checkpointId);\n      SimpleDataUtil.assertTableRows(tablePath, tableRows);\n      assertSnapshotSize(4);\n      assertMaxCommittedCheckpointId(newJobId, checkpointId);\n    }\n  }\n","realPath":"flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergFilesCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":427,"status":"M"}],"commitId":"b9634c9511c8a028074e8e5bdc54f0db47058668","commitMessage":"@@@Flink: maintain the complete data files into manifest before checkpoint finished. (#1477)\n\n","date":"2020-10-28 12:06:53","modifiedFileCount":"3","status":"M","submitter":"openinx"},{"authorTime":"2020-12-19 01:57:04","codes":[{"authorDate":"2020-12-19 01:57:04","commitOrder":3,"curCode":"  public void testCommitTxnWithoutDataFiles() throws Exception {\n    long checkpointId = 0;\n    long timestamp = 0;\n    JobID jobId = new JobID();\n    try (OneInputStreamOperatorTestHarness<WriteResult, Void> harness = createStreamSink(jobId)) {\n      harness.setup();\n      harness.open();\n\n      SimpleDataUtil.assertTableRows(table, Lists.newArrayList());\n      assertSnapshotSize(0);\n      assertMaxCommittedCheckpointId(jobId, -1L);\n\n      \r\n      \r\n      for (int i = 1; i <= 3; i++) {\n        harness.snapshot(++checkpointId, ++timestamp);\n        assertFlinkManifests(0);\n\n        harness.notifyOfCompletedCheckpoint(checkpointId);\n        assertFlinkManifests(0);\n\n        assertSnapshotSize(i);\n        assertMaxCommittedCheckpointId(jobId, checkpointId);\n      }\n    }\n  }\n","date":"2020-12-19 01:57:04","endLine":143,"groupId":"102211","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testCommitTxnWithoutDataFiles","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/5e/c246b7af0789e09bc0d60caa755747bb78b8a1.src","preCode":"  public void testCommitTxnWithoutDataFiles() throws Exception {\n    long checkpointId = 0;\n    long timestamp = 0;\n    JobID jobId = new JobID();\n    try (OneInputStreamOperatorTestHarness<DataFile, Void> harness = createStreamSink(jobId)) {\n      harness.setup();\n      harness.open();\n\n      SimpleDataUtil.assertTableRows(tablePath, Lists.newArrayList());\n      assertSnapshotSize(0);\n      assertMaxCommittedCheckpointId(jobId, -1L);\n\n      \r\n      \r\n      for (int i = 1; i <= 3; i++) {\n        harness.snapshot(++checkpointId, ++timestamp);\n        assertFlinkManifests(0);\n\n        harness.notifyOfCompletedCheckpoint(checkpointId);\n        assertFlinkManifests(0);\n\n        assertSnapshotSize(i);\n        assertMaxCommittedCheckpointId(jobId, checkpointId);\n      }\n    }\n  }\n","realPath":"flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergFilesCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":118,"status":"M"},{"authorDate":"2020-12-19 01:57:04","commitOrder":3,"curCode":"  public void testStartAnotherJobToWriteSameTable() throws Exception {\n    long checkpointId = 0;\n    long timestamp = 0;\n    List<RowData> rows = Lists.newArrayList();\n    List<RowData> tableRows = Lists.newArrayList();\n\n    JobID oldJobId = new JobID();\n    try (OneInputStreamOperatorTestHarness<WriteResult, Void> harness = createStreamSink(oldJobId)) {\n      harness.setup();\n      harness.open();\n\n      assertSnapshotSize(0);\n      assertMaxCommittedCheckpointId(oldJobId, -1L);\n\n      for (int i = 1; i <= 3; i++) {\n        rows.add(SimpleDataUtil.createRowData(i, \"hello\" + i));\n        tableRows.addAll(rows);\n\n        DataFile dataFile = writeDataFile(String.format(\"data-%d\", i), rows);\n        harness.processElement(of(dataFile), ++timestamp);\n        harness.snapshot(++checkpointId, ++timestamp);\n        assertFlinkManifests(1);\n\n        harness.notifyOfCompletedCheckpoint(checkpointId);\n        assertFlinkManifests(0);\n\n        SimpleDataUtil.assertTableRows(table, tableRows);\n        assertSnapshotSize(i);\n        assertMaxCommittedCheckpointId(oldJobId, checkpointId);\n      }\n    }\n\n    \r\n    checkpointId = 0;\n    timestamp = 0;\n    JobID newJobId = new JobID();\n    try (OneInputStreamOperatorTestHarness<WriteResult, Void> harness = createStreamSink(newJobId)) {\n      harness.setup();\n      harness.open();\n\n      assertSnapshotSize(3);\n      assertMaxCommittedCheckpointId(oldJobId, 3);\n      assertMaxCommittedCheckpointId(newJobId, -1);\n\n      rows.add(SimpleDataUtil.createRowData(2, \"world\"));\n      tableRows.addAll(rows);\n\n      DataFile dataFile = writeDataFile(\"data-new-1\", rows);\n      harness.processElement(of(dataFile), ++timestamp);\n      harness.snapshot(++checkpointId, ++timestamp);\n      assertFlinkManifests(1);\n\n      harness.notifyOfCompletedCheckpoint(checkpointId);\n      assertFlinkManifests(0);\n      SimpleDataUtil.assertTableRows(table, tableRows);\n      assertSnapshotSize(4);\n      assertMaxCommittedCheckpointId(newJobId, checkpointId);\n    }\n  }\n","date":"2020-12-19 01:57:04","endLine":495,"groupId":"102211","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testStartAnotherJobToWriteSameTable","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-iceberg-10-0.7/blobInfo/CC_OUT/blobs/5e/c246b7af0789e09bc0d60caa755747bb78b8a1.src","preCode":"  public void testStartAnotherJobToWriteSameTable() throws Exception {\n    long checkpointId = 0;\n    long timestamp = 0;\n    List<RowData> rows = Lists.newArrayList();\n    List<RowData> tableRows = Lists.newArrayList();\n\n    JobID oldJobId = new JobID();\n    try (OneInputStreamOperatorTestHarness<DataFile, Void> harness = createStreamSink(oldJobId)) {\n      harness.setup();\n      harness.open();\n\n      assertSnapshotSize(0);\n      assertMaxCommittedCheckpointId(oldJobId, -1L);\n\n      for (int i = 1; i <= 3; i++) {\n        rows.add(SimpleDataUtil.createRowData(i, \"hello\" + i));\n        tableRows.addAll(rows);\n\n        DataFile dataFile = writeDataFile(String.format(\"data-%d\", i), rows);\n        harness.processElement(dataFile, ++timestamp);\n        harness.snapshot(++checkpointId, ++timestamp);\n        assertFlinkManifests(1);\n\n        harness.notifyOfCompletedCheckpoint(checkpointId);\n        assertFlinkManifests(0);\n\n        SimpleDataUtil.assertTableRows(tablePath, tableRows);\n        assertSnapshotSize(i);\n        assertMaxCommittedCheckpointId(oldJobId, checkpointId);\n      }\n    }\n\n    \r\n    checkpointId = 0;\n    timestamp = 0;\n    JobID newJobId = new JobID();\n    try (OneInputStreamOperatorTestHarness<DataFile, Void> harness = createStreamSink(newJobId)) {\n      harness.setup();\n      harness.open();\n\n      assertSnapshotSize(3);\n      assertMaxCommittedCheckpointId(oldJobId, 3);\n      assertMaxCommittedCheckpointId(newJobId, -1);\n\n      rows.add(SimpleDataUtil.createRowData(2, \"world\"));\n      tableRows.addAll(rows);\n\n      DataFile dataFile = writeDataFile(\"data-new-1\", rows);\n      harness.processElement(dataFile, ++timestamp);\n      harness.snapshot(++checkpointId, ++timestamp);\n      assertFlinkManifests(1);\n\n      harness.notifyOfCompletedCheckpoint(checkpointId);\n      assertFlinkManifests(0);\n      SimpleDataUtil.assertTableRows(tablePath, tableRows);\n      assertSnapshotSize(4);\n      assertMaxCommittedCheckpointId(newJobId, checkpointId);\n    }\n  }\n","realPath":"flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergFilesCommitter.java","repoName":"iceberg","snippetEndLine":0,"snippetStartLine":0,"startLine":437,"status":"M"}],"commitId":"77c5617c102c2ab27fbf35cac3fd75380a887d5d","commitMessage":"@@@Flink: Commit both data files and delete files (#1939)\n\n","date":"2020-12-19 01:57:04","modifiedFileCount":"9","status":"M","submitter":"openinx"}]
