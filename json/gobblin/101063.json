[{"authorTime":"2018-11-05 10:04:26","codes":[{"authorDate":"2018-11-05 10:04:26","commitOrder":4,"curCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1:HDFS-1:hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-2\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName1, expectedJobName1);\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-2\", \"HDFS-2\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName2, expectedJobName2);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-2\", \"HDFS-4\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName3, expectedJobName3);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-4\", \"ADLS-1\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName4, expectedJobName4);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","date":"2018-11-05 10:04:26","endLine":468,"groupId":"4876","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompileFlowAfterFirstEdgeDeletion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/54/792f96d937fc956b9edaa8aafde4410eb829ef.src","preCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1:HDFS-1:hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-2\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName1, expectedJobName1);\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-2\", \"HDFS-2\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName2, expectedJobName2);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-2\", \"HDFS-4\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName3, expectedJobName3);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-4\", \"ADLS-1\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName4, expectedJobName4);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":349,"status":"MB"},{"authorDate":"2018-11-05 10:04:26","commitOrder":4,"curCode":"  public void testCompileFlowAfterSecondEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-2:HDFS-2:hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    \r\n    Assert.assertTrue(jobDag.isEmpty());\n  }\n","date":"2018-11-05 10:04:26","endLine":480,"groupId":"4876","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testCompileFlowAfterSecondEdgeDeletion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/54/792f96d937fc956b9edaa8aafde4410eb829ef.src","preCode":"  public void testCompileFlowAfterSecondEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-2:HDFS-2:hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    \r\n    Assert.assertTrue(jobDag.isEmpty());\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":471,"status":"B"}],"commitId":"49974214a088b60681938a73d40b2124b18cd2bc","commitMessage":"@@@[GOBBLIN-624] Handle dataset retention in multi-hop flow compiler.\n\nCloses #2493 from sv2000/retentionConfig\n","date":"2018-11-05 10:04:26","modifiedFileCount":"14","status":"M","submitter":"suvasude"},{"authorTime":"2019-07-16 01:53:15","codes":[{"authorDate":"2019-07-16 01:53:15","commitOrder":5,"curCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1_HDFS-1_hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-2\", \"localToHdfs\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName1, expectedJobName1);\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-2\", \"HDFS-2\", \"hdfsConvertToJsonAndEncrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName2, expectedJobName2);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-2\", \"HDFS-4\", \"hdfsToHdfs\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName3, expectedJobName3);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-4\", \"ADLS-1\", \"hdfsToAdl\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName4, expectedJobName4);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","date":"2019-07-16 01:53:15","endLine":518,"groupId":"4876","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompileFlowAfterFirstEdgeDeletion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/c7/317141258e263a117f99b5abbbbe763fefb728.src","preCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1:HDFS-1:hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-2\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName1, expectedJobName1);\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-2\", \"HDFS-2\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName2, expectedJobName2);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-2\", \"HDFS-4\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName3, expectedJobName3);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-4\", \"ADLS-1\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName4, expectedJobName4);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":399,"status":"M"},{"authorDate":"2019-07-16 01:53:15","commitOrder":5,"curCode":"  public void testCompileFlowAfterSecondEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-2_HDFS-2_hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    \r\n    Assert.assertTrue(jobDag.isEmpty());\n  }\n","date":"2019-07-16 01:53:15","endLine":530,"groupId":"4876","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testCompileFlowAfterSecondEdgeDeletion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/c7/317141258e263a117f99b5abbbbe763fefb728.src","preCode":"  public void testCompileFlowAfterSecondEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-2:HDFS-2:hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    \r\n    Assert.assertTrue(jobDag.isEmpty());\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":521,"status":"M"}],"commitId":"86515b99fac8d4dd12ca99c79674f0faf572736f","commitMessage":"@@@[GOBBLIN-810] Include flow edge ID in job name\n\nCloses #2675 from jack-moseley/job-name-conflict\n","date":"2019-07-16 01:53:15","modifiedFileCount":"5","status":"M","submitter":"Jack Moseley"},{"authorTime":"2019-07-16 01:53:15","codes":[{"authorDate":"2019-10-03 01:49:10","commitOrder":6,"curCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1_HDFS-1_hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-2\", \"localToHdfs\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName1.startsWith(expectedJobName1));\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-2\", \"HDFS-2\", \"hdfsConvertToJsonAndEncrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName2.startsWith(expectedJobName2));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-2\", \"HDFS-4\", \"hdfsToHdfs\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName3.startsWith(expectedJobName3));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-4\", \"ADLS-1\", \"hdfsToAdl\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName4.startsWith(expectedJobName4));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","date":"2019-10-03 01:49:10","endLine":518,"groupId":"4876","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompileFlowAfterFirstEdgeDeletion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/b0/faa6d86f372254412071b608fa5170cbc72d25.src","preCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1_HDFS-1_hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-2\", \"localToHdfs\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName1, expectedJobName1);\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-2\", \"HDFS-2\", \"hdfsConvertToJsonAndEncrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName2, expectedJobName2);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-2\", \"HDFS-4\", \"hdfsToHdfs\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName3, expectedJobName3);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-4\", \"ADLS-1\", \"hdfsToAdl\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName4, expectedJobName4);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":399,"status":"M"},{"authorDate":"2019-07-16 01:53:15","commitOrder":6,"curCode":"  public void testCompileFlowAfterSecondEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-2_HDFS-2_hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    \r\n    Assert.assertTrue(jobDag.isEmpty());\n  }\n","date":"2019-07-16 01:53:15","endLine":530,"groupId":"4876","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testCompileFlowAfterSecondEdgeDeletion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/c7/317141258e263a117f99b5abbbbe763fefb728.src","preCode":"  public void testCompileFlowAfterSecondEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-2_HDFS-2_hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    \r\n    Assert.assertTrue(jobDag.isEmpty());\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":521,"status":"N"}],"commitId":"37078ed6a40f73b2560863e3f73e0e1d67c05c87","commitMessage":"@@@[GOBBLIN-853] Support multiple paths specified in flow config\n\nCloses #2709 from jack-moseley/split-flowspec\n","date":"2019-10-03 01:49:10","modifiedFileCount":"4","status":"M","submitter":"Jack Moseley"},{"authorTime":"2019-07-16 01:53:15","codes":[{"authorDate":"2019-10-11 11:52:37","commitOrder":7,"curCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1_HDFS-1_hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-2\", \"localToHdfs\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName1.startsWith(expectedJobName1));\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(new Path(jobConfig.getString(\"gobblin.dataset.pattern\")), new Path(from));\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-2\", \"HDFS-2\", \"hdfsConvertToJsonAndEncrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName2.startsWith(expectedJobName2));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-2\", \"HDFS-4\", \"hdfsToHdfs\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName3.startsWith(expectedJobName3));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-4\", \"ADLS-1\", \"hdfsToAdl\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName4.startsWith(expectedJobName4));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","date":"2019-10-11 11:52:37","endLine":519,"groupId":"4876","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompileFlowAfterFirstEdgeDeletion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/17/6c2d6f9306bfe64f97dce6cc84f2f104b6f6b6.src","preCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1_HDFS-1_hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-2\", \"localToHdfs\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName1.startsWith(expectedJobName1));\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-2\", \"HDFS-2\", \"hdfsConvertToJsonAndEncrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName2.startsWith(expectedJobName2));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-2\", \"HDFS-4\", \"hdfsToHdfs\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName3.startsWith(expectedJobName3));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-4\", \"ADLS-1\", \"hdfsToAdl\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName4.startsWith(expectedJobName4));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":400,"status":"M"},{"authorDate":"2019-07-16 01:53:15","commitOrder":7,"curCode":"  public void testCompileFlowAfterSecondEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-2_HDFS-2_hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    \r\n    Assert.assertTrue(jobDag.isEmpty());\n  }\n","date":"2019-07-16 01:53:15","endLine":530,"groupId":"4876","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testCompileFlowAfterSecondEdgeDeletion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/c7/317141258e263a117f99b5abbbbe763fefb728.src","preCode":"  public void testCompileFlowAfterSecondEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-2_HDFS-2_hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    \r\n    Assert.assertTrue(jobDag.isEmpty());\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":521,"status":"N"}],"commitId":"baf2abe35cedeb7dec01528cbcca659c4fd73fa9","commitMessage":"@@@[GOBBLIN-894] Add option to combine datasets into a single flow\n\nCloses #2749 from jack-moseley/combine-flows\n","date":"2019-10-11 11:52:37","modifiedFileCount":"9","status":"M","submitter":"Jack Moseley"},{"authorTime":"2021-06-05 06:46:57","codes":[{"authorDate":"2019-10-11 11:52:37","commitOrder":8,"curCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1_HDFS-1_hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-2\", \"localToHdfs\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName1.startsWith(expectedJobName1));\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(new Path(jobConfig.getString(\"gobblin.dataset.pattern\")), new Path(from));\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-2\", \"HDFS-2\", \"hdfsConvertToJsonAndEncrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName2.startsWith(expectedJobName2));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-2\", \"HDFS-4\", \"hdfsToHdfs\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName3.startsWith(expectedJobName3));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-4\", \"ADLS-1\", \"hdfsToAdl\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName4.startsWith(expectedJobName4));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","date":"2019-10-11 11:52:37","endLine":519,"groupId":"101063","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompileFlowAfterFirstEdgeDeletion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/17/6c2d6f9306bfe64f97dce6cc84f2f104b6f6b6.src","preCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1_HDFS-1_hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-2\", \"localToHdfs\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName1.startsWith(expectedJobName1));\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(new Path(jobConfig.getString(\"gobblin.dataset.pattern\")), new Path(from));\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-2\", \"HDFS-2\", \"hdfsConvertToJsonAndEncrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName2.startsWith(expectedJobName2));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-2\", \"HDFS-4\", \"hdfsToHdfs\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName3.startsWith(expectedJobName3));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-4\", \"ADLS-1\", \"hdfsToAdl\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName4.startsWith(expectedJobName4));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":400,"status":"N"},{"authorDate":"2021-06-05 06:46:57","commitOrder":8,"curCode":"  public void testCompileFlowAfterSecondEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-2_HDFS-2_hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    \r\n    Assert.assertEquals(jobDag, null);\n  }\n","date":"2021-06-05 06:46:57","endLine":531,"groupId":"101063","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testCompileFlowAfterSecondEdgeDeletion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/14/f2078b30acb0f4b0718e2897fb8eaca2b7487b.src","preCode":"  public void testCompileFlowAfterSecondEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-2_HDFS-2_hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    \r\n    Assert.assertTrue(jobDag.isEmpty());\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":522,"status":"M"}],"commitId":"381b659d3da7a241d6f038696396e6bba690120e","commitMessage":"@@@[GOBBLIN-1453] Improve error reporting on failed flow compilations and fix bugs wher?\n\nCloses #3291 from Will-Lo/modify-flow-compilation-\nerror-reporting\n","date":"2021-06-05 06:46:57","modifiedFileCount":"11","status":"M","submitter":"William Lo"}]
