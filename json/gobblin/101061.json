[{"authorTime":"2018-08-10 00:40:43","codes":[{"authorDate":"2018-08-10 00:40:43","commitOrder":1,"curCode":"  public void testCompileFlow() throws URISyntaxException, IOException {\n    FlowSpec spec = createFlowSpec(\"flow/flow.conf\", \"LocalFS-1\", \"ADLS-1\");\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    Dag.DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobSpecWithExecutor = startNode.getValue();\n    JobSpec jobSpec = jobSpecWithExecutor.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobSpecWithExecutor = secondHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:convert-to-json-and-encrypt\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobSpecWithExecutor = thirdHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobSpecWithExecutor = fourthHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-ADL\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban03.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","date":"2018-08-10 00:40:43","endLine":255,"groupId":"7543","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompileFlow","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/b8/bac02a79aa29ff4919c2c7aa9f21d12cb29b5d.src","preCode":"  public void testCompileFlow() throws URISyntaxException, IOException {\n    FlowSpec spec = createFlowSpec(\"flow/flow.conf\", \"LocalFS-1\", \"ADLS-1\");\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    Dag.DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobSpecWithExecutor = startNode.getValue();\n    JobSpec jobSpec = jobSpecWithExecutor.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobSpecWithExecutor = secondHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:convert-to-json-and-encrypt\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobSpecWithExecutor = thirdHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobSpecWithExecutor = fourthHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-ADL\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban03.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":158,"status":"B"},{"authorDate":"2018-08-10 00:40:43","commitOrder":1,"curCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1:HDFS-1:hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow.conf\", \"LocalFS-1\", \"ADLS-1\");\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    Dag.DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:convert-to-json-and-encrypt\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-ADL\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","date":"2018-08-10 00:40:43","endLine":359,"groupId":"7544","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testCompileFlowAfterFirstEdgeDeletion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/b8/bac02a79aa29ff4919c2c7aa9f21d12cb29b5d.src","preCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1:HDFS-1:hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow.conf\", \"LocalFS-1\", \"ADLS-1\");\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    Dag.DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:convert-to-json-and-encrypt\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-ADL\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":258,"status":"B"}],"commitId":"ef26d287d95edcc5208daed651e8ef8e09efa5d0","commitMessage":"@@@[GOBBLIN-554] Change signature of SpecCompiler#compileFlow() to return a DAG of JobSpecs instead of a HashMap.\n\nCloses #2415 from sv2000/dag1\n","date":"2018-08-10 00:40:43","modifiedFileCount":"6","status":"B","submitter":"suvasude"},{"authorTime":"2018-10-11 02:07:07","codes":[{"authorDate":"2018-10-11 02:07:07","commitOrder":2,"curCode":"  public void testCompileFlow() throws URISyntaxException, IOException {\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\");\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    Dag.DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobSpecWithExecutor = startNode.getValue();\n    JobSpec jobSpec = jobSpecWithExecutor.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobSpecWithExecutor = secondHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:convert-to-json-and-encrypt\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobSpecWithExecutor = thirdHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobSpecWithExecutor = fourthHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-ADL\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban03.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","date":"2018-10-11 02:07:07","endLine":270,"groupId":"7543","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompileFlow","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/85/74dac521e5fb3937f3d499622039c70716c785.src","preCode":"  public void testCompileFlow() throws URISyntaxException, IOException {\n    FlowSpec spec = createFlowSpec(\"flow/flow.conf\", \"LocalFS-1\", \"ADLS-1\");\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    Dag.DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobSpecWithExecutor = startNode.getValue();\n    JobSpec jobSpec = jobSpecWithExecutor.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobSpecWithExecutor = secondHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:convert-to-json-and-encrypt\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobSpecWithExecutor = thirdHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobSpecWithExecutor = fourthHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-ADL\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban03.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":173,"status":"M"},{"authorDate":"2018-10-11 02:07:07","commitOrder":2,"curCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1:HDFS-1:hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\");\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    Dag.DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:convert-to-json-and-encrypt\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-ADL\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","date":"2018-10-11 02:07:07","endLine":374,"groupId":"7544","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testCompileFlowAfterFirstEdgeDeletion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/85/74dac521e5fb3937f3d499622039c70716c785.src","preCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1:HDFS-1:hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow.conf\", \"LocalFS-1\", \"ADLS-1\");\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    Dag.DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:convert-to-json-and-encrypt\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-ADL\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":273,"status":"M"}],"commitId":"b9149bfd948342163abd15b6b6dc12f9fc83d3f8","commitMessage":"@@@[GOBBLIN-606] Fix duplicate addition of FlowEdge to path for single-hop paths in Multi-hop flow compiler.[]\n\nCloses #2472 from sv2000/pathFinderBug\n","date":"2018-10-11 02:07:07","modifiedFileCount":"3","status":"M","submitter":"sv2000"},{"authorTime":"2018-10-11 12:09:49","codes":[{"authorDate":"2018-10-11 12:09:49","commitOrder":3,"curCode":"  public void testCompileFlow() throws URISyntaxException, IOException {\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\");\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    Dag.DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobSpecWithExecutor = startNode.getValue();\n    JobSpec jobSpec = jobSpecWithExecutor.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp-HDFS-HDFS\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName1.startsWith(expectedJobName1));\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobSpecWithExecutor = secondHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"convert-to-json-and-encrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName2.startsWith(expectedJobName2));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobSpecWithExecutor = thirdHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp-HDFS-HDFS\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName3.startsWith(expectedJobName3));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobSpecWithExecutor = fourthHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp-HDFS-ADL\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName4.startsWith(expectedJobName4));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban03.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","date":"2018-10-11 12:09:49","endLine":288,"groupId":"7543","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompileFlow","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/c0/4b2b9e0fcd514e7cbacf53a621f451eb0dead9.src","preCode":"  public void testCompileFlow() throws URISyntaxException, IOException {\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\");\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    Dag.DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobSpecWithExecutor = startNode.getValue();\n    JobSpec jobSpec = jobSpecWithExecutor.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobSpecWithExecutor = secondHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:convert-to-json-and-encrypt\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobSpecWithExecutor = thirdHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobSpecWithExecutor = fourthHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-ADL\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban03.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":174,"status":"M"},{"authorDate":"2018-10-11 12:09:49","commitOrder":3,"curCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1:HDFS-1:hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\");\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    Dag.DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp-HDFS-HDFS\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName1.startsWith(expectedJobName1));\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"convert-to-json-and-encrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName2.startsWith(expectedJobName2));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp-HDFS-HDFS\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName3.startsWith(expectedJobName3));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp-HDFS-ADL\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName4.startsWith(expectedJobName4));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","date":"2018-10-11 12:09:49","endLine":410,"groupId":"7544","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testCompileFlowAfterFirstEdgeDeletion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/c0/4b2b9e0fcd514e7cbacf53a621f451eb0dead9.src","preCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1:HDFS-1:hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\");\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    Dag.DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:convert-to-json-and-encrypt\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-HDFS\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    Assert.assertEquals(jobConfig.getString(\"job.name\"), \"testFlowGroup:testFlowName:Distcp-HDFS-ADL\");\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":291,"status":"M"}],"commitId":"2509f3a3d43ecc35caebe6e4b5ce283b035867b4","commitMessage":"@@@[GOBBLIN-604] Map dependencies in job templates to the job names in compiled JobSpecs.\n\nCloses #2470 from sv2000/dependencies\n","date":"2018-10-11 12:09:49","modifiedFileCount":"5","status":"M","submitter":"sv2000"},{"authorTime":"2018-11-05 10:04:26","codes":[{"authorDate":"2018-11-05 10:04:26","commitOrder":4,"curCode":"  public void testCompileFlow() throws URISyntaxException, IOException {\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobSpecWithExecutor = startNode.getValue();\n    JobSpec jobSpec = jobSpecWithExecutor.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-1\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName1, expectedJobName1);\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobSpecWithExecutor = secondHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-1\", \"HDFS-1\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName2, expectedJobName2);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobSpecWithExecutor = thirdHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-1\", \"HDFS-3\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName3, expectedJobName3);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobSpecWithExecutor = fourthHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-3\", \"ADLS-1\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName4, expectedJobName4);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban03.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","date":"2018-11-05 10:04:26","endLine":296,"groupId":"1154","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompileFlow","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/54/792f96d937fc956b9edaa8aafde4410eb829ef.src","preCode":"  public void testCompileFlow() throws URISyntaxException, IOException {\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\");\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    Dag.DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobSpecWithExecutor = startNode.getValue();\n    JobSpec jobSpec = jobSpecWithExecutor.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp-HDFS-HDFS\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName1.startsWith(expectedJobName1));\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobSpecWithExecutor = secondHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"convert-to-json-and-encrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName2.startsWith(expectedJobName2));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobSpecWithExecutor = thirdHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp-HDFS-HDFS\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName3.startsWith(expectedJobName3));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobSpecWithExecutor = fourthHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp-HDFS-ADL\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName4.startsWith(expectedJobName4));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban03.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":182,"status":"M"},{"authorDate":"2018-11-05 10:04:26","commitOrder":4,"curCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1:HDFS-1:hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-2\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName1, expectedJobName1);\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-2\", \"HDFS-2\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName2, expectedJobName2);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-2\", \"HDFS-4\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName3, expectedJobName3);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-4\", \"ADLS-1\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName4, expectedJobName4);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","date":"2018-11-05 10:04:26","endLine":468,"groupId":"4876","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testCompileFlowAfterFirstEdgeDeletion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/54/792f96d937fc956b9edaa8aafde4410eb829ef.src","preCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1:HDFS-1:hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\");\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    Dag.DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp-HDFS-HDFS\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName1.startsWith(expectedJobName1));\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"convert-to-json-and-encrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName2.startsWith(expectedJobName2));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp-HDFS-HDFS\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName3.startsWith(expectedJobName3));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    Dag.DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp-HDFS-ADL\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName4.startsWith(expectedJobName4));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":349,"status":"M"}],"commitId":"49974214a088b60681938a73d40b2124b18cd2bc","commitMessage":"@@@[GOBBLIN-624] Handle dataset retention in multi-hop flow compiler.\n\nCloses #2493 from sv2000/retentionConfig\n","date":"2018-11-05 10:04:26","modifiedFileCount":"14","status":"M","submitter":"suvasude"},{"authorTime":"2019-07-16 01:53:15","codes":[{"authorDate":"2019-07-16 01:53:15","commitOrder":5,"curCode":"  public void testCompileFlow() throws URISyntaxException, IOException {\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobSpecWithExecutor = startNode.getValue();\n    JobSpec jobSpec = jobSpecWithExecutor.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-1\", \"localToHdfs\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName1, expectedJobName1);\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobSpecWithExecutor = secondHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-1\", \"HDFS-1\", \"hdfsConvertToJsonAndEncrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName2, expectedJobName2);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobSpecWithExecutor = thirdHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-1\", \"HDFS-3\", \"hdfsToHdfs\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName3, expectedJobName3);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobSpecWithExecutor = fourthHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-3\", \"ADLS-1\", \"hdfsToAdl\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName4, expectedJobName4);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban03.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","date":"2019-07-16 01:53:15","endLine":344,"groupId":"4872","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompileFlow","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/c7/317141258e263a117f99b5abbbbe763fefb728.src","preCode":"  public void testCompileFlow() throws URISyntaxException, IOException {\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobSpecWithExecutor = startNode.getValue();\n    JobSpec jobSpec = jobSpecWithExecutor.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-1\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName1, expectedJobName1);\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobSpecWithExecutor = secondHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-1\", \"HDFS-1\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName2, expectedJobName2);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobSpecWithExecutor = thirdHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-1\", \"HDFS-3\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName3, expectedJobName3);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobSpecWithExecutor = fourthHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-3\", \"ADLS-1\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName4, expectedJobName4);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban03.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":230,"status":"M"},{"authorDate":"2019-07-16 01:53:15","commitOrder":5,"curCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1_HDFS-1_hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-2\", \"localToHdfs\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName1, expectedJobName1);\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-2\", \"HDFS-2\", \"hdfsConvertToJsonAndEncrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName2, expectedJobName2);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-2\", \"HDFS-4\", \"hdfsToHdfs\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName3, expectedJobName3);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-4\", \"ADLS-1\", \"hdfsToAdl\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName4, expectedJobName4);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","date":"2019-07-16 01:53:15","endLine":518,"groupId":"4876","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testCompileFlowAfterFirstEdgeDeletion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/c7/317141258e263a117f99b5abbbbe763fefb728.src","preCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1:HDFS-1:hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-2\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName1, expectedJobName1);\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-2\", \"HDFS-2\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName2, expectedJobName2);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-2\", \"HDFS-4\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName3, expectedJobName3);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-4\", \"ADLS-1\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName4, expectedJobName4);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":399,"status":"M"}],"commitId":"86515b99fac8d4dd12ca99c79674f0faf572736f","commitMessage":"@@@[GOBBLIN-810] Include flow edge ID in job name\n\nCloses #2675 from jack-moseley/job-name-conflict\n","date":"2019-07-16 01:53:15","modifiedFileCount":"5","status":"M","submitter":"Jack Moseley"},{"authorTime":"2019-10-03 01:49:10","codes":[{"authorDate":"2019-10-03 01:49:10","commitOrder":6,"curCode":"  public void testCompileFlow() throws URISyntaxException, IOException {\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobSpecWithExecutor = startNode.getValue();\n    JobSpec jobSpec = jobSpecWithExecutor.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-1\", \"localToHdfs\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName1.startsWith(expectedJobName1));\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobSpecWithExecutor = secondHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-1\", \"HDFS-1\", \"hdfsConvertToJsonAndEncrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName2.startsWith(expectedJobName2));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobSpecWithExecutor = thirdHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-1\", \"HDFS-3\", \"hdfsToHdfs\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName3.startsWith(expectedJobName3));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobSpecWithExecutor = fourthHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-3\", \"ADLS-1\", \"hdfsToAdl\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName4.startsWith(expectedJobName4));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban03.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","date":"2019-10-03 01:49:10","endLine":344,"groupId":"4872","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompileFlow","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/b0/faa6d86f372254412071b608fa5170cbc72d25.src","preCode":"  public void testCompileFlow() throws URISyntaxException, IOException {\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobSpecWithExecutor = startNode.getValue();\n    JobSpec jobSpec = jobSpecWithExecutor.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-1\", \"localToHdfs\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName1, expectedJobName1);\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobSpecWithExecutor = secondHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-1\", \"HDFS-1\", \"hdfsConvertToJsonAndEncrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName2, expectedJobName2);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobSpecWithExecutor = thirdHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-1\", \"HDFS-3\", \"hdfsToHdfs\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName3, expectedJobName3);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobSpecWithExecutor = fourthHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-3\", \"ADLS-1\", \"hdfsToAdl\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName4, expectedJobName4);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban03.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":230,"status":"M"},{"authorDate":"2019-10-03 01:49:10","commitOrder":6,"curCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1_HDFS-1_hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-2\", \"localToHdfs\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName1.startsWith(expectedJobName1));\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-2\", \"HDFS-2\", \"hdfsConvertToJsonAndEncrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName2.startsWith(expectedJobName2));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-2\", \"HDFS-4\", \"hdfsToHdfs\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName3.startsWith(expectedJobName3));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-4\", \"ADLS-1\", \"hdfsToAdl\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName4.startsWith(expectedJobName4));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","date":"2019-10-03 01:49:10","endLine":518,"groupId":"4876","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testCompileFlowAfterFirstEdgeDeletion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/b0/faa6d86f372254412071b608fa5170cbc72d25.src","preCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1_HDFS-1_hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-2\", \"localToHdfs\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName1, expectedJobName1);\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-2\", \"HDFS-2\", \"hdfsConvertToJsonAndEncrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName2, expectedJobName2);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-2\", \"HDFS-4\", \"hdfsToHdfs\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName3, expectedJobName3);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-4\", \"ADLS-1\", \"hdfsToAdl\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertEquals(jobName4, expectedJobName4);\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":399,"status":"M"}],"commitId":"37078ed6a40f73b2560863e3f73e0e1d67c05c87","commitMessage":"@@@[GOBBLIN-853] Support multiple paths specified in flow config\n\nCloses #2709 from jack-moseley/split-flowspec\n","date":"2019-10-03 01:49:10","modifiedFileCount":"4","status":"M","submitter":"Jack Moseley"},{"authorTime":"2019-10-11 11:52:37","codes":[{"authorDate":"2019-10-11 11:52:37","commitOrder":7,"curCode":"  public void testCompileFlow() throws URISyntaxException, IOException {\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobSpecWithExecutor = startNode.getValue();\n    JobSpec jobSpec = jobSpecWithExecutor.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-1\", \"localToHdfs\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName1.startsWith(expectedJobName1));\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(new Path(jobConfig.getString(\"gobblin.dataset.pattern\")), new Path(from));\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobSpecWithExecutor = secondHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-1\", \"HDFS-1\", \"hdfsConvertToJsonAndEncrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName2.startsWith(expectedJobName2));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobSpecWithExecutor = thirdHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-1\", \"HDFS-3\", \"hdfsToHdfs\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName3.startsWith(expectedJobName3));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobSpecWithExecutor = fourthHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-3\", \"ADLS-1\", \"hdfsToAdl\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName4.startsWith(expectedJobName4));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban03.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","date":"2019-10-11 11:52:37","endLine":345,"groupId":"101061","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompileFlow","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/17/6c2d6f9306bfe64f97dce6cc84f2f104b6f6b6.src","preCode":"  public void testCompileFlow() throws URISyntaxException, IOException {\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobSpecWithExecutor = startNode.getValue();\n    JobSpec jobSpec = jobSpecWithExecutor.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-1\", \"localToHdfs\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName1.startsWith(expectedJobName1));\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobSpecWithExecutor = secondHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-1\", \"HDFS-1\", \"hdfsConvertToJsonAndEncrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName2.startsWith(expectedJobName2));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobSpecWithExecutor = thirdHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-1\", \"HDFS-3\", \"hdfsToHdfs\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName3.startsWith(expectedJobName3));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn01.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban01.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobSpecWithExecutor = fourthHopNode.getValue();\n    jobConfig = jobSpecWithExecutor.getJobSpec().getConfig();\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-3\", \"ADLS-1\", \"hdfsToAdl\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName4.startsWith(expectedJobName4));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn03.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobSpecWithExecutor.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban03.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":231,"status":"M"},{"authorDate":"2019-10-11 11:52:37","commitOrder":7,"curCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1_HDFS-1_hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-2\", \"localToHdfs\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName1.startsWith(expectedJobName1));\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(new Path(jobConfig.getString(\"gobblin.dataset.pattern\")), new Path(from));\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-2\", \"HDFS-2\", \"hdfsConvertToJsonAndEncrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName2.startsWith(expectedJobName2));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-2\", \"HDFS-4\", \"hdfsToHdfs\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName3.startsWith(expectedJobName3));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-4\", \"ADLS-1\", \"hdfsToAdl\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName4.startsWith(expectedJobName4));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","date":"2019-10-11 11:52:37","endLine":519,"groupId":"101061","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testCompileFlowAfterFirstEdgeDeletion","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/17/6c2d6f9306bfe64f97dce6cc84f2f104b6f6b6.src","preCode":"  public void testCompileFlowAfterFirstEdgeDeletion() throws URISyntaxException, IOException {\n    \r\n    this.flowGraph.deleteFlowEdge(\"HDFS-1_HDFS-1_hdfsConvertToJsonAndEncrypt\");\n\n    FlowSpec spec = createFlowSpec(\"flow/flow1.conf\", \"LocalFS-1\", \"ADLS-1\", false, false);\n    Dag<JobExecutionPlan> jobDag = this.specCompiler.compileFlow(spec);\n\n    Assert.assertEquals(jobDag.getNodes().size(), 4);\n    Assert.assertEquals(jobDag.getStartNodes().size(), 1);\n    Assert.assertEquals(jobDag.getEndNodes().size(), 1);\n\n    \r\n    DagNode<JobExecutionPlan> startNode = jobDag.getStartNodes().get(0);\n    JobExecutionPlan jobExecutionPlan = startNode.getValue();\n    JobSpec jobSpec = jobExecutionPlan.getJobSpec();\n\n    \r\n    Config jobConfig = jobSpec.getConfig();\n    String flowGroup = \"testFlowGroup\";\n    String flowName = \"testFlowName\";\n    String expectedJobName1 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"LocalFS-1\", \"HDFS-2\", \"localToHdfs\");\n    String jobName1 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName1.startsWith(expectedJobName1));\n    String from = jobConfig.getString(\"from\");\n    String to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/out/testTeam/testDataset\");\n    String sourceFsUri = jobConfig.getString(\"fs.uri\");\n    Assert.assertEquals(sourceFsUri, \"file:///\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), sourceFsUri);\n    Assert.assertEquals(jobConfig.getString(\"state.store.fs.uri\"), sourceFsUri);\n    String targetFsUri = jobConfig.getString(\"target.filebased.fs.uri\");\n    Assert.assertEquals(targetFsUri, \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"writer.fs.uri\"), targetFsUri);\n    Assert.assertEquals(jobConfig.getString(\"gobblin.dataset.pattern\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"java\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.runtime.local.LocalJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"LOCAL\");\n    \r\n    SpecExecutor specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"fs:///\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.runtime.spec_executorInstance.InMemorySpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(startNode).size(), 1);\n    DagNode<JobExecutionPlan> secondHopNode = jobDag.getChildren(startNode).get(0);\n    jobExecutionPlan = secondHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName2 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"ConvertToJsonAndEncrypt\", \"HDFS-2\", \"HDFS-2\", \"hdfsConvertToJsonAndEncrypt\");\n    String jobName2 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName2.startsWith(expectedJobName2));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName1);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/out/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.data.directory\"), from);\n    Assert.assertEquals(jobConfig.getString(\"data.publisher.final.dir\"), to);\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(secondHopNode).size(), 1);\n    DagNode<JobExecutionPlan> thirdHopNode = jobDag.getChildren(secondHopNode).get(0);\n    jobExecutionPlan = thirdHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n    String expectedJobName3 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"Distcp\", \"HDFS-2\", \"HDFS-4\", \"hdfsToHdfs\");\n    String jobName3 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName3.startsWith(expectedJobName3));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName2);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn02.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban02.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getChildren(thirdHopNode).size(), 1);\n    DagNode<JobExecutionPlan> fourthHopNode = jobDag.getChildren(thirdHopNode).get(0);\n    jobExecutionPlan = fourthHopNode.getValue();\n    jobConfig = jobExecutionPlan.getJobSpec().getConfig();\n\n    String expectedJobName4 = Joiner.on(JobExecutionPlan.Factory.JOB_NAME_COMPONENT_SEPARATION_CHAR).\n        join(flowGroup, flowName, \"DistcpToADL\", \"HDFS-4\", \"ADLS-1\", \"hdfsToAdl\");\n    String jobName4 = jobConfig.getString(ConfigurationKeys.JOB_NAME_KEY);\n    Assert.assertTrue(jobName4.startsWith(expectedJobName4));\n    Assert.assertEquals(jobConfig.getString(ConfigurationKeys.JOB_DEPENDENCIES), jobName3);\n    from = jobConfig.getString(\"from\");\n    to = jobConfig.getString(\"to\");\n    Assert.assertEquals(from, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(to, \"/data/encrypted/testTeam/testDataset\");\n    Assert.assertEquals(jobConfig.getString(\"source.filebased.fs.uri\"), \"hdfs://hadoopnn04.grid.linkedin.com:8888/\");\n    Assert.assertEquals(jobConfig.getString(\"target.filebased.fs.uri\"), \"adl://azuredatalakestore.net/\");\n    Assert.assertEquals(jobConfig.getString(\"type\"), \"hadoopJava\");\n    Assert.assertEquals(jobConfig.getString(\"job.class\"), \"org.apache.gobblin.azkaban.AzkabanJobLauncher\");\n    Assert.assertEquals(jobConfig.getString(\"launcher.type\"), \"MAPREDUCE\");\n    Assert.assertEquals(jobConfig.getString(\"dfs.adls.oauth2.client.id\"), \"1234\");\n    Assert.assertEquals(jobConfig.getString(\"writer.encrypted.dfs.adls.oauth2.credential\"), \"credential\");\n    Assert.assertEquals(jobConfig.getString(\"encrypt.key.loc\"), \"/user/testUser/master.password\");\n    \r\n    specExecutor = jobExecutionPlan.getSpecExecutor();\n    Assert.assertEquals(specExecutor.getUri().toString(), \"https://azkaban04.gobblin.net:8443\");\n    Assert.assertEquals(specExecutor.getClass().getCanonicalName(), \"org.apache.gobblin.service.modules.flow.MultiHopFlowCompilerTest.TestAzkabanSpecExecutor\");\n\n    \r\n    Assert.assertEquals(jobDag.getEndNodes().get(0), fourthHopNode);\n  }\n","realPath":"gobblin-service/src/test/java/org/apache/gobblin/service/modules/flow/MultiHopFlowCompilerTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":400,"status":"M"}],"commitId":"baf2abe35cedeb7dec01528cbcca659c4fd73fa9","commitMessage":"@@@[GOBBLIN-894] Add option to combine datasets into a single flow\n\nCloses #2749 from jack-moseley/combine-flows\n","date":"2019-10-11 11:52:37","modifiedFileCount":"9","status":"M","submitter":"Jack Moseley"}]
