[{"authorTime":"2017-07-31 15:51:25","codes":[{"authorDate":"2017-07-31 15:51:25","commitOrder":1,"curCode":"  private void processNonPartitionedTable(final ConvertibleHiveDataset hiveDataset) throws IOException {\n    try {\n      \r\n      final long updateTime = this.updateProvider.getUpdateTime(hiveDataset.getTable());\n\n      log.info(String.format(\"Validating table: %s\", hiveDataset.getTable()));\n\n      for (final String format : hiveDataset.getDestFormats()) {\n        Optional<ConvertibleHiveDataset.ConversionConfig> conversionConfigOptional = hiveDataset.getConversionConfigForFormat(format);\n        if (conversionConfigOptional.isPresent()) {\n          ConvertibleHiveDataset.ConversionConfig conversionConfig = conversionConfigOptional.get();\n          String orcTableName = conversionConfig.getDestinationTableName();\n          String orcTableDatabase = conversionConfig.getDestinationDbName();\n          Pair<Optional<org.apache.hadoop.hive.metastore.api.Table>, Optional<List<Partition>>> destinationMeta =\n              getDestinationTableMeta(orcTableDatabase, orcTableName, this.props);\n\n          \r\n          final List<String> validationQueries =\n              HiveValidationQueryGenerator.generateCountValidationQueries(hiveDataset, Optional.<Partition> absent(), conversionConfig);\n          final List<String> dataValidationQueries =\n              Lists.newArrayList(HiveValidationQueryGenerator.generateDataValidationQuery(hiveDataset.getTable().getTableName(), hiveDataset.getTable()\n                  .getDbName(), destinationMeta.getKey().get(), Optional.<Partition> absent(), this.isNestedORC));\n\n          this.futures.add(this.exec.submit(new Callable<Void>() {\n            @Override\n            public Void call() throws Exception {\n\n              \r\n              log.debug(String.format(\"Going to execute queries: %s for format: %s\", validationQueries, format));\n              List<Long> rowCounts = ValidationJob.this.getValidationOutputFromHive(validationQueries);\n              log.debug(String.format(\"Going to execute queries: %s for format: %s\", dataValidationQueries, format));\n              List<Long> rowDataValidatedCount = ValidationJob.this.getValidationOutputFromHive(dataValidationQueries);\n              \r\n              validateAndPopulateReport(hiveDataset.getTable().getCompleteName(), updateTime, rowCounts, rowDataValidatedCount.get(0));\n\n              return null;\n            }\n          }));\n        } else {\n          log.warn(String.format(\"No config found for format: %s So skipping table: %s for this format\", format, hiveDataset.getTable().getCompleteName()));\n        }\n      }\n    } catch (UncheckedExecutionException e) {\n      log.warn(String.format(\"Not validating table: %s %s\", hiveDataset.getTable().getCompleteName(), e.getMessage()));\n    } catch (UpdateNotFoundException e) {\n      log.warn(String\n          .format(\"Not validating table: %s as update time was not found. %s\", hiveDataset.getTable().getCompleteName(),\n              e.getMessage()));\n    }\n  }\n","date":"2017-07-31 15:51:25","endLine":412,"groupId":"6891","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"processNonPartitionedTable","params":"(finalConvertibleHiveDatasethiveDataset)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/8b/f2c21013cb6df87bd796f56eb1c72ff3343f01.src","preCode":"  private void processNonPartitionedTable(final ConvertibleHiveDataset hiveDataset) throws IOException {\n    try {\n      \r\n      final long updateTime = this.updateProvider.getUpdateTime(hiveDataset.getTable());\n\n      log.info(String.format(\"Validating table: %s\", hiveDataset.getTable()));\n\n      for (final String format : hiveDataset.getDestFormats()) {\n        Optional<ConvertibleHiveDataset.ConversionConfig> conversionConfigOptional = hiveDataset.getConversionConfigForFormat(format);\n        if (conversionConfigOptional.isPresent()) {\n          ConvertibleHiveDataset.ConversionConfig conversionConfig = conversionConfigOptional.get();\n          String orcTableName = conversionConfig.getDestinationTableName();\n          String orcTableDatabase = conversionConfig.getDestinationDbName();\n          Pair<Optional<org.apache.hadoop.hive.metastore.api.Table>, Optional<List<Partition>>> destinationMeta =\n              getDestinationTableMeta(orcTableDatabase, orcTableName, this.props);\n\n          \r\n          final List<String> validationQueries =\n              HiveValidationQueryGenerator.generateCountValidationQueries(hiveDataset, Optional.<Partition> absent(), conversionConfig);\n          final List<String> dataValidationQueries =\n              Lists.newArrayList(HiveValidationQueryGenerator.generateDataValidationQuery(hiveDataset.getTable().getTableName(), hiveDataset.getTable()\n                  .getDbName(), destinationMeta.getKey().get(), Optional.<Partition> absent(), this.isNestedORC));\n\n          this.futures.add(this.exec.submit(new Callable<Void>() {\n            @Override\n            public Void call() throws Exception {\n\n              \r\n              log.debug(String.format(\"Going to execute queries: %s for format: %s\", validationQueries, format));\n              List<Long> rowCounts = ValidationJob.this.getValidationOutputFromHive(validationQueries);\n              log.debug(String.format(\"Going to execute queries: %s for format: %s\", dataValidationQueries, format));\n              List<Long> rowDataValidatedCount = ValidationJob.this.getValidationOutputFromHive(dataValidationQueries);\n              \r\n              validateAndPopulateReport(hiveDataset.getTable().getCompleteName(), updateTime, rowCounts, rowDataValidatedCount.get(0));\n\n              return null;\n            }\n          }));\n        } else {\n          log.warn(String.format(\"No config found for format: %s So skipping table: %s for this format\", format, hiveDataset.getTable().getCompleteName()));\n        }\n      }\n    } catch (UncheckedExecutionException e) {\n      log.warn(String.format(\"Not validating table: %s %s\", hiveDataset.getTable().getCompleteName(), e.getMessage()));\n    } catch (UpdateNotFoundException e) {\n      log.warn(String\n          .format(\"Not validating table: %s as update time was not found. %s\", hiveDataset.getTable().getCompleteName(),\n              e.getMessage()));\n    }\n  }\n","realPath":"gobblin-modules/gobblin-azkaban/src/main/java/org/apache/gobblin/data/management/conversion/hive/validation/ValidationJob.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":363,"status":"B"},{"authorDate":"2017-07-31 15:51:25","commitOrder":1,"curCode":"  private void processPartitionedTable(ConvertibleHiveDataset hiveDataset, AutoReturnableObject<IMetaStoreClient> client) throws IOException {\n\n    \r\n    List<Partition> sourcePartitions = HiveUtils.getPartitions(client.get(), hiveDataset.getTable(), Optional.<String> absent());\n\n    for (final String format : hiveDataset.getDestFormats()) {\n      Optional<ConvertibleHiveDataset.ConversionConfig> conversionConfigOptional = hiveDataset.getConversionConfigForFormat(format);\n\n      if (conversionConfigOptional.isPresent()) {\n\n        \r\n        ConvertibleHiveDataset.ConversionConfig conversionConfig = conversionConfigOptional.get();\n        String orcTableName = conversionConfig.getDestinationTableName();\n        String orcTableDatabase = conversionConfig.getDestinationDbName();\n        Pair<Optional<org.apache.hadoop.hive.metastore.api.Table>, Optional<List<Partition>>> destinationMeta =\n            getDestinationTableMeta(orcTableDatabase, orcTableName, this.props);\n\n        \r\n        for (final Partition sourcePartition : sourcePartitions) {\n          try {\n            final long updateTime = this.updateProvider.getUpdateTime(sourcePartition);\n            if (shouldValidate(sourcePartition)) {\n              log.info(String.format(\"Validating partition: %s\", sourcePartition.getCompleteName()));\n\n              \r\n              final List<String> countValidationQueries =\n                  HiveValidationQueryGenerator.generateCountValidationQueries(hiveDataset, Optional.of(sourcePartition), conversionConfig);\n              final List<String> dataValidationQueries =\n                  Lists.newArrayList(HiveValidationQueryGenerator.generateDataValidationQuery(hiveDataset.getTable().getTableName(), hiveDataset.getTable()\n                      .getDbName(), destinationMeta.getKey().get(), Optional.of(sourcePartition), this.isNestedORC));\n\n              this.futures.add(this.exec.submit(new Callable<Void>() {\n                @Override\n                public Void call() throws Exception {\n\n                  \r\n                  log.debug(String.format(\"Going to execute count validation queries queries: %s for format: %s \"\n                      + \"and partition %s\", countValidationQueries, format, sourcePartition.getCompleteName()));\n                  List<Long> rowCounts = ValidationJob.this.getValidationOutputFromHive(countValidationQueries);\n                  log.debug(String.format(\"Going to execute data validation queries: %s for format: %s and partition %s\",\n                      dataValidationQueries, format, sourcePartition.getCompleteName()));\n                  List<Long> rowDataValidatedCount = ValidationJob.this.getValidationOutputFromHive(dataValidationQueries);\n\n                  \r\n                  validateAndPopulateReport(sourcePartition.getCompleteName(), updateTime, rowCounts, rowDataValidatedCount.get(0));\n\n                  return null;\n                }\n              }));\n\n            } else {\n              log.debug(String.format(\"Not validating partition: %s as updateTime: %s is not in range of max look back: %s \" + \"and skip recent than: %s\",\n                  sourcePartition.getCompleteName(), updateTime, this.maxLookBackTime, this.skipRecentThanTime));\n            }\n          } catch (UncheckedExecutionException e) {\n            log.warn(\n                String.format(\"Not validating partition: %s %s\", sourcePartition.getCompleteName(), e.getMessage()));\n          } catch (UpdateNotFoundException e) {\n            log.warn(String.format(\"Not validating partition: %s as update time was not found. %s\",\n                sourcePartition.getCompleteName(), e.getMessage()));\n          }\n        }\n      } else {\n        log.info(String.format(\"No conversion config found for format %s. Ignoring data validation\", format));\n      }\n    }\n  }\n","date":"2017-07-31 15:51:25","endLine":487,"groupId":"6122","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"processPartitionedTable","params":"(ConvertibleHiveDatasethiveDataset@AutoReturnableObject<IMetaStoreClient>client)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/8b/f2c21013cb6df87bd796f56eb1c72ff3343f01.src","preCode":"  private void processPartitionedTable(ConvertibleHiveDataset hiveDataset, AutoReturnableObject<IMetaStoreClient> client) throws IOException {\n\n    \r\n    List<Partition> sourcePartitions = HiveUtils.getPartitions(client.get(), hiveDataset.getTable(), Optional.<String> absent());\n\n    for (final String format : hiveDataset.getDestFormats()) {\n      Optional<ConvertibleHiveDataset.ConversionConfig> conversionConfigOptional = hiveDataset.getConversionConfigForFormat(format);\n\n      if (conversionConfigOptional.isPresent()) {\n\n        \r\n        ConvertibleHiveDataset.ConversionConfig conversionConfig = conversionConfigOptional.get();\n        String orcTableName = conversionConfig.getDestinationTableName();\n        String orcTableDatabase = conversionConfig.getDestinationDbName();\n        Pair<Optional<org.apache.hadoop.hive.metastore.api.Table>, Optional<List<Partition>>> destinationMeta =\n            getDestinationTableMeta(orcTableDatabase, orcTableName, this.props);\n\n        \r\n        for (final Partition sourcePartition : sourcePartitions) {\n          try {\n            final long updateTime = this.updateProvider.getUpdateTime(sourcePartition);\n            if (shouldValidate(sourcePartition)) {\n              log.info(String.format(\"Validating partition: %s\", sourcePartition.getCompleteName()));\n\n              \r\n              final List<String> countValidationQueries =\n                  HiveValidationQueryGenerator.generateCountValidationQueries(hiveDataset, Optional.of(sourcePartition), conversionConfig);\n              final List<String> dataValidationQueries =\n                  Lists.newArrayList(HiveValidationQueryGenerator.generateDataValidationQuery(hiveDataset.getTable().getTableName(), hiveDataset.getTable()\n                      .getDbName(), destinationMeta.getKey().get(), Optional.of(sourcePartition), this.isNestedORC));\n\n              this.futures.add(this.exec.submit(new Callable<Void>() {\n                @Override\n                public Void call() throws Exception {\n\n                  \r\n                  log.debug(String.format(\"Going to execute count validation queries queries: %s for format: %s \"\n                      + \"and partition %s\", countValidationQueries, format, sourcePartition.getCompleteName()));\n                  List<Long> rowCounts = ValidationJob.this.getValidationOutputFromHive(countValidationQueries);\n                  log.debug(String.format(\"Going to execute data validation queries: %s for format: %s and partition %s\",\n                      dataValidationQueries, format, sourcePartition.getCompleteName()));\n                  List<Long> rowDataValidatedCount = ValidationJob.this.getValidationOutputFromHive(dataValidationQueries);\n\n                  \r\n                  validateAndPopulateReport(sourcePartition.getCompleteName(), updateTime, rowCounts, rowDataValidatedCount.get(0));\n\n                  return null;\n                }\n              }));\n\n            } else {\n              log.debug(String.format(\"Not validating partition: %s as updateTime: %s is not in range of max look back: %s \" + \"and skip recent than: %s\",\n                  sourcePartition.getCompleteName(), updateTime, this.maxLookBackTime, this.skipRecentThanTime));\n            }\n          } catch (UncheckedExecutionException e) {\n            log.warn(\n                String.format(\"Not validating partition: %s %s\", sourcePartition.getCompleteName(), e.getMessage()));\n          } catch (UpdateNotFoundException e) {\n            log.warn(String.format(\"Not validating partition: %s as update time was not found. %s\",\n                sourcePartition.getCompleteName(), e.getMessage()));\n          }\n        }\n      } else {\n        log.info(String.format(\"No conversion config found for format %s. Ignoring data validation\", format));\n      }\n    }\n  }\n","realPath":"gobblin-modules/gobblin-azkaban/src/main/java/org/apache/gobblin/data/management/conversion/hive/validation/ValidationJob.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":421,"status":"B"}],"commitId":"5457af88d56b8fb89b172129fd1ff24ecdd4eba8","commitMessage":"@@@Merge pull request #2031 from abti/move_packages_to_apache\n","date":"2017-07-31 15:51:25","modifiedFileCount":"2","status":"B","submitter":"Abhishek Tiwari"},{"authorTime":"2017-09-06 02:23:29","codes":[{"authorDate":"2017-09-06 02:23:29","commitOrder":2,"curCode":"  private void processNonPartitionedTable(final ConvertibleHiveDataset hiveDataset) throws IOException {\n    try {\n      \r\n      final long updateTime = this.updateProvider.getUpdateTime(hiveDataset.getTable());\n\n      log.info(String.format(\"Validating table: %s\", hiveDataset.getTable()));\n\n      for (final String format : hiveDataset.getDestFormats()) {\n        Optional<ConvertibleHiveDataset.ConversionConfig> conversionConfigOptional = hiveDataset.getConversionConfigForFormat(format);\n        if (conversionConfigOptional.isPresent()) {\n          ConvertibleHiveDataset.ConversionConfig conversionConfig = conversionConfigOptional.get();\n          String orcTableName = conversionConfig.getDestinationTableName();\n          String orcTableDatabase = conversionConfig.getDestinationDbName();\n          Pair<Optional<org.apache.hadoop.hive.metastore.api.Table>, Optional<List<Partition>>> destinationMeta =\n              HiveConverterUtils.getDestinationTableMeta(orcTableDatabase, orcTableName, this.props);\n\n          \r\n          final List<String> validationQueries =\n              HiveValidationQueryGenerator.generateCountValidationQueries(hiveDataset, Optional.<Partition> absent(), conversionConfig);\n          final List<String> dataValidationQueries =\n              Lists.newArrayList(HiveValidationQueryGenerator.generateDataValidationQuery(hiveDataset.getTable().getTableName(), hiveDataset.getTable()\n                  .getDbName(), destinationMeta.getKey().get(), Optional.<Partition> absent(), this.isNestedORC));\n\n          this.futures.add(this.exec.submit(new Callable<Void>() {\n            @Override\n            public Void call() throws Exception {\n\n              \r\n              log.debug(String.format(\"Going to execute queries: %s for format: %s\", validationQueries, format));\n              List<Long> rowCounts = ValidationJob.this.getValidationOutputFromHive(validationQueries);\n              log.debug(String.format(\"Going to execute queries: %s for format: %s\", dataValidationQueries, format));\n              List<Long> rowDataValidatedCount = ValidationJob.this.getValidationOutputFromHive(dataValidationQueries);\n              \r\n              validateAndPopulateReport(hiveDataset.getTable().getCompleteName(), updateTime, rowCounts, rowDataValidatedCount.get(0));\n\n              return null;\n            }\n          }));\n        } else {\n          log.warn(String.format(\"No config found for format: %s So skipping table: %s for this format\", format, hiveDataset.getTable().getCompleteName()));\n        }\n      }\n    } catch (UncheckedExecutionException e) {\n      log.warn(String.format(\"Not validating table: %s %s\", hiveDataset.getTable().getCompleteName(), e.getMessage()));\n    } catch (UpdateNotFoundException e) {\n      log.warn(String\n          .format(\"Not validating table: %s as update time was not found. %s\", hiveDataset.getTable().getCompleteName(),\n              e.getMessage()));\n    }\n  }\n","date":"2017-09-06 02:23:29","endLine":410,"groupId":"6891","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"processNonPartitionedTable","params":"(finalConvertibleHiveDatasethiveDataset)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/92/22d485a05fcbb6ec128d67234d33e339ede1d4.src","preCode":"  private void processNonPartitionedTable(final ConvertibleHiveDataset hiveDataset) throws IOException {\n    try {\n      \r\n      final long updateTime = this.updateProvider.getUpdateTime(hiveDataset.getTable());\n\n      log.info(String.format(\"Validating table: %s\", hiveDataset.getTable()));\n\n      for (final String format : hiveDataset.getDestFormats()) {\n        Optional<ConvertibleHiveDataset.ConversionConfig> conversionConfigOptional = hiveDataset.getConversionConfigForFormat(format);\n        if (conversionConfigOptional.isPresent()) {\n          ConvertibleHiveDataset.ConversionConfig conversionConfig = conversionConfigOptional.get();\n          String orcTableName = conversionConfig.getDestinationTableName();\n          String orcTableDatabase = conversionConfig.getDestinationDbName();\n          Pair<Optional<org.apache.hadoop.hive.metastore.api.Table>, Optional<List<Partition>>> destinationMeta =\n              getDestinationTableMeta(orcTableDatabase, orcTableName, this.props);\n\n          \r\n          final List<String> validationQueries =\n              HiveValidationQueryGenerator.generateCountValidationQueries(hiveDataset, Optional.<Partition> absent(), conversionConfig);\n          final List<String> dataValidationQueries =\n              Lists.newArrayList(HiveValidationQueryGenerator.generateDataValidationQuery(hiveDataset.getTable().getTableName(), hiveDataset.getTable()\n                  .getDbName(), destinationMeta.getKey().get(), Optional.<Partition> absent(), this.isNestedORC));\n\n          this.futures.add(this.exec.submit(new Callable<Void>() {\n            @Override\n            public Void call() throws Exception {\n\n              \r\n              log.debug(String.format(\"Going to execute queries: %s for format: %s\", validationQueries, format));\n              List<Long> rowCounts = ValidationJob.this.getValidationOutputFromHive(validationQueries);\n              log.debug(String.format(\"Going to execute queries: %s for format: %s\", dataValidationQueries, format));\n              List<Long> rowDataValidatedCount = ValidationJob.this.getValidationOutputFromHive(dataValidationQueries);\n              \r\n              validateAndPopulateReport(hiveDataset.getTable().getCompleteName(), updateTime, rowCounts, rowDataValidatedCount.get(0));\n\n              return null;\n            }\n          }));\n        } else {\n          log.warn(String.format(\"No config found for format: %s So skipping table: %s for this format\", format, hiveDataset.getTable().getCompleteName()));\n        }\n      }\n    } catch (UncheckedExecutionException e) {\n      log.warn(String.format(\"Not validating table: %s %s\", hiveDataset.getTable().getCompleteName(), e.getMessage()));\n    } catch (UpdateNotFoundException e) {\n      log.warn(String\n          .format(\"Not validating table: %s as update time was not found. %s\", hiveDataset.getTable().getCompleteName(),\n              e.getMessage()));\n    }\n  }\n","realPath":"gobblin-modules/gobblin-azkaban/src/main/java/org/apache/gobblin/data/management/conversion/hive/validation/ValidationJob.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":361,"status":"M"},{"authorDate":"2017-09-06 02:23:29","commitOrder":2,"curCode":"  private void processPartitionedTable(ConvertibleHiveDataset hiveDataset, AutoReturnableObject<IMetaStoreClient> client) throws IOException {\n\n    \r\n    List<Partition> sourcePartitions = HiveUtils.getPartitions(client.get(), hiveDataset.getTable(), Optional.<String> absent());\n\n    for (final String format : hiveDataset.getDestFormats()) {\n      Optional<ConvertibleHiveDataset.ConversionConfig> conversionConfigOptional = hiveDataset.getConversionConfigForFormat(format);\n\n      if (conversionConfigOptional.isPresent()) {\n\n        \r\n        ConvertibleHiveDataset.ConversionConfig conversionConfig = conversionConfigOptional.get();\n        String orcTableName = conversionConfig.getDestinationTableName();\n        String orcTableDatabase = conversionConfig.getDestinationDbName();\n        Pair<Optional<org.apache.hadoop.hive.metastore.api.Table>, Optional<List<Partition>>> destinationMeta =\n            HiveConverterUtils.getDestinationTableMeta(orcTableDatabase, orcTableName, this.props);\n\n        \r\n        for (final Partition sourcePartition : sourcePartitions) {\n          try {\n            final long updateTime = this.updateProvider.getUpdateTime(sourcePartition);\n            if (shouldValidate(sourcePartition)) {\n              log.info(String.format(\"Validating partition: %s\", sourcePartition.getCompleteName()));\n\n              \r\n              final List<String> countValidationQueries =\n                  HiveValidationQueryGenerator.generateCountValidationQueries(hiveDataset, Optional.of(sourcePartition), conversionConfig);\n              final List<String> dataValidationQueries =\n                  Lists.newArrayList(HiveValidationQueryGenerator.generateDataValidationQuery(hiveDataset.getTable().getTableName(), hiveDataset.getTable()\n                      .getDbName(), destinationMeta.getKey().get(), Optional.of(sourcePartition), this.isNestedORC));\n\n              this.futures.add(this.exec.submit(new Callable<Void>() {\n                @Override\n                public Void call() throws Exception {\n\n                  \r\n                  log.debug(String.format(\"Going to execute count validation queries queries: %s for format: %s \"\n                      + \"and partition %s\", countValidationQueries, format, sourcePartition.getCompleteName()));\n                  List<Long> rowCounts = ValidationJob.this.getValidationOutputFromHive(countValidationQueries);\n                  log.debug(String.format(\"Going to execute data validation queries: %s for format: %s and partition %s\",\n                      dataValidationQueries, format, sourcePartition.getCompleteName()));\n                  List<Long> rowDataValidatedCount = ValidationJob.this.getValidationOutputFromHive(dataValidationQueries);\n\n                  \r\n                  validateAndPopulateReport(sourcePartition.getCompleteName(), updateTime, rowCounts, rowDataValidatedCount.get(0));\n\n                  return null;\n                }\n              }));\n\n            } else {\n              log.debug(String.format(\"Not validating partition: %s as updateTime: %s is not in range of max look back: %s \" + \"and skip recent than: %s\",\n                  sourcePartition.getCompleteName(), updateTime, this.maxLookBackTime, this.skipRecentThanTime));\n            }\n          } catch (UncheckedExecutionException e) {\n            log.warn(\n                String.format(\"Not validating partition: %s %s\", sourcePartition.getCompleteName(), e.getMessage()));\n          } catch (UpdateNotFoundException e) {\n            log.warn(String.format(\"Not validating partition: %s as update time was not found. %s\",\n                sourcePartition.getCompleteName(), e.getMessage()));\n          }\n        }\n      } else {\n        log.info(String.format(\"No conversion config found for format %s. Ignoring data validation\", format));\n      }\n    }\n  }\n","date":"2017-09-06 02:23:29","endLine":485,"groupId":"6122","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"processPartitionedTable","params":"(ConvertibleHiveDatasethiveDataset@AutoReturnableObject<IMetaStoreClient>client)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/92/22d485a05fcbb6ec128d67234d33e339ede1d4.src","preCode":"  private void processPartitionedTable(ConvertibleHiveDataset hiveDataset, AutoReturnableObject<IMetaStoreClient> client) throws IOException {\n\n    \r\n    List<Partition> sourcePartitions = HiveUtils.getPartitions(client.get(), hiveDataset.getTable(), Optional.<String> absent());\n\n    for (final String format : hiveDataset.getDestFormats()) {\n      Optional<ConvertibleHiveDataset.ConversionConfig> conversionConfigOptional = hiveDataset.getConversionConfigForFormat(format);\n\n      if (conversionConfigOptional.isPresent()) {\n\n        \r\n        ConvertibleHiveDataset.ConversionConfig conversionConfig = conversionConfigOptional.get();\n        String orcTableName = conversionConfig.getDestinationTableName();\n        String orcTableDatabase = conversionConfig.getDestinationDbName();\n        Pair<Optional<org.apache.hadoop.hive.metastore.api.Table>, Optional<List<Partition>>> destinationMeta =\n            getDestinationTableMeta(orcTableDatabase, orcTableName, this.props);\n\n        \r\n        for (final Partition sourcePartition : sourcePartitions) {\n          try {\n            final long updateTime = this.updateProvider.getUpdateTime(sourcePartition);\n            if (shouldValidate(sourcePartition)) {\n              log.info(String.format(\"Validating partition: %s\", sourcePartition.getCompleteName()));\n\n              \r\n              final List<String> countValidationQueries =\n                  HiveValidationQueryGenerator.generateCountValidationQueries(hiveDataset, Optional.of(sourcePartition), conversionConfig);\n              final List<String> dataValidationQueries =\n                  Lists.newArrayList(HiveValidationQueryGenerator.generateDataValidationQuery(hiveDataset.getTable().getTableName(), hiveDataset.getTable()\n                      .getDbName(), destinationMeta.getKey().get(), Optional.of(sourcePartition), this.isNestedORC));\n\n              this.futures.add(this.exec.submit(new Callable<Void>() {\n                @Override\n                public Void call() throws Exception {\n\n                  \r\n                  log.debug(String.format(\"Going to execute count validation queries queries: %s for format: %s \"\n                      + \"and partition %s\", countValidationQueries, format, sourcePartition.getCompleteName()));\n                  List<Long> rowCounts = ValidationJob.this.getValidationOutputFromHive(countValidationQueries);\n                  log.debug(String.format(\"Going to execute data validation queries: %s for format: %s and partition %s\",\n                      dataValidationQueries, format, sourcePartition.getCompleteName()));\n                  List<Long> rowDataValidatedCount = ValidationJob.this.getValidationOutputFromHive(dataValidationQueries);\n\n                  \r\n                  validateAndPopulateReport(sourcePartition.getCompleteName(), updateTime, rowCounts, rowDataValidatedCount.get(0));\n\n                  return null;\n                }\n              }));\n\n            } else {\n              log.debug(String.format(\"Not validating partition: %s as updateTime: %s is not in range of max look back: %s \" + \"and skip recent than: %s\",\n                  sourcePartition.getCompleteName(), updateTime, this.maxLookBackTime, this.skipRecentThanTime));\n            }\n          } catch (UncheckedExecutionException e) {\n            log.warn(\n                String.format(\"Not validating partition: %s %s\", sourcePartition.getCompleteName(), e.getMessage()));\n          } catch (UpdateNotFoundException e) {\n            log.warn(String.format(\"Not validating partition: %s as update time was not found. %s\",\n                sourcePartition.getCompleteName(), e.getMessage()));\n          }\n        }\n      } else {\n        log.info(String.format(\"No conversion config found for format %s. Ignoring data validation\", format));\n      }\n    }\n  }\n","realPath":"gobblin-modules/gobblin-azkaban/src/main/java/org/apache/gobblin/data/management/conversion/hive/validation/ValidationJob.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":419,"status":"M"}],"commitId":"8a374f207bc7838bcfb144d644b6243c78dc122d","commitMessage":"@@@[GOBBLIN-181] Create HiveTask using customized Gobblin Task\n\nCloses #2062 from arjun4084346/materializer\n","date":"2017-09-06 02:23:29","modifiedFileCount":"5","status":"M","submitter":"Arjun"},{"authorTime":"2017-09-12 18:05:57","codes":[{"authorDate":"2017-09-12 18:05:57","commitOrder":3,"curCode":"  private void processNonPartitionedTable(final ConvertibleHiveDataset hiveDataset) throws IOException {\n    try {\n      \r\n      final long updateTime = this.updateProvider.getUpdateTime(hiveDataset.getTable());\n\n      log.info(String.format(\"Validating table: %s\", hiveDataset.getTable()));\n\n      for (final String format : hiveDataset.getDestFormats()) {\n        Optional<ConvertibleHiveDataset.ConversionConfig> conversionConfigOptional = hiveDataset.getConversionConfigForFormat(format);\n        if (conversionConfigOptional.isPresent()) {\n          ConvertibleHiveDataset.ConversionConfig conversionConfig = conversionConfigOptional.get();\n          String orcTableName = conversionConfig.getDestinationTableName();\n          String orcTableDatabase = conversionConfig.getDestinationDbName();\n          Pair<Optional<org.apache.hadoop.hive.metastore.api.Table>, Optional<List<Partition>>> destinationMeta =\n              HiveConverterUtils.getDestinationTableMeta(orcTableDatabase, orcTableName, this.props);\n\n          \r\n          final List<String> validationQueries =\n              HiveValidationQueryGenerator.generateCountValidationQueries(hiveDataset, Optional.<Partition> absent(), conversionConfig);\n          final List<String> dataValidationQueries =\n              Lists.newArrayList(HiveValidationQueryGenerator.generateDataValidationQuery(hiveDataset.getTable().getTableName(), hiveDataset.getTable()\n                  .getDbName(), destinationMeta.getKey().get(), Optional.<Partition> absent(), this.isNestedORC));\n\n          this.futures.add(this.exec.submit(new Callable<Void>() {\n            @Override\n            public Void call() throws Exception {\n\n              \r\n              log.debug(String.format(\"Going to execute queries: %s for format: %s\", validationQueries, format));\n              List<Long> rowCounts = ValidationJob.this.getValidationOutputFromHive(validationQueries);\n              log.debug(String.format(\"Going to execute queries: %s for format: %s\", dataValidationQueries, format));\n              List<Long> rowDataValidatedCount = ValidationJob.this.getValidationOutputFromHive(dataValidationQueries);\n              \r\n              validateAndPopulateReport(hiveDataset.getTable().getCompleteName(), updateTime, rowCounts, rowDataValidatedCount);\n\n              return null;\n            }\n          }));\n        } else {\n          log.warn(String.format(\"No config found for format: %s So skipping table: %s for this format\", format, hiveDataset.getTable().getCompleteName()));\n        }\n      }\n    } catch (UncheckedExecutionException e) {\n      log.warn(String.format(\"Not validating table: %s %s\", hiveDataset.getTable().getCompleteName(), e.getMessage()));\n    } catch (UpdateNotFoundException e) {\n      log.warn(String\n          .format(\"Not validating table: %s as update time was not found. %s\", hiveDataset.getTable().getCompleteName(),\n              e.getMessage()));\n    }\n  }\n","date":"2017-09-12 18:05:57","endLine":410,"groupId":"10193","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"processNonPartitionedTable","params":"(finalConvertibleHiveDatasethiveDataset)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/1a/618037acedf78be3601c62f67866b6451b1915.src","preCode":"  private void processNonPartitionedTable(final ConvertibleHiveDataset hiveDataset) throws IOException {\n    try {\n      \r\n      final long updateTime = this.updateProvider.getUpdateTime(hiveDataset.getTable());\n\n      log.info(String.format(\"Validating table: %s\", hiveDataset.getTable()));\n\n      for (final String format : hiveDataset.getDestFormats()) {\n        Optional<ConvertibleHiveDataset.ConversionConfig> conversionConfigOptional = hiveDataset.getConversionConfigForFormat(format);\n        if (conversionConfigOptional.isPresent()) {\n          ConvertibleHiveDataset.ConversionConfig conversionConfig = conversionConfigOptional.get();\n          String orcTableName = conversionConfig.getDestinationTableName();\n          String orcTableDatabase = conversionConfig.getDestinationDbName();\n          Pair<Optional<org.apache.hadoop.hive.metastore.api.Table>, Optional<List<Partition>>> destinationMeta =\n              HiveConverterUtils.getDestinationTableMeta(orcTableDatabase, orcTableName, this.props);\n\n          \r\n          final List<String> validationQueries =\n              HiveValidationQueryGenerator.generateCountValidationQueries(hiveDataset, Optional.<Partition> absent(), conversionConfig);\n          final List<String> dataValidationQueries =\n              Lists.newArrayList(HiveValidationQueryGenerator.generateDataValidationQuery(hiveDataset.getTable().getTableName(), hiveDataset.getTable()\n                  .getDbName(), destinationMeta.getKey().get(), Optional.<Partition> absent(), this.isNestedORC));\n\n          this.futures.add(this.exec.submit(new Callable<Void>() {\n            @Override\n            public Void call() throws Exception {\n\n              \r\n              log.debug(String.format(\"Going to execute queries: %s for format: %s\", validationQueries, format));\n              List<Long> rowCounts = ValidationJob.this.getValidationOutputFromHive(validationQueries);\n              log.debug(String.format(\"Going to execute queries: %s for format: %s\", dataValidationQueries, format));\n              List<Long> rowDataValidatedCount = ValidationJob.this.getValidationOutputFromHive(dataValidationQueries);\n              \r\n              validateAndPopulateReport(hiveDataset.getTable().getCompleteName(), updateTime, rowCounts, rowDataValidatedCount.get(0));\n\n              return null;\n            }\n          }));\n        } else {\n          log.warn(String.format(\"No config found for format: %s So skipping table: %s for this format\", format, hiveDataset.getTable().getCompleteName()));\n        }\n      }\n    } catch (UncheckedExecutionException e) {\n      log.warn(String.format(\"Not validating table: %s %s\", hiveDataset.getTable().getCompleteName(), e.getMessage()));\n    } catch (UpdateNotFoundException e) {\n      log.warn(String\n          .format(\"Not validating table: %s as update time was not found. %s\", hiveDataset.getTable().getCompleteName(),\n              e.getMessage()));\n    }\n  }\n","realPath":"gobblin-modules/gobblin-azkaban/src/main/java/org/apache/gobblin/data/management/conversion/hive/validation/ValidationJob.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":361,"status":"M"},{"authorDate":"2017-09-12 18:05:57","commitOrder":3,"curCode":"  private void processPartitionedTable(ConvertibleHiveDataset hiveDataset, AutoReturnableObject<IMetaStoreClient> client) throws IOException {\n\n    \r\n    List<Partition> sourcePartitions = HiveUtils.getPartitions(client.get(), hiveDataset.getTable(), Optional.<String> absent());\n\n    for (final String format : hiveDataset.getDestFormats()) {\n      Optional<ConvertibleHiveDataset.ConversionConfig> conversionConfigOptional = hiveDataset.getConversionConfigForFormat(format);\n\n      if (conversionConfigOptional.isPresent()) {\n\n        \r\n        ConvertibleHiveDataset.ConversionConfig conversionConfig = conversionConfigOptional.get();\n        String orcTableName = conversionConfig.getDestinationTableName();\n        String orcTableDatabase = conversionConfig.getDestinationDbName();\n        Pair<Optional<org.apache.hadoop.hive.metastore.api.Table>, Optional<List<Partition>>> destinationMeta =\n            HiveConverterUtils.getDestinationTableMeta(orcTableDatabase, orcTableName, this.props);\n\n        \r\n        for (final Partition sourcePartition : sourcePartitions) {\n          try {\n            final long updateTime = this.updateProvider.getUpdateTime(sourcePartition);\n            if (shouldValidate(sourcePartition)) {\n              log.info(String.format(\"Validating partition: %s\", sourcePartition.getCompleteName()));\n\n              \r\n              final List<String> countValidationQueries =\n                  HiveValidationQueryGenerator.generateCountValidationQueries(hiveDataset, Optional.of(sourcePartition), conversionConfig);\n              final List<String> dataValidationQueries =\n                  Lists.newArrayList(HiveValidationQueryGenerator.generateDataValidationQuery(hiveDataset.getTable().getTableName(), hiveDataset.getTable()\n                      .getDbName(), destinationMeta.getKey().get(), Optional.of(sourcePartition), this.isNestedORC));\n\n              this.futures.add(this.exec.submit(new Callable<Void>() {\n                @Override\n                public Void call() throws Exception {\n\n                  \r\n                  log.debug(String.format(\"Going to execute count validation queries queries: %s for format: %s \"\n                      + \"and partition %s\", countValidationQueries, format, sourcePartition.getCompleteName()));\n                  List<Long> rowCounts = ValidationJob.this.getValidationOutputFromHive(countValidationQueries);\n                  log.debug(String.format(\"Going to execute data validation queries: %s for format: %s and partition %s\",\n                      dataValidationQueries, format, sourcePartition.getCompleteName()));\n                  List<Long> rowDataValidatedCount = ValidationJob.this.getValidationOutputFromHive(dataValidationQueries);\n\n                  \r\n                  validateAndPopulateReport(sourcePartition.getCompleteName(), updateTime, rowCounts, rowDataValidatedCount);\n\n                  return null;\n                }\n              }));\n\n            } else {\n              log.debug(String.format(\"Not validating partition: %s as updateTime: %s is not in range of max look back: %s \" + \"and skip recent than: %s\",\n                  sourcePartition.getCompleteName(), updateTime, this.maxLookBackTime, this.skipRecentThanTime));\n            }\n          } catch (UncheckedExecutionException e) {\n            log.warn(\n                String.format(\"Not validating partition: %s %s\", sourcePartition.getCompleteName(), e.getMessage()));\n          } catch (UpdateNotFoundException e) {\n            log.warn(String.format(\"Not validating partition: %s as update time was not found. %s\",\n                sourcePartition.getCompleteName(), e.getMessage()));\n          }\n        }\n      } else {\n        log.info(String.format(\"No conversion config found for format %s. Ignoring data validation\", format));\n      }\n    }\n  }\n","date":"2017-09-12 18:05:57","endLine":485,"groupId":"10193","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"processPartitionedTable","params":"(ConvertibleHiveDatasethiveDataset@AutoReturnableObject<IMetaStoreClient>client)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/1a/618037acedf78be3601c62f67866b6451b1915.src","preCode":"  private void processPartitionedTable(ConvertibleHiveDataset hiveDataset, AutoReturnableObject<IMetaStoreClient> client) throws IOException {\n\n    \r\n    List<Partition> sourcePartitions = HiveUtils.getPartitions(client.get(), hiveDataset.getTable(), Optional.<String> absent());\n\n    for (final String format : hiveDataset.getDestFormats()) {\n      Optional<ConvertibleHiveDataset.ConversionConfig> conversionConfigOptional = hiveDataset.getConversionConfigForFormat(format);\n\n      if (conversionConfigOptional.isPresent()) {\n\n        \r\n        ConvertibleHiveDataset.ConversionConfig conversionConfig = conversionConfigOptional.get();\n        String orcTableName = conversionConfig.getDestinationTableName();\n        String orcTableDatabase = conversionConfig.getDestinationDbName();\n        Pair<Optional<org.apache.hadoop.hive.metastore.api.Table>, Optional<List<Partition>>> destinationMeta =\n            HiveConverterUtils.getDestinationTableMeta(orcTableDatabase, orcTableName, this.props);\n\n        \r\n        for (final Partition sourcePartition : sourcePartitions) {\n          try {\n            final long updateTime = this.updateProvider.getUpdateTime(sourcePartition);\n            if (shouldValidate(sourcePartition)) {\n              log.info(String.format(\"Validating partition: %s\", sourcePartition.getCompleteName()));\n\n              \r\n              final List<String> countValidationQueries =\n                  HiveValidationQueryGenerator.generateCountValidationQueries(hiveDataset, Optional.of(sourcePartition), conversionConfig);\n              final List<String> dataValidationQueries =\n                  Lists.newArrayList(HiveValidationQueryGenerator.generateDataValidationQuery(hiveDataset.getTable().getTableName(), hiveDataset.getTable()\n                      .getDbName(), destinationMeta.getKey().get(), Optional.of(sourcePartition), this.isNestedORC));\n\n              this.futures.add(this.exec.submit(new Callable<Void>() {\n                @Override\n                public Void call() throws Exception {\n\n                  \r\n                  log.debug(String.format(\"Going to execute count validation queries queries: %s for format: %s \"\n                      + \"and partition %s\", countValidationQueries, format, sourcePartition.getCompleteName()));\n                  List<Long> rowCounts = ValidationJob.this.getValidationOutputFromHive(countValidationQueries);\n                  log.debug(String.format(\"Going to execute data validation queries: %s for format: %s and partition %s\",\n                      dataValidationQueries, format, sourcePartition.getCompleteName()));\n                  List<Long> rowDataValidatedCount = ValidationJob.this.getValidationOutputFromHive(dataValidationQueries);\n\n                  \r\n                  validateAndPopulateReport(sourcePartition.getCompleteName(), updateTime, rowCounts, rowDataValidatedCount.get(0));\n\n                  return null;\n                }\n              }));\n\n            } else {\n              log.debug(String.format(\"Not validating partition: %s as updateTime: %s is not in range of max look back: %s \" + \"and skip recent than: %s\",\n                  sourcePartition.getCompleteName(), updateTime, this.maxLookBackTime, this.skipRecentThanTime));\n            }\n          } catch (UncheckedExecutionException e) {\n            log.warn(\n                String.format(\"Not validating partition: %s %s\", sourcePartition.getCompleteName(), e.getMessage()));\n          } catch (UpdateNotFoundException e) {\n            log.warn(String.format(\"Not validating partition: %s as update time was not found. %s\",\n                sourcePartition.getCompleteName(), e.getMessage()));\n          }\n        }\n      } else {\n        log.info(String.format(\"No conversion config found for format %s. Ignoring data validation\", format));\n      }\n    }\n  }\n","realPath":"gobblin-modules/gobblin-azkaban/src/main/java/org/apache/gobblin/data/management/conversion/hive/validation/ValidationJob.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":419,"status":"M"}],"commitId":"1507b6055d3f058cc17b1accb01c897a21b9513c","commitMessage":"@@@[GOBBLIN-247] Fixed ArrayIndexOutOfBoundsException for avro-to-orc Validation Job\n\nCloses #2100 from aditya1105/avro-to-orc\n","date":"2017-09-12 18:05:57","modifiedFileCount":"1","status":"M","submitter":"aditya1105"}]
