[{"authorTime":"2021-02-05 04:13:05","codes":[{"authorDate":"2021-03-24 06:21:10","commitOrder":2,"curCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    GenericRecord genericRecord = recordEnvelope.getRecord();\n    GobblinMetadataChangeEvent gmce =\n        (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n    if (whiteistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n      write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n    } else {\n      log.info(String.format(\"Skip table %s.%s since it's blacklisted\", tableSpec.getTable().getDbName(),\n          tableSpec.getTable().getTableName()));\n    }\n  }\n","date":"2021-03-24 06:21:10","endLine":284,"groupId":"3095","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"writeEnvelope","params":"(RecordEnvelope<GenericRecord>recordEnvelope@Map<String@Collection<HiveSpec>>newSpecsMap@Map<String@Collection<HiveSpec>>oldSpecsMap@HiveSpectableSpec)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/a4/550bc37d61a68b8a861b145dd7242a1bda03a8.src","preCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    GenericRecord genericRecord = recordEnvelope.getRecord();\n    GobblinMetadataChangeEvent gmce =\n        (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n    if (whiteistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n      write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n    } else {\n      log.info(String.format(\"Skip table %s.%s since it's blacklisted\", tableSpec.getTable().getDbName(),\n          tableSpec.getTable().getTableName()));\n    }\n  }\n","realPath":"gobblin-hive-registration/src/main/java/org/apache/gobblin/hive/writer/HiveMetadataWriter.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":273,"status":"B"},{"authorDate":"2021-02-05 04:13:05","commitOrder":2,"curCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    Lock readLock = readWriteLock.readLock();\n    readLock.lock();\n    try {\n      GenericRecord genericRecord = recordEnvelope.getRecord();\n      GobblinMetadataChangeEvent gmce =\n          (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n      if (whiteistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n        TableIdentifier tid = TableIdentifier.of(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName());\n        String topicPartition = tableTopicpartitionMap.computeIfAbsent(tid,\n            t -> ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getTopicPartition().toString());\n        Long currentWatermark = getCurrentWaterMark(tid, topicPartition);\n        Long currentOffset =\n            ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getLwm().getValue();\n        if (currentOffset > currentWatermark) {\n          if (currentWatermark == DEFAULT_WATERMARK) {\n            \r\n            tableMetadataMap.computeIfAbsent(tid, t -> new TableMetadata()).lowWatermark =\n                Optional.of(currentOffset - 1);\n          }\n          tableMetadataMap.get(tid).datasetName = gmce.getDatasetIdentifier().getNativeName();\n          write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n          tableCurrentWaterMarkMap.put(tid,\n              ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getLwm().getValue());\n        } else {\n          log.warn(String.format(\"Skip processing record %s since it has lower watermark\", genericRecord.toString()));\n        }\n      } else {\n        log.info(String.format(\"Skip table %s.%s since it's blacklisted\", tableSpec.getTable().getDbName(),\n            tableSpec.getTable().getTableName()));\n      }\n    } finally {\n      readLock.unlock();\n    }\n  }\n","date":"2021-02-05 04:13:05","endLine":799,"groupId":"3095","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"writeEnvelope","params":"(RecordEnvelope<GenericRecord>recordEnvelope@Map<String@Collection<HiveSpec>>newSpecsMap@Map<String@Collection<HiveSpec>>oldSpecsMap@HiveSpectableSpec)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/ed/89f1a290336f9bf5e926db9648556071bd5f36.src","preCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    Lock readLock = readWriteLock.readLock();\n    readLock.lock();\n    try {\n      GenericRecord genericRecord = recordEnvelope.getRecord();\n      GobblinMetadataChangeEvent gmce =\n          (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n      if (whiteistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n        TableIdentifier tid = TableIdentifier.of(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName());\n        String topicPartition = tableTopicpartitionMap.computeIfAbsent(tid,\n            t -> ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getTopicPartition().toString());\n        Long currentWatermark = getCurrentWaterMark(tid, topicPartition);\n        Long currentOffset =\n            ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getLwm().getValue();\n        if (currentOffset > currentWatermark) {\n          if (currentWatermark == DEFAULT_WATERMARK) {\n            \r\n            tableMetadataMap.computeIfAbsent(tid, t -> new TableMetadata()).lowWatermark =\n                Optional.of(currentOffset - 1);\n          }\n          tableMetadataMap.get(tid).datasetName = gmce.getDatasetIdentifier().getNativeName();\n          write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n          tableCurrentWaterMarkMap.put(tid,\n              ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getLwm().getValue());\n        } else {\n          log.warn(String.format(\"Skip processing record %s since it has lower watermark\", genericRecord.toString()));\n        }\n      } else {\n        log.info(String.format(\"Skip table %s.%s since it's blacklisted\", tableSpec.getTable().getDbName(),\n            tableSpec.getTable().getTableName()));\n      }\n    } finally {\n      readLock.unlock();\n    }\n  }\n","realPath":"gobblin-iceberg/src/main/java/org/apache/gobblin/iceberg/writer/IcebergMetadataWriter.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":764,"status":"NB"}],"commitId":"d9ae5353c74fdcd385835fca9b586b3fdb90971b","commitMessage":"@@@[GOBBLIN-1396] Enable HiveWriter to consume gmce and register into hive MetadataStore\n\n[GOBBLIN-1396]Enable HiveWriter to consume gmce\nand register into hive MetadataStore\n\nbug fix\n\nadd default branch to pass styleCheck\n\nbug fix to avoid dead lock\n\naddress comments\n\naddress comments\n\nenable rewrite/drop file in HiveMetadataWriter\n\nremove unintentional change\n\naddress comments\n\nforce to set schema literal\n\nCloses #3234 from ZihanLi58/GOBBLIN-1396\n","date":"2021-03-24 06:21:10","modifiedFileCount":"5","status":"M","submitter":"Zihan Li"},{"authorTime":"2021-02-05 04:13:05","codes":[{"authorDate":"2021-05-12 05:59:06","commitOrder":3,"curCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    GenericRecord genericRecord = recordEnvelope.getRecord();\n    GobblinMetadataChangeEvent gmce =\n        (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n    if (whiteistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n      write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n    } else {\n      log.debug(String.format(\"Skip table %s.%s since it's blacklisted\", tableSpec.getTable().getDbName(),\n          tableSpec.getTable().getTableName()));\n    }\n  }\n","date":"2021-05-14 06:37:46","endLine":285,"groupId":"3095","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"writeEnvelope","params":"(RecordEnvelope<GenericRecord>recordEnvelope@Map<String@Collection<HiveSpec>>newSpecsMap@Map<String@Collection<HiveSpec>>oldSpecsMap@HiveSpectableSpec)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/b1/1948c9cbf818cf856b7738a18d6f659dff5853.src","preCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    GenericRecord genericRecord = recordEnvelope.getRecord();\n    GobblinMetadataChangeEvent gmce =\n        (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n    if (whiteistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n      write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n    } else {\n      log.info(String.format(\"Skip table %s.%s since it's blacklisted\", tableSpec.getTable().getDbName(),\n          tableSpec.getTable().getTableName()));\n    }\n  }\n","realPath":"gobblin-hive-registration/src/main/java/org/apache/gobblin/hive/writer/HiveMetadataWriter.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":274,"status":"M"},{"authorDate":"2021-02-05 04:13:05","commitOrder":3,"curCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    Lock readLock = readWriteLock.readLock();\n    readLock.lock();\n    try {\n      GenericRecord genericRecord = recordEnvelope.getRecord();\n      GobblinMetadataChangeEvent gmce =\n          (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n      if (whiteistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n        TableIdentifier tid = TableIdentifier.of(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName());\n        String topicPartition = tableTopicpartitionMap.computeIfAbsent(tid,\n            t -> ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getTopicPartition().toString());\n        Long currentWatermark = getCurrentWaterMark(tid, topicPartition);\n        Long currentOffset =\n            ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getLwm().getValue();\n        if (currentOffset > currentWatermark) {\n          if (currentWatermark == DEFAULT_WATERMARK) {\n            \r\n            tableMetadataMap.computeIfAbsent(tid, t -> new TableMetadata()).lowWatermark =\n                Optional.of(currentOffset - 1);\n          }\n          tableMetadataMap.get(tid).datasetName = gmce.getDatasetIdentifier().getNativeName();\n          write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n          tableCurrentWaterMarkMap.put(tid,\n              ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getLwm().getValue());\n        } else {\n          log.warn(String.format(\"Skip processing record %s since it has lower watermark\", genericRecord.toString()));\n        }\n      } else {\n        log.info(String.format(\"Skip table %s.%s since it's blacklisted\", tableSpec.getTable().getDbName(),\n            tableSpec.getTable().getTableName()));\n      }\n    } finally {\n      readLock.unlock();\n    }\n  }\n","date":"2021-02-05 04:13:05","endLine":799,"groupId":"3095","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"writeEnvelope","params":"(RecordEnvelope<GenericRecord>recordEnvelope@Map<String@Collection<HiveSpec>>newSpecsMap@Map<String@Collection<HiveSpec>>oldSpecsMap@HiveSpectableSpec)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/ed/89f1a290336f9bf5e926db9648556071bd5f36.src","preCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    Lock readLock = readWriteLock.readLock();\n    readLock.lock();\n    try {\n      GenericRecord genericRecord = recordEnvelope.getRecord();\n      GobblinMetadataChangeEvent gmce =\n          (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n      if (whiteistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n        TableIdentifier tid = TableIdentifier.of(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName());\n        String topicPartition = tableTopicpartitionMap.computeIfAbsent(tid,\n            t -> ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getTopicPartition().toString());\n        Long currentWatermark = getCurrentWaterMark(tid, topicPartition);\n        Long currentOffset =\n            ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getLwm().getValue();\n        if (currentOffset > currentWatermark) {\n          if (currentWatermark == DEFAULT_WATERMARK) {\n            \r\n            tableMetadataMap.computeIfAbsent(tid, t -> new TableMetadata()).lowWatermark =\n                Optional.of(currentOffset - 1);\n          }\n          tableMetadataMap.get(tid).datasetName = gmce.getDatasetIdentifier().getNativeName();\n          write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n          tableCurrentWaterMarkMap.put(tid,\n              ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getLwm().getValue());\n        } else {\n          log.warn(String.format(\"Skip processing record %s since it has lower watermark\", genericRecord.toString()));\n        }\n      } else {\n        log.info(String.format(\"Skip table %s.%s since it's blacklisted\", tableSpec.getTable().getDbName(),\n            tableSpec.getTable().getTableName()));\n      }\n    } finally {\n      readLock.unlock();\n    }\n  }\n","realPath":"gobblin-iceberg/src/main/java/org/apache/gobblin/iceberg/writer/IcebergMetadataWriter.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":764,"status":"N"}],"commitId":"2c314647f803ef7d91b90d276199e7b58d5f8d7d","commitMessage":"@@@[GOBBLIN-1442]Fix the bug of NoSuchElement Exception in HiveWriter\n","date":"2021-05-14 06:37:46","modifiedFileCount":"1","status":"M","submitter":"Zihan Li"},{"authorTime":"2021-02-05 04:13:05","codes":[{"authorDate":"2021-07-16 03:33:00","commitOrder":4,"curCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    GenericRecord genericRecord = recordEnvelope.getRecord();\n    GobblinMetadataChangeEvent gmce =\n        (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n    if (whitelistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n      write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n    } else {\n      log.debug(String.format(\"Skip table %s.%s since it's not selected\", tableSpec.getTable().getDbName(),\n          tableSpec.getTable().getTableName()));\n    }\n  }\n","date":"2021-07-16 03:33:00","endLine":322,"groupId":"3095","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"writeEnvelope","params":"(RecordEnvelope<GenericRecord>recordEnvelope@Map<String@Collection<HiveSpec>>newSpecsMap@Map<String@Collection<HiveSpec>>oldSpecsMap@HiveSpectableSpec)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/55/fcd2b71fd0b39c429e342f36231d9eb7050404.src","preCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    GenericRecord genericRecord = recordEnvelope.getRecord();\n    GobblinMetadataChangeEvent gmce =\n        (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n    if (whiteistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n      write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n    } else {\n      log.debug(String.format(\"Skip table %s.%s since it's blacklisted\", tableSpec.getTable().getDbName(),\n          tableSpec.getTable().getTableName()));\n    }\n  }\n","realPath":"gobblin-hive-registration/src/main/java/org/apache/gobblin/hive/writer/HiveMetadataWriter.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":311,"status":"M"},{"authorDate":"2021-02-05 04:13:05","commitOrder":4,"curCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    Lock readLock = readWriteLock.readLock();\n    readLock.lock();\n    try {\n      GenericRecord genericRecord = recordEnvelope.getRecord();\n      GobblinMetadataChangeEvent gmce =\n          (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n      if (whiteistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n        TableIdentifier tid = TableIdentifier.of(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName());\n        String topicPartition = tableTopicpartitionMap.computeIfAbsent(tid,\n            t -> ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getTopicPartition().toString());\n        Long currentWatermark = getCurrentWaterMark(tid, topicPartition);\n        Long currentOffset =\n            ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getLwm().getValue();\n        if (currentOffset > currentWatermark) {\n          if (currentWatermark == DEFAULT_WATERMARK) {\n            \r\n            tableMetadataMap.computeIfAbsent(tid, t -> new TableMetadata()).lowWatermark =\n                Optional.of(currentOffset - 1);\n          }\n          tableMetadataMap.get(tid).datasetName = gmce.getDatasetIdentifier().getNativeName();\n          write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n          tableCurrentWaterMarkMap.put(tid,\n              ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getLwm().getValue());\n        } else {\n          log.warn(String.format(\"Skip processing record %s since it has lower watermark\", genericRecord.toString()));\n        }\n      } else {\n        log.info(String.format(\"Skip table %s.%s since it's blacklisted\", tableSpec.getTable().getDbName(),\n            tableSpec.getTable().getTableName()));\n      }\n    } finally {\n      readLock.unlock();\n    }\n  }\n","date":"2021-02-05 04:13:05","endLine":799,"groupId":"3095","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"writeEnvelope","params":"(RecordEnvelope<GenericRecord>recordEnvelope@Map<String@Collection<HiveSpec>>newSpecsMap@Map<String@Collection<HiveSpec>>oldSpecsMap@HiveSpectableSpec)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/ed/89f1a290336f9bf5e926db9648556071bd5f36.src","preCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    Lock readLock = readWriteLock.readLock();\n    readLock.lock();\n    try {\n      GenericRecord genericRecord = recordEnvelope.getRecord();\n      GobblinMetadataChangeEvent gmce =\n          (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n      if (whiteistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n        TableIdentifier tid = TableIdentifier.of(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName());\n        String topicPartition = tableTopicpartitionMap.computeIfAbsent(tid,\n            t -> ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getTopicPartition().toString());\n        Long currentWatermark = getCurrentWaterMark(tid, topicPartition);\n        Long currentOffset =\n            ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getLwm().getValue();\n        if (currentOffset > currentWatermark) {\n          if (currentWatermark == DEFAULT_WATERMARK) {\n            \r\n            tableMetadataMap.computeIfAbsent(tid, t -> new TableMetadata()).lowWatermark =\n                Optional.of(currentOffset - 1);\n          }\n          tableMetadataMap.get(tid).datasetName = gmce.getDatasetIdentifier().getNativeName();\n          write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n          tableCurrentWaterMarkMap.put(tid,\n              ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getLwm().getValue());\n        } else {\n          log.warn(String.format(\"Skip processing record %s since it has lower watermark\", genericRecord.toString()));\n        }\n      } else {\n        log.info(String.format(\"Skip table %s.%s since it's blacklisted\", tableSpec.getTable().getDbName(),\n            tableSpec.getTable().getTableName()));\n      }\n    } finally {\n      readLock.unlock();\n    }\n  }\n","realPath":"gobblin-iceberg/src/main/java/org/apache/gobblin/iceberg/writer/IcebergMetadataWriter.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":764,"status":"N"}],"commitId":"5fb30cb16f80ce6ea848e61c36e3c1d48b8bdd0f","commitMessage":"@@@[GOBBLIN-1486] Documentation improvement for Gobblin Metadata Ingestion pipeline (#3325)\n\nFixing javadoc and some typo while reading the code about metadata ingestion pipeline. This PR covers mostly the part of HiveMetadataWriter and GobblinMCEWriter.  there will be a follow up on IcebergMetadataWriter.","date":"2021-07-16 03:33:00","modifiedFileCount":"5","status":"M","submitter":"Lei"},{"authorTime":"2021-08-07 02:52:37","codes":[{"authorDate":"2021-07-16 03:33:00","commitOrder":5,"curCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    GenericRecord genericRecord = recordEnvelope.getRecord();\n    GobblinMetadataChangeEvent gmce =\n        (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n    if (whitelistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n      write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n    } else {\n      log.debug(String.format(\"Skip table %s.%s since it's not selected\", tableSpec.getTable().getDbName(),\n          tableSpec.getTable().getTableName()));\n    }\n  }\n","date":"2021-07-16 03:33:00","endLine":322,"groupId":"3095","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"writeEnvelope","params":"(RecordEnvelope<GenericRecord>recordEnvelope@Map<String@Collection<HiveSpec>>newSpecsMap@Map<String@Collection<HiveSpec>>oldSpecsMap@HiveSpectableSpec)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/55/fcd2b71fd0b39c429e342f36231d9eb7050404.src","preCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    GenericRecord genericRecord = recordEnvelope.getRecord();\n    GobblinMetadataChangeEvent gmce =\n        (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n    if (whitelistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n      write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n    } else {\n      log.debug(String.format(\"Skip table %s.%s since it's not selected\", tableSpec.getTable().getDbName(),\n          tableSpec.getTable().getTableName()));\n    }\n  }\n","realPath":"gobblin-hive-registration/src/main/java/org/apache/gobblin/hive/writer/HiveMetadataWriter.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":311,"status":"N"},{"authorDate":"2021-08-07 02:52:37","commitOrder":5,"curCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    Lock readLock = readWriteLock.readLock();\n    readLock.lock();\n    try {\n      GenericRecord genericRecord = recordEnvelope.getRecord();\n      GobblinMetadataChangeEvent gmce =\n          (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n      if (whitelistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n        TableIdentifier tid = TableIdentifier.of(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName());\n        String topicPartition = tableTopicPartitionMap.computeIfAbsent(tid,\n            t -> ((KafkaWatermark) recordEnvelope.getWatermark()).getTopicPartition().toString());\n        Long currentWatermark = getAndPersistCurrentWatermark(tid, topicPartition);\n        Long currentOffset = ((KafkaWatermark) recordEnvelope.getWatermark()).getLwm().getValue();\n\n        if (currentOffset > currentWatermark) {\n          if (currentWatermark == DEFAULT_WATERMARK) {\n            \r\n            tableMetadataMap.computeIfAbsent(tid, t -> new TableMetadata()).lowWatermark =\n                Optional.of(currentOffset - 1);\n          }\n          tableMetadataMap.get(tid).setDatasetName(gmce.getDatasetIdentifier().getNativeName());\n          write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n          tableCurrentWatermarkMap.put(tid, currentOffset);\n        } else {\n          log.warn(String.format(\"Skip processing record %s since it has lower watermark\", genericRecord.toString()));\n        }\n      } else {\n        log.info(String.format(\"Skip table %s.%s since it's not selected\", tableSpec.getTable().getDbName(),\n            tableSpec.getTable().getTableName()));\n      }\n    } finally {\n      readLock.unlock();\n    }\n  }\n","date":"2021-08-07 02:52:37","endLine":855,"groupId":"3095","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"writeEnvelope","params":"(RecordEnvelope<GenericRecord>recordEnvelope@Map<String@Collection<HiveSpec>>newSpecsMap@Map<String@Collection<HiveSpec>>oldSpecsMap@HiveSpectableSpec)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/3a/bd491a3ac113c5f9f82a2627054fb1a53dd3e8.src","preCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    Lock readLock = readWriteLock.readLock();\n    readLock.lock();\n    try {\n      GenericRecord genericRecord = recordEnvelope.getRecord();\n      GobblinMetadataChangeEvent gmce =\n          (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n      if (whiteistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n        TableIdentifier tid = TableIdentifier.of(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName());\n        String topicPartition = tableTopicpartitionMap.computeIfAbsent(tid,\n            t -> ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getTopicPartition().toString());\n        Long currentWatermark = getCurrentWaterMark(tid, topicPartition);\n        Long currentOffset =\n            ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getLwm().getValue();\n        if (currentOffset > currentWatermark) {\n          if (currentWatermark == DEFAULT_WATERMARK) {\n            \r\n            tableMetadataMap.computeIfAbsent(tid, t -> new TableMetadata()).lowWatermark =\n                Optional.of(currentOffset - 1);\n          }\n          tableMetadataMap.get(tid).datasetName = gmce.getDatasetIdentifier().getNativeName();\n          write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n          tableCurrentWaterMarkMap.put(tid,\n              ((KafkaStreamingExtractor.KafkaWatermark) recordEnvelope.getWatermark()).getLwm().getValue());\n        } else {\n          log.warn(String.format(\"Skip processing record %s since it has lower watermark\", genericRecord.toString()));\n        }\n      } else {\n        log.info(String.format(\"Skip table %s.%s since it's blacklisted\", tableSpec.getTable().getDbName(),\n            tableSpec.getTable().getTableName()));\n      }\n    } finally {\n      readLock.unlock();\n    }\n  }\n","realPath":"gobblin-iceberg/src/main/java/org/apache/gobblin/iceberg/writer/IcebergMetadataWriter.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":821,"status":"M"}],"commitId":"dd205150d13d9ad9cd77a604d0bc577e19e9c628","commitMessage":"@@@[GOBBLIN-1501] Documentation + Code Cleaning for IcebergMetadataWriter  (#3347)\n\n* Add more comments in IcebergMetadataWriter while reading thru the code base\n\n* Fix unit tests post force the compile-time casting of gmce","date":"2021-08-07 02:52:37","modifiedFileCount":"7","status":"M","submitter":"Lei"},{"authorTime":"2021-09-11 03:02:42","codes":[{"authorDate":"2021-07-16 03:33:00","commitOrder":6,"curCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    GenericRecord genericRecord = recordEnvelope.getRecord();\n    GobblinMetadataChangeEvent gmce =\n        (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n    if (whitelistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n      write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n    } else {\n      log.debug(String.format(\"Skip table %s.%s since it's not selected\", tableSpec.getTable().getDbName(),\n          tableSpec.getTable().getTableName()));\n    }\n  }\n","date":"2021-07-16 03:33:00","endLine":322,"groupId":"101221","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"writeEnvelope","params":"(RecordEnvelope<GenericRecord>recordEnvelope@Map<String@Collection<HiveSpec>>newSpecsMap@Map<String@Collection<HiveSpec>>oldSpecsMap@HiveSpectableSpec)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/55/fcd2b71fd0b39c429e342f36231d9eb7050404.src","preCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    GenericRecord genericRecord = recordEnvelope.getRecord();\n    GobblinMetadataChangeEvent gmce =\n        (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n    if (whitelistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n      write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n    } else {\n      log.debug(String.format(\"Skip table %s.%s since it's not selected\", tableSpec.getTable().getDbName(),\n          tableSpec.getTable().getTableName()));\n    }\n  }\n","realPath":"gobblin-hive-registration/src/main/java/org/apache/gobblin/hive/writer/HiveMetadataWriter.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":311,"status":"N"},{"authorDate":"2021-09-11 03:02:42","commitOrder":6,"curCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    Lock readLock = readWriteLock.readLock();\n    readLock.lock();\n    try {\n      GenericRecord genericRecord = recordEnvelope.getRecord();\n      GobblinMetadataChangeEvent gmce =\n          (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n      if (whitelistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n        TableIdentifier tid = TableIdentifier.of(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName());\n        String topicPartition = tableTopicPartitionMap.computeIfAbsent(tid,\n            t -> ((KafkaWatermark) recordEnvelope.getWatermark()).getTopicPartition().toString());\n        Long currentWatermark = getAndPersistCurrentWatermark(tid, topicPartition);\n        Long currentOffset = ((KafkaWatermark) recordEnvelope.getWatermark()).getLwm().getValue();\n\n        if (currentOffset > currentWatermark) {\n          if (currentWatermark == DEFAULT_WATERMARK) {\n            \r\n            tableMetadataMap.computeIfAbsent(tid, t -> new TableMetadata()).lowWatermark =\n                Optional.of(currentOffset - 1);\n          }\n          tableMetadataMap.get(tid).setDatasetName(gmce.getDatasetIdentifier().getNativeName());\n          if(this.completenessEnabled && this.completenessWhitelistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n            tableMetadataMap.get(tid).setCompletenessEnabled(true);\n          }\n\n          write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n          tableCurrentWatermarkMap.put(tid, currentOffset);\n        } else {\n          log.warn(String.format(\"Skip processing record %s since it has lower watermark\", genericRecord.toString()));\n        }\n      } else {\n        log.info(String.format(\"Skip table %s.%s since it's not selected\", tableSpec.getTable().getDbName(),\n            tableSpec.getTable().getTableName()));\n      }\n    } finally {\n      readLock.unlock();\n    }\n  }\n","date":"2021-09-11 03:02:42","endLine":1039,"groupId":"101221","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"writeEnvelope","params":"(RecordEnvelope<GenericRecord>recordEnvelope@Map<String@Collection<HiveSpec>>newSpecsMap@Map<String@Collection<HiveSpec>>oldSpecsMap@HiveSpectableSpec)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/f6/7e614feb8dc35aa5959cd88436b866743079bb.src","preCode":"  public void writeEnvelope(RecordEnvelope<GenericRecord> recordEnvelope, Map<String, Collection<HiveSpec>> newSpecsMap,\n      Map<String, Collection<HiveSpec>> oldSpecsMap, HiveSpec tableSpec) throws IOException {\n    Lock readLock = readWriteLock.readLock();\n    readLock.lock();\n    try {\n      GenericRecord genericRecord = recordEnvelope.getRecord();\n      GobblinMetadataChangeEvent gmce =\n          (GobblinMetadataChangeEvent) SpecificData.get().deepCopy(genericRecord.getSchema(), genericRecord);\n      if (whitelistBlacklist.acceptTable(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName())) {\n        TableIdentifier tid = TableIdentifier.of(tableSpec.getTable().getDbName(), tableSpec.getTable().getTableName());\n        String topicPartition = tableTopicPartitionMap.computeIfAbsent(tid,\n            t -> ((KafkaWatermark) recordEnvelope.getWatermark()).getTopicPartition().toString());\n        Long currentWatermark = getAndPersistCurrentWatermark(tid, topicPartition);\n        Long currentOffset = ((KafkaWatermark) recordEnvelope.getWatermark()).getLwm().getValue();\n\n        if (currentOffset > currentWatermark) {\n          if (currentWatermark == DEFAULT_WATERMARK) {\n            \r\n            tableMetadataMap.computeIfAbsent(tid, t -> new TableMetadata()).lowWatermark =\n                Optional.of(currentOffset - 1);\n          }\n          tableMetadataMap.get(tid).setDatasetName(gmce.getDatasetIdentifier().getNativeName());\n          write(gmce, newSpecsMap, oldSpecsMap, tableSpec);\n          tableCurrentWatermarkMap.put(tid, currentOffset);\n        } else {\n          log.warn(String.format(\"Skip processing record %s since it has lower watermark\", genericRecord.toString()));\n        }\n      } else {\n        log.info(String.format(\"Skip table %s.%s since it's not selected\", tableSpec.getTable().getDbName(),\n            tableSpec.getTable().getTableName()));\n      }\n    } finally {\n      readLock.unlock();\n    }\n  }\n","realPath":"gobblin-iceberg/src/main/java/org/apache/gobblin/iceberg/writer/IcebergMetadataWriter.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":1001,"status":"M"}],"commitId":"47707df00a6884ada5974a5f5203408ce1efb890","commitMessage":"@@@[GOBBLIN-1533] Add completeness watermark to iceberg tables (#3385)\n\n* [GOBBLIN-1533] Add completeness watermark to iceberg tables\n\n* updated hive metadata writer test\n\n* Add apache header\n\n* Added correct default partition type\n\n* Fixed kafka audit url and logic to get topic name for iceberg table\n\n* Changes based on review\n\n* Make audit check granularity configurable\n\n* Added additional optimization to check for current hour during completion watermark calculation\n\n* optimization to skip audit check if its upto date by checking the seconds from epoch between current watermark and now\n\n* fixed test case\n\n* Replace hours from epoch with duration\n\n* Moved logging\n\n* Update partition spec with late field even when schema has been updated","date":"2021-09-11 03:02:42","modifiedFileCount":"8","status":"M","submitter":"vbohra"}]
