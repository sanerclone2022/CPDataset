[{"authorTime":"2018-09-15 03:31:02","codes":[{"authorDate":"2017-12-09 03:31:22","commitOrder":3,"curCode":"  public void testPublishSingleTask()\n      throws IOException {\n    WorkUnitState state = buildTaskState(1);\n    LineageInfo lineageInfo = LineageInfo.getLineageInfo(state.getTaskBroker()).get();\n    DatasetDescriptor source = new DatasetDescriptor(\"kafka\", \"testTopic\");\n    lineageInfo.setSource(source, state);\n    BaseDataPublisher publisher = new BaseDataPublisher(state);\n    publisher.publishData(state);\n    Assert.assertTrue(state.contains(\"gobblin.event.lineage.branch.0.destination\"));\n    Assert.assertFalse(state.contains(\"gobblin.event.lineage.branch.1.destination\"));\n  }\n","date":"2017-12-09 03:31:22","endLine":546,"groupId":"1844","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testPublishSingleTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/d4/6d6e39ed5e737e2138e2282cc316cea21336f9.src","preCode":"  public void testPublishSingleTask()\n      throws IOException {\n    WorkUnitState state = buildTaskState(1);\n    LineageInfo lineageInfo = LineageInfo.getLineageInfo(state.getTaskBroker()).get();\n    DatasetDescriptor source = new DatasetDescriptor(\"kafka\", \"testTopic\");\n    lineageInfo.setSource(source, state);\n    BaseDataPublisher publisher = new BaseDataPublisher(state);\n    publisher.publishData(state);\n    Assert.assertTrue(state.contains(\"gobblin.event.lineage.branch.0.destination\"));\n    Assert.assertFalse(state.contains(\"gobblin.event.lineage.branch.1.destination\"));\n  }\n","realPath":"gobblin-core/src/test/java/org/apache/gobblin/publisher/BaseDataPublisherTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":536,"status":"NB"},{"authorDate":"2018-09-15 03:31:02","commitOrder":3,"curCode":"  public void testPublishedPartitionsLineage()\n      throws IOException {\n    int numBranches = 2;\n    int numPartitionsPerBranch = 2;\n\n    WorkUnitState state = buildTaskState(numBranches);\n    LineageInfo lineageInfo = LineageInfo.getLineageInfo(state.getTaskBroker()).get();\n    DatasetDescriptor source = new DatasetDescriptor(\"kafka\", \"testTopic\");\n    lineageInfo.setSource(source, state);\n    BaseDataPublisher publisher = new BaseDataPublisher(state);\n\n    \r\n    DatasetDescriptor datasetAtWriter = new DatasetDescriptor(\"dummy\", \"dummy\");\n    for (int i = 0; i < numBranches; i++) {\n      List<PartitionDescriptor> partitions = new ArrayList<>();\n      for (int j = 0; j < numPartitionsPerBranch; j++) {\n        \r\n        partitions.add(new PartitionDescriptor(\"partition\" + i + j, datasetAtWriter));\n      }\n      String partitionsKey = \"writer.\" + i + \".partitions\";\n      state.setProp(partitionsKey, GSON.toJson(partitions, PARTITION_LIST_TYPE));\n    }\n\n    publisher.publish(ImmutableList.of(state));\n\n    Assert.assertTrue(state.contains(\"gobblin.event.lineage.branch.0.destination\"));\n    Assert.assertTrue(state.contains(\"gobblin.event.lineage.branch.1.destination\"));\n\n    Collection<LineageEventBuilder> events = LineageInfo.load(ImmutableList.of(state));\n    Assert.assertTrue(events.size() == 4);\n\n    \r\n    for (int i = 0; i < numBranches; i++) {\n      String outputPath = String.format(\"/data/output/branch%d/namespace/table\", i);\n      DatasetDescriptor destinationDataset = new DatasetDescriptor(\"file\", outputPath);\n      destinationDataset.addMetadata(\"fsUri\", \"file:///\");\n      destinationDataset.addMetadata(\"branch\", \"\" + i);\n\n      for (int j = 0; j < numPartitionsPerBranch; j++) {\n        LineageEventBuilder event = find(events, \"partition\" + i + j);\n        Assert.assertTrue(null != event);\n        Assert.assertEquals(event.getSource(), source);\n        Assert.assertEquals(event.getDestination(),\n            \r\n            new PartitionDescriptor(\"partition\" + i + j, destinationDataset));\n      }\n    }\n  }\n","date":"2018-09-15 03:31:02","endLine":631,"groupId":"1844","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testPublishedPartitionsLineage","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/46/9b72f579d34760075cc30ed934e7554e72bdb4.src","preCode":"  public void testPublishedPartitionsLineage()\n      throws IOException {\n    int numBranches = 2;\n    int numPartitionsPerBranch = 2;\n\n    WorkUnitState state = buildTaskState(numBranches);\n    LineageInfo lineageInfo = LineageInfo.getLineageInfo(state.getTaskBroker()).get();\n    DatasetDescriptor source = new DatasetDescriptor(\"kafka\", \"testTopic\");\n    lineageInfo.setSource(source, state);\n    BaseDataPublisher publisher = new BaseDataPublisher(state);\n\n    \r\n    DatasetDescriptor datasetAtWriter = new DatasetDescriptor(\"dummy\", \"dummy\");\n    for (int i = 0; i < numBranches; i++) {\n      List<PartitionDescriptor> partitions = new ArrayList<>();\n      for (int j = 0; j < numPartitionsPerBranch; j++) {\n        \r\n        partitions.add(new PartitionDescriptor(\"partition\" + i + j, datasetAtWriter));\n      }\n      String partitionsKey = \"writer.\" + i + \".partitions\";\n      state.setProp(partitionsKey, GSON.toJson(partitions, PARTITION_LIST_TYPE));\n    }\n\n    publisher.publish(ImmutableList.of(state));\n\n    Assert.assertTrue(state.contains(\"gobblin.event.lineage.branch.0.destination\"));\n    Assert.assertTrue(state.contains(\"gobblin.event.lineage.branch.1.destination\"));\n\n    Collection<LineageEventBuilder> events = LineageInfo.load(ImmutableList.of(state));\n    Assert.assertTrue(events.size() == 4);\n\n    \r\n    for (int i = 0; i < numBranches; i++) {\n      String outputPath = String.format(\"/data/output/branch%d/namespace/table\", i);\n      DatasetDescriptor destinationDataset = new DatasetDescriptor(\"file\", outputPath);\n      destinationDataset.addMetadata(\"fsUri\", \"file:///\");\n      destinationDataset.addMetadata(\"branch\", \"\" + i);\n\n      for (int j = 0; j < numPartitionsPerBranch; j++) {\n        LineageEventBuilder event = find(events, \"partition\" + i + j);\n        Assert.assertTrue(null != event);\n        Assert.assertEquals(event.getSource(), source);\n        Assert.assertEquals(event.getDestination(),\n            \r\n            new PartitionDescriptor(\"partition\" + i + j, destinationDataset));\n      }\n    }\n  }\n","realPath":"gobblin-core/src/test/java/org/apache/gobblin/publisher/BaseDataPublisherTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":584,"status":"B"}],"commitId":"ef59a1517575f41671b0ec4ffa6ac53b3648e30c","commitMessage":"@@@[GOBBLIN-587] Implement partition level lineage for fs based destination\n\nCloses #2453 from zxcware/pd\n","date":"2018-09-15 03:31:02","modifiedFileCount":"20","status":"M","submitter":"zhchen"},{"authorTime":"2021-01-13 04:20:19","codes":[{"authorDate":"2017-12-09 03:31:22","commitOrder":4,"curCode":"  public void testPublishSingleTask()\n      throws IOException {\n    WorkUnitState state = buildTaskState(1);\n    LineageInfo lineageInfo = LineageInfo.getLineageInfo(state.getTaskBroker()).get();\n    DatasetDescriptor source = new DatasetDescriptor(\"kafka\", \"testTopic\");\n    lineageInfo.setSource(source, state);\n    BaseDataPublisher publisher = new BaseDataPublisher(state);\n    publisher.publishData(state);\n    Assert.assertTrue(state.contains(\"gobblin.event.lineage.branch.0.destination\"));\n    Assert.assertFalse(state.contains(\"gobblin.event.lineage.branch.1.destination\"));\n  }\n","date":"2017-12-09 03:31:22","endLine":546,"groupId":"101239","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testPublishSingleTask","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/d4/6d6e39ed5e737e2138e2282cc316cea21336f9.src","preCode":"  public void testPublishSingleTask()\n      throws IOException {\n    WorkUnitState state = buildTaskState(1);\n    LineageInfo lineageInfo = LineageInfo.getLineageInfo(state.getTaskBroker()).get();\n    DatasetDescriptor source = new DatasetDescriptor(\"kafka\", \"testTopic\");\n    lineageInfo.setSource(source, state);\n    BaseDataPublisher publisher = new BaseDataPublisher(state);\n    publisher.publishData(state);\n    Assert.assertTrue(state.contains(\"gobblin.event.lineage.branch.0.destination\"));\n    Assert.assertFalse(state.contains(\"gobblin.event.lineage.branch.1.destination\"));\n  }\n","realPath":"gobblin-core/src/test/java/org/apache/gobblin/publisher/BaseDataPublisherTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":536,"status":"N"},{"authorDate":"2021-01-13 04:20:19","commitOrder":4,"curCode":"  public void testPublishedPartitionsLineage()\n      throws IOException {\n    int numBranches = 2;\n    int numPartitionsPerBranch = 2;\n\n    WorkUnitState state = buildTaskState(numBranches);\n    LineageInfo lineageInfo = LineageInfo.getLineageInfo(state.getTaskBroker()).get();\n    DatasetDescriptor source = new DatasetDescriptor(\"kafka\", \"testTopic\");\n    lineageInfo.setSource(source, state);\n    BaseDataPublisher publisher = new BaseDataPublisher(state);\n\n    \r\n    DatasetDescriptor datasetAtWriter = new DatasetDescriptor(\"dummy\", \"dummy\");\n    for (int i = 0; i < numBranches; i++) {\n      List<PartitionDescriptor> partitions = new ArrayList<>();\n      for (int j = 0; j < numPartitionsPerBranch; j++) {\n        \r\n        partitions.add(new PartitionDescriptor(\"partition\" + i + j, datasetAtWriter));\n      }\n      String partitionsKey = \"writer.\" + i + \".partitions\";\n      state.setProp(partitionsKey, GSON.toJson(partitions, PARTITION_LIST_TYPE));\n    }\n\n    publisher.publish(ImmutableList.of(state));\n\n    Assert.assertTrue(state.contains(\"gobblin.event.lineage.branch.0.destination\"));\n    Assert.assertTrue(state.contains(\"gobblin.event.lineage.branch.1.destination\"));\n\n    Collection<LineageEventBuilder> events = LineageInfo.load(ImmutableList.of(state));\n    Assert.assertTrue(events.size() == 4);\n\n    \r\n    for (int i = 0; i < numBranches; i++) {\n      String outputPath = String.format(\"/data/output/branch%d/namespace/table\", i);\n      DatasetDescriptor destinationDataset = new DatasetDescriptor(\"file\", URI.create(\"file:///\"), outputPath);\n      destinationDataset.addMetadata(\"fsUri\", \"file:///\");\n      destinationDataset.addMetadata(\"branch\", \"\" + i);\n\n      for (int j = 0; j < numPartitionsPerBranch; j++) {\n        LineageEventBuilder event = find(events, \"partition\" + i + j);\n        Assert.assertTrue(null != event);\n        Assert.assertEquals(event.getSource(), source);\n        Assert.assertEquals(event.getDestination(),\n            \r\n            new PartitionDescriptor(\"partition\" + i + j, destinationDataset));\n      }\n    }\n  }\n","date":"2021-01-13 04:20:19","endLine":629,"groupId":"101239","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"testPublishedPartitionsLineage","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-gobblin-10-0.7/blobInfo/CC_OUT/blobs/c8/d52c059e265bd331efe84d0ce019bc1d971dcc.src","preCode":"  public void testPublishedPartitionsLineage()\n      throws IOException {\n    int numBranches = 2;\n    int numPartitionsPerBranch = 2;\n\n    WorkUnitState state = buildTaskState(numBranches);\n    LineageInfo lineageInfo = LineageInfo.getLineageInfo(state.getTaskBroker()).get();\n    DatasetDescriptor source = new DatasetDescriptor(\"kafka\", \"testTopic\");\n    lineageInfo.setSource(source, state);\n    BaseDataPublisher publisher = new BaseDataPublisher(state);\n\n    \r\n    DatasetDescriptor datasetAtWriter = new DatasetDescriptor(\"dummy\", \"dummy\");\n    for (int i = 0; i < numBranches; i++) {\n      List<PartitionDescriptor> partitions = new ArrayList<>();\n      for (int j = 0; j < numPartitionsPerBranch; j++) {\n        \r\n        partitions.add(new PartitionDescriptor(\"partition\" + i + j, datasetAtWriter));\n      }\n      String partitionsKey = \"writer.\" + i + \".partitions\";\n      state.setProp(partitionsKey, GSON.toJson(partitions, PARTITION_LIST_TYPE));\n    }\n\n    publisher.publish(ImmutableList.of(state));\n\n    Assert.assertTrue(state.contains(\"gobblin.event.lineage.branch.0.destination\"));\n    Assert.assertTrue(state.contains(\"gobblin.event.lineage.branch.1.destination\"));\n\n    Collection<LineageEventBuilder> events = LineageInfo.load(ImmutableList.of(state));\n    Assert.assertTrue(events.size() == 4);\n\n    \r\n    for (int i = 0; i < numBranches; i++) {\n      String outputPath = String.format(\"/data/output/branch%d/namespace/table\", i);\n      DatasetDescriptor destinationDataset = new DatasetDescriptor(\"file\", outputPath);\n      destinationDataset.addMetadata(\"fsUri\", \"file:///\");\n      destinationDataset.addMetadata(\"branch\", \"\" + i);\n\n      for (int j = 0; j < numPartitionsPerBranch; j++) {\n        LineageEventBuilder event = find(events, \"partition\" + i + j);\n        Assert.assertTrue(null != event);\n        Assert.assertEquals(event.getSource(), source);\n        Assert.assertEquals(event.getDestination(),\n            \r\n            new PartitionDescriptor(\"partition\" + i + j, destinationDataset));\n      }\n    }\n  }\n","realPath":"gobblin-core/src/test/java/org/apache/gobblin/publisher/BaseDataPublisherTest.java","repoName":"gobblin","snippetEndLine":0,"snippetStartLine":0,"startLine":582,"status":"M"}],"commitId":"f2283c0f8fbff0a5305aee54884359b2cba5ac65","commitMessage":"@@@[GOBBLIN-1339] Add cluster name to dataset descriptor\n\n[GOBBLIN-1339] Add cluster name to dataset\ndescriptor\n\nWe use dataset descriptors to track lineage.\nPreviously.  it\nonly included the platform name (hive. hdfs) and\npath of the\ndataset. As a result.  we could not differentiate\nthe data copy\nbetween multiple production clusters.  as the\ndataset descriptors\nwere the same for them. We add an optional cluster\nname to\naddress that.\n\nThis change will be used for data copy audit\nsystem.\n\nHive and file-based copy code is updated to\ninclude cluster names.\n\nUse full storage system url instead of just well-\nknown name\n\nCloses #3178 from aplex/dataset-cluster\n","date":"2021-01-13 04:20:19","modifiedFileCount":"13","status":"M","submitter":"aprokofiev"}]
