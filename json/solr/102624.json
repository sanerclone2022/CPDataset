[{"authorTime":"2017-02-15 08:24:25","codes":[{"authorDate":"2017-02-15 08:24:25","commitOrder":1,"curCode":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    zeros = zeros && !ft.isPointField() && sf.indexed(); \r\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            \r\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    \r\n    \r\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      \r\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      \r\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { \r\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        \r\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      \r\n      \r\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n","date":"2017-02-15 08:24:25","endLine":405,"groupId":"59341","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"getCountsSingleValue","params":"(SolrIndexSearchersearcher@DocSetdocs@StringfieldName@intoffset@intlimit@intmincount@booleanmissing@Stringsort)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/a7/2eeeede995d56f7807a4a61644f62e7b28558f.src","preCode":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    zeros = zeros && !ft.isPointField() && sf.indexed(); \r\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            \r\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    \r\n    \r\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      \r\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      \r\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { \r\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        \r\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      \r\n      \r\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n","realPath":"solr/core/src/java/org/apache/solr/request/NumericFacets.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":171,"status":"B"},{"authorDate":"2017-02-15 08:24:25","commitOrder":1,"curCode":"  private static NamedList<Integer> getCountsMultiValued(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    \r\n    \r\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    assert sf.multiValued();\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(false);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    SortedNumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        longs = DocValues.getSortedNumeric(ctx.reader(), fieldName);\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        long l = longs.nextValue(); \r\n        hashTable.add(l, 1);\n        for (int i = 1; i < longs.docValueCount(); i++) {\n          long lnew = longs.nextValue();\n          if (lnew > l) { \r\n            hashTable.add(lnew, 1);\n          }\n          l = lnew;\n         }\n        \n      } else {\n        ++missingCount;\n      }\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      \r\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final NamedList<Integer> result = new NamedList<>(Math.max(pq.size() - offset + 1, 1));\n    final Deque<Entry> counts = new ArrayDeque<>(pq.size() - offset);\n    while (pq.size() > offset) {\n      counts.addFirst(pq.pop());\n    }\n    \n    for (Entry entry : counts) {\n      result.add(bitsToStringValue(ft, entry.bits), entry.count); \r\n    }\n    \n    \r\n    \r\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n","date":"2017-02-15 08:24:25","endLine":504,"groupId":"22693","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"getCountsMultiValued","params":"(SolrIndexSearchersearcher@DocSetdocs@StringfieldName@intoffset@intlimit@intmincount@booleanmissing@Stringsort)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/a7/2eeeede995d56f7807a4a61644f62e7b28558f.src","preCode":"  private static NamedList<Integer> getCountsMultiValued(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    \r\n    \r\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    assert sf.multiValued();\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(false);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    SortedNumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        longs = DocValues.getSortedNumeric(ctx.reader(), fieldName);\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        long l = longs.nextValue(); \r\n        hashTable.add(l, 1);\n        for (int i = 1; i < longs.docValueCount(); i++) {\n          long lnew = longs.nextValue();\n          if (lnew > l) { \r\n            hashTable.add(lnew, 1);\n          }\n          l = lnew;\n         }\n        \n      } else {\n        ++missingCount;\n      }\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      \r\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final NamedList<Integer> result = new NamedList<>(Math.max(pq.size() - offset + 1, 1));\n    final Deque<Entry> counts = new ArrayDeque<>(pq.size() - offset);\n    while (pq.size() > offset) {\n      counts.addFirst(pq.pop());\n    }\n    \n    for (Entry entry : counts) {\n      result.add(bitsToStringValue(ft, entry.bits), entry.count); \r\n    }\n    \n    \r\n    \r\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n","realPath":"solr/core/src/java/org/apache/solr/request/NumericFacets.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":407,"status":"B"}],"commitId":"26298f35df118aad186e6eaf1ea5c2e5f50d607f","commitMessage":"@@@Merge remote-tracking branch 'origin/master'\n","date":"2017-02-15 08:24:25","modifiedFileCount":"40","status":"B","submitter":"Noble Paul"},{"authorTime":"2017-08-01 06:21:49","codes":[{"authorDate":"2017-08-01 06:21:49","commitOrder":2,"curCode":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    if (zeros && ft.isPointField()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is Points-based\");\n    }\n\n    zeros = zeros && !ft.isPointField() && sf.indexed(); \r\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            \r\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    \r\n    \r\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      \r\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      \r\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { \r\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        \r\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      \r\n      \r\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n","date":"2017-08-01 06:22:15","endLine":412,"groupId":"59341","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"getCountsSingleValue","params":"(SolrIndexSearchersearcher@DocSetdocs@StringfieldName@intoffset@intlimit@intmincount@booleanmissing@Stringsort)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/f9/f38b3dbaf0ed23fb99bf6a5135d0f60b3c7172.src","preCode":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    zeros = zeros && !ft.isPointField() && sf.indexed(); \r\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            \r\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    \r\n    \r\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      \r\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      \r\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { \r\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        \r\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      \r\n      \r\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n","realPath":"solr/core/src/java/org/apache/solr/request/NumericFacets.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":173,"status":"M"},{"authorDate":"2017-08-01 06:21:49","commitOrder":2,"curCode":"  private static NamedList<Integer> getCountsMultiValued(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    \r\n    \r\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    assert sf.multiValued();\n\n    if (zeros && ft.isPointField()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is Points-based\");\n    }\n\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(false);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    SortedNumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        longs = DocValues.getSortedNumeric(ctx.reader(), fieldName);\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        long l = longs.nextValue(); \r\n        hashTable.add(l, 1);\n        for (int i = 1; i < longs.docValueCount(); i++) {\n          long lnew = longs.nextValue();\n          if (lnew > l) { \r\n            hashTable.add(lnew, 1);\n          }\n          l = lnew;\n         }\n        \n      } else {\n        ++missingCount;\n      }\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      \r\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final NamedList<Integer> result = new NamedList<>(Math.max(pq.size() - offset + 1, 1));\n    final Deque<Entry> counts = new ArrayDeque<>(pq.size() - offset);\n    while (pq.size() > offset) {\n      counts.addFirst(pq.pop());\n    }\n    \n    for (Entry entry : counts) {\n      result.add(bitsToStringValue(ft, entry.bits), entry.count); \r\n    }\n    \n    \r\n    \r\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n","date":"2017-08-01 06:22:15","endLine":518,"groupId":"22693","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"getCountsMultiValued","params":"(SolrIndexSearchersearcher@DocSetdocs@StringfieldName@intoffset@intlimit@intmincount@booleanmissing@Stringsort)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/f9/f38b3dbaf0ed23fb99bf6a5135d0f60b3c7172.src","preCode":"  private static NamedList<Integer> getCountsMultiValued(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    \r\n    \r\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    assert sf.multiValued();\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(false);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    SortedNumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        longs = DocValues.getSortedNumeric(ctx.reader(), fieldName);\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        long l = longs.nextValue(); \r\n        hashTable.add(l, 1);\n        for (int i = 1; i < longs.docValueCount(); i++) {\n          long lnew = longs.nextValue();\n          if (lnew > l) { \r\n            hashTable.add(lnew, 1);\n          }\n          l = lnew;\n         }\n        \n      } else {\n        ++missingCount;\n      }\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      \r\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final NamedList<Integer> result = new NamedList<>(Math.max(pq.size() - offset + 1, 1));\n    final Deque<Entry> counts = new ArrayDeque<>(pq.size() - offset);\n    while (pq.size() > offset) {\n      counts.addFirst(pq.pop());\n    }\n    \n    for (Entry entry : counts) {\n      result.add(bitsToStringValue(ft, entry.bits), entry.count); \r\n    }\n    \n    \r\n    \r\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n","realPath":"solr/core/src/java/org/apache/solr/request/NumericFacets.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":414,"status":"M"}],"commitId":"3a405971b9e06e2004e0d66ae1b82f530de969f2","commitMessage":"@@@SOLR-10033: Provide a clear exception when attempting to facet with facet.mincount=0 over points fields\n","date":"2017-08-01 06:22:15","modifiedFileCount":"2","status":"M","submitter":"Steve Rowe"},{"authorTime":"2017-08-01 21:40:45","codes":[{"authorDate":"2017-08-01 21:40:45","commitOrder":3,"curCode":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    zeros = zeros && !ft.isPointField() && sf.indexed(); \r\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            \r\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    \r\n    \r\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      \r\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      \r\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { \r\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        \r\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      \r\n      \r\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n","date":"2017-08-01 21:42:09","endLine":406,"groupId":"59341","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"getCountsSingleValue","params":"(SolrIndexSearchersearcher@DocSetdocs@StringfieldName@intoffset@intlimit@intmincount@booleanmissing@Stringsort)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/fd/17f1f7397e294c2ac52962b8a6ec34153116dd.src","preCode":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    if (zeros && ft.isPointField()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is Points-based\");\n    }\n\n    zeros = zeros && !ft.isPointField() && sf.indexed(); \r\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            \r\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    \r\n    \r\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      \r\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      \r\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { \r\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        \r\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      \r\n      \r\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n","realPath":"solr/core/src/java/org/apache/solr/request/NumericFacets.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":172,"status":"M"},{"authorDate":"2017-08-01 21:40:45","commitOrder":3,"curCode":"  private static NamedList<Integer> getCountsMultiValued(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    \r\n    \r\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    assert sf.multiValued();\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(false);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    SortedNumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        longs = DocValues.getSortedNumeric(ctx.reader(), fieldName);\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        long l = longs.nextValue(); \r\n        hashTable.add(l, 1);\n        for (int i = 1; i < longs.docValueCount(); i++) {\n          long lnew = longs.nextValue();\n          if (lnew > l) { \r\n            hashTable.add(lnew, 1);\n          }\n          l = lnew;\n         }\n        \n      } else {\n        ++missingCount;\n      }\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      \r\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final NamedList<Integer> result = new NamedList<>(Math.max(pq.size() - offset + 1, 1));\n    final Deque<Entry> counts = new ArrayDeque<>(pq.size() - offset);\n    while (pq.size() > offset) {\n      counts.addFirst(pq.pop());\n    }\n    \n    for (Entry entry : counts) {\n      result.add(bitsToStringValue(ft, entry.bits), entry.count); \r\n    }\n    \n    \r\n    \r\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n","date":"2017-08-01 21:42:09","endLine":505,"groupId":"22693","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"getCountsMultiValued","params":"(SolrIndexSearchersearcher@DocSetdocs@StringfieldName@intoffset@intlimit@intmincount@booleanmissing@Stringsort)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/fd/17f1f7397e294c2ac52962b8a6ec34153116dd.src","preCode":"  private static NamedList<Integer> getCountsMultiValued(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    \r\n    \r\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    assert sf.multiValued();\n\n    if (zeros && ft.isPointField()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is Points-based\");\n    }\n\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(false);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    SortedNumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        longs = DocValues.getSortedNumeric(ctx.reader(), fieldName);\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        long l = longs.nextValue(); \r\n        hashTable.add(l, 1);\n        for (int i = 1; i < longs.docValueCount(); i++) {\n          long lnew = longs.nextValue();\n          if (lnew > l) { \r\n            hashTable.add(lnew, 1);\n          }\n          l = lnew;\n         }\n        \n      } else {\n        ++missingCount;\n      }\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      \r\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final NamedList<Integer> result = new NamedList<>(Math.max(pq.size() - offset + 1, 1));\n    final Deque<Entry> counts = new ArrayDeque<>(pq.size() - offset);\n    while (pq.size() > offset) {\n      counts.addFirst(pq.pop());\n    }\n    \n    for (Entry entry : counts) {\n      result.add(bitsToStringValue(ft, entry.bits), entry.count); \r\n    }\n    \n    \r\n    \r\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n","realPath":"solr/core/src/java/org/apache/solr/request/NumericFacets.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":408,"status":"M"}],"commitId":"7d6c154fd9cdb72d808d97ed8f3f99517c1663a8","commitMessage":"@@@Revert \"SOLR-10033: Provide a clear exception when attempting to facet with facet.mincount=0 over points fields\"\n\nThis reverts commit 3a405971b9e06e2004e0d66ae1b82f530de969f2.\n","date":"2017-08-01 21:42:09","modifiedFileCount":"2","status":"M","submitter":"Steve Rowe"},{"authorTime":"2019-06-10 14:56:21","codes":[{"authorDate":"2017-08-01 21:40:45","commitOrder":4,"curCode":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    zeros = zeros && !ft.isPointField() && sf.indexed(); \r\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            \r\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    \r\n    \r\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      \r\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      \r\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { \r\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        \r\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      \r\n      \r\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n","date":"2017-08-01 21:42:09","endLine":406,"groupId":"59341","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"getCountsSingleValue","params":"(SolrIndexSearchersearcher@DocSetdocs@StringfieldName@intoffset@intlimit@intmincount@booleanmissing@Stringsort)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/fd/17f1f7397e294c2ac52962b8a6ec34153116dd.src","preCode":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    zeros = zeros && !ft.isPointField() && sf.indexed(); \r\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            \r\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    \r\n    \r\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      \r\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      \r\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { \r\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        \r\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      \r\n      \r\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n","realPath":"solr/core/src/java/org/apache/solr/request/NumericFacets.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":172,"status":"N"},{"authorDate":"2019-06-10 14:56:21","commitOrder":4,"curCode":"  private static NamedList<Integer> getCountsMultiValued(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    \r\n    \r\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    assert sf.multiValued();\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(false);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    SortedNumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        longs = DocValues.getSortedNumeric(ctx.reader(), fieldName);\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        long l = longs.nextValue(); \r\n        hashTable.add(l, 1);\n        for (int i = 1, count = longs.docValueCount(); i < count; i++) {\n          long lnew = longs.nextValue();\n          if (lnew > l) { \r\n            hashTable.add(lnew, 1);\n          }\n          l = lnew;\n         }\n        \n      } else {\n        ++missingCount;\n      }\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      \r\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final NamedList<Integer> result = new NamedList<>(Math.max(pq.size() - offset + 1, 1));\n    final Deque<Entry> counts = new ArrayDeque<>(pq.size() - offset);\n    while (pq.size() > offset) {\n      counts.addFirst(pq.pop());\n    }\n    \n    for (Entry entry : counts) {\n      result.add(bitsToStringValue(ft, entry.bits), entry.count); \r\n    }\n    \n    \r\n    \r\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n","date":"2019-06-10 14:56:21","endLine":505,"groupId":"22693","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"getCountsMultiValued","params":"(SolrIndexSearchersearcher@DocSetdocs@StringfieldName@intoffset@intlimit@intmincount@booleanmissing@Stringsort)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/4e/f9f26b08ba31a8afba93d695acf1bfc43ead9a.src","preCode":"  private static NamedList<Integer> getCountsMultiValued(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    \r\n    \r\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    assert sf.multiValued();\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(false);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    SortedNumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        longs = DocValues.getSortedNumeric(ctx.reader(), fieldName);\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        long l = longs.nextValue(); \r\n        hashTable.add(l, 1);\n        for (int i = 1; i < longs.docValueCount(); i++) {\n          long lnew = longs.nextValue();\n          if (lnew > l) { \r\n            hashTable.add(lnew, 1);\n          }\n          l = lnew;\n         }\n        \n      } else {\n        ++missingCount;\n      }\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      \r\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final NamedList<Integer> result = new NamedList<>(Math.max(pq.size() - offset + 1, 1));\n    final Deque<Entry> counts = new ArrayDeque<>(pq.size() - offset);\n    while (pq.size() > offset) {\n      counts.addFirst(pq.pop());\n    }\n    \n    for (Entry entry : counts) {\n      result.add(bitsToStringValue(ft, entry.bits), entry.count); \r\n    }\n    \n    \r\n    \r\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n","realPath":"solr/core/src/java/org/apache/solr/request/NumericFacets.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":408,"status":"M"}],"commitId":"97ca9df7ef3733acd4babf10610797e36ac1d996","commitMessage":"@@@LUCENE-8834: Cache the SortedNumericDocValues.docValueCount() value whenever it is used in a loop (#698)\n\n","date":"2019-06-10 14:56:21","modifiedFileCount":"10","status":"M","submitter":"Tim Underwood"},{"authorTime":"2019-08-19 23:16:04","codes":[{"authorDate":"2019-08-19 23:16:04","commitOrder":5,"curCode":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    zeros = zeros && !ft.isPointField() && sf.indexed(); \r\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            \r\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    final NamedList<Integer> result = new NamedList<>();\n    if (limit == 0) {\n      return finalize(result, missingCount, missing);\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final ValueSource vs = ft.getValueSource(sf, null);\n\n    \r\n    \r\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      \r\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      \r\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { \r\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        \r\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      \r\n      \r\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    return finalize(result, missingCount, missing);\n  }\n","date":"2019-08-19 23:16:04","endLine":407,"groupId":"102624","id":9,"instanceNumber":1,"isCurCommit":1,"methodName":"getCountsSingleValue","params":"(SolrIndexSearchersearcher@DocSetdocs@StringfieldName@intoffset@intlimit@intmincount@booleanmissing@Stringsort)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/c1/85cb5a0e5e2cb2551c9441790195e3c400107e.src","preCode":"  private static NamedList<Integer> getCountsSingleValue(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    boolean zeros = mincount <= 0;\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    final NumberType numericType = ft.getNumberType();\n    if (numericType == null) {\n      throw new IllegalStateException();\n    }\n    zeros = zeros && !ft.isPointField() && sf.indexed(); \r\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(true);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    NumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        switch (numericType) {\n          case LONG:\n          case DATE:\n          case INTEGER:\n            \r\n            longs = DocValues.getNumeric(ctx.reader(), fieldName);\n            break;\n          case FLOAT:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          case DOUBLE:\n            \r\n            longs = new FilterNumericDocValues(DocValues.getNumeric(ctx.reader(), fieldName)) {\n              @Override\n              public long longValue() throws IOException {\n                long bits = super.longValue();\n                if (bits < 0) bits ^= 0x7fffffffffffffffL;\n                return bits;\n              }\n            };\n            break;\n          default:\n            throw new AssertionError(\"Unexpected type: \" + numericType);\n        }\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        hashTable.add(doc, longs.longValue(), 1);\n      } else {\n        ++missingCount;\n      }\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e.docID = hashTable.docIDs[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final ValueSource vs = ft.getValueSource(sf, null);\n    final NamedList<Integer> result = new NamedList<>();\n\n    \r\n    \r\n    if (!zeros || FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      \r\n      final Deque<Entry> counts = new ArrayDeque<>();\n      while (pq.size() > offset) {\n        counts.addFirst(pq.pop());\n      }\n      \n      \r\n      for (Entry entry : counts) {\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        result.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n\n      if (zeros && (limit < 0 || result.size() < limit)) { \r\n        if (!sf.indexed() && !sf.hasDocValues()) {\n          throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_MINCOUNT + \"=0 on field \" + sf.getName() + \" which is neither indexed nor docValues\");\n        }\n        \r\n        final Set<String> alreadySeen = new HashSet<>();\n        while (pq.size() > 0) {\n          Entry entry = pq.pop();\n          final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n          final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n          alreadySeen.add(values.strVal(entry.docID - leaves.get(readerIdx).docBase));\n        }\n        for (int i = 0; i < result.size(); ++i) {\n          alreadySeen.add(result.getName(i));\n        }\n        final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n        if (terms != null) {\n          final String prefixStr = TrieField.getMainValuePrefix(ft);\n          final BytesRef prefix;\n          if (prefixStr != null) {\n            prefix = new BytesRef(prefixStr);\n          } else {\n            prefix = new BytesRef();\n          }\n          final TermsEnum termsEnum = terms.iterator();\n          BytesRef term;\n          switch (termsEnum.seekCeil(prefix)) {\n            case FOUND:\n            case NOT_FOUND:\n              term = termsEnum.term();\n              break;\n            case END:\n              term = null;\n              break;\n            default:\n              throw new AssertionError();\n          }\n          final CharsRefBuilder spare = new CharsRefBuilder();\n          for (int skipped = hashTable.size; skipped < offset && term != null && StringHelper.startsWith(term, prefix); ) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              ++skipped;\n            }\n            term = termsEnum.next();\n          }\n          for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n            ft.indexedToReadable(term, spare);\n            final String termStr = spare.toString();\n            if (!alreadySeen.contains(termStr)) {\n              result.add(termStr, 0);\n            }\n          }\n        }\n      }\n    } else {\n      \r\n      \r\n      if (!sf.indexed()) {\n        throw new IllegalStateException(\"Cannot use \" + FacetParams.FACET_SORT + \"=\" + FacetParams.FACET_SORT_INDEX + \" on a field which is not indexed\");\n      }\n      final Map<String, Integer> counts = new HashMap<>();\n      while (pq.size() > 0) {\n        final Entry entry = pq.pop();\n        final int readerIdx = ReaderUtil.subIndex(entry.docID, leaves);\n        final FunctionValues values = vs.getValues(Collections.emptyMap(), leaves.get(readerIdx));\n        counts.put(values.strVal(entry.docID - leaves.get(readerIdx).docBase), entry.count);\n      }\n      final Terms terms = searcher.getSlowAtomicReader().terms(fieldName);\n      if (terms != null) {\n        final String prefixStr = TrieField.getMainValuePrefix(ft);\n        final BytesRef prefix;\n        if (prefixStr != null) {\n          prefix = new BytesRef(prefixStr);\n        } else {\n          prefix = new BytesRef();\n        }\n        final TermsEnum termsEnum = terms.iterator();\n        BytesRef term;\n        switch (termsEnum.seekCeil(prefix)) {\n          case FOUND:\n          case NOT_FOUND:\n            term = termsEnum.term();\n            break;\n          case END:\n            term = null;\n            break;\n          default:\n            throw new AssertionError();\n        }\n        final CharsRefBuilder spare = new CharsRefBuilder();\n        for (int i = 0; i < offset && term != null && StringHelper.startsWith(term, prefix); ++i) {\n          term = termsEnum.next();\n        }\n        for ( ; term != null && StringHelper.startsWith(term, prefix) && (limit < 0 || result.size() < limit); term = termsEnum.next()) {\n          ft.indexedToReadable(term, spare);\n          final String termStr = spare.toString();\n          Integer count = counts.get(termStr);\n          if (count == null) {\n            count = 0;\n          }\n          result.add(termStr, count);\n        }\n      }\n    }\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n","realPath":"solr/core/src/java/org/apache/solr/request/NumericFacets.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":172,"status":"M"},{"authorDate":"2019-08-19 23:16:04","commitOrder":5,"curCode":"  private static NamedList<Integer> getCountsMultiValued(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    \r\n    \r\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    assert sf.multiValued();\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(false);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    SortedNumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        longs = DocValues.getSortedNumeric(ctx.reader(), fieldName);\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        long l = longs.nextValue(); \r\n        hashTable.add(l, 1);\n        for (int i = 1, count = longs.docValueCount(); i < count; i++) {\n          long lnew = longs.nextValue();\n          if (lnew > l) { \r\n            hashTable.add(lnew, 1);\n          }\n          l = lnew;\n         }\n        \n      } else {\n        ++missingCount;\n      }\n    }\n\n    if (limit == 0) {\n      NamedList<Integer> result = new NamedList<>();\n      return finalize(result, missingCount, missing);\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      \r\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final NamedList<Integer> result = new NamedList<>(Math.max(pq.size() - offset + 1, 1));\n    final Deque<Entry> counts = new ArrayDeque<>(pq.size() - offset);\n    while (pq.size() > offset) {\n      counts.addFirst(pq.pop());\n    }\n    \n    for (Entry entry : counts) {\n      result.add(bitsToStringValue(ft, entry.bits), entry.count); \r\n    }\n    \n    \r\n    \r\n\n    return finalize(result, missingCount, missing);\n  }\n","date":"2019-08-19 23:16:04","endLine":508,"groupId":"102624","id":10,"instanceNumber":2,"isCurCommit":1,"methodName":"getCountsMultiValued","params":"(SolrIndexSearchersearcher@DocSetdocs@StringfieldName@intoffset@intlimit@intmincount@booleanmissing@Stringsort)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/c1/85cb5a0e5e2cb2551c9441790195e3c400107e.src","preCode":"  private static NamedList<Integer> getCountsMultiValued(SolrIndexSearcher searcher, DocSet docs, String fieldName, int offset, int limit, int mincount, boolean missing, String sort) throws IOException {\n    \r\n    \r\n    mincount = Math.max(mincount, 1);\n    final SchemaField sf = searcher.getSchema().getField(fieldName);\n    final FieldType ft = sf.getType();\n    assert sf.multiValued();\n    final List<LeafReaderContext> leaves = searcher.getIndexReader().leaves();\n\n    \r\n    final HashTable hashTable = new HashTable(false);\n    final Iterator<LeafReaderContext> ctxIt = leaves.iterator();\n    LeafReaderContext ctx = null;\n    SortedNumericDocValues longs = null;\n    int missingCount = 0;\n    for (DocIterator docsIt = docs.iterator(); docsIt.hasNext(); ) {\n      final int doc = docsIt.nextDoc();\n      if (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc()) {\n        do {\n          ctx = ctxIt.next();\n        } while (ctx == null || doc >= ctx.docBase + ctx.reader().maxDoc());\n        assert doc >= ctx.docBase;\n        longs = DocValues.getSortedNumeric(ctx.reader(), fieldName);\n      }\n      int valuesDocID = longs.docID();\n      if (valuesDocID < doc - ctx.docBase) {\n        valuesDocID = longs.advance(doc - ctx.docBase);\n      }\n      if (valuesDocID == doc - ctx.docBase) {\n        long l = longs.nextValue(); \r\n        hashTable.add(l, 1);\n        for (int i = 1, count = longs.docValueCount(); i < count; i++) {\n          long lnew = longs.nextValue();\n          if (lnew > l) { \r\n            hashTable.add(lnew, 1);\n          }\n          l = lnew;\n         }\n        \n      } else {\n        ++missingCount;\n      }\n    }\n\n    \r\n    final int pqSize = limit < 0 ? hashTable.size : Math.min(offset + limit, hashTable.size);\n    final PriorityQueue<Entry> pq;\n    if (FacetParams.FACET_SORT_COUNT.equals(sort) || FacetParams.FACET_SORT_COUNT_LEGACY.equals(sort)) {\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          if (a.count < b.count || (a.count == b.count && a.bits > b.bits)) {\n            return true;\n          } else {\n            return false;\n          }\n        }\n      };\n    } else {\n      \r\n      pq = new PriorityQueue<Entry>(pqSize) {\n        @Override\n        protected boolean lessThan(Entry a, Entry b) {\n          return a.bits > b.bits;\n        }\n      };\n    }\n    Entry e = null;\n    for (int i = 0; i < hashTable.bits.length; ++i) {\n      if (hashTable.counts[i] >= mincount) {\n        if (e == null) {\n          e = new Entry();\n        }\n        e.bits = hashTable.bits[i];\n        e.count = hashTable.counts[i];\n        e = pq.insertWithOverflow(e);\n      }\n    }\n\n    \r\n    final NamedList<Integer> result = new NamedList<>(Math.max(pq.size() - offset + 1, 1));\n    final Deque<Entry> counts = new ArrayDeque<>(pq.size() - offset);\n    while (pq.size() > offset) {\n      counts.addFirst(pq.pop());\n    }\n    \n    for (Entry entry : counts) {\n      result.add(bitsToStringValue(ft, entry.bits), entry.count); \r\n    }\n    \n    \r\n    \r\n\n    if (missing) {\n      result.add(null, missingCount);\n    }\n    return result;\n  }\n","realPath":"solr/core/src/java/org/apache/solr/request/NumericFacets.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":409,"status":"M"}],"commitId":"0654c2496d3d624f0ddfbe6e31e1755e157b2266","commitMessage":"@@@SOLR-6328: return missing count for facet.missing=true even if limit=0\n\n* facet.missing is independent of facet.limit. So.  even for limit=0. \n  missing counts should be return if facet.missing=true\n","date":"2019-08-19 23:16:04","modifiedFileCount":"8","status":"M","submitter":"Munendra S N"}]
