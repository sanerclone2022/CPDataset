[{"authorTime":"2016-05-26 01:42:11","codes":[{"authorDate":"2016-05-26 01:42:11","commitOrder":1,"curCode":"  public void testRandom() throws Exception {\n    Directory dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = TestUtil.randomRealisticUnicodeString(random());\n      \r\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    \r\n    if (random().nextInt(10) == 7) {\n      \r\n      Codec codec = TestUtil.alwaysPostingsFormat(TestUtil.getPostingsFormatWithOrds(random()));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new LegacyIntField(\"id\", id, Field.Store.YES));\n      \n      final int termCount = TestUtil.nextInt(random(), 0, 20 * RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random().nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newStringField(\"field\", termsArray[ord].utf8ToString(), Field.Store.NO);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(LeafReaderContext ctx : r.leaves()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + ctx.reader());\n      }\n      verify(ctx.reader(), idToOrds, termsArray, null);\n    }\n\n    \r\n    \r\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    LeafReader slowR = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(slowR);\n    verify(slowR, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purgeByCacheKey(slowR.getCoreCacheKey());\n\n    r.close();\n    dir.close();\n  }\n","date":"2016-05-26 01:42:11","endLine":225,"groupId":"3836","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testRandom","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/f1/627a6ee350ed9eae9bc720a9e4d13b1f0aeee3.src","preCode":"  public void testRandom() throws Exception {\n    Directory dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = TestUtil.randomRealisticUnicodeString(random());\n      \r\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    \r\n    if (random().nextInt(10) == 7) {\n      \r\n      Codec codec = TestUtil.alwaysPostingsFormat(TestUtil.getPostingsFormatWithOrds(random()));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new LegacyIntField(\"id\", id, Field.Store.YES));\n      \n      final int termCount = TestUtil.nextInt(random(), 0, 20 * RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random().nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newStringField(\"field\", termsArray[ord].utf8ToString(), Field.Store.NO);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(LeafReaderContext ctx : r.leaves()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + ctx.reader());\n      }\n      verify(ctx.reader(), idToOrds, termsArray, null);\n    }\n\n    \r\n    \r\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    LeafReader slowR = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(slowR);\n    verify(slowR, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purgeByCacheKey(slowR.getCoreCacheKey());\n\n    r.close();\n    dir.close();\n  }\n","realPath":"solr/core/src/test/org/apache/solr/uninverting/TestDocTermOrds.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":139,"status":"B"},{"authorDate":"2016-05-26 01:42:11","commitOrder":1,"curCode":"  public void testRandomWithPrefix() throws Exception {\n    Directory dir = newDirectory();\n\n    final Set<String> prefixes = new HashSet<>();\n    final int numPrefix = TestUtil.nextInt(random(), 2, 7);\n    if (VERBOSE) {\n      System.out.println(\"TEST: use \" + numPrefix + \" prefixes\");\n    }\n    while(prefixes.size() < numPrefix) {\n      prefixes.add(TestUtil.randomRealisticUnicodeString(random()));\n      \r\n    }\n    final String[] prefixesArray = prefixes.toArray(new String[prefixes.size()]);\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = prefixesArray[random().nextInt(prefixesArray.length)] + TestUtil.randomRealisticUnicodeString(random());\n      \r\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    \r\n    if (random().nextInt(10) == 7) {\n      Codec codec = TestUtil.alwaysPostingsFormat(TestUtil.getPostingsFormatWithOrds(random()));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new LegacyIntField(\"id\", id, Field.Store.YES));\n      \n      final int termCount = TestUtil.nextInt(random(), 0, 20 * RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random().nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newStringField(\"field\", termsArray[ord].utf8ToString(), Field.Store.NO);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n    \n    LeafReader slowR = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(slowR);\n    for(String prefix : prefixesArray) {\n\n      final BytesRef prefixRef = prefix == null ? null : new BytesRef(prefix);\n\n      final int[][] idToOrdsPrefix = new int[NUM_DOCS][];\n      for(int id=0;id<NUM_DOCS;id++) {\n        final int[] docOrds = idToOrds[id];\n        final List<Integer> newOrds = new ArrayList<>();\n        for(int ord : idToOrds[id]) {\n          if (StringHelper.startsWith(termsArray[ord], prefixRef)) {\n            newOrds.add(ord);\n          }\n        }\n        final int[] newOrdsArray = new int[newOrds.size()];\n        int upto = 0;\n        for(int ord : newOrds) {\n          newOrdsArray[upto++] = ord;\n        }\n        idToOrdsPrefix[id] = newOrdsArray;\n      }\n\n      for(LeafReaderContext ctx : r.leaves()) {\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: sub=\" + ctx.reader());\n        }\n        verify(ctx.reader(), idToOrdsPrefix, termsArray, prefixRef);\n      }\n\n      \r\n      \r\n      if (VERBOSE) {\n        System.out.println(\"TEST: top reader\");\n      }\n      verify(slowR, idToOrdsPrefix, termsArray, prefixRef);\n    }\n\n    FieldCache.DEFAULT.purgeByCacheKey(slowR.getCoreCacheKey());\n\n    r.close();\n    dir.close();\n  }\n","date":"2016-05-26 01:42:11","endLine":345,"groupId":"3836","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testRandomWithPrefix","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/f1/627a6ee350ed9eae9bc720a9e4d13b1f0aeee3.src","preCode":"  public void testRandomWithPrefix() throws Exception {\n    Directory dir = newDirectory();\n\n    final Set<String> prefixes = new HashSet<>();\n    final int numPrefix = TestUtil.nextInt(random(), 2, 7);\n    if (VERBOSE) {\n      System.out.println(\"TEST: use \" + numPrefix + \" prefixes\");\n    }\n    while(prefixes.size() < numPrefix) {\n      prefixes.add(TestUtil.randomRealisticUnicodeString(random()));\n      \r\n    }\n    final String[] prefixesArray = prefixes.toArray(new String[prefixes.size()]);\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = prefixesArray[random().nextInt(prefixesArray.length)] + TestUtil.randomRealisticUnicodeString(random());\n      \r\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    \r\n    if (random().nextInt(10) == 7) {\n      Codec codec = TestUtil.alwaysPostingsFormat(TestUtil.getPostingsFormatWithOrds(random()));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new LegacyIntField(\"id\", id, Field.Store.YES));\n      \n      final int termCount = TestUtil.nextInt(random(), 0, 20 * RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random().nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newStringField(\"field\", termsArray[ord].utf8ToString(), Field.Store.NO);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n    \n    LeafReader slowR = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(slowR);\n    for(String prefix : prefixesArray) {\n\n      final BytesRef prefixRef = prefix == null ? null : new BytesRef(prefix);\n\n      final int[][] idToOrdsPrefix = new int[NUM_DOCS][];\n      for(int id=0;id<NUM_DOCS;id++) {\n        final int[] docOrds = idToOrds[id];\n        final List<Integer> newOrds = new ArrayList<>();\n        for(int ord : idToOrds[id]) {\n          if (StringHelper.startsWith(termsArray[ord], prefixRef)) {\n            newOrds.add(ord);\n          }\n        }\n        final int[] newOrdsArray = new int[newOrds.size()];\n        int upto = 0;\n        for(int ord : newOrds) {\n          newOrdsArray[upto++] = ord;\n        }\n        idToOrdsPrefix[id] = newOrdsArray;\n      }\n\n      for(LeafReaderContext ctx : r.leaves()) {\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: sub=\" + ctx.reader());\n        }\n        verify(ctx.reader(), idToOrdsPrefix, termsArray, prefixRef);\n      }\n\n      \r\n      \r\n      if (VERBOSE) {\n        System.out.println(\"TEST: top reader\");\n      }\n      verify(slowR, idToOrdsPrefix, termsArray, prefixRef);\n    }\n\n    FieldCache.DEFAULT.purgeByCacheKey(slowR.getCoreCacheKey());\n\n    r.close();\n    dir.close();\n  }\n","realPath":"solr/core/src/test/org/apache/solr/uninverting/TestDocTermOrds.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":227,"status":"B"}],"commitId":"366f8d18fe3ac9c9fb93493fe60d2bf4157de032","commitMessage":"@@@Merge remote-tracking branch 'origin/master'\n","date":"2016-05-26 01:42:11","modifiedFileCount":"50","status":"B","submitter":"Noble Paul"},{"authorTime":"2017-02-28 21:21:30","codes":[{"authorDate":"2017-02-28 21:21:30","commitOrder":2,"curCode":"  public void testRandom() throws Exception {\n    Directory dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = TestUtil.randomRealisticUnicodeString(random());\n      \r\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    \r\n    if (random().nextInt(10) == 7) {\n      \r\n      Codec codec = TestUtil.alwaysPostingsFormat(TestUtil.getPostingsFormatWithOrds(random()));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new LegacyIntField(\"id\", id, Field.Store.YES));\n      \n      final int termCount = TestUtil.nextInt(random(), 0, 20 * RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random().nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newStringField(\"field\", termsArray[ord].utf8ToString(), Field.Store.NO);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(LeafReaderContext ctx : r.leaves()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + ctx.reader());\n      }\n      verify(ctx.reader(), idToOrds, termsArray, null);\n    }\n\n    \r\n    \r\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    LeafReader slowR = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(slowR);\n    verify(slowR, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purgeByCacheKey(slowR.getCoreCacheHelper().getKey());\n\n    r.close();\n    dir.close();\n  }\n","date":"2017-02-28 21:46:45","endLine":225,"groupId":"103320","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testRandom","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/69/b89b4209007e1b2194ce29fec2871d892071cd.src","preCode":"  public void testRandom() throws Exception {\n    Directory dir = newDirectory();\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = TestUtil.randomRealisticUnicodeString(random());\n      \r\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    \r\n    if (random().nextInt(10) == 7) {\n      \r\n      Codec codec = TestUtil.alwaysPostingsFormat(TestUtil.getPostingsFormatWithOrds(random()));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new LegacyIntField(\"id\", id, Field.Store.YES));\n      \n      final int termCount = TestUtil.nextInt(random(), 0, 20 * RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random().nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newStringField(\"field\", termsArray[ord].utf8ToString(), Field.Store.NO);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n\n    for(LeafReaderContext ctx : r.leaves()) {\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: sub=\" + ctx.reader());\n      }\n      verify(ctx.reader(), idToOrds, termsArray, null);\n    }\n\n    \r\n    \r\n    if (VERBOSE) {\n      System.out.println(\"TEST: top reader\");\n    }\n    LeafReader slowR = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(slowR);\n    verify(slowR, idToOrds, termsArray, null);\n\n    FieldCache.DEFAULT.purgeByCacheKey(slowR.getCoreCacheKey());\n\n    r.close();\n    dir.close();\n  }\n","realPath":"solr/core/src/test/org/apache/solr/uninverting/TestDocTermOrds.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":139,"status":"M"},{"authorDate":"2017-02-28 21:21:30","commitOrder":2,"curCode":"  public void testRandomWithPrefix() throws Exception {\n    Directory dir = newDirectory();\n\n    final Set<String> prefixes = new HashSet<>();\n    final int numPrefix = TestUtil.nextInt(random(), 2, 7);\n    if (VERBOSE) {\n      System.out.println(\"TEST: use \" + numPrefix + \" prefixes\");\n    }\n    while(prefixes.size() < numPrefix) {\n      prefixes.add(TestUtil.randomRealisticUnicodeString(random()));\n      \r\n    }\n    final String[] prefixesArray = prefixes.toArray(new String[prefixes.size()]);\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = prefixesArray[random().nextInt(prefixesArray.length)] + TestUtil.randomRealisticUnicodeString(random());\n      \r\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    \r\n    if (random().nextInt(10) == 7) {\n      Codec codec = TestUtil.alwaysPostingsFormat(TestUtil.getPostingsFormatWithOrds(random()));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new LegacyIntField(\"id\", id, Field.Store.YES));\n      \n      final int termCount = TestUtil.nextInt(random(), 0, 20 * RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random().nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newStringField(\"field\", termsArray[ord].utf8ToString(), Field.Store.NO);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n    \n    LeafReader slowR = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(slowR);\n    for(String prefix : prefixesArray) {\n\n      final BytesRef prefixRef = prefix == null ? null : new BytesRef(prefix);\n\n      final int[][] idToOrdsPrefix = new int[NUM_DOCS][];\n      for(int id=0;id<NUM_DOCS;id++) {\n        final int[] docOrds = idToOrds[id];\n        final List<Integer> newOrds = new ArrayList<>();\n        for(int ord : idToOrds[id]) {\n          if (StringHelper.startsWith(termsArray[ord], prefixRef)) {\n            newOrds.add(ord);\n          }\n        }\n        final int[] newOrdsArray = new int[newOrds.size()];\n        int upto = 0;\n        for(int ord : newOrds) {\n          newOrdsArray[upto++] = ord;\n        }\n        idToOrdsPrefix[id] = newOrdsArray;\n      }\n\n      for(LeafReaderContext ctx : r.leaves()) {\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: sub=\" + ctx.reader());\n        }\n        verify(ctx.reader(), idToOrdsPrefix, termsArray, prefixRef);\n      }\n\n      \r\n      \r\n      if (VERBOSE) {\n        System.out.println(\"TEST: top reader\");\n      }\n      verify(slowR, idToOrdsPrefix, termsArray, prefixRef);\n    }\n\n    FieldCache.DEFAULT.purgeByCacheKey(slowR.getCoreCacheHelper().getKey());\n\n    r.close();\n    dir.close();\n  }\n","date":"2017-02-28 21:46:45","endLine":345,"groupId":"103320","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testRandomWithPrefix","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/69/b89b4209007e1b2194ce29fec2871d892071cd.src","preCode":"  public void testRandomWithPrefix() throws Exception {\n    Directory dir = newDirectory();\n\n    final Set<String> prefixes = new HashSet<>();\n    final int numPrefix = TestUtil.nextInt(random(), 2, 7);\n    if (VERBOSE) {\n      System.out.println(\"TEST: use \" + numPrefix + \" prefixes\");\n    }\n    while(prefixes.size() < numPrefix) {\n      prefixes.add(TestUtil.randomRealisticUnicodeString(random()));\n      \r\n    }\n    final String[] prefixesArray = prefixes.toArray(new String[prefixes.size()]);\n\n    final int NUM_TERMS = atLeast(20);\n    final Set<BytesRef> terms = new HashSet<>();\n    while(terms.size() < NUM_TERMS) {\n      final String s = prefixesArray[random().nextInt(prefixesArray.length)] + TestUtil.randomRealisticUnicodeString(random());\n      \r\n      if (s.length() > 0) {\n        terms.add(new BytesRef(s));\n      }\n    }\n    final BytesRef[] termsArray = terms.toArray(new BytesRef[terms.size()]);\n    Arrays.sort(termsArray);\n    \n    final int NUM_DOCS = atLeast(100);\n\n    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));\n\n    \r\n    if (random().nextInt(10) == 7) {\n      Codec codec = TestUtil.alwaysPostingsFormat(TestUtil.getPostingsFormatWithOrds(random()));\n      conf.setCodec(codec);\n    }\n    \n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir, conf);\n\n    final int[][] idToOrds = new int[NUM_DOCS][];\n    final Set<Integer> ordsForDocSet = new HashSet<>();\n\n    for(int id=0;id<NUM_DOCS;id++) {\n      Document doc = new Document();\n\n      doc.add(new LegacyIntField(\"id\", id, Field.Store.YES));\n      \n      final int termCount = TestUtil.nextInt(random(), 0, 20 * RANDOM_MULTIPLIER);\n      while(ordsForDocSet.size() < termCount) {\n        ordsForDocSet.add(random().nextInt(termsArray.length));\n      }\n      final int[] ordsForDoc = new int[termCount];\n      int upto = 0;\n      if (VERBOSE) {\n        System.out.println(\"TEST: doc id=\" + id);\n      }\n      for(int ord : ordsForDocSet) {\n        ordsForDoc[upto++] = ord;\n        Field field = newStringField(\"field\", termsArray[ord].utf8ToString(), Field.Store.NO);\n        if (VERBOSE) {\n          System.out.println(\"  f=\" + termsArray[ord].utf8ToString());\n        }\n        doc.add(field);\n      }\n      ordsForDocSet.clear();\n      Arrays.sort(ordsForDoc);\n      idToOrds[id] = ordsForDoc;\n      w.addDocument(doc);\n    }\n    \n    final DirectoryReader r = w.getReader();\n    w.close();\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: reader=\" + r);\n    }\n    \n    LeafReader slowR = SlowCompositeReaderWrapper.wrap(r);\n    TestUtil.checkReader(slowR);\n    for(String prefix : prefixesArray) {\n\n      final BytesRef prefixRef = prefix == null ? null : new BytesRef(prefix);\n\n      final int[][] idToOrdsPrefix = new int[NUM_DOCS][];\n      for(int id=0;id<NUM_DOCS;id++) {\n        final int[] docOrds = idToOrds[id];\n        final List<Integer> newOrds = new ArrayList<>();\n        for(int ord : idToOrds[id]) {\n          if (StringHelper.startsWith(termsArray[ord], prefixRef)) {\n            newOrds.add(ord);\n          }\n        }\n        final int[] newOrdsArray = new int[newOrds.size()];\n        int upto = 0;\n        for(int ord : newOrds) {\n          newOrdsArray[upto++] = ord;\n        }\n        idToOrdsPrefix[id] = newOrdsArray;\n      }\n\n      for(LeafReaderContext ctx : r.leaves()) {\n        if (VERBOSE) {\n          System.out.println(\"\\nTEST: sub=\" + ctx.reader());\n        }\n        verify(ctx.reader(), idToOrdsPrefix, termsArray, prefixRef);\n      }\n\n      \r\n      \r\n      if (VERBOSE) {\n        System.out.println(\"TEST: top reader\");\n      }\n      verify(slowR, idToOrdsPrefix, termsArray, prefixRef);\n    }\n\n    FieldCache.DEFAULT.purgeByCacheKey(slowR.getCoreCacheKey());\n\n    r.close();\n    dir.close();\n  }\n","realPath":"solr/core/src/test/org/apache/solr/uninverting/TestDocTermOrds.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":227,"status":"M"}],"commitId":"df6f83072303b4891a296b700a50c743284d3c30","commitMessage":"@@@LUCENE-7410: Make cache keys and close listeners less trappy.\n","date":"2017-02-28 21:46:45","modifiedFileCount":"75","status":"M","submitter":"Adrien Grand"}]
