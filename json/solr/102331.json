[{"authorTime":"2011-07-10 07:01:53","codes":[{"authorDate":"2011-07-10 07:01:53","commitOrder":1,"curCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<Token>();\n    TokenStream ts = analyzer.reusableTokenStream(\"\", new StringReader(q));\n    ts.reset();\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    while (ts.incrementToken()){\n      Token token = new Token();\n      token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n      token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n      token.setType(typeAtt.type());\n      token.setFlags(flagsAtt.getFlags());\n      token.setPayload(payloadAtt.getPayload());\n      token.setPositionIncrement(posIncAtt.getPositionIncrement());\n      result.add(token);\n    }\n    return result;\n  }\n","date":"2011-07-10 07:01:53","endLine":497,"groupId":"31259","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"getTokens","params":"(Stringq@Analyzeranalyzer)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/5d/37913207b138fe81adaa2c7b317957ed96b6d9.src","preCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<Token>();\n    TokenStream ts = analyzer.reusableTokenStream(\"\", new StringReader(q));\n    ts.reset();\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    while (ts.incrementToken()){\n      Token token = new Token();\n      token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n      token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n      token.setType(typeAtt.type());\n      token.setFlags(flagsAtt.getFlags());\n      token.setPayload(payloadAtt.getPayload());\n      token.setPositionIncrement(posIncAtt.getPositionIncrement());\n      result.add(token);\n    }\n    return result;\n  }\n","realPath":"solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":474,"status":"B"},{"authorDate":"2011-07-10 07:01:53","commitOrder":1,"curCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<Token>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_40);\n    TokenStream ts = analyzer.tokenStream(\"\", new StringReader(origQuery));\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    try {\n      ts.reset();\n      while (ts.incrementToken()){\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    return result;\n  }\n","date":"2011-07-10 07:01:53","endLine":70,"groupId":"51395","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"convert","params":"(StringorigQuery)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/0d/cfee46b166c4123fb4f8d9367a310cc177e3b9.src","preCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<Token>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_40);\n    TokenStream ts = analyzer.tokenStream(\"\", new StringReader(origQuery));\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    try {\n      ts.reset();\n      while (ts.incrementToken()){\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    return result;\n  }\n","realPath":"solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"B"}],"commitId":"429093b236e30940d69edc8869346819c337cd10","commitMessage":"@@@SOLR-2452: Rewrote Solr build system (tighter integration with the Lucene build system) and restructured Solr internal and contrib modules\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1144761 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2011-07-10 07:01:53","modifiedFileCount":"0","status":"B","submitter":"Steven Rowe"},{"authorTime":"2011-07-10 07:01:53","codes":[{"authorDate":"2011-08-08 19:55:03","commitOrder":2,"curCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<Token>();\n    assert analyzer != null;\n    TokenStream ts = analyzer.reusableTokenStream(\"\", new StringReader(q));\n    ts.reset();\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    while (ts.incrementToken()){\n      Token token = new Token();\n      token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n      token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n      token.setType(typeAtt.type());\n      token.setFlags(flagsAtt.getFlags());\n      token.setPayload(payloadAtt.getPayload());\n      token.setPositionIncrement(posIncAtt.getPositionIncrement());\n      result.add(token);\n    }\n    return result;\n  }\n","date":"2011-08-08 19:55:03","endLine":498,"groupId":"31259","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"getTokens","params":"(Stringq@Analyzeranalyzer)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/34/9896737e3ec3576fe82f4fe1348872c6a90ef9.src","preCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<Token>();\n    TokenStream ts = analyzer.reusableTokenStream(\"\", new StringReader(q));\n    ts.reset();\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    while (ts.incrementToken()){\n      Token token = new Token();\n      token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n      token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n      token.setType(typeAtt.type());\n      token.setFlags(flagsAtt.getFlags());\n      token.setPayload(payloadAtt.getPayload());\n      token.setPositionIncrement(posIncAtt.getPositionIncrement());\n      result.add(token);\n    }\n    return result;\n  }\n","realPath":"solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":474,"status":"M"},{"authorDate":"2011-07-10 07:01:53","commitOrder":2,"curCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<Token>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_40);\n    TokenStream ts = analyzer.tokenStream(\"\", new StringReader(origQuery));\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    try {\n      ts.reset();\n      while (ts.incrementToken()){\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    return result;\n  }\n","date":"2011-07-10 07:01:53","endLine":70,"groupId":"51395","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"convert","params":"(StringorigQuery)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/0d/cfee46b166c4123fb4f8d9367a310cc177e3b9.src","preCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<Token>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_40);\n    TokenStream ts = analyzer.tokenStream(\"\", new StringReader(origQuery));\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    try {\n      ts.reset();\n      while (ts.incrementToken()){\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    return result;\n  }\n","realPath":"solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"N"}],"commitId":"718f42479f52f1e178a131c908498a7f45f84a95","commitMessage":"@@@SOLR-2688: switch solr 4.0 example to DirectSpellChecker\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1154935 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2011-08-08 19:55:03","modifiedFileCount":"2","status":"M","submitter":"Robert Muir"},{"authorTime":"2011-07-10 07:01:53","codes":[{"authorDate":"2011-09-26 03:10:17","commitOrder":3,"curCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<Token>();\n    assert analyzer != null;\n    TokenStream ts = analyzer.reusableTokenStream(\"\", new StringReader(q));\n    ts.reset();\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    while (ts.incrementToken()){\n      Token token = new Token();\n      token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n      token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n      token.setType(typeAtt.type());\n      token.setFlags(flagsAtt.getFlags());\n      token.setPayload(payloadAtt.getPayload());\n      token.setPositionIncrement(posIncAtt.getPositionIncrement());\n      result.add(token);\n    }\n    ts.end();\n    ts.close();\n    return result;\n  }\n","date":"2011-09-26 03:10:17","endLine":500,"groupId":"31259","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"getTokens","params":"(Stringq@Analyzeranalyzer)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/f1/982e3c961c0ab8a18059c1486aa330899e3e06.src","preCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<Token>();\n    assert analyzer != null;\n    TokenStream ts = analyzer.reusableTokenStream(\"\", new StringReader(q));\n    ts.reset();\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    while (ts.incrementToken()){\n      Token token = new Token();\n      token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n      token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n      token.setType(typeAtt.type());\n      token.setFlags(flagsAtt.getFlags());\n      token.setPayload(payloadAtt.getPayload());\n      token.setPositionIncrement(posIncAtt.getPositionIncrement());\n      result.add(token);\n    }\n    return result;\n  }\n","realPath":"solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":474,"status":"M"},{"authorDate":"2011-07-10 07:01:53","commitOrder":3,"curCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<Token>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_40);\n    TokenStream ts = analyzer.tokenStream(\"\", new StringReader(origQuery));\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    try {\n      ts.reset();\n      while (ts.incrementToken()){\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    return result;\n  }\n","date":"2011-07-10 07:01:53","endLine":70,"groupId":"51395","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"convert","params":"(StringorigQuery)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/0d/cfee46b166c4123fb4f8d9367a310cc177e3b9.src","preCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<Token>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_40);\n    TokenStream ts = analyzer.tokenStream(\"\", new StringReader(origQuery));\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    try {\n      ts.reset();\n      while (ts.incrementToken()){\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    return result;\n  }\n","realPath":"solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"N"}],"commitId":"1e577e15b5be16f498317020b9c26722f0df5c42","commitMessage":"@@@LUCENE-3456: use MockTokenizer instead of WhitespaceTokenizer in test configs\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1175529 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2011-09-26 03:10:17","modifiedFileCount":"5","status":"M","submitter":"Robert Muir"},{"authorTime":"2011-09-27 12:07:38","codes":[{"authorDate":"2011-09-26 03:10:17","commitOrder":4,"curCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<Token>();\n    assert analyzer != null;\n    TokenStream ts = analyzer.reusableTokenStream(\"\", new StringReader(q));\n    ts.reset();\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    while (ts.incrementToken()){\n      Token token = new Token();\n      token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n      token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n      token.setType(typeAtt.type());\n      token.setFlags(flagsAtt.getFlags());\n      token.setPayload(payloadAtt.getPayload());\n      token.setPositionIncrement(posIncAtt.getPositionIncrement());\n      result.add(token);\n    }\n    ts.end();\n    ts.close();\n    return result;\n  }\n","date":"2011-09-26 03:10:17","endLine":500,"groupId":"31259","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"getTokens","params":"(Stringq@Analyzeranalyzer)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/f1/982e3c961c0ab8a18059c1486aa330899e3e06.src","preCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<Token>();\n    assert analyzer != null;\n    TokenStream ts = analyzer.reusableTokenStream(\"\", new StringReader(q));\n    ts.reset();\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    while (ts.incrementToken()){\n      Token token = new Token();\n      token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n      token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n      token.setType(typeAtt.type());\n      token.setFlags(flagsAtt.getFlags());\n      token.setPayload(payloadAtt.getPayload());\n      token.setPositionIncrement(posIncAtt.getPositionIncrement());\n      result.add(token);\n    }\n    ts.end();\n    ts.close();\n    return result;\n  }\n","realPath":"solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":474,"status":"N"},{"authorDate":"2011-09-27 12:07:38","commitOrder":4,"curCode":"  public Collection<Token> convert(String origQuery) {\n    try {\n      Collection<Token> result = new HashSet<Token>();\n      WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_40);\n      TokenStream ts = analyzer.reusableTokenStream(\"\", new StringReader(origQuery));\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n\n      ts.reset();\n\n      while (ts.incrementToken()) {\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n      ts.end();\n      ts.close();\n      \n      return result;\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2011-09-27 12:07:38","endLine":75,"groupId":"51395","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"convert","params":"(StringorigQuery)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/df/9eb75c6ebde86a613d38e576e1ce53f19cf7f4.src","preCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<Token>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_40);\n    TokenStream ts = analyzer.tokenStream(\"\", new StringReader(origQuery));\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    try {\n      ts.reset();\n      while (ts.incrementToken()){\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    return result;\n  }\n","realPath":"solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":43,"status":"M"}],"commitId":"fe3982c746606e595073675063dd42d0698b644d","commitMessage":"@@@LUCENE-3455: Moved remaining Analysis consumers over to using reusableTokenStream\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1176191 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2011-09-27 12:07:38","modifiedFileCount":"7","status":"M","submitter":"Christopher John Male"},{"authorTime":"2011-09-28 13:26:54","codes":[{"authorDate":"2011-09-28 13:26:54","commitOrder":5,"curCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<Token>();\n    assert analyzer != null;\n    TokenStream ts = analyzer.tokenStream(\"\", new StringReader(q));\n    ts.reset();\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    while (ts.incrementToken()){\n      Token token = new Token();\n      token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n      token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n      token.setType(typeAtt.type());\n      token.setFlags(flagsAtt.getFlags());\n      token.setPayload(payloadAtt.getPayload());\n      token.setPositionIncrement(posIncAtt.getPositionIncrement());\n      result.add(token);\n    }\n    ts.end();\n    ts.close();\n    return result;\n  }\n","date":"2011-09-28 13:26:54","endLine":500,"groupId":"31259","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"getTokens","params":"(Stringq@Analyzeranalyzer)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/63/1e33a29a94105a77b27e6d464a6b9d585a3719.src","preCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<Token>();\n    assert analyzer != null;\n    TokenStream ts = analyzer.reusableTokenStream(\"\", new StringReader(q));\n    ts.reset();\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    while (ts.incrementToken()){\n      Token token = new Token();\n      token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n      token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n      token.setType(typeAtt.type());\n      token.setFlags(flagsAtt.getFlags());\n      token.setPayload(payloadAtt.getPayload());\n      token.setPositionIncrement(posIncAtt.getPositionIncrement());\n      result.add(token);\n    }\n    ts.end();\n    ts.close();\n    return result;\n  }\n","realPath":"solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":474,"status":"M"},{"authorDate":"2011-09-28 13:26:54","commitOrder":5,"curCode":"  public Collection<Token> convert(String origQuery) {\n    try {\n      Collection<Token> result = new HashSet<Token>();\n      WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_40);\n      TokenStream ts = analyzer.tokenStream(\"\", new StringReader(origQuery));\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n\n      ts.reset();\n\n      while (ts.incrementToken()) {\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n      ts.end();\n      ts.close();\n      \n      return result;\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2011-09-28 13:26:54","endLine":75,"groupId":"51395","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"convert","params":"(StringorigQuery)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/fe/e6848ffa606d931b38d7700659171beb1f333c.src","preCode":"  public Collection<Token> convert(String origQuery) {\n    try {\n      Collection<Token> result = new HashSet<Token>();\n      WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_40);\n      TokenStream ts = analyzer.reusableTokenStream(\"\", new StringReader(origQuery));\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n\n      ts.reset();\n\n      while (ts.incrementToken()) {\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n      ts.end();\n      ts.close();\n      \n      return result;\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":43,"status":"M"}],"commitId":"67c13bd2fe57d73a824f163f9c73018fa51a1a65","commitMessage":"@@@LUCENE-3455: Renamed Analyzer.reusableTokenStream to Analyzer.tokenStream\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1176728 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2011-09-28 13:26:54","modifiedFileCount":"53","status":"M","submitter":"Christopher John Male"},{"authorTime":"2013-07-09 01:55:48","codes":[{"authorDate":"2013-07-09 01:55:48","commitOrder":6,"curCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<Token>();\n    assert analyzer != null;\n    TokenStream ts = analyzer.tokenStream(\"\", q);\n    ts.reset();\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    while (ts.incrementToken()){\n      Token token = new Token();\n      token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n      token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n      token.setType(typeAtt.type());\n      token.setFlags(flagsAtt.getFlags());\n      token.setPayload(payloadAtt.getPayload());\n      token.setPositionIncrement(posIncAtt.getPositionIncrement());\n      result.add(token);\n    }\n    ts.end();\n    ts.close();\n    return result;\n  }\n","date":"2013-07-09 01:55:48","endLine":489,"groupId":"31259","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"getTokens","params":"(Stringq@Analyzeranalyzer)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/1d/eec69da6a9e48e7150eaf663b39e3c07264857.src","preCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<Token>();\n    assert analyzer != null;\n    TokenStream ts = analyzer.tokenStream(\"\", new StringReader(q));\n    ts.reset();\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    while (ts.incrementToken()){\n      Token token = new Token();\n      token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n      token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n      token.setType(typeAtt.type());\n      token.setFlags(flagsAtt.getFlags());\n      token.setPayload(payloadAtt.getPayload());\n      token.setPositionIncrement(posIncAtt.getPositionIncrement());\n      result.add(token);\n    }\n    ts.end();\n    ts.close();\n    return result;\n  }\n","realPath":"solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":463,"status":"M"},{"authorDate":"2013-07-09 01:55:48","commitOrder":6,"curCode":"  public Collection<Token> convert(String origQuery) {\n    try {\n      Collection<Token> result = new HashSet<Token>();\n      WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_40);\n      TokenStream ts = analyzer.tokenStream(\"\", origQuery);\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n\n      ts.reset();\n\n      while (ts.incrementToken()) {\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n      ts.end();\n      ts.close();\n      \n      return result;\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2013-07-09 01:55:48","endLine":74,"groupId":"51395","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"convert","params":"(StringorigQuery)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/a9/ad3b9b627723e30650155c1a31370540d1bf92.src","preCode":"  public Collection<Token> convert(String origQuery) {\n    try {\n      Collection<Token> result = new HashSet<Token>();\n      WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_40);\n      TokenStream ts = analyzer.tokenStream(\"\", new StringReader(origQuery));\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n\n      ts.reset();\n\n      while (ts.incrementToken()) {\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n      ts.end();\n      ts.close();\n      \n      return result;\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"M"}],"commitId":"f092795fe94ba727f7368b63d8eb1ecd39749fc4","commitMessage":"@@@LUCENE-5097: Analyzer now has an additional tokenStream(String fieldName.  String text) method.  so wrapping by StringReader for common use is no longer needed. This method uses an internal reuseable reader.  which was previously only used by the Field class.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1500862 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2013-07-09 01:55:48","modifiedFileCount":"59","status":"M","submitter":"Uwe Schindler"},{"authorTime":"2013-10-07 14:24:25","codes":[{"authorDate":"2013-10-07 14:24:25","commitOrder":7,"curCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<Token>();\n    assert analyzer != null;\n    try (TokenStream ts = analyzer.tokenStream(\"\", q)) {\n      ts.reset();\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      \n      while (ts.incrementToken()){\n        Token token = new Token();\n        token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        result.add(token);\n      }\n      ts.end();\n      return result;\n    }\n  }\n","date":"2013-10-07 14:24:25","endLine":489,"groupId":"31259","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"getTokens","params":"(Stringq@Analyzeranalyzer)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/35/a6c66592f1cde7081a902cd26e861131397dd4.src","preCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<Token>();\n    assert analyzer != null;\n    TokenStream ts = analyzer.tokenStream(\"\", q);\n    ts.reset();\n    \r\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n    TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n    FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n    \n    while (ts.incrementToken()){\n      Token token = new Token();\n      token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n      token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n      token.setType(typeAtt.type());\n      token.setFlags(flagsAtt.getFlags());\n      token.setPayload(payloadAtt.getPayload());\n      token.setPositionIncrement(posIncAtt.getPositionIncrement());\n      result.add(token);\n    }\n    ts.end();\n    ts.close();\n    return result;\n  }\n","realPath":"solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":463,"status":"M"},{"authorDate":"2013-10-07 14:24:25","commitOrder":7,"curCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<Token>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_40);\n    \n    try (TokenStream ts = analyzer.tokenStream(\"\", origQuery)) {\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n\n      ts.reset();\n\n      while (ts.incrementToken()) {\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n      ts.end();      \n      return result;\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2013-10-07 14:24:25","endLine":72,"groupId":"51395","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"convert","params":"(StringorigQuery)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/15/ec62f6a2f2124e83a8afdfba9ba2b2fae3b2b2.src","preCode":"  public Collection<Token> convert(String origQuery) {\n    try {\n      Collection<Token> result = new HashSet<Token>();\n      WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_40);\n      TokenStream ts = analyzer.tokenStream(\"\", origQuery);\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n\n      ts.reset();\n\n      while (ts.incrementToken()) {\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n      ts.end();\n      ts.close();\n      \n      return result;\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"M"}],"commitId":"64a795b6e3dd82b9c579ee1652db64da7df56c6b","commitMessage":"@@@LUCENE-5259: convert analysis consumers to try-with-resources\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1529770 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2013-10-07 14:24:25","modifiedFileCount":"41","status":"M","submitter":"Robert Muir"},{"authorTime":"2013-12-10 06:53:38","codes":[{"authorDate":"2013-10-07 14:24:25","commitOrder":8,"curCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<Token>();\n    assert analyzer != null;\n    try (TokenStream ts = analyzer.tokenStream(\"\", q)) {\n      ts.reset();\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      \n      while (ts.incrementToken()){\n        Token token = new Token();\n        token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        result.add(token);\n      }\n      ts.end();\n      return result;\n    }\n  }\n","date":"2013-10-07 14:24:25","endLine":489,"groupId":"31259","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"getTokens","params":"(Stringq@Analyzeranalyzer)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/35/a6c66592f1cde7081a902cd26e861131397dd4.src","preCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<Token>();\n    assert analyzer != null;\n    try (TokenStream ts = analyzer.tokenStream(\"\", q)) {\n      ts.reset();\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      \n      while (ts.incrementToken()){\n        Token token = new Token();\n        token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        result.add(token);\n      }\n      ts.end();\n      return result;\n    }\n  }\n","realPath":"solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":463,"status":"N"},{"authorDate":"2013-12-10 06:53:38","commitOrder":8,"curCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<Token>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(LuceneTestCase.TEST_VERSION_CURRENT);\n    \n    try (TokenStream ts = analyzer.tokenStream(\"\", origQuery)) {\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n\n      ts.reset();\n\n      while (ts.incrementToken()) {\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n      ts.end();      \n      return result;\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2013-12-10 06:53:38","endLine":72,"groupId":"51395","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"convert","params":"(StringorigQuery)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/69/d70ab644aea556939732ab8dbd4e6b86f7e79d.src","preCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<Token>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_40);\n    \n    try (TokenStream ts = analyzer.tokenStream(\"\", origQuery)) {\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n\n      ts.reset();\n\n      while (ts.incrementToken()) {\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n      ts.end();      \n      return result;\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"M"}],"commitId":"a1461ad9b12fe4d9a62fc7a4aff2c7361584dedc","commitMessage":"@@@LUCENE-5364: Replace hard-coded Version.LUCENE_XY that doesn't have to be hard-coded (because of back-compat testing or version dependent behavior.  or demo code that should exemplify pinning versions in user code).  with Version.LUCENE_CURRENT in non-test code.  or with LuceneTestCase.TEST_VERSION_CURRENT in test code; upgrade hard-coded Version.LUCENE_XY constants that should track the next release version to the next release version if they aren't already there.  and put a token near them so that they can be found and upgraded when the next release version changes: ':Post-Release-Update-Version.LUCENE_XY:'\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1549701 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2013-12-10 06:53:38","modifiedFileCount":"28","status":"M","submitter":"Steven Rowe"},{"authorTime":"2014-03-12 22:39:17","codes":[{"authorDate":"2014-03-12 22:39:17","commitOrder":9,"curCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<>();\n    assert analyzer != null;\n    try (TokenStream ts = analyzer.tokenStream(\"\", q)) {\n      ts.reset();\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      \n      while (ts.incrementToken()){\n        Token token = new Token();\n        token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        result.add(token);\n      }\n      ts.end();\n      return result;\n    }\n  }\n","date":"2014-03-12 22:39:17","endLine":498,"groupId":"31259","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"getTokens","params":"(Stringq@Analyzeranalyzer)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/35/44630c1341207ac6e106a0dea9058121005b47.src","preCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<Token>();\n    assert analyzer != null;\n    try (TokenStream ts = analyzer.tokenStream(\"\", q)) {\n      ts.reset();\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      \n      while (ts.incrementToken()){\n        Token token = new Token();\n        token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        result.add(token);\n      }\n      ts.end();\n      return result;\n    }\n  }\n","realPath":"solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":472,"status":"M"},{"authorDate":"2014-03-12 22:39:17","commitOrder":9,"curCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(LuceneTestCase.TEST_VERSION_CURRENT);\n    \n    try (TokenStream ts = analyzer.tokenStream(\"\", origQuery)) {\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n\n      ts.reset();\n\n      while (ts.incrementToken()) {\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n      ts.end();      \n      return result;\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2014-03-12 22:39:17","endLine":72,"groupId":"51395","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"convert","params":"(StringorigQuery)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/c8/ca39ef635c6f970da60778dd3ce9fcc80e928e.src","preCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<Token>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(LuceneTestCase.TEST_VERSION_CURRENT);\n    \n    try (TokenStream ts = analyzer.tokenStream(\"\", origQuery)) {\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n\n      ts.reset();\n\n      while (ts.incrementToken()) {\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n      ts.end();      \n      return result;\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"M"}],"commitId":"3b67b17493216f6b0c81a981073fd5f61eace6f4","commitMessage":"@@@LUCENE-5512: remove redundant typing (diamond operator) in trunk\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1576755 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2014-03-12 22:39:17","modifiedFileCount":"1241","status":"M","submitter":"Robert Muir"},{"authorTime":"2014-07-31 06:16:16","codes":[{"authorDate":"2014-03-12 22:39:17","commitOrder":10,"curCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<>();\n    assert analyzer != null;\n    try (TokenStream ts = analyzer.tokenStream(\"\", q)) {\n      ts.reset();\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      \n      while (ts.incrementToken()){\n        Token token = new Token();\n        token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        result.add(token);\n      }\n      ts.end();\n      return result;\n    }\n  }\n","date":"2014-03-12 22:39:17","endLine":498,"groupId":"31259","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"getTokens","params":"(Stringq@Analyzeranalyzer)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/35/44630c1341207ac6e106a0dea9058121005b47.src","preCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<>();\n    assert analyzer != null;\n    try (TokenStream ts = analyzer.tokenStream(\"\", q)) {\n      ts.reset();\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      \n      while (ts.incrementToken()){\n        Token token = new Token();\n        token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        result.add(token);\n      }\n      ts.end();\n      return result;\n    }\n  }\n","realPath":"solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":472,"status":"N"},{"authorDate":"2014-07-31 06:16:16","commitOrder":10,"curCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer();\n    \n    try (TokenStream ts = analyzer.tokenStream(\"\", origQuery)) {\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n\n      ts.reset();\n\n      while (ts.incrementToken()) {\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n      ts.end();      \n      return result;\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2014-07-31 06:16:16","endLine":72,"groupId":"51395","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"convert","params":"(StringorigQuery)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/60/140f280378c2b66f4e7df7529389fdb39cd8d3.src","preCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(LuceneTestCase.TEST_VERSION_CURRENT);\n    \n    try (TokenStream ts = analyzer.tokenStream(\"\", origQuery)) {\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n\n      ts.reset();\n\n      while (ts.incrementToken()) {\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n      ts.end();      \n      return result;\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"M"}],"commitId":"0368c604cc6bfcabf9c7f1c2afd0dd2e0fbb4a96","commitMessage":"@@@LUCENE-5859: remove dead code: changes no runtime behavior.  these are all unused variables\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1614778 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2014-07-31 06:16:16","modifiedFileCount":"258","status":"M","submitter":"Robert Muir"},{"authorTime":"2014-07-31 18:44:39","codes":[{"authorDate":"2014-03-12 22:39:17","commitOrder":11,"curCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<>();\n    assert analyzer != null;\n    try (TokenStream ts = analyzer.tokenStream(\"\", q)) {\n      ts.reset();\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      \n      while (ts.incrementToken()){\n        Token token = new Token();\n        token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        result.add(token);\n      }\n      ts.end();\n      return result;\n    }\n  }\n","date":"2014-03-12 22:39:17","endLine":498,"groupId":"31259","id":21,"instanceNumber":1,"isCurCommit":0,"methodName":"getTokens","params":"(Stringq@Analyzeranalyzer)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/35/44630c1341207ac6e106a0dea9058121005b47.src","preCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<>();\n    assert analyzer != null;\n    try (TokenStream ts = analyzer.tokenStream(\"\", q)) {\n      ts.reset();\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      \n      while (ts.incrementToken()){\n        Token token = new Token();\n        token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        result.add(token);\n      }\n      ts.end();\n      return result;\n    }\n  }\n","realPath":"solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":472,"status":"N"},{"authorDate":"2014-07-31 18:44:39","commitOrder":11,"curCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(LuceneTestCase.TEST_VERSION_CURRENT);\n    \n    try (TokenStream ts = analyzer.tokenStream(\"\", origQuery)) {\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n\n      ts.reset();\n\n      while (ts.incrementToken()) {\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n      ts.end();      \n      return result;\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2014-07-31 18:44:39","endLine":72,"groupId":"51395","id":22,"instanceNumber":2,"isCurCommit":0,"methodName":"convert","params":"(StringorigQuery)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/c8/ca39ef635c6f970da60778dd3ce9fcc80e928e.src","preCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer();\n    \n    try (TokenStream ts = analyzer.tokenStream(\"\", origQuery)) {\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n\n      ts.reset();\n\n      while (ts.incrementToken()) {\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n      ts.end();      \n      return result;\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"M"}],"commitId":"e6d29d223b14778cf22682268539534160458089","commitMessage":"@@@LUCENE-5859: Literally add back dead code to please a bunch of fucking babies\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1614852 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2014-07-31 18:44:39","modifiedFileCount":"258","status":"M","submitter":"Robert Muir"},{"authorTime":"2014-08-09 06:42:48","codes":[{"authorDate":"2014-03-12 22:39:17","commitOrder":12,"curCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<>();\n    assert analyzer != null;\n    try (TokenStream ts = analyzer.tokenStream(\"\", q)) {\n      ts.reset();\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      \n      while (ts.incrementToken()){\n        Token token = new Token();\n        token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        result.add(token);\n      }\n      ts.end();\n      return result;\n    }\n  }\n","date":"2014-03-12 22:39:17","endLine":498,"groupId":"31259","id":23,"instanceNumber":1,"isCurCommit":0,"methodName":"getTokens","params":"(Stringq@Analyzeranalyzer)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/35/44630c1341207ac6e106a0dea9058121005b47.src","preCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<>();\n    assert analyzer != null;\n    try (TokenStream ts = analyzer.tokenStream(\"\", q)) {\n      ts.reset();\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      \n      while (ts.incrementToken()){\n        Token token = new Token();\n        token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        result.add(token);\n      }\n      ts.end();\n      return result;\n    }\n  }\n","realPath":"solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":472,"status":"N"},{"authorDate":"2014-08-09 06:42:48","commitOrder":12,"curCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer();\n    \n    try (TokenStream ts = analyzer.tokenStream(\"\", origQuery)) {\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n\n      ts.reset();\n\n      while (ts.incrementToken()) {\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n      ts.end();      \n      return result;\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2014-08-09 06:42:48","endLine":72,"groupId":"51395","id":24,"instanceNumber":2,"isCurCommit":0,"methodName":"convert","params":"(StringorigQuery)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/60/140f280378c2b66f4e7df7529389fdb39cd8d3.src","preCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(LuceneTestCase.TEST_VERSION_CURRENT);\n    \n    try (TokenStream ts = analyzer.tokenStream(\"\", origQuery)) {\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n\n      ts.reset();\n\n      while (ts.incrementToken()) {\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n      ts.end();      \n      return result;\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":42,"status":"M"}],"commitId":"9938a39a872d4f232f718b2672d0245cae658e0b","commitMessage":"@@@LUCENE-5859: Remove Version from Analyzer constructors\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1616901 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2014-08-09 06:42:48","modifiedFileCount":"284","status":"M","submitter":"Ryan Ernst"},{"authorTime":"2018-02-26 14:13:26","codes":[{"authorDate":"2014-03-12 22:39:17","commitOrder":13,"curCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<>();\n    assert analyzer != null;\n    try (TokenStream ts = analyzer.tokenStream(\"\", q)) {\n      ts.reset();\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      \n      while (ts.incrementToken()){\n        Token token = new Token();\n        token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        result.add(token);\n      }\n      ts.end();\n      return result;\n    }\n  }\n","date":"2014-03-12 22:39:17","endLine":498,"groupId":"102331","id":25,"instanceNumber":1,"isCurCommit":0,"methodName":"getTokens","params":"(Stringq@Analyzeranalyzer)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/35/44630c1341207ac6e106a0dea9058121005b47.src","preCode":"  private Collection<Token> getTokens(String q, Analyzer analyzer) throws IOException {\n    Collection<Token> result = new ArrayList<>();\n    assert analyzer != null;\n    try (TokenStream ts = analyzer.tokenStream(\"\", q)) {\n      ts.reset();\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n      \n      while (ts.incrementToken()){\n        Token token = new Token();\n        token.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        token.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        token.setType(typeAtt.type());\n        token.setFlags(flagsAtt.getFlags());\n        token.setPayload(payloadAtt.getPayload());\n        token.setPositionIncrement(posIncAtt.getPositionIncrement());\n        result.add(token);\n      }\n      ts.end();\n      return result;\n    }\n  }\n","realPath":"solr/core/src/java/org/apache/solr/handler/component/SpellCheckComponent.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":472,"status":"N"},{"authorDate":"2018-02-26 14:13:26","commitOrder":13,"curCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<>();\n\n    try (WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(); TokenStream ts = analyzer.tokenStream(\"\", origQuery)) {\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n\n      ts.reset();\n\n      while (ts.incrementToken()) {\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n      ts.end();      \n      return result;\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","date":"2018-02-26 14:13:47","endLine":69,"groupId":"102331","id":26,"instanceNumber":2,"isCurCommit":1,"methodName":"convert","params":"(StringorigQuery)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/bd/647b31f90beb2d7a42e54ed0531d0c1ca108a3.src","preCode":"  public Collection<Token> convert(String origQuery) {\n    Collection<Token> result = new HashSet<>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer();\n    \n    try (TokenStream ts = analyzer.tokenStream(\"\", origQuery)) {\n      \r\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      TypeAttribute typeAtt = ts.addAttribute(TypeAttribute.class);\n      FlagsAttribute flagsAtt = ts.addAttribute(FlagsAttribute.class);\n      PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.addAttribute(PositionIncrementAttribute.class);\n\n      ts.reset();\n\n      while (ts.incrementToken()) {\n        Token tok = new Token();\n        tok.copyBuffer(termAtt.buffer(), 0, termAtt.length());\n        tok.setOffset(offsetAtt.startOffset(), offsetAtt.endOffset());\n        tok.setFlags(flagsAtt.getFlags());\n        tok.setPayload(payloadAtt.getPayload());\n        tok.setPositionIncrement(posIncAtt.getPositionIncrement());\n        tok.setType(typeAtt.type());\n        result.add(tok);\n      }\n      ts.end();      \n      return result;\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/spelling/SimpleQueryConverter.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":40,"status":"M"}],"commitId":"6164643882f6f2bf371e53376cc9f0a2a184b150","commitMessage":"@@@SOLR-10809: Get precommit lint warnings out of Solr core\n","date":"2018-02-26 14:13:47","modifiedFileCount":"52","status":"M","submitter":"Erick Erickson"}]
