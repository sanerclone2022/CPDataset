[{"authorTime":"2011-07-10 07:01:53","codes":[{"authorDate":"2011-07-10 07:01:53","commitOrder":1,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"  wh������t������v������r  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"  whatever  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expecting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2011-07-10 07:01:53","endLine":335,"groupId":"20976","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/c5/8552190da6309909fd12ef73f42767c08f77ee.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"  wh������t������v������r  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"  whatever  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expecting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":310,"status":"B"},{"authorDate":"2011-07-10 07:01:53","commitOrder":1,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  \n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expecting the 'WhitespaceTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 3, new int[]{4,4,4,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 4, new int[]{6,6,6,4}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 2, new int[]{3,3,3,2,2}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 3, new int[]{4,4,4,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 4, new int[]{6,6,6,4,4}, null, false));\n  }\n","date":"2011-07-10 07:01:53","endLine":335,"groupId":"18506","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/50/6029ceb7c45a1dcd0f2e5f98d3aab0c6d969a8.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  \n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expecting the 'WhitespaceTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 3, new int[]{4,4,4,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 4, new int[]{6,6,6,4}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 2, new int[]{3,3,3,2,2}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 3, new int[]{4,4,4,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 4, new int[]{6,6,6,4,4}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/DocumentAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":205,"status":"B"}],"commitId":"429093b236e30940d69edc8869346819c337cd10","commitMessage":"@@@SOLR-2452: Rewrote Solr build system (tighter integration with the Lucene build system) and restructured Solr internal and contrib modules\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1144761 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2011-07-10 07:01:53","modifiedFileCount":"0","status":"B","submitter":"Steven Rowe"},{"authorTime":"2011-09-26 03:10:17","codes":[{"authorDate":"2011-09-26 03:10:17","commitOrder":2,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"  wh������t������v������r  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"  whatever  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2011-09-26 03:10:17","endLine":336,"groupId":"20976","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/ba/05631e89e0a6cb0142062bc2373f495294f1bd.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"  wh������t������v������r  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"  whatever  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expecting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":311,"status":"M"},{"authorDate":"2011-09-26 03:10:17","commitOrder":2,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  \n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting the 'MockTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 3, new int[]{4,4,4,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 4, new int[]{6,6,6,4}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 2, new int[]{3,3,3,2,2}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 3, new int[]{4,4,4,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 4, new int[]{6,6,6,4,4}, null, false));\n  }\n","date":"2011-09-26 03:10:17","endLine":336,"groupId":"18506","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/5d/eb533ec8e333ba51b45b6e2c4f7de0bf661934.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  \n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expecting the 'WhitespaceTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 3, new int[]{4,4,4,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 4, new int[]{6,6,6,4}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 2, new int[]{3,3,3,2,2}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 3, new int[]{4,4,4,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 4, new int[]{6,6,6,4,4}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/DocumentAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":206,"status":"M"}],"commitId":"1e577e15b5be16f498317020b9c26722f0df5c42","commitMessage":"@@@LUCENE-3456: use MockTokenizer instead of WhitespaceTokenizer in test configs\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1175529 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2011-09-26 03:10:17","modifiedFileCount":"5","status":"M","submitter":"Robert Muir"},{"authorTime":"2011-09-26 03:10:17","codes":[{"authorDate":"2012-01-22 13:20:46","commitOrder":3,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2012-01-22 13:20:46","endLine":336,"groupId":"43613","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/52/7f5188b1f6e548a33be4191a8b5288ffae061d.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"  wh������t������v������r  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"  whatever  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":311,"status":"M"},{"authorDate":"2011-09-26 03:10:17","commitOrder":3,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  \n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting the 'MockTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 3, new int[]{4,4,4,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 4, new int[]{6,6,6,4}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 2, new int[]{3,3,3,2,2}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 3, new int[]{4,4,4,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 4, new int[]{6,6,6,4,4}, null, false));\n  }\n","date":"2011-09-26 03:10:17","endLine":336,"groupId":"18506","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/5d/eb533ec8e333ba51b45b6e2c4f7de0bf661934.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  \n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting the 'MockTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 3, new int[]{4,4,4,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 4, new int[]{6,6,6,4}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 2, new int[]{3,3,3,2,2}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 3, new int[]{4,4,4,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 4, new int[]{6,6,6,4,4}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/DocumentAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":206,"status":"N"}],"commitId":"f3a363708ff95f28b5f29fe2cc27c1cd9fd0fca9","commitMessage":"@@@LUCENE-3690: Re-implemented HTMLStripCharFilter as a JFlex-generated scanner.  Fixes LUCENE-2208.  SOLR-882.  and SOLR-42.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1234452 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2012-01-22 13:20:46","modifiedFileCount":"6","status":"M","submitter":"Steven Rowe"},{"authorTime":"2013-04-22 22:26:55","codes":[{"authorDate":"2013-04-22 22:26:55","commitOrder":4,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2013-04-22 22:26:55","endLine":335,"groupId":"43613","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/43/18bcc8d45affbe06d426faeabf81eb4455e231.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":310,"status":"M"},{"authorDate":"2013-04-22 22:26:55","commitOrder":4,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  \n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting the 'MockTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 3, new int[]{4,4,4,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 4, new int[]{6,6,6,4}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 2, new int[]{3,3,3,2,2}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 3, new int[]{4,4,4,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 4, new int[]{6,6,6,4,4}, null, false));\n  }\n","date":"2013-04-22 22:26:55","endLine":337,"groupId":"18506","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/cc/e1dd5ba70fc97bf4599de279b95dd36b403fd1.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  \n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting the 'MockTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 3, new int[]{4,4,4,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 4, new int[]{6,6,6,4}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 2, new int[]{3,3,3,2,2}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 3, new int[]{4,4,4,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 4, new int[]{6,6,6,4,4}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/DocumentAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":207,"status":"M"}],"commitId":"28b7111dc79fb0095814d617a5879fb93e70fb9c","commitMessage":"@@@SOLR-3251: Dynamically add fields to schema.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1470539 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2013-04-22 22:26:55","modifiedFileCount":"120","status":"M","submitter":"Steven Rowe"},{"authorTime":"2013-05-05 02:18:08","codes":[{"authorDate":"2013-04-22 22:26:55","commitOrder":5,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2013-04-22 22:26:55","endLine":335,"groupId":"43613","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/43/18bcc8d45affbe06d426faeabf81eb4455e231.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":310,"status":"N"},{"authorDate":"2013-05-05 02:18:08","commitOrder":5,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  \n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting the 'MockTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3,3,3}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6,6,6}, null, false));\n  }\n","date":"2013-05-05 02:18:08","endLine":337,"groupId":"18506","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/5d/9091314498d360c092fd84aa47f731853ec212.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  \n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting the 'MockTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 3, new int[]{4,4,4,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 4, new int[]{6,6,6,4}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 2, new int[]{3,3,3,2,2}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 3, new int[]{4,4,4,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 4, new int[]{6,6,6,4,4}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/DocumentAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":207,"status":"M"}],"commitId":"8a7f2b6cc4575878f2b9cfd105e76429f3d6107b","commitMessage":"@@@LUCENE-4963: Deprecate broken TokenFilter options.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1479148 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2013-05-05 02:18:08","modifiedFileCount":"52","status":"M","submitter":"Adrien Grand"},{"authorTime":"2016-06-15 04:38:04","codes":[{"authorDate":"2013-04-22 22:26:55","commitOrder":6,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2013-04-22 22:26:55","endLine":335,"groupId":"43613","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/43/18bcc8d45affbe06d426faeabf81eb4455e231.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":310,"status":"N"},{"authorDate":"2016-06-15 04:38:04","commitOrder":6,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  \n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting the 'MockTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3,3,3}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6,6,6}, null, false));\n  }\n","date":"2016-06-15 04:38:04","endLine":337,"groupId":"18506","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/6e/3d82c42cf28852bb7e5a99826ac3e3b8589226.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  \n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting the 'MockTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3,3,3}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6,6,6}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/DocumentAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":207,"status":"M"}],"commitId":"87016b5f0ce7a895447ee19d3f567f4135cae2a6","commitMessage":"@@@LUCENE-7318: graduate StandardAnalyzer and make it the default for IndexWriterConfig\n","date":"2016-06-15 04:38:04","modifiedFileCount":"182","status":"M","submitter":"Mike McCandless"},{"authorTime":"2016-10-03 03:18:22","codes":[{"authorDate":"2013-04-22 22:26:55","commitOrder":7,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2013-04-22 22:26:55","endLine":335,"groupId":"43613","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/43/18bcc8d45affbe06d426faeabf81eb4455e231.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":310,"status":"N"},{"authorDate":"2016-10-03 03:18:22","commitOrder":7,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  \n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting the 'MockTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3,3,3}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6,6,6}, null, false));\n  }\n","date":"2016-10-03 03:18:22","endLine":337,"groupId":"18506","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/d3/b0ab0e78f76fb16499c45d45747e6e71847f39.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  \n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting the 'MockTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3,3,3}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6,6,6}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/DocumentAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":207,"status":"M"}],"commitId":"3182cd9872af3cb73a1f73e36de68b8d76646e59","commitMessage":"@@@Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/lucene-solr\n","date":"2016-10-03 03:18:22","modifiedFileCount":"570","status":"M","submitter":"Karl Wright"},{"authorTime":"2017-07-28 23:07:44","codes":[{"authorDate":"2013-04-22 22:26:55","commitOrder":8,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2013-04-22 22:26:55","endLine":335,"groupId":"43613","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/43/18bcc8d45affbe06d426faeabf81eb4455e231.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":310,"status":"N"},{"authorDate":"2017-07-28 23:07:44","commitOrder":8,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n    document.addField(\"number_l_p\", 88L);\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n    String name;\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n    queryResult = idResult.get(\"query\");\n    assertEquals(\"Only the default analyzer should be applied\", 1, queryResult.size());\n    name = queryResult.getName(0);\n    assertTrue(\"Only the default analyzer should be applied\", name.matches(\"org.apache.solr.schema.FieldType\\\\$DefaultAnalyzer.*\"));\n    tokenList = (List<NamedList>) queryResult.getVal(0);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = idResult.get(\"index\");\n    assertEquals(\"The id field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"1\");\n    assertEquals(\"Only the default analyzer should be applied\", 1, valueResult.size());\n    name = queryResult.getName(0);\n    assertTrue(\"Only the default analyzer should be applied\", name.matches(\"org.apache.solr.schema.FieldType\\\\$DefaultAnalyzer.*\"));\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"The 'id' field value has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"1\", null, \"word\", 0, 1, 1, new int[]{1}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> number_l_p_Result = documentResult.get(\"number_l_p\");\n    assertNotNull(\"an analysis for the 'number_l_p' field should be returned\", number_l_p_Result);\n    indexResult = number_l_p_Result.get(\"index\");\n    assertEquals(\"The number_l_p field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"88\");\n    assertEquals(\"Only the default analyzer should be applied\", 1, valueResult.size());\n    name = queryResult.getName(0);\n    assertTrue(\"Only the default analyzer should be applied\", name.matches(\"org.apache.solr.schema.FieldType\\\\$DefaultAnalyzer.*\"));\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"The 'number_l_p' field value has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"88\", null, \"word\", 0, 2, 1, new int[]{1}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting the 'MockTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3,3,3}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6,6,6}, null, false));\n  }\n","date":"2017-07-28 23:07:44","endLine":348,"groupId":"18506","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/7f/195263054a31e1b1c67fc1524893e20397c413.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  \n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting the 'MockTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3,3,3}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6,6,6}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/DocumentAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":207,"status":"M"}],"commitId":"51b68404883f9cff4a130ebc378adb04dbd73a3e","commitMessage":"@@@SOLR-11155: /analysis/field and /analysis/document requests should support points fields.\n","date":"2017-07-28 23:07:44","modifiedFileCount":"5","status":"M","submitter":"Steve Rowe"},{"authorTime":"2018-06-15 15:45:55","codes":[{"authorDate":"2013-04-22 22:26:55","commitOrder":9,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2013-04-22 22:26:55","endLine":335,"groupId":"43613","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/43/18bcc8d45affbe06d426faeabf81eb4455e231.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":310,"status":"N"},{"authorDate":"2018-06-15 15:45:55","commitOrder":9,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n    document.addField(\"number_l_p\", 88L);\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n    String name;\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n    queryResult = idResult.get(\"query\");\n    assertEquals(\"Only the default analyzer should be applied\", 1, queryResult.size());\n    name = queryResult.getName(0);\n    assertTrue(\"Only the default analyzer should be applied\", name.matches(\"org.apache.solr.schema.FieldType\\\\$DefaultAnalyzer.*\"));\n    tokenList = (List<NamedList>) queryResult.getVal(0);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = idResult.get(\"index\");\n    assertEquals(\"The id field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"1\");\n    assertEquals(\"Only the default analyzer should be applied\", 1, valueResult.size());\n    name = queryResult.getName(0);\n    assertTrue(\"Only the default analyzer should be applied\", name.matches(\"org.apache.solr.schema.FieldType\\\\$DefaultAnalyzer.*\"));\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"The 'id' field value has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"1\", null, \"word\", 0, 1, 1, new int[]{1}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> number_l_p_Result = documentResult.get(\"number_l_p\");\n    assertNotNull(\"an analysis for the 'number_l_p' field should be returned\", number_l_p_Result);\n    indexResult = number_l_p_Result.get(\"index\");\n    assertEquals(\"The number_l_p field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"88\");\n    assertEquals(\"Only the default analyzer should be applied\", 1, valueResult.size());\n    name = queryResult.getName(0);\n    assertTrue(\"Only the default analyzer should be applied\", name.matches(\"org.apache.solr.schema.FieldType\\\\$DefaultAnalyzer.*\"));\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"The 'number_l_p' field value has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"88\", null, \"word\", 0, 2, 1, new int[]{1}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting the 'MockTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3,3}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6,6}, null, false));\n  }\n","date":"2018-06-27 19:05:23","endLine":335,"groupId":"3979","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/34/fb186c802b69f7840651a3d7569c4200b4cae9.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n    document.addField(\"number_l_p\", 88L);\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n    String name;\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n    queryResult = idResult.get(\"query\");\n    assertEquals(\"Only the default analyzer should be applied\", 1, queryResult.size());\n    name = queryResult.getName(0);\n    assertTrue(\"Only the default analyzer should be applied\", name.matches(\"org.apache.solr.schema.FieldType\\\\$DefaultAnalyzer.*\"));\n    tokenList = (List<NamedList>) queryResult.getVal(0);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = idResult.get(\"index\");\n    assertEquals(\"The id field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"1\");\n    assertEquals(\"Only the default analyzer should be applied\", 1, valueResult.size());\n    name = queryResult.getName(0);\n    assertTrue(\"Only the default analyzer should be applied\", name.matches(\"org.apache.solr.schema.FieldType\\\\$DefaultAnalyzer.*\"));\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"The 'id' field value has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"1\", null, \"word\", 0, 1, 1, new int[]{1}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> number_l_p_Result = documentResult.get(\"number_l_p\");\n    assertNotNull(\"an analysis for the 'number_l_p' field should be returned\", number_l_p_Result);\n    indexResult = number_l_p_Result.get(\"index\");\n    assertEquals(\"The number_l_p field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"88\");\n    assertEquals(\"Only the default analyzer should be applied\", 1, valueResult.size());\n    name = queryResult.getName(0);\n    assertTrue(\"Only the default analyzer should be applied\", name.matches(\"org.apache.solr.schema.FieldType\\\\$DefaultAnalyzer.*\"));\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"The 'number_l_p' field value has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"88\", null, \"word\", 0, 2, 1, new int[]{1}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting the 'MockTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expecting the 'StandardFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3,3,3}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6,6,6}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/DocumentAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":207,"status":"M"}],"commitId":"0fbe3d257e173b78bd7f9681967351613a7254ab","commitMessage":"@@@LUCENE-8356: Remove StandardFilter from Solr schemas\n","date":"2018-06-27 19:05:23","modifiedFileCount":"4","status":"M","submitter":"Alan Woodward"},{"authorTime":"2018-06-15 15:45:55","codes":[{"authorDate":"2020-06-18 05:51:41","commitOrder":10,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n\n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2020-06-18 05:51:41","endLine":392,"groupId":"103459","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/4d/905e2c5b68b6e91834b09c7e840a4749b85f43.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n\n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":362,"status":"M"},{"authorDate":"2018-06-15 15:45:55","commitOrder":10,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n    document.addField(\"number_l_p\", 88L);\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n    String name;\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n    queryResult = idResult.get(\"query\");\n    assertEquals(\"Only the default analyzer should be applied\", 1, queryResult.size());\n    name = queryResult.getName(0);\n    assertTrue(\"Only the default analyzer should be applied\", name.matches(\"org.apache.solr.schema.FieldType\\\\$DefaultAnalyzer.*\"));\n    tokenList = (List<NamedList>) queryResult.getVal(0);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = idResult.get(\"index\");\n    assertEquals(\"The id field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"1\");\n    assertEquals(\"Only the default analyzer should be applied\", 1, valueResult.size());\n    name = queryResult.getName(0);\n    assertTrue(\"Only the default analyzer should be applied\", name.matches(\"org.apache.solr.schema.FieldType\\\\$DefaultAnalyzer.*\"));\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"The 'id' field value has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"1\", null, \"word\", 0, 1, 1, new int[]{1}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> number_l_p_Result = documentResult.get(\"number_l_p\");\n    assertNotNull(\"an analysis for the 'number_l_p' field should be returned\", number_l_p_Result);\n    indexResult = number_l_p_Result.get(\"index\");\n    assertEquals(\"The number_l_p field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"88\");\n    assertEquals(\"Only the default analyzer should be applied\", 1, valueResult.size());\n    name = queryResult.getName(0);\n    assertTrue(\"Only the default analyzer should be applied\", name.matches(\"org.apache.solr.schema.FieldType\\\\$DefaultAnalyzer.*\"));\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"The 'number_l_p' field value has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"88\", null, \"word\", 0, 2, 1, new int[]{1}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting the 'MockTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3,3}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6,6}, null, false));\n  }\n","date":"2018-06-27 19:05:23","endLine":335,"groupId":"103459","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/34/fb186c802b69f7840651a3d7569c4200b4cae9.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    SolrInputDocument document = new SolrInputDocument();\n    document.addField(\"id\", 1);\n    document.addField(\"whitetok\", \"Jumping Jack\");\n    document.addField(\"text\", \"The Fox Jumped Over The Dogs\");\n    document.addField(\"number_l_p\", 88L);\n\n    DocumentAnalysisRequest request = new DocumentAnalysisRequest()\n            .setQuery(\"JUMPING\")\n            .setShowMatch(true)\n            .addDocument(document);\n\n    NamedList<Object> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertNotNull(\"result is null and it shouldn't be\", result);\n    NamedList<NamedList<NamedList<Object>>> documentResult = (NamedList<NamedList<NamedList<Object>>>) result.get(\"1\");\n    assertNotNull(\"An analysis for document with key '1' should be returned\", documentResult);\n\n    NamedList<Object> queryResult;\n    List<NamedList> tokenList;\n    NamedList<Object> indexResult;\n    NamedList<List<NamedList>> valueResult;\n    String name;\n\n    \r\n    NamedList<NamedList<Object>> idResult = documentResult.get(\"id\");\n    assertNotNull(\"an analysis for the 'id' field should be returned\", idResult);\n    queryResult = idResult.get(\"query\");\n    assertEquals(\"Only the default analyzer should be applied\", 1, queryResult.size());\n    name = queryResult.getName(0);\n    assertTrue(\"Only the default analyzer should be applied\", name.matches(\"org.apache.solr.schema.FieldType\\\\$DefaultAnalyzer.*\"));\n    tokenList = (List<NamedList>) queryResult.getVal(0);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = idResult.get(\"index\");\n    assertEquals(\"The id field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"1\");\n    assertEquals(\"Only the default analyzer should be applied\", 1, valueResult.size());\n    name = queryResult.getName(0);\n    assertTrue(\"Only the default analyzer should be applied\", name.matches(\"org.apache.solr.schema.FieldType\\\\$DefaultAnalyzer.*\"));\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"The 'id' field value has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"1\", null, \"word\", 0, 1, 1, new int[]{1}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> number_l_p_Result = documentResult.get(\"number_l_p\");\n    assertNotNull(\"an analysis for the 'number_l_p' field should be returned\", number_l_p_Result);\n    indexResult = number_l_p_Result.get(\"index\");\n    assertEquals(\"The number_l_p field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"88\");\n    assertEquals(\"Only the default analyzer should be applied\", 1, valueResult.size());\n    name = queryResult.getName(0);\n    assertTrue(\"Only the default analyzer should be applied\", name.matches(\"org.apache.solr.schema.FieldType\\\\$DefaultAnalyzer.*\"));\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"The 'number_l_p' field value has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"88\", null, \"word\", 0, 2, 1, new int[]{1}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> whitetokResult = documentResult.get(\"whitetok\");\n    assertNotNull(\"an analysis for the 'whitetok' field should be returned\", whitetokResult);\n    queryResult = whitetokResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting the 'MockTokenizer' to be applied on the query for the 'whitetok' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    indexResult = whitetokResult.get(\"index\");\n    assertEquals(\"The 'whitetok' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"Jumping Jack\");\n    tokenList = valueResult.getVal(0);\n    assertEquals(\"Expecting 2 tokens to be present\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"Jumping\", null, \"word\", 0, 7, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Jack\", null, \"word\", 8, 12, 2, new int[]{2}, null, false));\n\n    \r\n    NamedList<NamedList<Object>> textResult = documentResult.get(\"text\");\n    assertNotNull(\"an analysis for the 'text' field should be returned\", textResult);\n    queryResult = textResult.get(\"query\");\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"JUMPING\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jumping\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1}, null, false));\n    tokenList = (List<NamedList>) queryResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the query for the 'text' field\", tokenList);\n    assertEquals(\"Query has only one token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 0, 7, 1, new int[]{1,1,1,1}, null, false));\n    indexResult = textResult.get(\"index\");\n    assertEquals(\"The 'text' field has only a single value\", 1, indexResult.size());\n    valueResult = (NamedList<List<NamedList>>) indexResult.get(\"The Fox Jumped Over The Dogs\");\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting the 'StandardTokenizer' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"Fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"Jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"Over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"The\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"Dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expecting the 'LowerCaseFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 6 tokens\", 6, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 20, 23, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expecting the 'StopFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens after stop word removal\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6}, null, false));\n    tokenList = valueResult.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expecting the 'PorterStemFilter' to be applied on the index for the 'text' field\", tokenList);\n    assertEquals(\"Expecting 4 tokens\", 4, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 4, 7, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 8, 14, 3, new int[]{3,3,3,3}, null, true));\n    assertToken(tokenList.get(2), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 15, 19, 4, new int[]{4,4,4,4}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 24, 28, 6, new int[]{6,6,6,6}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/DocumentAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":207,"status":"N"}],"commitId":"b01e249c9ec724b6df120a5d731020cfe4de3fce","commitMessage":"@@@SOLR-14574: Fix or suppress warnings in solr/core/src/test (part 1)\n","date":"2020-06-18 05:51:41","modifiedFileCount":"213","status":"M","submitter":"Erick Erickson"}]
