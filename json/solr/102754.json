[{"authorTime":"2013-06-27 04:18:33","codes":[{"authorDate":"2013-06-27 04:18:33","commitOrder":1,"curCode":"  private void assertInputsEquals(String name, Directory fsDir, HdfsDirectory hdfs) throws IOException {\n    int reads = random.nextInt(MAX_NUMBER_OF_READS);\n    IndexInput fsInput = fsDir.openInput(name,new IOContext());\n    IndexInput hdfsInput = hdfs.openInput(name,new IOContext());\n    assertEquals(fsInput.length(), hdfsInput.length());\n    int fileLength = (int) fsInput.length();\n    for (int i = 0; i < reads; i++) {\n      int nextInt = Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE,fileLength);\n      byte[] fsBuf = new byte[random.nextInt(nextInt > 0 ? nextInt : 1) + MIN_BUFFER_SIZE];\n      byte[] hdfsBuf = new byte[fsBuf.length];\n      int offset = random.nextInt(fsBuf.length);\n      \n      nextInt = fsBuf.length - offset;\n      int length = random.nextInt(nextInt > 0 ? nextInt : 1);\n      nextInt = fileLength - length;\n      int pos = random.nextInt(nextInt > 0 ? nextInt : 1);\n      fsInput.seek(pos);\n      fsInput.readBytes(fsBuf, offset, length);\n      hdfsInput.seek(pos);\n      hdfsInput.readBytes(hdfsBuf, offset, length);\n      for (int f = offset; f < length; f++) {\n        if (fsBuf[f] != hdfsBuf[f]) {\n          fail();\n        }\n      }\n    }\n    fsInput.close();\n    hdfsInput.close();\n  }\n","date":"2013-06-27 04:18:33","endLine":197,"groupId":"46118","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"assertInputsEquals","params":"(Stringname@DirectoryfsDir@HdfsDirectoryhdfs)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/23/491121d547df544be288604169b891f945cc0d.src","preCode":"  private void assertInputsEquals(String name, Directory fsDir, HdfsDirectory hdfs) throws IOException {\n    int reads = random.nextInt(MAX_NUMBER_OF_READS);\n    IndexInput fsInput = fsDir.openInput(name,new IOContext());\n    IndexInput hdfsInput = hdfs.openInput(name,new IOContext());\n    assertEquals(fsInput.length(), hdfsInput.length());\n    int fileLength = (int) fsInput.length();\n    for (int i = 0; i < reads; i++) {\n      int nextInt = Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE,fileLength);\n      byte[] fsBuf = new byte[random.nextInt(nextInt > 0 ? nextInt : 1) + MIN_BUFFER_SIZE];\n      byte[] hdfsBuf = new byte[fsBuf.length];\n      int offset = random.nextInt(fsBuf.length);\n      \n      nextInt = fsBuf.length - offset;\n      int length = random.nextInt(nextInt > 0 ? nextInt : 1);\n      nextInt = fileLength - length;\n      int pos = random.nextInt(nextInt > 0 ? nextInt : 1);\n      fsInput.seek(pos);\n      fsInput.readBytes(fsBuf, offset, length);\n      hdfsInput.seek(pos);\n      hdfsInput.readBytes(hdfsBuf, offset, length);\n      for (int f = offset; f < length; f++) {\n        if (fsBuf[f] != hdfsBuf[f]) {\n          fail();\n        }\n      }\n    }\n    fsInput.close();\n    hdfsInput.close();\n  }\n","realPath":"solr/core/src/test/org/apache/solr/store/hdfs/HdfsDirectoryTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":169,"status":"B"},{"authorDate":"2013-06-27 04:18:33","commitOrder":1,"curCode":"  private void assertInputsEquals(String name, Directory fsDir, Directory hdfs) throws IOException {\n    int reads = random.nextInt(MAX_NUMBER_OF_READS);\n    IndexInput fsInput = fsDir.openInput(name, new IOContext());\n    IndexInput hdfsInput = hdfs.openInput(name, new IOContext());\n    assertEquals(fsInput.length(), hdfsInput.length());\n    int fileLength = (int) fsInput.length();\n    for (int i = 0; i < reads; i++) {\n      byte[] fsBuf = new byte[random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE, fileLength)) + MIN_BUFFER_SIZE];\n      byte[] hdfsBuf = new byte[fsBuf.length];\n      int offset = random.nextInt(fsBuf.length);\n      int length = random.nextInt(fsBuf.length - offset);\n      int pos = random.nextInt(fileLength - length);\n      fsInput.seek(pos);\n      fsInput.readBytes(fsBuf, offset, length);\n      hdfsInput.seek(pos);\n      hdfsInput.readBytes(hdfsBuf, offset, length);\n      for (int f = offset; f < length; f++) {\n        if (fsBuf[f] != hdfsBuf[f]) {\n          fail(\"read [\" + i + \"]\");\n        }\n      }\n    }\n    fsInput.close();\n    hdfsInput.close();\n  }\n","date":"2013-06-27 04:18:33","endLine":193,"groupId":"18510","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"assertInputsEquals","params":"(Stringname@DirectoryfsDir@Directoryhdfs)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/b8/29dcf549f4ef81f823643118c167935f6138b3.src","preCode":"  private void assertInputsEquals(String name, Directory fsDir, Directory hdfs) throws IOException {\n    int reads = random.nextInt(MAX_NUMBER_OF_READS);\n    IndexInput fsInput = fsDir.openInput(name, new IOContext());\n    IndexInput hdfsInput = hdfs.openInput(name, new IOContext());\n    assertEquals(fsInput.length(), hdfsInput.length());\n    int fileLength = (int) fsInput.length();\n    for (int i = 0; i < reads; i++) {\n      byte[] fsBuf = new byte[random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE, fileLength)) + MIN_BUFFER_SIZE];\n      byte[] hdfsBuf = new byte[fsBuf.length];\n      int offset = random.nextInt(fsBuf.length);\n      int length = random.nextInt(fsBuf.length - offset);\n      int pos = random.nextInt(fileLength - length);\n      fsInput.seek(pos);\n      fsInput.readBytes(fsBuf, offset, length);\n      hdfsInput.seek(pos);\n      hdfsInput.readBytes(hdfsBuf, offset, length);\n      for (int f = offset; f < length; f++) {\n        if (fsBuf[f] != hdfsBuf[f]) {\n          fail(\"read [\" + i + \"]\");\n        }\n      }\n    }\n    fsInput.close();\n    hdfsInput.close();\n  }\n","realPath":"solr/core/src/test/org/apache/solr/store/blockcache/BlockDirectoryTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":169,"status":"B"}],"commitId":"b9e1537a7e12e6c15622452e48d8ca8c23aa98c4","commitMessage":"@@@SOLR-4916: Add support to write and read Solr index files and transaction log files to and from HDFS.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1497072 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2013-06-27 04:18:33","modifiedFileCount":"42","status":"B","submitter":"Mark Robert Miller"},{"authorTime":"2013-07-10 23:05:52","codes":[{"authorDate":"2013-06-27 04:18:33","commitOrder":2,"curCode":"  private void assertInputsEquals(String name, Directory fsDir, HdfsDirectory hdfs) throws IOException {\n    int reads = random.nextInt(MAX_NUMBER_OF_READS);\n    IndexInput fsInput = fsDir.openInput(name,new IOContext());\n    IndexInput hdfsInput = hdfs.openInput(name,new IOContext());\n    assertEquals(fsInput.length(), hdfsInput.length());\n    int fileLength = (int) fsInput.length();\n    for (int i = 0; i < reads; i++) {\n      int nextInt = Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE,fileLength);\n      byte[] fsBuf = new byte[random.nextInt(nextInt > 0 ? nextInt : 1) + MIN_BUFFER_SIZE];\n      byte[] hdfsBuf = new byte[fsBuf.length];\n      int offset = random.nextInt(fsBuf.length);\n      \n      nextInt = fsBuf.length - offset;\n      int length = random.nextInt(nextInt > 0 ? nextInt : 1);\n      nextInt = fileLength - length;\n      int pos = random.nextInt(nextInt > 0 ? nextInt : 1);\n      fsInput.seek(pos);\n      fsInput.readBytes(fsBuf, offset, length);\n      hdfsInput.seek(pos);\n      hdfsInput.readBytes(hdfsBuf, offset, length);\n      for (int f = offset; f < length; f++) {\n        if (fsBuf[f] != hdfsBuf[f]) {\n          fail();\n        }\n      }\n    }\n    fsInput.close();\n    hdfsInput.close();\n  }\n","date":"2013-06-27 04:18:33","endLine":197,"groupId":"102754","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"assertInputsEquals","params":"(Stringname@DirectoryfsDir@HdfsDirectoryhdfs)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/23/491121d547df544be288604169b891f945cc0d.src","preCode":"  private void assertInputsEquals(String name, Directory fsDir, HdfsDirectory hdfs) throws IOException {\n    int reads = random.nextInt(MAX_NUMBER_OF_READS);\n    IndexInput fsInput = fsDir.openInput(name,new IOContext());\n    IndexInput hdfsInput = hdfs.openInput(name,new IOContext());\n    assertEquals(fsInput.length(), hdfsInput.length());\n    int fileLength = (int) fsInput.length();\n    for (int i = 0; i < reads; i++) {\n      int nextInt = Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE,fileLength);\n      byte[] fsBuf = new byte[random.nextInt(nextInt > 0 ? nextInt : 1) + MIN_BUFFER_SIZE];\n      byte[] hdfsBuf = new byte[fsBuf.length];\n      int offset = random.nextInt(fsBuf.length);\n      \n      nextInt = fsBuf.length - offset;\n      int length = random.nextInt(nextInt > 0 ? nextInt : 1);\n      nextInt = fileLength - length;\n      int pos = random.nextInt(nextInt > 0 ? nextInt : 1);\n      fsInput.seek(pos);\n      fsInput.readBytes(fsBuf, offset, length);\n      hdfsInput.seek(pos);\n      hdfsInput.readBytes(hdfsBuf, offset, length);\n      for (int f = offset; f < length; f++) {\n        if (fsBuf[f] != hdfsBuf[f]) {\n          fail();\n        }\n      }\n    }\n    fsInput.close();\n    hdfsInput.close();\n  }\n","realPath":"solr/core/src/test/org/apache/solr/store/hdfs/HdfsDirectoryTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":169,"status":"N"},{"authorDate":"2013-07-10 23:05:52","commitOrder":2,"curCode":"  private void assertInputsEquals(String name, Directory fsDir, Directory hdfs) throws IOException {\n    int reads = random.nextInt(MAX_NUMBER_OF_READS);\n    IndexInput fsInput = fsDir.openInput(name, new IOContext());\n    IndexInput hdfsInput = hdfs.openInput(name, new IOContext());\n    assertEquals(fsInput.length(), hdfsInput.length());\n    int fileLength = (int) fsInput.length();\n    for (int i = 0; i < reads; i++) {\n      int rnd;\n      if (fileLength == 0) {\n        rnd = 0;\n      } else {\n        rnd = random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE, fileLength));\n      }\n\n      byte[] fsBuf = new byte[rnd + MIN_BUFFER_SIZE];\n      byte[] hdfsBuf = new byte[fsBuf.length];\n      int offset = random.nextInt(fsBuf.length);\n      int length = random.nextInt(fsBuf.length - offset);\n      \n      int pos;\n      if (fileLength == 0) {\n        pos = 0;\n      } else {\n        pos = random.nextInt(fileLength - length);\n      }\n    \n      fsInput.seek(pos);\n      fsInput.readBytes(fsBuf, offset, length);\n      hdfsInput.seek(pos);\n      hdfsInput.readBytes(hdfsBuf, offset, length);\n      for (int f = offset; f < length; f++) {\n        if (fsBuf[f] != hdfsBuf[f]) {\n          fail(\"read [\" + i + \"]\");\n        }\n      }\n    }\n    fsInput.close();\n    hdfsInput.close();\n  }\n","date":"2013-07-10 23:05:52","endLine":213,"groupId":"102754","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"assertInputsEquals","params":"(Stringname@DirectoryfsDir@Directoryhdfs)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/9d/edebd1e4f0f518d9443f22ef97c76bf5afac4d.src","preCode":"  private void assertInputsEquals(String name, Directory fsDir, Directory hdfs) throws IOException {\n    int reads = random.nextInt(MAX_NUMBER_OF_READS);\n    IndexInput fsInput = fsDir.openInput(name, new IOContext());\n    IndexInput hdfsInput = hdfs.openInput(name, new IOContext());\n    assertEquals(fsInput.length(), hdfsInput.length());\n    int fileLength = (int) fsInput.length();\n    for (int i = 0; i < reads; i++) {\n      byte[] fsBuf = new byte[random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE, fileLength)) + MIN_BUFFER_SIZE];\n      byte[] hdfsBuf = new byte[fsBuf.length];\n      int offset = random.nextInt(fsBuf.length);\n      int length = random.nextInt(fsBuf.length - offset);\n      int pos = random.nextInt(fileLength - length);\n      fsInput.seek(pos);\n      fsInput.readBytes(fsBuf, offset, length);\n      hdfsInput.seek(pos);\n      hdfsInput.readBytes(hdfsBuf, offset, length);\n      for (int f = offset; f < length; f++) {\n        if (fsBuf[f] != hdfsBuf[f]) {\n          fail(\"read [\" + i + \"]\");\n        }\n      }\n    }\n    fsInput.close();\n    hdfsInput.close();\n  }\n","realPath":"solr/core/src/test/org/apache/solr/store/blockcache/BlockDirectoryTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":175,"status":"M"}],"commitId":"2310ab22f0abf7732d87480d6f9539082d4e0a9f","commitMessage":"@@@SOLR-5026: org.apache.solr.store.blockcache.BlockDirectoryTest.testRandomAccessWrites would pass a 0 to random#next on 0 file size\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1501781 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2013-07-10 23:05:52","modifiedFileCount":"1","status":"M","submitter":"Mark Robert Miller"}]
