[{"authorTime":"2013-06-27 04:18:33","codes":[{"authorDate":"2013-06-27 04:18:33","commitOrder":1,"curCode":"  private void createFile(String name, Directory fsDir, HdfsDirectory hdfs) throws IOException {\n    int writes = random.nextInt(MAX_NUMBER_OF_WRITES);\n    int fileLength = random.nextInt(MAX_FILE_SIZE - MIN_FILE_SIZE) + MIN_FILE_SIZE;\n    IndexOutput fsOutput = fsDir.createOutput(name, new IOContext());\n    fsOutput.setLength(fileLength);\n    IndexOutput hdfsOutput = hdfs.createOutput(name, new IOContext());\n    hdfsOutput.setLength(fileLength);\n    for (int i = 0; i < writes; i++) {\n      byte[] buf = new byte[random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE,fileLength)) + MIN_BUFFER_SIZE];\n      random.nextBytes(buf);\n      int offset = random.nextInt(buf.length);\n      int length = random.nextInt(buf.length - offset);\n      fsOutput.writeBytes(buf, offset, length);\n      hdfsOutput.writeBytes(buf, offset, length);\n    }\n    fsOutput.close();\n    hdfsOutput.close();\n  }\n","date":"2013-06-27 04:18:33","endLine":216,"groupId":"3877","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"createFile","params":"(Stringname@DirectoryfsDir@HdfsDirectoryhdfs)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/23/491121d547df544be288604169b891f945cc0d.src","preCode":"  private void createFile(String name, Directory fsDir, HdfsDirectory hdfs) throws IOException {\n    int writes = random.nextInt(MAX_NUMBER_OF_WRITES);\n    int fileLength = random.nextInt(MAX_FILE_SIZE - MIN_FILE_SIZE) + MIN_FILE_SIZE;\n    IndexOutput fsOutput = fsDir.createOutput(name, new IOContext());\n    fsOutput.setLength(fileLength);\n    IndexOutput hdfsOutput = hdfs.createOutput(name, new IOContext());\n    hdfsOutput.setLength(fileLength);\n    for (int i = 0; i < writes; i++) {\n      byte[] buf = new byte[random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE,fileLength)) + MIN_BUFFER_SIZE];\n      random.nextBytes(buf);\n      int offset = random.nextInt(buf.length);\n      int length = random.nextInt(buf.length - offset);\n      fsOutput.writeBytes(buf, offset, length);\n      hdfsOutput.writeBytes(buf, offset, length);\n    }\n    fsOutput.close();\n    hdfsOutput.close();\n  }\n","realPath":"solr/core/src/test/org/apache/solr/store/hdfs/HdfsDirectoryTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":199,"status":"B"},{"authorDate":"2013-06-27 04:18:33","commitOrder":1,"curCode":"  private void createFile(String name, Directory fsDir, Directory hdfs) throws IOException {\n    int writes = random.nextInt(MAX_NUMBER_OF_WRITES);\n    int fileLength = random.nextInt(MAX_FILE_SIZE - MIN_FILE_SIZE) + MIN_FILE_SIZE;\n    IndexOutput fsOutput = fsDir.createOutput(name, IOContext.DEFAULT);\n    IndexOutput hdfsOutput = hdfs.createOutput(name, IOContext.DEFAULT);\n    for (int i = 0; i < writes; i++) {\n      byte[] buf = new byte[random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE, fileLength)) + MIN_BUFFER_SIZE];\n      random.nextBytes(buf);\n      int offset = random.nextInt(buf.length);\n      int length = random.nextInt(buf.length - offset);\n      fsOutput.writeBytes(buf, offset, length);\n      hdfsOutput.writeBytes(buf, offset, length);\n    }\n    fsOutput.close();\n    hdfsOutput.close();\n  }\n","date":"2013-06-27 04:18:33","endLine":210,"groupId":"18511","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"createFile","params":"(Stringname@DirectoryfsDir@Directoryhdfs)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/b8/29dcf549f4ef81f823643118c167935f6138b3.src","preCode":"  private void createFile(String name, Directory fsDir, Directory hdfs) throws IOException {\n    int writes = random.nextInt(MAX_NUMBER_OF_WRITES);\n    int fileLength = random.nextInt(MAX_FILE_SIZE - MIN_FILE_SIZE) + MIN_FILE_SIZE;\n    IndexOutput fsOutput = fsDir.createOutput(name, IOContext.DEFAULT);\n    IndexOutput hdfsOutput = hdfs.createOutput(name, IOContext.DEFAULT);\n    for (int i = 0; i < writes; i++) {\n      byte[] buf = new byte[random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE, fileLength)) + MIN_BUFFER_SIZE];\n      random.nextBytes(buf);\n      int offset = random.nextInt(buf.length);\n      int length = random.nextInt(buf.length - offset);\n      fsOutput.writeBytes(buf, offset, length);\n      hdfsOutput.writeBytes(buf, offset, length);\n    }\n    fsOutput.close();\n    hdfsOutput.close();\n  }\n","realPath":"solr/core/src/test/org/apache/solr/store/blockcache/BlockDirectoryTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":195,"status":"B"}],"commitId":"b9e1537a7e12e6c15622452e48d8ca8c23aa98c4","commitMessage":"@@@SOLR-4916: Add support to write and read Solr index files and transaction log files to and from HDFS.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1497072 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2013-06-27 04:18:33","modifiedFileCount":"42","status":"B","submitter":"Mark Robert Miller"},{"authorTime":"2013-06-27 04:18:33","codes":[{"authorDate":"2014-04-12 02:46:43","commitOrder":2,"curCode":"  private void createFile(String name, Directory fsDir, HdfsDirectory hdfs) throws IOException {\n    int writes = random.nextInt(MAX_NUMBER_OF_WRITES);\n    int fileLength = random.nextInt(MAX_FILE_SIZE - MIN_FILE_SIZE) + MIN_FILE_SIZE;\n    IndexOutput fsOutput = fsDir.createOutput(name, new IOContext());\n    IndexOutput hdfsOutput = hdfs.createOutput(name, new IOContext());\n    for (int i = 0; i < writes; i++) {\n      byte[] buf = new byte[random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE,fileLength)) + MIN_BUFFER_SIZE];\n      random.nextBytes(buf);\n      int offset = random.nextInt(buf.length);\n      int length = random.nextInt(buf.length - offset);\n      fsOutput.writeBytes(buf, offset, length);\n      hdfsOutput.writeBytes(buf, offset, length);\n    }\n    fsOutput.close();\n    hdfsOutput.close();\n  }\n","date":"2014-04-12 02:46:43","endLine":210,"groupId":"102755","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"createFile","params":"(Stringname@DirectoryfsDir@HdfsDirectoryhdfs)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/e5/5e5b9f75f6e6238dcc6679f48889f550249369.src","preCode":"  private void createFile(String name, Directory fsDir, HdfsDirectory hdfs) throws IOException {\n    int writes = random.nextInt(MAX_NUMBER_OF_WRITES);\n    int fileLength = random.nextInt(MAX_FILE_SIZE - MIN_FILE_SIZE) + MIN_FILE_SIZE;\n    IndexOutput fsOutput = fsDir.createOutput(name, new IOContext());\n    fsOutput.setLength(fileLength);\n    IndexOutput hdfsOutput = hdfs.createOutput(name, new IOContext());\n    hdfsOutput.setLength(fileLength);\n    for (int i = 0; i < writes; i++) {\n      byte[] buf = new byte[random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE,fileLength)) + MIN_BUFFER_SIZE];\n      random.nextBytes(buf);\n      int offset = random.nextInt(buf.length);\n      int length = random.nextInt(buf.length - offset);\n      fsOutput.writeBytes(buf, offset, length);\n      hdfsOutput.writeBytes(buf, offset, length);\n    }\n    fsOutput.close();\n    hdfsOutput.close();\n  }\n","realPath":"solr/core/src/test/org/apache/solr/store/hdfs/HdfsDirectoryTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":195,"status":"M"},{"authorDate":"2013-06-27 04:18:33","commitOrder":2,"curCode":"  private void createFile(String name, Directory fsDir, Directory hdfs) throws IOException {\n    int writes = random.nextInt(MAX_NUMBER_OF_WRITES);\n    int fileLength = random.nextInt(MAX_FILE_SIZE - MIN_FILE_SIZE) + MIN_FILE_SIZE;\n    IndexOutput fsOutput = fsDir.createOutput(name, IOContext.DEFAULT);\n    IndexOutput hdfsOutput = hdfs.createOutput(name, IOContext.DEFAULT);\n    for (int i = 0; i < writes; i++) {\n      byte[] buf = new byte[random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE, fileLength)) + MIN_BUFFER_SIZE];\n      random.nextBytes(buf);\n      int offset = random.nextInt(buf.length);\n      int length = random.nextInt(buf.length - offset);\n      fsOutput.writeBytes(buf, offset, length);\n      hdfsOutput.writeBytes(buf, offset, length);\n    }\n    fsOutput.close();\n    hdfsOutput.close();\n  }\n","date":"2013-06-27 04:18:33","endLine":210,"groupId":"102755","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"createFile","params":"(Stringname@DirectoryfsDir@Directoryhdfs)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/b8/29dcf549f4ef81f823643118c167935f6138b3.src","preCode":"  private void createFile(String name, Directory fsDir, Directory hdfs) throws IOException {\n    int writes = random.nextInt(MAX_NUMBER_OF_WRITES);\n    int fileLength = random.nextInt(MAX_FILE_SIZE - MIN_FILE_SIZE) + MIN_FILE_SIZE;\n    IndexOutput fsOutput = fsDir.createOutput(name, IOContext.DEFAULT);\n    IndexOutput hdfsOutput = hdfs.createOutput(name, IOContext.DEFAULT);\n    for (int i = 0; i < writes; i++) {\n      byte[] buf = new byte[random.nextInt(Math.min(MAX_BUFFER_SIZE - MIN_BUFFER_SIZE, fileLength)) + MIN_BUFFER_SIZE];\n      random.nextBytes(buf);\n      int offset = random.nextInt(buf.length);\n      int length = random.nextInt(buf.length - offset);\n      fsOutput.writeBytes(buf, offset, length);\n      hdfsOutput.writeBytes(buf, offset, length);\n    }\n    fsOutput.close();\n    hdfsOutput.close();\n  }\n","realPath":"solr/core/src/test/org/apache/solr/store/blockcache/BlockDirectoryTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":195,"status":"N"}],"commitId":"bb639e561dec12e5a38f2f1b83bee7b027f8ec14","commitMessage":"@@@LUCENE-5582: remove IndexOutput.length/setLength\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1586743 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2014-04-12 02:46:43","modifiedFileCount":"17","status":"M","submitter":"Michael McCandless"}]
