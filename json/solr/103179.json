[{"authorTime":"2013-06-27 04:18:33","codes":[{"authorDate":"2013-06-27 04:18:33","commitOrder":1,"curCode":"  public void testRemoveOldLogs() throws Exception {\n    try {\n      DirectUpdateHandler2.commitOnClose = false;\n      final Semaphore logReplay = new Semaphore(0);\n      final Semaphore logReplayFinish = new Semaphore(0);\n\n      UpdateLog.testing_logReplayHook = new Runnable() {\n        @Override\n        public void run() {\n          try {\n            assertTrue(logReplay.tryAcquire(timeout, TimeUnit.SECONDS));\n          } catch (Exception e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      UpdateLog.testing_logReplayFinishHook = new Runnable() {\n        @Override\n        public void run() {\n          logReplayFinish.release();\n        }\n      };\n\n\n      clearIndex();\n      assertU(commit());\n\n      String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n \n      \n      Configuration conf = new Configuration();\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs;\n      try {\n        URI uri = new URI(hdfsUri);\n        fs = FileSystem.newInstance(uri, conf);\n      } catch (IOException e) {\n        throw new RuntimeException(e);\n      } catch (URISyntaxException e) {\n        throw new RuntimeException(e);\n      }\n      \n      h.close();\n\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      createCore();\n\n      int start = 0;\n      int maxReq = 50;\n\n      LinkedList<Long> versions = new LinkedList<Long>();\n      addDocs(10, start, versions); start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(10, start, versions);  start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      assertEquals(2, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(1, start, versions);  start+=1;\n      h.close();\n      createCore();      \r\n\n      \r\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      logReplay.release(1000);\n      assertTrue(logReplayFinish.tryAcquire(timeout, TimeUnit.SECONDS));\n\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      \r\n      \r\n      \r\n      addDocs(1, start, new LinkedList<Long>()); \r\n      h.close();\n      files = HdfsUpdateLog.getLogList(fs, new Path(logDir));;\n      Arrays.sort(files);\n\n      FSDataOutputStream dos = fs.create(new Path(new Path(logDir), files[files.length-1]), (short)1);\n      dos.writeUTF(\"This is a trashed log file that really shouldn't work at all, but we'll see..\");\n      dos.close();\n\n      ignoreException(\"Failure to open existing\");\n      createCore();\n      \r\n      assertJQ(req(\"qt\", \"/get\", \"getVersions\", \"\" + maxReq), \"/versions==\" + versions.subList(0, Math.min(maxReq, start)));\n      resetExceptionIgnores();\n\n    } finally {\n      DirectUpdateHandler2.commitOnClose = true;\n      UpdateLog.testing_logReplayHook = null;\n      UpdateLog.testing_logReplayFinishHook = null;\n    }\n  }\n","date":"2013-06-27 04:18:33","endLine":835,"groupId":"16199","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testRemoveOldLogs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/a7/86702222f9ed799ef808f1237c408e8402db44.src","preCode":"  public void testRemoveOldLogs() throws Exception {\n    try {\n      DirectUpdateHandler2.commitOnClose = false;\n      final Semaphore logReplay = new Semaphore(0);\n      final Semaphore logReplayFinish = new Semaphore(0);\n\n      UpdateLog.testing_logReplayHook = new Runnable() {\n        @Override\n        public void run() {\n          try {\n            assertTrue(logReplay.tryAcquire(timeout, TimeUnit.SECONDS));\n          } catch (Exception e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      UpdateLog.testing_logReplayFinishHook = new Runnable() {\n        @Override\n        public void run() {\n          logReplayFinish.release();\n        }\n      };\n\n\n      clearIndex();\n      assertU(commit());\n\n      String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n \n      \n      Configuration conf = new Configuration();\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs;\n      try {\n        URI uri = new URI(hdfsUri);\n        fs = FileSystem.newInstance(uri, conf);\n      } catch (IOException e) {\n        throw new RuntimeException(e);\n      } catch (URISyntaxException e) {\n        throw new RuntimeException(e);\n      }\n      \n      h.close();\n\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      createCore();\n\n      int start = 0;\n      int maxReq = 50;\n\n      LinkedList<Long> versions = new LinkedList<Long>();\n      addDocs(10, start, versions); start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(10, start, versions);  start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      assertEquals(2, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(1, start, versions);  start+=1;\n      h.close();\n      createCore();      \r\n\n      \r\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      logReplay.release(1000);\n      assertTrue(logReplayFinish.tryAcquire(timeout, TimeUnit.SECONDS));\n\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      \r\n      \r\n      \r\n      addDocs(1, start, new LinkedList<Long>()); \r\n      h.close();\n      files = HdfsUpdateLog.getLogList(fs, new Path(logDir));;\n      Arrays.sort(files);\n\n      FSDataOutputStream dos = fs.create(new Path(new Path(logDir), files[files.length-1]), (short)1);\n      dos.writeUTF(\"This is a trashed log file that really shouldn't work at all, but we'll see..\");\n      dos.close();\n\n      ignoreException(\"Failure to open existing\");\n      createCore();\n      \r\n      assertJQ(req(\"qt\", \"/get\", \"getVersions\", \"\" + maxReq), \"/versions==\" + versions.subList(0, Math.min(maxReq, start)));\n      resetExceptionIgnores();\n\n    } finally {\n      DirectUpdateHandler2.commitOnClose = true;\n      UpdateLog.testing_logReplayHook = null;\n      UpdateLog.testing_logReplayFinishHook = null;\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":714,"status":"B"},{"authorDate":"2013-06-27 04:18:33","commitOrder":1,"curCode":"  void deleteLogs() throws Exception {\n    String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n    Configuration conf = new Configuration();\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    FileSystem fs;\n    try {\n      URI uri = new URI(hdfsUri);\n      fs = FileSystem.newInstance(uri, conf);\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    } catch (URISyntaxException e) {\n      throw new RuntimeException(e);\n    }\n    h.close();\n\n    try {\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        \r\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n    } finally {\n      \r\n      \r\n      createCore();\n      assertJQ(req(\"q\",\"*:*\") ,\"/response/numFound==\");   \r\n    }\n  }\n","date":"2013-06-27 04:18:33","endLine":1150,"groupId":"24321","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"deleteLogs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/a7/86702222f9ed799ef808f1237c408e8402db44.src","preCode":"  void deleteLogs() throws Exception {\n    String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n    Configuration conf = new Configuration();\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    FileSystem fs;\n    try {\n      URI uri = new URI(hdfsUri);\n      fs = FileSystem.newInstance(uri, conf);\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    } catch (URISyntaxException e) {\n      throw new RuntimeException(e);\n    }\n    h.close();\n\n    try {\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        \r\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n    } finally {\n      \r\n      \r\n      createCore();\n      assertJQ(req(\"q\",\"*:*\") ,\"/response/numFound==\");   \r\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":1121,"status":"B"}],"commitId":"b9e1537a7e12e6c15622452e48d8ca8c23aa98c4","commitMessage":"@@@SOLR-4916: Add support to write and read Solr index files and transaction log files to and from HDFS.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1497072 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2013-06-27 04:18:33","modifiedFileCount":"42","status":"B","submitter":"Mark Robert Miller"},{"authorTime":"2013-07-05 09:00:50","codes":[{"authorDate":"2013-07-05 09:00:50","commitOrder":2,"curCode":"  public void testRemoveOldLogs() throws Exception {\n    try {\n      DirectUpdateHandler2.commitOnClose = false;\n      final Semaphore logReplay = new Semaphore(0);\n      final Semaphore logReplayFinish = new Semaphore(0);\n\n      UpdateLog.testing_logReplayHook = new Runnable() {\n        @Override\n        public void run() {\n          try {\n            assertTrue(logReplay.tryAcquire(timeout, TimeUnit.SECONDS));\n          } catch (Exception e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      UpdateLog.testing_logReplayFinishHook = new Runnable() {\n        @Override\n        public void run() {\n          logReplayFinish.release();\n        }\n      };\n\n\n      clearIndex();\n      assertU(commit());\n\n      String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n      \n      h.close();\n\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      createCore();\n\n      int start = 0;\n      int maxReq = 50;\n\n      LinkedList<Long> versions = new LinkedList<Long>();\n      addDocs(10, start, versions); start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(10, start, versions);  start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      assertEquals(2, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(1, start, versions);  start+=1;\n      h.close();\n      createCore();      \r\n\n      \r\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      logReplay.release(1000);\n      assertTrue(logReplayFinish.tryAcquire(timeout, TimeUnit.SECONDS));\n\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      \r\n      \r\n      \r\n      addDocs(1, start, new LinkedList<Long>()); \r\n      h.close();\n      files = HdfsUpdateLog.getLogList(fs, new Path(logDir));;\n      Arrays.sort(files);\n\n      FSDataOutputStream dos = fs.create(new Path(new Path(logDir), files[files.length-1]), (short)1);\n      dos.writeUTF(\"This is a trashed log file that really shouldn't work at all, but we'll see..\");\n      dos.close();\n\n      ignoreException(\"Failure to open existing\");\n      createCore();\n      \r\n      assertJQ(req(\"qt\", \"/get\", \"getVersions\", \"\" + maxReq), \"/versions==\" + versions.subList(0, Math.min(maxReq, start)));\n      resetExceptionIgnores();\n\n    } finally {\n      DirectUpdateHandler2.commitOnClose = true;\n      UpdateLog.testing_logReplayHook = null;\n      UpdateLog.testing_logReplayFinishHook = null;\n    }\n  }\n","date":"2013-07-05 09:00:50","endLine":835,"groupId":"16199","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testRemoveOldLogs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/69/a849ef0ab08d971500a7037645b9a07171c1fc.src","preCode":"  public void testRemoveOldLogs() throws Exception {\n    try {\n      DirectUpdateHandler2.commitOnClose = false;\n      final Semaphore logReplay = new Semaphore(0);\n      final Semaphore logReplayFinish = new Semaphore(0);\n\n      UpdateLog.testing_logReplayHook = new Runnable() {\n        @Override\n        public void run() {\n          try {\n            assertTrue(logReplay.tryAcquire(timeout, TimeUnit.SECONDS));\n          } catch (Exception e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      UpdateLog.testing_logReplayFinishHook = new Runnable() {\n        @Override\n        public void run() {\n          logReplayFinish.release();\n        }\n      };\n\n\n      clearIndex();\n      assertU(commit());\n\n      String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n \n      \n      Configuration conf = new Configuration();\n      conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n      FileSystem fs;\n      try {\n        URI uri = new URI(hdfsUri);\n        fs = FileSystem.newInstance(uri, conf);\n      } catch (IOException e) {\n        throw new RuntimeException(e);\n      } catch (URISyntaxException e) {\n        throw new RuntimeException(e);\n      }\n      \n      h.close();\n\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      createCore();\n\n      int start = 0;\n      int maxReq = 50;\n\n      LinkedList<Long> versions = new LinkedList<Long>();\n      addDocs(10, start, versions); start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(10, start, versions);  start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      assertEquals(2, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(1, start, versions);  start+=1;\n      h.close();\n      createCore();      \r\n\n      \r\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      logReplay.release(1000);\n      assertTrue(logReplayFinish.tryAcquire(timeout, TimeUnit.SECONDS));\n\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      \r\n      \r\n      \r\n      addDocs(1, start, new LinkedList<Long>()); \r\n      h.close();\n      files = HdfsUpdateLog.getLogList(fs, new Path(logDir));;\n      Arrays.sort(files);\n\n      FSDataOutputStream dos = fs.create(new Path(new Path(logDir), files[files.length-1]), (short)1);\n      dos.writeUTF(\"This is a trashed log file that really shouldn't work at all, but we'll see..\");\n      dos.close();\n\n      ignoreException(\"Failure to open existing\");\n      createCore();\n      \r\n      assertJQ(req(\"qt\", \"/get\", \"getVersions\", \"\" + maxReq), \"/versions==\" + versions.subList(0, Math.min(maxReq, start)));\n      resetExceptionIgnores();\n\n    } finally {\n      DirectUpdateHandler2.commitOnClose = true;\n      UpdateLog.testing_logReplayHook = null;\n      UpdateLog.testing_logReplayFinishHook = null;\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":727,"status":"M"},{"authorDate":"2013-07-05 09:00:50","commitOrder":2,"curCode":"  void deleteLogs() throws Exception {\n    String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n\n    h.close();\n\n    try {\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        \r\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n    } finally {\n      \r\n      \r\n      createCore();\n      assertJQ(req(\"q\",\"*:*\") ,\"/response/numFound==\");   \r\n    }\n  }\n","date":"2013-07-05 09:00:50","endLine":1107,"groupId":"24321","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"deleteLogs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/69/a849ef0ab08d971500a7037645b9a07171c1fc.src","preCode":"  void deleteLogs() throws Exception {\n    String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n    Configuration conf = new Configuration();\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    FileSystem fs;\n    try {\n      URI uri = new URI(hdfsUri);\n      fs = FileSystem.newInstance(uri, conf);\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    } catch (URISyntaxException e) {\n      throw new RuntimeException(e);\n    }\n    h.close();\n\n    try {\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        \r\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n    } finally {\n      \r\n      \r\n      createCore();\n      assertJQ(req(\"q\",\"*:*\") ,\"/response/numFound==\");   \r\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":1088,"status":"M"}],"commitId":"a69b1b0e3db2d85a52ec10de37ed3facc50fd81f","commitMessage":"@@@SOLR-5007: clean up resource management\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1499894 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2013-07-05 09:00:50","modifiedFileCount":"1","status":"M","submitter":"Mark Robert Miller"},{"authorTime":"2013-07-05 09:00:50","codes":[{"authorDate":"2014-03-12 22:39:17","commitOrder":3,"curCode":"  public void testRemoveOldLogs() throws Exception {\n    try {\n      DirectUpdateHandler2.commitOnClose = false;\n      final Semaphore logReplay = new Semaphore(0);\n      final Semaphore logReplayFinish = new Semaphore(0);\n\n      UpdateLog.testing_logReplayHook = new Runnable() {\n        @Override\n        public void run() {\n          try {\n            assertTrue(logReplay.tryAcquire(timeout, TimeUnit.SECONDS));\n          } catch (Exception e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      UpdateLog.testing_logReplayFinishHook = new Runnable() {\n        @Override\n        public void run() {\n          logReplayFinish.release();\n        }\n      };\n\n\n      clearIndex();\n      assertU(commit());\n\n      String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n      \n      h.close();\n\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      createCore();\n\n      int start = 0;\n      int maxReq = 50;\n\n      LinkedList<Long> versions = new LinkedList<>();\n      addDocs(10, start, versions); start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(10, start, versions);  start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      assertEquals(2, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(1, start, versions);  start+=1;\n      h.close();\n      createCore();      \r\n\n      \r\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      logReplay.release(1000);\n      assertTrue(logReplayFinish.tryAcquire(timeout, TimeUnit.SECONDS));\n\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      \r\n      \r\n      \r\n      addDocs(1, start, new LinkedList<Long>()); \r\n      h.close();\n      files = HdfsUpdateLog.getLogList(fs, new Path(logDir));;\n      Arrays.sort(files);\n\n      FSDataOutputStream dos = fs.create(new Path(new Path(logDir), files[files.length-1]), (short)1);\n      dos.writeUTF(\"This is a trashed log file that really shouldn't work at all, but we'll see..\");\n      dos.close();\n\n      ignoreException(\"Failure to open existing\");\n      createCore();\n      \r\n      assertJQ(req(\"qt\", \"/get\", \"getVersions\", \"\" + maxReq), \"/versions==\" + versions.subList(0, Math.min(maxReq, start)));\n      resetExceptionIgnores();\n\n    } finally {\n      DirectUpdateHandler2.commitOnClose = true;\n      UpdateLog.testing_logReplayHook = null;\n      UpdateLog.testing_logReplayFinishHook = null;\n    }\n  }\n","date":"2014-03-12 22:39:17","endLine":835,"groupId":"16199","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testRemoveOldLogs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/9d/e9d3bf73f9faa9b5a0ac605af2edb00c101d18.src","preCode":"  public void testRemoveOldLogs() throws Exception {\n    try {\n      DirectUpdateHandler2.commitOnClose = false;\n      final Semaphore logReplay = new Semaphore(0);\n      final Semaphore logReplayFinish = new Semaphore(0);\n\n      UpdateLog.testing_logReplayHook = new Runnable() {\n        @Override\n        public void run() {\n          try {\n            assertTrue(logReplay.tryAcquire(timeout, TimeUnit.SECONDS));\n          } catch (Exception e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      UpdateLog.testing_logReplayFinishHook = new Runnable() {\n        @Override\n        public void run() {\n          logReplayFinish.release();\n        }\n      };\n\n\n      clearIndex();\n      assertU(commit());\n\n      String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n      \n      h.close();\n\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      createCore();\n\n      int start = 0;\n      int maxReq = 50;\n\n      LinkedList<Long> versions = new LinkedList<Long>();\n      addDocs(10, start, versions); start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(10, start, versions);  start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      assertEquals(2, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(1, start, versions);  start+=1;\n      h.close();\n      createCore();      \r\n\n      \r\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      logReplay.release(1000);\n      assertTrue(logReplayFinish.tryAcquire(timeout, TimeUnit.SECONDS));\n\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      \r\n      \r\n      \r\n      addDocs(1, start, new LinkedList<Long>()); \r\n      h.close();\n      files = HdfsUpdateLog.getLogList(fs, new Path(logDir));;\n      Arrays.sort(files);\n\n      FSDataOutputStream dos = fs.create(new Path(new Path(logDir), files[files.length-1]), (short)1);\n      dos.writeUTF(\"This is a trashed log file that really shouldn't work at all, but we'll see..\");\n      dos.close();\n\n      ignoreException(\"Failure to open existing\");\n      createCore();\n      \r\n      assertJQ(req(\"qt\", \"/get\", \"getVersions\", \"\" + maxReq), \"/versions==\" + versions.subList(0, Math.min(maxReq, start)));\n      resetExceptionIgnores();\n\n    } finally {\n      DirectUpdateHandler2.commitOnClose = true;\n      UpdateLog.testing_logReplayHook = null;\n      UpdateLog.testing_logReplayFinishHook = null;\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":727,"status":"M"},{"authorDate":"2013-07-05 09:00:50","commitOrder":3,"curCode":"  void deleteLogs() throws Exception {\n    String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n\n    h.close();\n\n    try {\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        \r\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n    } finally {\n      \r\n      \r\n      createCore();\n      assertJQ(req(\"q\",\"*:*\") ,\"/response/numFound==\");   \r\n    }\n  }\n","date":"2013-07-05 09:00:50","endLine":1107,"groupId":"24321","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"deleteLogs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/69/a849ef0ab08d971500a7037645b9a07171c1fc.src","preCode":"  void deleteLogs() throws Exception {\n    String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n\n    h.close();\n\n    try {\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        \r\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n    } finally {\n      \r\n      \r\n      createCore();\n      assertJQ(req(\"q\",\"*:*\") ,\"/response/numFound==\");   \r\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":1088,"status":"N"}],"commitId":"3b67b17493216f6b0c81a981073fd5f61eace6f4","commitMessage":"@@@LUCENE-5512: remove redundant typing (diamond operator) in trunk\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1576755 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2014-03-12 22:39:17","modifiedFileCount":"1241","status":"M","submitter":"Robert Muir"},{"authorTime":"2013-07-05 09:00:50","codes":[{"authorDate":"2016-03-04 21:23:47","commitOrder":4,"curCode":"  public void testRemoveOldLogs() throws Exception {\n    try {\n      DirectUpdateHandler2.commitOnClose = false;\n      final Semaphore logReplay = new Semaphore(0);\n      final Semaphore logReplayFinish = new Semaphore(0);\n\n      UpdateLog.testing_logReplayHook = () -> {\n        try {\n          assertTrue(logReplay.tryAcquire(timeout, TimeUnit.SECONDS));\n        } catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      };\n\n      UpdateLog.testing_logReplayFinishHook = new Runnable() {\n        @Override\n        public void run() {\n          logReplayFinish.release();\n        }\n      };\n\n\n      clearIndex();\n      assertU(commit());\n\n      String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n      \n      h.close();\n\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      createCore();\n\n      int start = 0;\n      int maxReq = 50;\n\n      LinkedList<Long> versions = new LinkedList<>();\n      addDocs(10, start, versions); start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(10, start, versions);  start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      assertEquals(2, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(1, start, versions);  start+=1;\n      h.close();\n      createCore();      \r\n\n      \r\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      logReplay.release(1000);\n      assertTrue(logReplayFinish.tryAcquire(timeout, TimeUnit.SECONDS));\n\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      \r\n      \r\n      \r\n      addDocs(1, start, new LinkedList<Long>()); \r\n      h.close();\n      files = HdfsUpdateLog.getLogList(fs, new Path(logDir));;\n      Arrays.sort(files);\n\n      FSDataOutputStream dos = fs.create(new Path(new Path(logDir), files[files.length-1]), (short)1);\n      dos.writeUTF(\"This is a trashed log file that really shouldn't work at all, but we'll see..\");\n      dos.close();\n\n      ignoreException(\"Failure to open existing\");\n      createCore();\n      \r\n      assertJQ(req(\"qt\", \"/get\", \"getVersions\", \"\" + maxReq), \"/versions==\" + versions.subList(0, Math.min(maxReq, start)));\n      resetExceptionIgnores();\n\n    } finally {\n      DirectUpdateHandler2.commitOnClose = true;\n      UpdateLog.testing_logReplayHook = null;\n      UpdateLog.testing_logReplayFinishHook = null;\n    }\n  }\n","date":"2016-03-04 21:23:47","endLine":816,"groupId":"16199","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testRemoveOldLogs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/d0/d61be5bcff6d6322edc3120b63273fc12281f3.src","preCode":"  public void testRemoveOldLogs() throws Exception {\n    try {\n      DirectUpdateHandler2.commitOnClose = false;\n      final Semaphore logReplay = new Semaphore(0);\n      final Semaphore logReplayFinish = new Semaphore(0);\n\n      UpdateLog.testing_logReplayHook = new Runnable() {\n        @Override\n        public void run() {\n          try {\n            assertTrue(logReplay.tryAcquire(timeout, TimeUnit.SECONDS));\n          } catch (Exception e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n\n      UpdateLog.testing_logReplayFinishHook = new Runnable() {\n        @Override\n        public void run() {\n          logReplayFinish.release();\n        }\n      };\n\n\n      clearIndex();\n      assertU(commit());\n\n      String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n      \n      h.close();\n\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      createCore();\n\n      int start = 0;\n      int maxReq = 50;\n\n      LinkedList<Long> versions = new LinkedList<>();\n      addDocs(10, start, versions); start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(10, start, versions);  start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      assertEquals(2, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(1, start, versions);  start+=1;\n      h.close();\n      createCore();      \r\n\n      \r\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      logReplay.release(1000);\n      assertTrue(logReplayFinish.tryAcquire(timeout, TimeUnit.SECONDS));\n\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      \r\n      \r\n      \r\n      addDocs(1, start, new LinkedList<Long>()); \r\n      h.close();\n      files = HdfsUpdateLog.getLogList(fs, new Path(logDir));;\n      Arrays.sort(files);\n\n      FSDataOutputStream dos = fs.create(new Path(new Path(logDir), files[files.length-1]), (short)1);\n      dos.writeUTF(\"This is a trashed log file that really shouldn't work at all, but we'll see..\");\n      dos.close();\n\n      ignoreException(\"Failure to open existing\");\n      createCore();\n      \r\n      assertJQ(req(\"qt\", \"/get\", \"getVersions\", \"\" + maxReq), \"/versions==\" + versions.subList(0, Math.min(maxReq, start)));\n      resetExceptionIgnores();\n\n    } finally {\n      DirectUpdateHandler2.commitOnClose = true;\n      UpdateLog.testing_logReplayHook = null;\n      UpdateLog.testing_logReplayFinishHook = null;\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":711,"status":"M"},{"authorDate":"2013-07-05 09:00:50","commitOrder":4,"curCode":"  void deleteLogs() throws Exception {\n    String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n\n    h.close();\n\n    try {\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        \r\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n    } finally {\n      \r\n      \r\n      createCore();\n      assertJQ(req(\"q\",\"*:*\") ,\"/response/numFound==\");   \r\n    }\n  }\n","date":"2013-07-05 09:00:50","endLine":1107,"groupId":"24321","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"deleteLogs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/69/a849ef0ab08d971500a7037645b9a07171c1fc.src","preCode":"  void deleteLogs() throws Exception {\n    String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n\n    h.close();\n\n    try {\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        \r\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n    } finally {\n      \r\n      \r\n      createCore();\n      assertJQ(req(\"q\",\"*:*\") ,\"/response/numFound==\");   \r\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":1088,"status":"N"}],"commitId":"c27dd8e03866082686ab3d17f7350f91975a8efc","commitMessage":"@@@SOLR-8750 : Use lambdas in code where SAM type interfaces are used\n","date":"2016-03-04 21:23:47","modifiedFileCount":"45","status":"M","submitter":"Noble Paul"},{"authorTime":"2013-07-05 09:00:50","codes":[{"authorDate":"2016-04-07 05:18:10","commitOrder":5,"curCode":"  public void testRemoveOldLogs() throws Exception {\n    try {\n      DirectUpdateHandler2.commitOnClose = false;\n      final Semaphore logReplay = new Semaphore(0);\n      final Semaphore logReplayFinish = new Semaphore(0);\n\n      UpdateLog.testing_logReplayHook = () -> {\n        try {\n          assertTrue(logReplay.tryAcquire(timeout, TimeUnit.SECONDS));\n        } catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      };\n\n      UpdateLog.testing_logReplayFinishHook = logReplayFinish::release;\n\n\n      clearIndex();\n      assertU(commit());\n\n      String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n      \n      h.close();\n\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      createCore();\n\n      int start = 0;\n      int maxReq = 50;\n\n      LinkedList<Long> versions = new LinkedList<>();\n      addDocs(10, start, versions); start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(10, start, versions);  start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      assertEquals(2, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(1, start, versions);  start+=1;\n      h.close();\n      createCore();      \r\n\n      \r\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      logReplay.release(1000);\n      assertTrue(logReplayFinish.tryAcquire(timeout, TimeUnit.SECONDS));\n\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      \r\n      \r\n      \r\n      addDocs(1, start, new LinkedList<Long>()); \r\n      h.close();\n      files = HdfsUpdateLog.getLogList(fs, new Path(logDir));;\n      Arrays.sort(files);\n\n      FSDataOutputStream dos = fs.create(new Path(new Path(logDir), files[files.length-1]), (short)1);\n      dos.writeUTF(\"This is a trashed log file that really shouldn't work at all, but we'll see..\");\n      dos.close();\n\n      ignoreException(\"Failure to open existing\");\n      createCore();\n      \r\n      assertJQ(req(\"qt\", \"/get\", \"getVersions\", \"\" + maxReq), \"/versions==\" + versions.subList(0, Math.min(maxReq, start)));\n      resetExceptionIgnores();\n\n    } finally {\n      DirectUpdateHandler2.commitOnClose = true;\n      UpdateLog.testing_logReplayHook = null;\n      UpdateLog.testing_logReplayFinishHook = null;\n    }\n  }\n","date":"2016-04-07 05:18:10","endLine":811,"groupId":"16199","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testRemoveOldLogs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/7d/e7a2bcff44c8912249bf7ed097c60c2d288d9a.src","preCode":"  public void testRemoveOldLogs() throws Exception {\n    try {\n      DirectUpdateHandler2.commitOnClose = false;\n      final Semaphore logReplay = new Semaphore(0);\n      final Semaphore logReplayFinish = new Semaphore(0);\n\n      UpdateLog.testing_logReplayHook = () -> {\n        try {\n          assertTrue(logReplay.tryAcquire(timeout, TimeUnit.SECONDS));\n        } catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      };\n\n      UpdateLog.testing_logReplayFinishHook = new Runnable() {\n        @Override\n        public void run() {\n          logReplayFinish.release();\n        }\n      };\n\n\n      clearIndex();\n      assertU(commit());\n\n      String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n      \n      h.close();\n\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      createCore();\n\n      int start = 0;\n      int maxReq = 50;\n\n      LinkedList<Long> versions = new LinkedList<>();\n      addDocs(10, start, versions); start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(10, start, versions);  start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      assertEquals(2, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(1, start, versions);  start+=1;\n      h.close();\n      createCore();      \r\n\n      \r\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      logReplay.release(1000);\n      assertTrue(logReplayFinish.tryAcquire(timeout, TimeUnit.SECONDS));\n\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      \r\n      \r\n      \r\n      addDocs(1, start, new LinkedList<Long>()); \r\n      h.close();\n      files = HdfsUpdateLog.getLogList(fs, new Path(logDir));;\n      Arrays.sort(files);\n\n      FSDataOutputStream dos = fs.create(new Path(new Path(logDir), files[files.length-1]), (short)1);\n      dos.writeUTF(\"This is a trashed log file that really shouldn't work at all, but we'll see..\");\n      dos.close();\n\n      ignoreException(\"Failure to open existing\");\n      createCore();\n      \r\n      assertJQ(req(\"qt\", \"/get\", \"getVersions\", \"\" + maxReq), \"/versions==\" + versions.subList(0, Math.min(maxReq, start)));\n      resetExceptionIgnores();\n\n    } finally {\n      DirectUpdateHandler2.commitOnClose = true;\n      UpdateLog.testing_logReplayHook = null;\n      UpdateLog.testing_logReplayFinishHook = null;\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":711,"status":"M"},{"authorDate":"2013-07-05 09:00:50","commitOrder":5,"curCode":"  void deleteLogs() throws Exception {\n    String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n\n    h.close();\n\n    try {\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        \r\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n    } finally {\n      \r\n      \r\n      createCore();\n      assertJQ(req(\"q\",\"*:*\") ,\"/response/numFound==\");   \r\n    }\n  }\n","date":"2013-07-05 09:00:50","endLine":1107,"groupId":"24321","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"deleteLogs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/69/a849ef0ab08d971500a7037645b9a07171c1fc.src","preCode":"  void deleteLogs() throws Exception {\n    String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n\n    h.close();\n\n    try {\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        \r\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n    } finally {\n      \r\n      \r\n      createCore();\n      assertJQ(req(\"q\",\"*:*\") ,\"/response/numFound==\");   \r\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":1088,"status":"N"}],"commitId":"eda2a319595d01efaf05ed4bc4574682b91e7aaf","commitMessage":"@@@Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/lucene-solr\n","date":"2016-04-07 05:18:10","modifiedFileCount":"41","status":"M","submitter":"Karl Wright"},{"authorTime":"2013-07-05 09:00:50","codes":[{"authorDate":"2019-03-13 01:06:19","commitOrder":6,"curCode":"  public void testRemoveOldLogs() throws Exception {\n    try {\n      DirectUpdateHandler2.commitOnClose = false;\n      final Semaphore logReplay = new Semaphore(0);\n      final Semaphore logReplayFinish = new Semaphore(0);\n\n      UpdateLog.testing_logReplayHook = () -> {\n        try {\n          assertTrue(logReplay.tryAcquire(timeout, TimeUnit.SECONDS));\n        } catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      };\n\n      UpdateLog.testing_logReplayFinishHook = logReplayFinish::release;\n\n\n      clearIndex();\n      assertU(commit());\n\n      String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n      \n      h.close();\n\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      createCore();\n\n      int start = 0;\n      int maxReq = 50;\n\n      LinkedList<Long> versions = new LinkedList<>();\n      addDocs(10, start, versions); start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(10, start, versions);  start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      assertEquals(2, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(1, start, versions);  start+=1;\n      h.close();\n      createCore();      \r\n\n      \r\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      logReplay.release(1000);\n      assertTrue(logReplayFinish.tryAcquire(timeout, TimeUnit.SECONDS));\n\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      \r\n      \r\n      \r\n      addDocs(1, start, new LinkedList<Long>()); \r\n      h.close();\n      files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      Arrays.sort(files);\n\n      FSDataOutputStream dos = fs.create(new Path(new Path(logDir), files[files.length-1]), (short)1);\n      dos.writeUTF(\"This is a trashed log file that really shouldn't work at all, but we'll see..\");\n      dos.close();\n\n      ignoreException(\"Failure to open existing\");\n      createCore();\n      \r\n      assertJQ(req(\"qt\", \"/get\", \"getVersions\", \"\" + maxReq), \"/versions==\" + versions.subList(0, Math.min(maxReq, start)));\n      resetExceptionIgnores();\n\n    } finally {\n      DirectUpdateHandler2.commitOnClose = true;\n      UpdateLog.testing_logReplayHook = null;\n      UpdateLog.testing_logReplayFinishHook = null;\n    }\n  }\n","date":"2019-03-19 03:25:36","endLine":757,"groupId":"16199","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testRemoveOldLogs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/9c/97c03babade9f376b936df8a9814e3135c7ba5.src","preCode":"  public void testRemoveOldLogs() throws Exception {\n    try {\n      DirectUpdateHandler2.commitOnClose = false;\n      final Semaphore logReplay = new Semaphore(0);\n      final Semaphore logReplayFinish = new Semaphore(0);\n\n      UpdateLog.testing_logReplayHook = () -> {\n        try {\n          assertTrue(logReplay.tryAcquire(timeout, TimeUnit.SECONDS));\n        } catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      };\n\n      UpdateLog.testing_logReplayFinishHook = logReplayFinish::release;\n\n\n      clearIndex();\n      assertU(commit());\n\n      String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n      \n      h.close();\n\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      createCore();\n\n      int start = 0;\n      int maxReq = 50;\n\n      LinkedList<Long> versions = new LinkedList<>();\n      addDocs(10, start, versions); start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(10, start, versions);  start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      assertEquals(2, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(1, start, versions);  start+=1;\n      h.close();\n      createCore();      \r\n\n      \r\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      logReplay.release(1000);\n      assertTrue(logReplayFinish.tryAcquire(timeout, TimeUnit.SECONDS));\n\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      \r\n      \r\n      \r\n      addDocs(1, start, new LinkedList<Long>()); \r\n      h.close();\n      files = HdfsUpdateLog.getLogList(fs, new Path(logDir));;\n      Arrays.sort(files);\n\n      FSDataOutputStream dos = fs.create(new Path(new Path(logDir), files[files.length-1]), (short)1);\n      dos.writeUTF(\"This is a trashed log file that really shouldn't work at all, but we'll see..\");\n      dos.close();\n\n      ignoreException(\"Failure to open existing\");\n      createCore();\n      \r\n      assertJQ(req(\"qt\", \"/get\", \"getVersions\", \"\" + maxReq), \"/versions==\" + versions.subList(0, Math.min(maxReq, start)));\n      resetExceptionIgnores();\n\n    } finally {\n      DirectUpdateHandler2.commitOnClose = true;\n      UpdateLog.testing_logReplayHook = null;\n      UpdateLog.testing_logReplayFinishHook = null;\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":657,"status":"M"},{"authorDate":"2013-07-05 09:00:50","commitOrder":6,"curCode":"  void deleteLogs() throws Exception {\n    String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n\n    h.close();\n\n    try {\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        \r\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n    } finally {\n      \r\n      \r\n      createCore();\n      assertJQ(req(\"q\",\"*:*\") ,\"/response/numFound==\");   \r\n    }\n  }\n","date":"2013-07-05 09:00:50","endLine":1107,"groupId":"24321","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"deleteLogs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/69/a849ef0ab08d971500a7037645b9a07171c1fc.src","preCode":"  void deleteLogs() throws Exception {\n    String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n\n    h.close();\n\n    try {\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        \r\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n    } finally {\n      \r\n      \r\n      createCore();\n      assertJQ(req(\"q\",\"*:*\") ,\"/response/numFound==\");   \r\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":1088,"status":"N"}],"commitId":"cf828163bdfa010c87f1171b6919e444bd0ff01c","commitMessage":"@@@SOLR-13330: Improve HDFS tests\n\nRelated JIRAs:\n* SOLR-11010\n* SOLR-11381\n* SOLR-12040\n* SOLR-13297\n\nChanges:\n* Consolidate hdfs configuration into HdfsTestUtil\n* Ensure socketTimeout long enough for HDFS tests\n* Ensure HdfsTestUtil.getClientConfiguration used in tests\n* Replace deprecated HDFS calls\n* Use try-with-resources to ensure closing of HDFS resources\n\nSigned-off-by: Kevin Risden <krisden@apache.org>\n","date":"2019-03-19 03:25:36","modifiedFileCount":"23","status":"M","submitter":"Kevin Risden"},{"authorTime":"2013-07-05 09:00:50","codes":[{"authorDate":"2019-12-08 09:03:25","commitOrder":7,"curCode":"  public void testRemoveOldLogs() throws Exception {\n    try {\n      DirectUpdateHandler2.commitOnClose = false;\n      final Semaphore logReplay = new Semaphore(0);\n      final Semaphore logReplayFinish = new Semaphore(0);\n\n      UpdateLog.testing_logReplayHook = () -> {\n        try {\n          assertTrue(logReplay.tryAcquire(TIMEOUT, TimeUnit.SECONDS));\n        } catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      };\n\n      UpdateLog.testing_logReplayFinishHook = logReplayFinish::release;\n\n\n      clearIndex();\n      assertU(commit());\n\n      String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n      \n      h.close();\n\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      createCore();\n\n      int start = 0;\n      int maxReq = 50;\n\n      LinkedList<Long> versions = new LinkedList<>();\n      addDocs(10, start, versions); start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(10, start, versions);  start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      assertEquals(2, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(1, start, versions);  start+=1;\n      h.close();\n      createCore();      \r\n\n      \r\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      logReplay.release(1000);\n      assertTrue(logReplayFinish.tryAcquire(TIMEOUT, TimeUnit.SECONDS));\n\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      \r\n      \r\n      \r\n      addDocs(1, start, new LinkedList<Long>()); \r\n      h.close();\n      files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      Arrays.sort(files);\n\n      FSDataOutputStream dos = fs.create(new Path(new Path(logDir), files[files.length-1]), (short)1);\n      dos.writeUTF(\"This is a trashed log file that really shouldn't work at all, but we'll see..\");\n      dos.close();\n\n      ignoreException(\"Failure to open existing\");\n      createCore();\n      \r\n      assertJQ(req(\"qt\", \"/get\", \"getVersions\", \"\" + maxReq), \"/versions==\" + versions.subList(0, Math.min(maxReq, start)));\n      resetExceptionIgnores();\n\n    } finally {\n      DirectUpdateHandler2.commitOnClose = true;\n      UpdateLog.testing_logReplayHook = null;\n      UpdateLog.testing_logReplayFinishHook = null;\n    }\n  }\n","date":"2019-12-11 05:15:24","endLine":757,"groupId":"16199","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testRemoveOldLogs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/a1/48d59599fd7cdd2bcd84eba7e57f8e321ef337.src","preCode":"  public void testRemoveOldLogs() throws Exception {\n    try {\n      DirectUpdateHandler2.commitOnClose = false;\n      final Semaphore logReplay = new Semaphore(0);\n      final Semaphore logReplayFinish = new Semaphore(0);\n\n      UpdateLog.testing_logReplayHook = () -> {\n        try {\n          assertTrue(logReplay.tryAcquire(timeout, TimeUnit.SECONDS));\n        } catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      };\n\n      UpdateLog.testing_logReplayFinishHook = logReplayFinish::release;\n\n\n      clearIndex();\n      assertU(commit());\n\n      String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n      \n      h.close();\n\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      createCore();\n\n      int start = 0;\n      int maxReq = 50;\n\n      LinkedList<Long> versions = new LinkedList<>();\n      addDocs(10, start, versions); start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(10, start, versions);  start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      assertEquals(2, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(1, start, versions);  start+=1;\n      h.close();\n      createCore();      \r\n\n      \r\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      logReplay.release(1000);\n      assertTrue(logReplayFinish.tryAcquire(timeout, TimeUnit.SECONDS));\n\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      \r\n      \r\n      \r\n      addDocs(1, start, new LinkedList<Long>()); \r\n      h.close();\n      files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      Arrays.sort(files);\n\n      FSDataOutputStream dos = fs.create(new Path(new Path(logDir), files[files.length-1]), (short)1);\n      dos.writeUTF(\"This is a trashed log file that really shouldn't work at all, but we'll see..\");\n      dos.close();\n\n      ignoreException(\"Failure to open existing\");\n      createCore();\n      \r\n      assertJQ(req(\"qt\", \"/get\", \"getVersions\", \"\" + maxReq), \"/versions==\" + versions.subList(0, Math.min(maxReq, start)));\n      resetExceptionIgnores();\n\n    } finally {\n      DirectUpdateHandler2.commitOnClose = true;\n      UpdateLog.testing_logReplayHook = null;\n      UpdateLog.testing_logReplayFinishHook = null;\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":657,"status":"M"},{"authorDate":"2013-07-05 09:00:50","commitOrder":7,"curCode":"  void deleteLogs() throws Exception {\n    String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n\n    h.close();\n\n    try {\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        \r\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n    } finally {\n      \r\n      \r\n      createCore();\n      assertJQ(req(\"q\",\"*:*\") ,\"/response/numFound==\");   \r\n    }\n  }\n","date":"2013-07-05 09:00:50","endLine":1107,"groupId":"24321","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"deleteLogs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/69/a849ef0ab08d971500a7037645b9a07171c1fc.src","preCode":"  void deleteLogs() throws Exception {\n    String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n\n    h.close();\n\n    try {\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        \r\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n    } finally {\n      \r\n      \r\n      createCore();\n      assertJQ(req(\"q\",\"*:*\") ,\"/response/numFound==\");   \r\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":1088,"status":"N"}],"commitId":"48775ea18e903f401d7ab0299947abdf32107b27","commitMessage":"@@@SOLR-14033: Fix Hadoop tests with security manager\n\nThis removes the Solr security manager hacks\nfor Hadoop. It does so by:\n* Using a fake group mapping class instead of ShellGroupMapping\n* Copies a few Hadoop classes and modifies them for tests with no Shell\n* Nulls out some of the static variables in the tests\n\nThe Hadoop files were copied from Apache Hadoop 3.2.0\nand copied to the test package to be only picked up\nduring tests. They were modified to remove the need to\nshell out for access. The assumption is that these\nHDFS integration tests only run on Unix based systems\nand therefore Windows compatibility was removed in some\nof the modified classes. The long term goal is to remove\nthese custom Hadoop classes. All the copied classes are\nin the org.apache.hadoop package.\n\nSigned-off-by: Kevin Risden <krisden@apache.org>\n","date":"2019-12-11 05:15:24","modifiedFileCount":"17","status":"M","submitter":"Kevin Risden"},{"authorTime":"2013-07-05 09:00:50","codes":[{"authorDate":"2020-01-17 02:47:06","commitOrder":8,"curCode":"  public void testRemoveOldLogs() throws Exception {\n    try {\n      TestInjection.skipIndexWriterCommitOnClose = true;\n      final Semaphore logReplay = new Semaphore(0);\n      final Semaphore logReplayFinish = new Semaphore(0);\n\n      UpdateLog.testing_logReplayHook = () -> {\n        try {\n          assertTrue(logReplay.tryAcquire(TIMEOUT, TimeUnit.SECONDS));\n        } catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      };\n\n      UpdateLog.testing_logReplayFinishHook = logReplayFinish::release;\n\n\n      clearIndex();\n      assertU(commit());\n\n      String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n      \n      h.close();\n\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      createCore();\n\n      int start = 0;\n      int maxReq = 50;\n\n      LinkedList<Long> versions = new LinkedList<>();\n      addDocs(10, start, versions); start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(10, start, versions);  start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      assertEquals(2, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(1, start, versions);  start+=1;\n      h.close();\n      createCore();      \r\n\n      \r\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      logReplay.release(1000);\n      assertTrue(logReplayFinish.tryAcquire(TIMEOUT, TimeUnit.SECONDS));\n\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      \r\n      \r\n      \r\n      addDocs(1, start, new LinkedList<Long>()); \r\n      h.close();\n      files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      Arrays.sort(files);\n\n      FSDataOutputStream dos = fs.create(new Path(new Path(logDir), files[files.length-1]), (short)1);\n      dos.writeUTF(\"This is a trashed log file that really shouldn't work at all, but we'll see..\");\n      dos.close();\n\n      ignoreException(\"Failure to open existing\");\n      createCore();\n      \r\n      assertJQ(req(\"qt\", \"/get\", \"getVersions\", \"\" + maxReq), \"/versions==\" + versions.subList(0, Math.min(maxReq, start)));\n      resetExceptionIgnores();\n\n    } finally {\n      UpdateLog.testing_logReplayHook = null;\n      UpdateLog.testing_logReplayFinishHook = null;\n    }\n  }\n","date":"2020-01-17 02:47:06","endLine":756,"groupId":"103179","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testRemoveOldLogs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/fa/146ea80edc483c7a0812725eb1e071fb397ddb.src","preCode":"  public void testRemoveOldLogs() throws Exception {\n    try {\n      DirectUpdateHandler2.commitOnClose = false;\n      final Semaphore logReplay = new Semaphore(0);\n      final Semaphore logReplayFinish = new Semaphore(0);\n\n      UpdateLog.testing_logReplayHook = () -> {\n        try {\n          assertTrue(logReplay.tryAcquire(TIMEOUT, TimeUnit.SECONDS));\n        } catch (Exception e) {\n          throw new RuntimeException(e);\n        }\n      };\n\n      UpdateLog.testing_logReplayFinishHook = logReplayFinish::release;\n\n\n      clearIndex();\n      assertU(commit());\n\n      String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n      \n      h.close();\n\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      createCore();\n\n      int start = 0;\n      int maxReq = 50;\n\n      LinkedList<Long> versions = new LinkedList<>();\n      addDocs(10, start, versions); start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(10, start, versions);  start+=10;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      assertEquals(2, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      addDocs(1, start, versions);  start+=1;\n      h.close();\n      createCore();      \r\n\n      \r\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      logReplay.release(1000);\n      assertTrue(logReplayFinish.tryAcquire(TIMEOUT, TimeUnit.SECONDS));\n\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      addDocs(105, start, versions);  start+=105;\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n      assertU(commit());\n      assertJQ(req(\"qt\",\"/get\", \"getVersions\",\"\"+maxReq), \"/versions==\" + versions.subList(0,Math.min(maxReq,start)));\n\n      \r\n      assertEquals(1, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n\n      \r\n      \r\n      \r\n      addDocs(1, start, new LinkedList<Long>()); \r\n      h.close();\n      files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      Arrays.sort(files);\n\n      FSDataOutputStream dos = fs.create(new Path(new Path(logDir), files[files.length-1]), (short)1);\n      dos.writeUTF(\"This is a trashed log file that really shouldn't work at all, but we'll see..\");\n      dos.close();\n\n      ignoreException(\"Failure to open existing\");\n      createCore();\n      \r\n      assertJQ(req(\"qt\", \"/get\", \"getVersions\", \"\" + maxReq), \"/versions==\" + versions.subList(0, Math.min(maxReq, start)));\n      resetExceptionIgnores();\n\n    } finally {\n      DirectUpdateHandler2.commitOnClose = true;\n      UpdateLog.testing_logReplayHook = null;\n      UpdateLog.testing_logReplayFinishHook = null;\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":657,"status":"M"},{"authorDate":"2013-07-05 09:00:50","commitOrder":8,"curCode":"  void deleteLogs() throws Exception {\n    String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n\n    h.close();\n\n    try {\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        \r\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n    } finally {\n      \r\n      \r\n      createCore();\n      assertJQ(req(\"q\",\"*:*\") ,\"/response/numFound==\");   \r\n    }\n  }\n","date":"2013-07-05 09:00:50","endLine":1107,"groupId":"103179","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"deleteLogs","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/69/a849ef0ab08d971500a7037645b9a07171c1fc.src","preCode":"  void deleteLogs() throws Exception {\n    String logDir = h.getCore().getUpdateHandler().getUpdateLog().getLogDir();\n\n    h.close();\n\n    try {\n      String[] files = HdfsUpdateLog.getLogList(fs, new Path(logDir));\n      for (String file : files) {\n        \r\n        fs.delete(new Path(logDir, file), false);\n      }\n\n      assertEquals(0, HdfsUpdateLog.getLogList(fs, new Path(logDir)).length);\n    } finally {\n      \r\n      \r\n      createCore();\n      assertJQ(req(\"q\",\"*:*\") ,\"/response/numFound==\");   \r\n    }\n  }\n","realPath":"solr/core/src/test/org/apache/solr/search/TestRecoveryHdfs.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":1088,"status":"N"}],"commitId":"5f2d7c4855987670489d68884c787e4cfb377fa9","commitMessage":"@@@SOLR-14184: Internal 'test' variable DirectUpdateHandler2.commitOnClose has been removed and replaced with TestInjection.skipIndexWriterCommitOnClose\n","date":"2020-01-17 02:47:06","modifiedFileCount":"13","status":"M","submitter":"Chris Hostetter"}]
