[{"authorTime":"2011-07-10 07:01:53","codes":[{"authorDate":"2011-07-10 07:01:53","commitOrder":1,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","date":"2011-07-10 07:01:53","endLine":307,"groupId":"32703","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/c5/8552190da6309909fd12ef73f42767c08f77ee.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":117,"status":"B"},{"authorDate":"2011-07-10 07:01:53","commitOrder":1,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"  wh������t������v������r  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"  whatever  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expecting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2011-07-10 07:01:53","endLine":335,"groupId":"20976","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/c5/8552190da6309909fd12ef73f42767c08f77ee.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"  wh������t������v������r  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"  whatever  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expecting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":310,"status":"B"}],"commitId":"429093b236e30940d69edc8869346819c337cd10","commitMessage":"@@@SOLR-2452: Rewrote Solr build system (tighter integration with the Lucene build system) and restructured Solr internal and contrib modules\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1144761 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2011-07-10 07:01:53","modifiedFileCount":"0","status":"B","submitter":"Steven Rowe"},{"authorTime":"2011-09-26 03:10:17","codes":[{"authorDate":"2011-09-26 03:10:17","commitOrder":2,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","date":"2011-09-26 03:10:17","endLine":308,"groupId":"32703","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/ba/05631e89e0a6cb0142062bc2373f495294f1bd.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":118,"status":"M"},{"authorDate":"2011-09-26 03:10:17","commitOrder":2,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"  wh������t������v������r  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"  whatever  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2011-09-26 03:10:17","endLine":336,"groupId":"20976","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/ba/05631e89e0a6cb0142062bc2373f495294f1bd.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"  wh������t������v������r  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"  whatever  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expecting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":311,"status":"M"}],"commitId":"1e577e15b5be16f498317020b9c26722f0df5c42","commitMessage":"@@@LUCENE-3456: use MockTokenizer instead of WhitespaceTokenizer in test configs\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1175529 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2011-09-26 03:10:17","modifiedFileCount":"5","status":"M","submitter":"Robert Muir"},{"authorTime":"2011-09-26 03:10:17","codes":[{"authorDate":"2011-09-26 05:10:50","commitOrder":3,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","date":"2011-09-26 05:10:50","endLine":308,"groupId":"32703","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/a6/2644b37b45af358f57a0d5aab4d487d7719800.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":118,"status":"M"},{"authorDate":"2011-09-26 03:10:17","commitOrder":3,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"  wh������t������v������r  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"  whatever  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2011-09-26 03:10:17","endLine":336,"groupId":"20976","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/ba/05631e89e0a6cb0142062bc2373f495294f1bd.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"  wh������t������v������r  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"  whatever  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":311,"status":"N"}],"commitId":"e7229ee7a4ea2a77d70df57c16920be903699be6","commitMessage":"@@@LUCENE-3456: more use of MockTokenizer\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1175579 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2011-09-26 05:10:50","modifiedFileCount":"1","status":"M","submitter":"Robert Muir"},{"authorTime":"2012-01-22 13:20:46","codes":[{"authorDate":"2011-09-26 05:10:50","commitOrder":4,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","date":"2011-09-26 05:10:50","endLine":308,"groupId":"32703","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/a6/2644b37b45af358f57a0d5aab4d487d7719800.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":118,"status":"N"},{"authorDate":"2012-01-22 13:20:46","commitOrder":4,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2012-01-22 13:20:46","endLine":336,"groupId":"43613","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/52/7f5188b1f6e548a33be4191a8b5288ffae061d.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"  wh������t������v������r  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"  whatever  \", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":311,"status":"M"}],"commitId":"f3a363708ff95f28b5f29fe2cc27c1cd9fd0fca9","commitMessage":"@@@LUCENE-3690: Re-implemented HTMLStripCharFilter as a JFlex-generated scanner.  Fixes LUCENE-2208.  SOLR-882.  and SOLR-42.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1234452 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2012-01-22 13:20:46","modifiedFileCount":"6","status":"M","submitter":"Steven Rowe"},{"authorTime":"2013-04-22 22:26:55","codes":[{"authorDate":"2013-04-22 22:26:55","commitOrder":5,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","date":"2013-04-22 22:26:55","endLine":307,"groupId":"32703","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/43/18bcc8d45affbe06d426faeabf81eb4455e231.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":117,"status":"M"},{"authorDate":"2013-04-22 22:26:55","commitOrder":5,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2013-04-22 22:26:55","endLine":335,"groupId":"43613","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/43/18bcc8d45affbe06d426faeabf81eb4455e231.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":310,"status":"M"}],"commitId":"28b7111dc79fb0095814d617a5879fb93e70fb9c","commitMessage":"@@@SOLR-3251: Dynamically add fields to schema.\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1470539 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2013-04-22 22:26:55","modifiedFileCount":"120","status":"M","submitter":"Steven Rowe"},{"authorTime":"2013-04-22 22:26:55","codes":[{"authorDate":"2013-05-05 02:18:08","commitOrder":6,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","date":"2013-05-05 02:18:08","endLine":307,"groupId":"32703","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/c8/2cedcd121dc934b69e857767d9a54b22dd5501.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":117,"status":"M"},{"authorDate":"2013-04-22 22:26:55","commitOrder":6,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2013-04-22 22:26:55","endLine":335,"groupId":"43613","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/43/18bcc8d45affbe06d426faeabf81eb4455e231.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":310,"status":"N"}],"commitId":"8a7f2b6cc4575878f2b9cfd105e76429f3d6107b","commitMessage":"@@@LUCENE-4963: Deprecate broken TokenFilter options.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1479148 13f79535-47bb-0310-9956-ffa450edef68\n","date":"2013-05-05 02:18:08","modifiedFileCount":"52","status":"M","submitter":"Adrien Grand"},{"authorTime":"2013-04-22 22:26:55","codes":[{"authorDate":"2016-06-15 04:38:04","commitOrder":7,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","date":"2016-06-15 04:38:04","endLine":354,"groupId":"32703","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/2e/d00cc840d07070853f0e7ed05fee64ca0337cd.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":164,"status":"M"},{"authorDate":"2013-04-22 22:26:55","commitOrder":7,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2013-04-22 22:26:55","endLine":335,"groupId":"43613","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/43/18bcc8d45affbe06d426faeabf81eb4455e231.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":310,"status":"N"}],"commitId":"87016b5f0ce7a895447ee19d3f567f4135cae2a6","commitMessage":"@@@LUCENE-7318: graduate StandardAnalyzer and make it the default for IndexWriterConfig\n","date":"2016-06-15 04:38:04","modifiedFileCount":"182","status":"M","submitter":"Mike McCandless"},{"authorTime":"2013-04-22 22:26:55","codes":[{"authorDate":"2016-10-03 03:18:22","commitOrder":8,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","date":"2016-10-03 03:18:22","endLine":354,"groupId":"32703","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/d2/ef55533337ad867095fb3306dda7ef30cd938a.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":164,"status":"M"},{"authorDate":"2013-04-22 22:26:55","commitOrder":8,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2013-04-22 22:26:55","endLine":335,"groupId":"43613","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/43/18bcc8d45affbe06d426faeabf81eb4455e231.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":310,"status":"N"}],"commitId":"3182cd9872af3cb73a1f73e36de68b8d76646e59","commitMessage":"@@@Merge branch 'master' of https://git-wip-us.apache.org/repos/asf/lucene-solr\n","date":"2016-10-03 03:18:22","modifiedFileCount":"570","status":"M","submitter":"Karl Wright"},{"authorTime":"2013-04-22 22:26:55","codes":[{"authorDate":"2018-06-15 15:45:55","commitOrder":9,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","date":"2018-06-27 19:05:23","endLine":350,"groupId":"4306","id":17,"instanceNumber":1,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/ef/7de391e780c85f3b8f46b1a7899c7ec69ef89f.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":178,"status":"M"},{"authorDate":"2013-04-22 22:26:55","commitOrder":9,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2013-04-22 22:26:55","endLine":335,"groupId":"43613","id":18,"instanceNumber":2,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/43/18bcc8d45affbe06d426faeabf81eb4455e231.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n    \n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":310,"status":"N"}],"commitId":"0fbe3d257e173b78bd7f9681967351613a7254ab","commitMessage":"@@@LUCENE-8356: Remove StandardFilter from Solr schemas\n","date":"2018-06-27 19:05:23","modifiedFileCount":"4","status":"M","submitter":"Alan Woodward"},{"authorTime":"2020-06-18 05:51:41","codes":[{"authorDate":"2020-06-18 05:51:41","commitOrder":10,"curCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    @SuppressWarnings({\"rawtypes\"})\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","date":"2020-06-18 05:51:41","endLine":359,"groupId":"103453","id":19,"instanceNumber":1,"isCurCommit":0,"methodName":"testHandleAnalysisRequest","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/4d/905e2c5b68b6e91834b09c7e840a4749b85f43.src","preCode":"  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":177,"status":"M"},{"authorDate":"2020-06-18 05:51:41","commitOrder":10,"curCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n\n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","date":"2020-06-18 05:51:41","endLine":392,"groupId":"103453","id":20,"instanceNumber":2,"isCurCommit":0,"methodName":"testCharFilterAnalysis","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-solr-10-0.7/blobInfo/CC_OUT/blobs/4d/905e2c5b68b6e91834b09c7e840a4749b85f43.src","preCode":"  public void testCharFilterAnalysis() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldType(\"charfilthtmlmap\");\n    request.setFieldValue(\"<html><body>wh������t������v������r</body></html>\");\n    request.setShowMatch(false);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"charfilthtmlmap\");\n    assertNotNull(\"expecting result for field type 'charfilthtmlmap'\", textType);\n\n    NamedList indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'charfilthtmlmap'\", indexPart);\n\n    assertEquals(\"\\n\\nwh������t������v������r\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.HTMLStripCharFilter\"));\n    assertEquals(\"\\n\\nwhatever\\n\\n\", indexPart.get(\"org.apache.lucene.analysis.charfilter.MappingCharFilter\"));\n\n    List<NamedList> tokenList = (List<NamedList>)indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"Expecting MockTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 1);\n    assertToken(tokenList.get(0), new TokenInfo(\"whatever\", null, \"word\", 12, 20, 1, new int[]{1}, null, false));\n  }\n","realPath":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest.java","repoName":"solr","snippetEndLine":0,"snippetStartLine":0,"startLine":362,"status":"M"}],"commitId":"b01e249c9ec724b6df120a5d731020cfe4de3fce","commitMessage":"@@@SOLR-14574: Fix or suppress warnings in solr/core/src/test (part 1)\n","date":"2020-06-18 05:51:41","modifiedFileCount":"213","status":"M","submitter":"Erick Erickson"}]
