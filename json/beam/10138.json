[{"authorTime":"2017-11-16 01:00:23","codes":[{"authorDate":"2019-11-20 22:28:02","commitOrder":3,"curCode":"  public SparkSideInputReader(\n      Map<PCollectionView<?>, WindowingStrategy<?, ?>> indexByView,\n      SideInputBroadcast broadcastStateData) {\n    for (PCollectionView<?> view : indexByView.keySet()) {\n      checkArgument(\n          Materializations.MULTIMAP_MATERIALIZATION_URN.equals(\n              view.getViewFn().getMaterialization().getUrn()),\n          \"This handler is only capable of dealing with %s materializations \"\n              + \"but was asked to handle %s for PCollectionView with tag %s.\",\n          Materializations.MULTIMAP_MATERIALIZATION_URN,\n          view.getViewFn().getMaterialization().getUrn(),\n          view.getTagInternal().getId());\n    }\n    sideInputs = new HashMap<>();\n    for (Map.Entry<PCollectionView<?>, WindowingStrategy<?, ?>> entry : indexByView.entrySet()) {\n      sideInputs.put(entry.getKey().getTagInternal(), entry.getValue());\n    }\n    this.broadcastStateData = broadcastStateData;\n  }\n","date":"2019-11-20 22:28:02","endLine":72,"groupId":"9006","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"SparkSideInputReader","params":"(Map<PCollectionView<?>@WindowingStrategy<?@?>>indexByView@SideInputBroadcastbroadcastStateData)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/5d/49767fc591f942b58288b3ac29fb77f5238ca8.src","preCode":"  public SparkSideInputReader(\n      Map<PCollectionView<?>, WindowingStrategy<?, ?>> indexByView,\n      SideInputBroadcast broadcastStateData) {\n    for (PCollectionView<?> view : indexByView.keySet()) {\n      checkArgument(\n          Materializations.MULTIMAP_MATERIALIZATION_URN.equals(\n              view.getViewFn().getMaterialization().getUrn()),\n          \"This handler is only capable of dealing with %s materializations \"\n              + \"but was asked to handle %s for PCollectionView with tag %s.\",\n          Materializations.MULTIMAP_MATERIALIZATION_URN,\n          view.getViewFn().getMaterialization().getUrn(),\n          view.getTagInternal().getId());\n    }\n    sideInputs = new HashMap<>();\n    for (Map.Entry<PCollectionView<?>, WindowingStrategy<?, ?>> entry : indexByView.entrySet()) {\n      sideInputs.put(entry.getKey().getTagInternal(), entry.getValue());\n    }\n    this.broadcastStateData = broadcastStateData;\n  }\n","realPath":"runners/spark/src/main/java/org/apache/beam/runners/spark/structuredstreaming/translation/batch/functions/SparkSideInputReader.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":54,"status":"B"},{"authorDate":"2017-11-16 01:00:23","commitOrder":3,"curCode":"  public FlinkSideInputReader(Map<PCollectionView<?>, WindowingStrategy<?, ?>> indexByView,\n                              RuntimeContext runtimeContext) {\n    for (PCollectionView<?> view : indexByView.keySet()) {\n      checkArgument(\n          Materializations.MULTIMAP_MATERIALIZATION_URN.equals(\n              view.getViewFn().getMaterialization().getUrn()),\n          \"This handler is only capable of dealing with %s materializations \"\n              + \"but was asked to handle %s for PCollectionView with tag %s.\",\n          Materializations.MULTIMAP_MATERIALIZATION_URN,\n          view.getViewFn().getMaterialization().getUrn(),\n          view.getTagInternal().getId());\n    }\n    sideInputs = new HashMap<>();\n    for (Map.Entry<PCollectionView<?>, WindowingStrategy<?, ?>> entry : indexByView.entrySet()) {\n      sideInputs.put(entry.getKey().getTagInternal(), entry.getValue());\n    }\n    this.runtimeContext = runtimeContext;\n  }\n","date":"2017-11-16 01:00:23","endLine":70,"groupId":"9006","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"FlinkSideInputReader","params":"(Map<PCollectionView<?>@WindowingStrategy<?@?>>indexByView@RuntimeContextruntimeContext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/fb/3f375fdd91b84ec59021dc9b6602c057e65da6.src","preCode":"  public FlinkSideInputReader(Map<PCollectionView<?>, WindowingStrategy<?, ?>> indexByView,\n                              RuntimeContext runtimeContext) {\n    for (PCollectionView<?> view : indexByView.keySet()) {\n      checkArgument(\n          Materializations.MULTIMAP_MATERIALIZATION_URN.equals(\n              view.getViewFn().getMaterialization().getUrn()),\n          \"This handler is only capable of dealing with %s materializations \"\n              + \"but was asked to handle %s for PCollectionView with tag %s.\",\n          Materializations.MULTIMAP_MATERIALIZATION_URN,\n          view.getViewFn().getMaterialization().getUrn(),\n          view.getTagInternal().getId());\n    }\n    sideInputs = new HashMap<>();\n    for (Map.Entry<PCollectionView<?>, WindowingStrategy<?, ?>> entry : indexByView.entrySet()) {\n      sideInputs.put(entry.getKey().getTagInternal(), entry.getValue());\n    }\n    this.runtimeContext = runtimeContext;\n  }\n","realPath":"runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkSideInputReader.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":53,"status":"NB"}],"commitId":"18059eecad850b6e30bc7e376e70937915dd11fb","commitMessage":"@@@Merge pull request #9866: [BEAM-8470] Create a new Spark runner based on Spark Structured streaming framework\n\n","date":"2019-11-20 22:28:02","modifiedFileCount":"3","status":"M","submitter":"Alexey Romanenko"},{"authorTime":"2020-06-18 04:27:46","codes":[{"authorDate":"2020-06-18 04:27:46","commitOrder":4,"curCode":"  public SparkSideInputReader(\n      Map<PCollectionView<?>, WindowingStrategy<?, ?>> indexByView,\n      SideInputBroadcast broadcastStateData) {\n    for (PCollectionView<?> view : indexByView.keySet()) {\n      checkArgument(\n          SUPPORTED_MATERIALIZATIONS.contains(view.getViewFn().getMaterialization().getUrn()),\n          \"This handler is only capable of dealing with %s materializations \"\n              + \"but was asked to handle %s for PCollectionView with tag %s.\",\n          SUPPORTED_MATERIALIZATIONS,\n          view.getViewFn().getMaterialization().getUrn(),\n          view.getTagInternal().getId());\n    }\n    sideInputs = new HashMap<>();\n    for (Map.Entry<PCollectionView<?>, WindowingStrategy<?, ?>> entry : indexByView.entrySet()) {\n      sideInputs.put(entry.getKey().getTagInternal(), entry.getValue());\n    }\n    this.broadcastStateData = broadcastStateData;\n  }\n","date":"2020-06-18 04:27:46","endLine":75,"groupId":"10138","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"SparkSideInputReader","params":"(Map<PCollectionView<?>@WindowingStrategy<?@?>>indexByView@SideInputBroadcastbroadcastStateData)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/56/e3ee81e757a2d730d7e5e2550de2a6a4444cd0.src","preCode":"  public SparkSideInputReader(\n      Map<PCollectionView<?>, WindowingStrategy<?, ?>> indexByView,\n      SideInputBroadcast broadcastStateData) {\n    for (PCollectionView<?> view : indexByView.keySet()) {\n      checkArgument(\n          Materializations.MULTIMAP_MATERIALIZATION_URN.equals(\n              view.getViewFn().getMaterialization().getUrn()),\n          \"This handler is only capable of dealing with %s materializations \"\n              + \"but was asked to handle %s for PCollectionView with tag %s.\",\n          Materializations.MULTIMAP_MATERIALIZATION_URN,\n          view.getViewFn().getMaterialization().getUrn(),\n          view.getTagInternal().getId());\n    }\n    sideInputs = new HashMap<>();\n    for (Map.Entry<PCollectionView<?>, WindowingStrategy<?, ?>> entry : indexByView.entrySet()) {\n      sideInputs.put(entry.getKey().getTagInternal(), entry.getValue());\n    }\n    this.broadcastStateData = broadcastStateData;\n  }\n","realPath":"runners/spark/src/main/java/org/apache/beam/runners/spark/structuredstreaming/translation/batch/functions/SparkSideInputReader.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":58,"status":"M"},{"authorDate":"2020-06-18 04:27:46","commitOrder":4,"curCode":"  public FlinkSideInputReader(\n      Map<PCollectionView<?>, WindowingStrategy<?, ?>> indexByView, RuntimeContext runtimeContext) {\n    for (PCollectionView<?> view : indexByView.keySet()) {\n      checkArgument(\n          SUPPORTED_MATERIALIZATIONS.contains(view.getViewFn().getMaterialization().getUrn()),\n          \"This handler is only capable of dealing with %s materializations \"\n              + \"but was asked to handle %s for PCollectionView with tag %s.\",\n          SUPPORTED_MATERIALIZATIONS,\n          view.getViewFn().getMaterialization().getUrn(),\n          view.getTagInternal().getId());\n    }\n    sideInputs = new HashMap<>();\n    for (Map.Entry<PCollectionView<?>, WindowingStrategy<?, ?>> entry : indexByView.entrySet()) {\n      sideInputs.put(entry.getKey().getTagInternal(), entry.getValue());\n    }\n    this.runtimeContext = runtimeContext;\n  }\n","date":"2020-06-18 04:27:46","endLine":68,"groupId":"10138","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"FlinkSideInputReader","params":"(Map<PCollectionView<?>@WindowingStrategy<?@?>>indexByView@RuntimeContextruntimeContext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/b1/2bcf998701163a14a0dd7c3af1f46de3b3c31f.src","preCode":"  public FlinkSideInputReader(\n      Map<PCollectionView<?>, WindowingStrategy<?, ?>> indexByView, RuntimeContext runtimeContext) {\n    for (PCollectionView<?> view : indexByView.keySet()) {\n      checkArgument(\n          Materializations.MULTIMAP_MATERIALIZATION_URN.equals(\n              view.getViewFn().getMaterialization().getUrn()),\n          \"This handler is only capable of dealing with %s materializations \"\n              + \"but was asked to handle %s for PCollectionView with tag %s.\",\n          Materializations.MULTIMAP_MATERIALIZATION_URN,\n          view.getViewFn().getMaterialization().getUrn(),\n          view.getTagInternal().getId());\n    }\n    sideInputs = new HashMap<>();\n    for (Map.Entry<PCollectionView<?>, WindowingStrategy<?, ?>> entry : indexByView.entrySet()) {\n      sideInputs.put(entry.getKey().getTagInternal(), entry.getValue());\n    }\n    this.runtimeContext = runtimeContext;\n  }\n","realPath":"runners/flink/src/main/java/org/apache/beam/runners/flink/translation/functions/FlinkSideInputReader.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":52,"status":"M"}],"commitId":"d1c6ef6c9803536dddc67e064e6da53c04ce0f60","commitMessage":"@@@[BEAM-10097.  BEAM-5982.  BEAM-3080] Use primitive views directly instead of transforming KV<Void.  Iterable<T>> to the view type via a naive mapping. (#11821)\n\n* [BEAM-10097.  BEAM-5982.  BEAM-3080] Use primitive views directly instead of transforming KV<Void.  Iterable<T>> to the view type via a naive mapping.\n\nThis does not impact non-portable runners since the new expansion is guarded by the experiment \"beam_fn_api\".\nThis currently has little benefit for portable runners since they still treat all views as in memory iterables of values but opens the door for them to meaningfully provide optimized versions.\nThe singleton and iterable views can't be translated except if using the Dataflow runner v2 experiment since runner v1 doesn't support the iterable access pattern.\n\n* fixup! Fix test after rebase on master","date":"2020-06-18 04:27:46","modifiedFileCount":"29","status":"M","submitter":"Lukasz Cwik"}]
