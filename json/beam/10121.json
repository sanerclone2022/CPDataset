[{"authorTime":"2017-02-20 17:40:26","codes":[{"authorDate":"2017-02-20 17:40:26","commitOrder":1,"curCode":"  public void testTrackSingle() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream.QueuedValues<Integer> queueStream =\n        CreateStream.fromQueue(Collections.<Iterable<Integer>>emptyList());\n\n    p.apply(queueStream).setCoder(VarIntCoder.of())\n        .apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.Bound.class,  -1));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","date":"2017-02-20 17:40:26","endLine":89,"groupId":"7379","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testTrackSingle","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/fb/e5777b5142130d8e7029b42294d3a431227c71.src","preCode":"  public void testTrackSingle() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream.QueuedValues<Integer> queueStream =\n        CreateStream.fromQueue(Collections.<Iterable<Integer>>emptyList());\n\n    p.apply(queueStream).setCoder(VarIntCoder.of())\n        .apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.Bound.class,  -1));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","realPath":"runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/TrackStreamingSourcesTest.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":73,"status":"B"},{"authorDate":"2017-02-20 17:40:26","commitOrder":1,"curCode":"  public void testTrackFlattened() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream.QueuedValues<Integer> queueStream1 =\n        CreateStream.fromQueue(Collections.<Iterable<Integer>>emptyList());\n    CreateStream.QueuedValues<Integer> queueStream2 =\n        CreateStream.fromQueue(Collections.<Iterable<Integer>>emptyList());\n\n    PCollection<Integer> pcol1 = p.apply(queueStream1).setCoder(VarIntCoder.of());\n    PCollection<Integer> pcol2 = p.apply(queueStream2).setCoder(VarIntCoder.of());\n    PCollection<Integer> flattened =\n        PCollectionList.of(pcol1).and(pcol2).apply(Flatten.<Integer>pCollections());\n    flattened.apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.Bound.class, -1, -2));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","date":"2017-02-20 17:40:26","endLine":113,"groupId":"7379","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testTrackFlattened","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/fb/e5777b5142130d8e7029b42294d3a431227c71.src","preCode":"  public void testTrackFlattened() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream.QueuedValues<Integer> queueStream1 =\n        CreateStream.fromQueue(Collections.<Iterable<Integer>>emptyList());\n    CreateStream.QueuedValues<Integer> queueStream2 =\n        CreateStream.fromQueue(Collections.<Iterable<Integer>>emptyList());\n\n    PCollection<Integer> pcol1 = p.apply(queueStream1).setCoder(VarIntCoder.of());\n    PCollection<Integer> pcol2 = p.apply(queueStream2).setCoder(VarIntCoder.of());\n    PCollection<Integer> flattened =\n        PCollectionList.of(pcol1).and(pcol2).apply(Flatten.<Integer>pCollections());\n    flattened.apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.Bound.class, -1, -2));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","realPath":"runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/TrackStreamingSourcesTest.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":92,"status":"B"}],"commitId":"aa45ccb0800741ddf5ee7ecc7965c85c1914acf7","commitMessage":"@@@This closes #1987\n","date":"2017-02-20 17:40:26","modifiedFileCount":"10","status":"B","submitter":"Sela"},{"authorTime":"2017-03-01 06:18:45","codes":[{"authorDate":"2017-03-01 06:18:45","commitOrder":2,"curCode":"  public void testTrackSingle() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> emptyStream =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    p.apply(emptyStream).apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.Bound.class,  0));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","date":"2017-03-01 06:18:45","endLine":88,"groupId":"21837","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testTrackSingle","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/b1/81a042820c56744288f6ddacf416e8e54700ba.src","preCode":"  public void testTrackSingle() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream.QueuedValues<Integer> queueStream =\n        CreateStream.fromQueue(Collections.<Iterable<Integer>>emptyList());\n\n    p.apply(queueStream).setCoder(VarIntCoder.of())\n        .apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.Bound.class,  -1));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","realPath":"runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/TrackStreamingSourcesTest.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":71,"status":"M"},{"authorDate":"2017-03-01 06:18:45","commitOrder":2,"curCode":"  public void testTrackFlattened() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> queueStream1 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n    CreateStream<Integer> queueStream2 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    PCollection<Integer> pcol1 = p.apply(queueStream1);\n    PCollection<Integer> pcol2 = p.apply(queueStream2);\n    PCollection<Integer> flattened =\n        PCollectionList.of(pcol1).and(pcol2).apply(Flatten.<Integer>pCollections());\n    flattened.apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.Bound.class, 0, 1));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","date":"2017-03-01 06:18:45","endLine":116,"groupId":"2158","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testTrackFlattened","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/b1/81a042820c56744288f6ddacf416e8e54700ba.src","preCode":"  public void testTrackFlattened() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream.QueuedValues<Integer> queueStream1 =\n        CreateStream.fromQueue(Collections.<Iterable<Integer>>emptyList());\n    CreateStream.QueuedValues<Integer> queueStream2 =\n        CreateStream.fromQueue(Collections.<Iterable<Integer>>emptyList());\n\n    PCollection<Integer> pcol1 = p.apply(queueStream1).setCoder(VarIntCoder.of());\n    PCollection<Integer> pcol2 = p.apply(queueStream2).setCoder(VarIntCoder.of());\n    PCollection<Integer> flattened =\n        PCollectionList.of(pcol1).and(pcol2).apply(Flatten.<Integer>pCollections());\n    flattened.apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.Bound.class, -1, -2));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","realPath":"runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/TrackStreamingSourcesTest.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":91,"status":"M"}],"commitId":"61e31e622ec5b1f3c4cd1417c859810689f2683c","commitMessage":"@@@This closes #2050\n","date":"2017-03-01 06:18:45","modifiedFileCount":"27","status":"M","submitter":"Sela"},{"authorTime":"2017-03-04 07:36:10","codes":[{"authorDate":"2017-03-04 07:36:10","commitOrder":3,"curCode":"  public void testTrackSingle() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> emptyStream =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    p.apply(emptyStream).apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.BoundMulti.class,  0));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","date":"2017-03-04 07:36:10","endLine":88,"groupId":"21837","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testTrackSingle","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/d6/6633b4c49b2d7720de19ed0310aaa7b61e3d12.src","preCode":"  public void testTrackSingle() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> emptyStream =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    p.apply(emptyStream).apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.Bound.class,  0));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","realPath":"runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/TrackStreamingSourcesTest.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":71,"status":"M"},{"authorDate":"2017-03-04 07:36:10","commitOrder":3,"curCode":"  public void testTrackFlattened() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> queueStream1 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n    CreateStream<Integer> queueStream2 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    PCollection<Integer> pcol1 = p.apply(queueStream1);\n    PCollection<Integer> pcol2 = p.apply(queueStream2);\n    PCollection<Integer> flattened =\n        PCollectionList.of(pcol1).and(pcol2).apply(Flatten.<Integer>pCollections());\n    flattened.apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.BoundMulti.class, 0, 1));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","date":"2017-03-04 07:36:10","endLine":116,"groupId":"2158","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testTrackFlattened","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/d6/6633b4c49b2d7720de19ed0310aaa7b61e3d12.src","preCode":"  public void testTrackFlattened() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> queueStream1 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n    CreateStream<Integer> queueStream2 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    PCollection<Integer> pcol1 = p.apply(queueStream1);\n    PCollection<Integer> pcol2 = p.apply(queueStream2);\n    PCollection<Integer> flattened =\n        PCollectionList.of(pcol1).and(pcol2).apply(Flatten.<Integer>pCollections());\n    flattened.apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.Bound.class, 0, 1));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","realPath":"runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/TrackStreamingSourcesTest.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":91,"status":"M"}],"commitId":"bb8cd72b98b261c781add91191d246a650de2cd3","commitMessage":"@@@This closes #2145\n","date":"2017-03-04 07:36:10","modifiedFileCount":"11","status":"M","submitter":"Thomas Groh"},{"authorTime":"2017-03-07 01:09:10","codes":[{"authorDate":"2017-03-07 01:09:10","commitOrder":4,"curCode":"  public void testTrackSingle() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> emptyStream =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    p.apply(emptyStream).apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.Bound.class,  0));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","date":"2017-03-07 01:09:10","endLine":88,"groupId":"21837","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testTrackSingle","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/b1/81a042820c56744288f6ddacf416e8e54700ba.src","preCode":"  public void testTrackSingle() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> emptyStream =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    p.apply(emptyStream).apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.BoundMulti.class,  0));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","realPath":"runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/TrackStreamingSourcesTest.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":71,"status":"M"},{"authorDate":"2017-03-07 01:09:10","commitOrder":4,"curCode":"  public void testTrackFlattened() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> queueStream1 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n    CreateStream<Integer> queueStream2 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    PCollection<Integer> pcol1 = p.apply(queueStream1);\n    PCollection<Integer> pcol2 = p.apply(queueStream2);\n    PCollection<Integer> flattened =\n        PCollectionList.of(pcol1).and(pcol2).apply(Flatten.<Integer>pCollections());\n    flattened.apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.Bound.class, 0, 1));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","date":"2017-03-07 01:09:10","endLine":116,"groupId":"2158","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testTrackFlattened","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/b1/81a042820c56744288f6ddacf416e8e54700ba.src","preCode":"  public void testTrackFlattened() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> queueStream1 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n    CreateStream<Integer> queueStream2 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    PCollection<Integer> pcol1 = p.apply(queueStream1);\n    PCollection<Integer> pcol2 = p.apply(queueStream2);\n    PCollection<Integer> flattened =\n        PCollectionList.of(pcol1).and(pcol2).apply(Flatten.<Integer>pCollections());\n    flattened.apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.BoundMulti.class, 0, 1));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","realPath":"runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/TrackStreamingSourcesTest.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":91,"status":"M"}],"commitId":"9cc8018b3ed945244bb311134ebd824016d1633f","commitMessage":"@@@This closes #2170\n","date":"2017-03-07 01:09:10","modifiedFileCount":"11","status":"M","submitter":"Thomas Groh"},{"authorTime":"2017-03-17 23:54:02","codes":[{"authorDate":"2017-03-17 23:54:02","commitOrder":5,"curCode":"  public void testTrackSingle() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> emptyStream =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    p.apply(emptyStream).apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.BoundMulti.class,  0));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","date":"2017-03-17 23:54:02","endLine":88,"groupId":"21837","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testTrackSingle","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/d6/6633b4c49b2d7720de19ed0310aaa7b61e3d12.src","preCode":"  public void testTrackSingle() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> emptyStream =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    p.apply(emptyStream).apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.Bound.class,  0));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","realPath":"runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/TrackStreamingSourcesTest.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":71,"status":"M"},{"authorDate":"2017-03-17 23:54:02","commitOrder":5,"curCode":"  public void testTrackFlattened() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> queueStream1 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n    CreateStream<Integer> queueStream2 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    PCollection<Integer> pcol1 = p.apply(queueStream1);\n    PCollection<Integer> pcol2 = p.apply(queueStream2);\n    PCollection<Integer> flattened =\n        PCollectionList.of(pcol1).and(pcol2).apply(Flatten.<Integer>pCollections());\n    flattened.apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.BoundMulti.class, 0, 1));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","date":"2017-03-17 23:54:02","endLine":116,"groupId":"2158","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testTrackFlattened","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/d6/6633b4c49b2d7720de19ed0310aaa7b61e3d12.src","preCode":"  public void testTrackFlattened() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> queueStream1 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n    CreateStream<Integer> queueStream2 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    PCollection<Integer> pcol1 = p.apply(queueStream1);\n    PCollection<Integer> pcol2 = p.apply(queueStream2);\n    PCollection<Integer> flattened =\n        PCollectionList.of(pcol1).and(pcol2).apply(Flatten.<Integer>pCollections());\n    flattened.apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.Bound.class, 0, 1));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","realPath":"runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/TrackStreamingSourcesTest.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":91,"status":"M"}],"commitId":"ef6a5008a012582a02007a630e78a7716e3175c3","commitMessage":"@@@This closes #2248\n","date":"2017-03-17 23:54:02","modifiedFileCount":"14","status":"M","submitter":"Thomas Groh"},{"authorTime":"2017-03-29 04:05:12","codes":[{"authorDate":"2017-03-29 04:05:12","commitOrder":6,"curCode":"  public void testTrackSingle() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> emptyStream =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    p.apply(emptyStream).apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.MultiOutput.class,  0));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","date":"2017-03-29 04:05:12","endLine":88,"groupId":"21837","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testTrackSingle","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/41/ccd0837dcb4e57be0ee17ac8e92f1e7551f24a.src","preCode":"  public void testTrackSingle() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> emptyStream =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    p.apply(emptyStream).apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.BoundMulti.class,  0));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","realPath":"runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/TrackStreamingSourcesTest.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":71,"status":"M"},{"authorDate":"2017-03-29 04:05:12","commitOrder":6,"curCode":"  public void testTrackFlattened() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> queueStream1 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n    CreateStream<Integer> queueStream2 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    PCollection<Integer> pcol1 = p.apply(queueStream1);\n    PCollection<Integer> pcol2 = p.apply(queueStream2);\n    PCollection<Integer> flattened =\n        PCollectionList.of(pcol1).and(pcol2).apply(Flatten.<Integer>pCollections());\n    flattened.apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.MultiOutput.class, 0, 1));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","date":"2017-03-29 04:05:12","endLine":116,"groupId":"2158","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testTrackFlattened","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/41/ccd0837dcb4e57be0ee17ac8e92f1e7551f24a.src","preCode":"  public void testTrackFlattened() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> queueStream1 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n    CreateStream<Integer> queueStream2 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    PCollection<Integer> pcol1 = p.apply(queueStream1);\n    PCollection<Integer> pcol2 = p.apply(queueStream2);\n    PCollection<Integer> flattened =\n        PCollectionList.of(pcol1).and(pcol2).apply(Flatten.<Integer>pCollections());\n    flattened.apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.BoundMulti.class, 0, 1));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","realPath":"runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/TrackStreamingSourcesTest.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":91,"status":"M"}],"commitId":"66283670d12a499f911e8a08c71652f16421e6f4","commitMessage":"@@@This closes #2153\n","date":"2017-03-29 04:05:12","modifiedFileCount":"48","status":"M","submitter":"Eugene Kirpichov"},{"authorTime":"2018-01-25 04:56:23","codes":[{"authorDate":"2017-03-29 04:05:12","commitOrder":7,"curCode":"  public void testTrackSingle() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> emptyStream =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    p.apply(emptyStream).apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.MultiOutput.class,  0));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","date":"2017-03-29 04:05:12","endLine":88,"groupId":"10121","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testTrackSingle","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/41/ccd0837dcb4e57be0ee17ac8e92f1e7551f24a.src","preCode":"  public void testTrackSingle() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> emptyStream =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    p.apply(emptyStream).apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.MultiOutput.class,  0));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","realPath":"runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/TrackStreamingSourcesTest.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":71,"status":"N"},{"authorDate":"2018-01-25 04:56:23","commitOrder":7,"curCode":"  public void testTrackFlattened() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> queueStream1 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n    CreateStream<Integer> queueStream2 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    PCollection<Integer> pcol1 = p.apply(queueStream1);\n    PCollection<Integer> pcol2 = p.apply(queueStream2);\n    PCollection<Integer> flattened =\n        PCollectionList.of(pcol1).and(pcol2).apply(Flatten.pCollections());\n    flattened.apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.MultiOutput.class, 0, 1));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","date":"2018-01-25 04:56:23","endLine":115,"groupId":"10121","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testTrackFlattened","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/85/05a36f9428cd4c3225ef2e64fcf0f0af523ef9.src","preCode":"  public void testTrackFlattened() {\n    options.setRunner(SparkRunner.class);\n    JavaSparkContext jsc = SparkContextFactory.getSparkContext(options);\n    JavaStreamingContext jssc = new JavaStreamingContext(jsc,\n        new org.apache.spark.streaming.Duration(options.getBatchIntervalMillis()));\n\n    Pipeline p = Pipeline.create(options);\n\n    CreateStream<Integer> queueStream1 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n    CreateStream<Integer> queueStream2 =\n        CreateStream.of(\n            VarIntCoder.of(),\n            Duration.millis(options.getBatchIntervalMillis())).emptyBatch();\n\n    PCollection<Integer> pcol1 = p.apply(queueStream1);\n    PCollection<Integer> pcol2 = p.apply(queueStream2);\n    PCollection<Integer> flattened =\n        PCollectionList.of(pcol1).and(pcol2).apply(Flatten.<Integer>pCollections());\n    flattened.apply(ParDo.of(new PassthroughFn<>()));\n\n    p.traverseTopologically(new StreamingSourceTracker(jssc, p, ParDo.MultiOutput.class, 0, 1));\n    assertThat(StreamingSourceTracker.numAssertions, equalTo(1));\n  }\n","realPath":"runners/spark/src/test/java/org/apache/beam/runners/spark/translation/streaming/TrackStreamingSourcesTest.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":90,"status":"M"}],"commitId":"0cbcf4ad1db7d820c5476d636f3a3d69062021a5","commitMessage":"@@@Merge pull request #4470 from jkff/java8\n\nLarge set of mechanical changes after switch to Java8","date":"2018-01-25 04:56:23","modifiedFileCount":"653","status":"M","submitter":"Eugene Kirpichov"}]
