[{"authorTime":"2019-11-20 22:28:02","codes":[{"authorDate":"2019-11-20 22:28:02","commitOrder":1,"curCode":"  public void translateTransform(\n      PTransform<PBegin, PCollection<T>> transform, TranslationContext context) {\n    AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>> rootTransform =\n        (AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>>)\n            context.getCurrentTransform();\n\n    BoundedSource<T> source;\n    try {\n      source = ReadTranslation.boundedSourceFromTransform(rootTransform);\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    SparkSession sparkSession = context.getSparkSession();\n\n    String serializedSource = Base64Serializer.serializeUnchecked(source);\n    Dataset<Row> rowDataset =\n        sparkSession\n            .read()\n            .format(sourceProviderClass)\n            .option(DatasetSourceBatch.BEAM_SOURCE_OPTION, serializedSource)\n            .option(\n                DatasetSourceBatch.DEFAULT_PARALLELISM,\n                String.valueOf(context.getSparkSession().sparkContext().defaultParallelism()))\n            .option(\n                DatasetSourceBatch.PIPELINE_OPTIONS, context.getSerializableOptions().toString())\n            .load();\n\n    \r\n    WindowedValue.FullWindowedValueCoder<T> windowedValueCoder =\n        WindowedValue.FullWindowedValueCoder.of(\n            source.getOutputCoder(), GlobalWindow.Coder.INSTANCE);\n\n    Dataset<WindowedValue<T>> dataset =\n        rowDataset.map(\n            RowHelpers.extractWindowedValueFromRowMapFunction(windowedValueCoder),\n            EncoderHelpers.fromBeamCoder(windowedValueCoder));\n\n    PCollection<T> output = (PCollection<T>) context.getOutput();\n    context.putDataset(output, dataset);\n  }\n","date":"2019-11-20 22:28:02","endLine":84,"groupId":"19074","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"translateTransform","params":"(PTransform<PBegin@PCollection<T>>transform@TranslationContextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/6a/f7f55877f4ad031ddce4d45661c3babb0d017f.src","preCode":"  public void translateTransform(\n      PTransform<PBegin, PCollection<T>> transform, TranslationContext context) {\n    AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>> rootTransform =\n        (AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>>)\n            context.getCurrentTransform();\n\n    BoundedSource<T> source;\n    try {\n      source = ReadTranslation.boundedSourceFromTransform(rootTransform);\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    SparkSession sparkSession = context.getSparkSession();\n\n    String serializedSource = Base64Serializer.serializeUnchecked(source);\n    Dataset<Row> rowDataset =\n        sparkSession\n            .read()\n            .format(sourceProviderClass)\n            .option(DatasetSourceBatch.BEAM_SOURCE_OPTION, serializedSource)\n            .option(\n                DatasetSourceBatch.DEFAULT_PARALLELISM,\n                String.valueOf(context.getSparkSession().sparkContext().defaultParallelism()))\n            .option(\n                DatasetSourceBatch.PIPELINE_OPTIONS, context.getSerializableOptions().toString())\n            .load();\n\n    \r\n    WindowedValue.FullWindowedValueCoder<T> windowedValueCoder =\n        WindowedValue.FullWindowedValueCoder.of(\n            source.getOutputCoder(), GlobalWindow.Coder.INSTANCE);\n\n    Dataset<WindowedValue<T>> dataset =\n        rowDataset.map(\n            RowHelpers.extractWindowedValueFromRowMapFunction(windowedValueCoder),\n            EncoderHelpers.fromBeamCoder(windowedValueCoder));\n\n    PCollection<T> output = (PCollection<T>) context.getOutput();\n    context.putDataset(output, dataset);\n  }\n","realPath":"runners/spark/src/main/java/org/apache/beam/runners/spark/structuredstreaming/translation/batch/ReadSourceTranslatorBatch.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":45,"status":"B"},{"authorDate":"2019-11-20 22:28:02","commitOrder":1,"curCode":"  public void translateTransform(\n      PTransform<PBegin, PCollection<T>> transform, TranslationContext context) {\n    AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>> rootTransform =\n        (AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>>)\n            context.getCurrentTransform();\n\n    UnboundedSource<T, UnboundedSource.CheckpointMark> source;\n    try {\n      source = ReadTranslation.unboundedSourceFromTransform(rootTransform);\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    SparkSession sparkSession = context.getSparkSession();\n\n    String serializedSource = Base64Serializer.serializeUnchecked(source);\n    Dataset<Row> rowDataset =\n        sparkSession\n            .readStream()\n            .format(sourceProviderClass)\n            .option(DatasetSourceStreaming.BEAM_SOURCE_OPTION, serializedSource)\n            .option(\n                DatasetSourceStreaming.DEFAULT_PARALLELISM,\n                String.valueOf(context.getSparkSession().sparkContext().defaultParallelism()))\n            .option(\n                DatasetSourceStreaming.PIPELINE_OPTIONS,\n                context.getSerializableOptions().toString())\n            .load();\n\n    \r\n    WindowedValue.FullWindowedValueCoder<T> windowedValueCoder =\n        WindowedValue.FullWindowedValueCoder.of(\n            source.getOutputCoder(), GlobalWindow.Coder.INSTANCE);\n    Dataset<WindowedValue<T>> dataset =\n        rowDataset.map(\n            RowHelpers.extractWindowedValueFromRowMapFunction(windowedValueCoder),\n            EncoderHelpers.fromBeamCoder(windowedValueCoder));\n\n    PCollection<T> output = (PCollection<T>) context.getOutput();\n    context.putDataset(output, dataset);\n  }\n","date":"2019-11-20 22:28:02","endLine":84,"groupId":"19074","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"translateTransform","params":"(PTransform<PBegin@PCollection<T>>transform@TranslationContextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/ea/10272507681778487f259fd7e32ffdf9525f5c.src","preCode":"  public void translateTransform(\n      PTransform<PBegin, PCollection<T>> transform, TranslationContext context) {\n    AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>> rootTransform =\n        (AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>>)\n            context.getCurrentTransform();\n\n    UnboundedSource<T, UnboundedSource.CheckpointMark> source;\n    try {\n      source = ReadTranslation.unboundedSourceFromTransform(rootTransform);\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    SparkSession sparkSession = context.getSparkSession();\n\n    String serializedSource = Base64Serializer.serializeUnchecked(source);\n    Dataset<Row> rowDataset =\n        sparkSession\n            .readStream()\n            .format(sourceProviderClass)\n            .option(DatasetSourceStreaming.BEAM_SOURCE_OPTION, serializedSource)\n            .option(\n                DatasetSourceStreaming.DEFAULT_PARALLELISM,\n                String.valueOf(context.getSparkSession().sparkContext().defaultParallelism()))\n            .option(\n                DatasetSourceStreaming.PIPELINE_OPTIONS,\n                context.getSerializableOptions().toString())\n            .load();\n\n    \r\n    WindowedValue.FullWindowedValueCoder<T> windowedValueCoder =\n        WindowedValue.FullWindowedValueCoder.of(\n            source.getOutputCoder(), GlobalWindow.Coder.INSTANCE);\n    Dataset<WindowedValue<T>> dataset =\n        rowDataset.map(\n            RowHelpers.extractWindowedValueFromRowMapFunction(windowedValueCoder),\n            EncoderHelpers.fromBeamCoder(windowedValueCoder));\n\n    PCollection<T> output = (PCollection<T>) context.getOutput();\n    context.putDataset(output, dataset);\n  }\n","realPath":"runners/spark/src/main/java/org/apache/beam/runners/spark/structuredstreaming/translation/streaming/ReadSourceTranslatorStreaming.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":45,"status":"B"}],"commitId":"18059eecad850b6e30bc7e376e70937915dd11fb","commitMessage":"@@@Merge pull request #9866: [BEAM-8470] Create a new Spark runner based on Spark Structured streaming framework\n\n","date":"2019-11-20 22:28:02","modifiedFileCount":"3","status":"B","submitter":"Alexey Romanenko"},{"authorTime":"2021-08-05 17:59:06","codes":[{"authorDate":"2021-08-05 17:59:06","commitOrder":2,"curCode":"  public void translateTransform(\n      PTransform<PBegin, PCollection<T>> transform, AbstractTranslationContext context) {\n    AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>> rootTransform =\n        (AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>>)\n            context.getCurrentTransform();\n\n    BoundedSource<T> source;\n    try {\n      source = ReadTranslation.boundedSourceFromTransform(rootTransform);\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    SparkSession sparkSession = context.getSparkSession();\n\n    String serializedSource = Base64Serializer.serializeUnchecked(source);\n    Dataset<Row> rowDataset =\n        sparkSession\n            .read()\n            .format(sourceProviderClass)\n            .option(BEAM_SOURCE_OPTION, serializedSource)\n            .option(\n                DEFAULT_PARALLELISM,\n                String.valueOf(context.getSparkSession().sparkContext().defaultParallelism()))\n            .option(\n                PIPELINE_OPTIONS, context.getSerializableOptions().toString())\n            .load();\n\n    \r\n    WindowedValue.FullWindowedValueCoder<T> windowedValueCoder =\n        WindowedValue.FullWindowedValueCoder.of(\n            source.getOutputCoder(), GlobalWindow.Coder.INSTANCE);\n\n    Dataset<WindowedValue<T>> dataset =\n        rowDataset.map(\n            RowHelpers.extractWindowedValueFromRowMapFunction(windowedValueCoder),\n            EncoderHelpers.fromBeamCoder(windowedValueCoder));\n\n    PCollection<T> output = (PCollection<T>) context.getOutput();\n    context.putDataset(output, dataset);\n  }\n","date":"2021-08-05 17:59:06","endLine":88,"groupId":"10141","id":3,"instanceNumber":1,"isCurCommit":1,"methodName":"translateTransform","params":"(PTransform<PBegin@PCollection<T>>transform@AbstractTranslationContextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/c0/7e882d6d13dd48e11d113f09ece406a0484f20.src","preCode":"  public void translateTransform(\n      PTransform<PBegin, PCollection<T>> transform, TranslationContext context) {\n    AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>> rootTransform =\n        (AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>>)\n            context.getCurrentTransform();\n\n    BoundedSource<T> source;\n    try {\n      source = ReadTranslation.boundedSourceFromTransform(rootTransform);\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    SparkSession sparkSession = context.getSparkSession();\n\n    String serializedSource = Base64Serializer.serializeUnchecked(source);\n    Dataset<Row> rowDataset =\n        sparkSession\n            .read()\n            .format(sourceProviderClass)\n            .option(DatasetSourceBatch.BEAM_SOURCE_OPTION, serializedSource)\n            .option(\n                DatasetSourceBatch.DEFAULT_PARALLELISM,\n                String.valueOf(context.getSparkSession().sparkContext().defaultParallelism()))\n            .option(\n                DatasetSourceBatch.PIPELINE_OPTIONS, context.getSerializableOptions().toString())\n            .load();\n\n    \r\n    WindowedValue.FullWindowedValueCoder<T> windowedValueCoder =\n        WindowedValue.FullWindowedValueCoder.of(\n            source.getOutputCoder(), GlobalWindow.Coder.INSTANCE);\n\n    Dataset<WindowedValue<T>> dataset =\n        rowDataset.map(\n            RowHelpers.extractWindowedValueFromRowMapFunction(windowedValueCoder),\n            EncoderHelpers.fromBeamCoder(windowedValueCoder));\n\n    PCollection<T> output = (PCollection<T>) context.getOutput();\n    context.putDataset(output, dataset);\n  }\n","realPath":"runners/spark/src/main/java/org/apache/beam/runners/spark/structuredstreaming/translation/batch/ReadSourceTranslatorBatch.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":49,"status":"M"},{"authorDate":"2021-08-05 17:59:06","commitOrder":2,"curCode":"  public void translateTransform(\n      PTransform<PBegin, PCollection<T>> transform, AbstractTranslationContext context) {\n    AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>> rootTransform =\n        (AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>>)\n            context.getCurrentTransform();\n\n    UnboundedSource<T, UnboundedSource.CheckpointMark> source;\n    try {\n      source = ReadTranslation.unboundedSourceFromTransform(rootTransform);\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    SparkSession sparkSession = context.getSparkSession();\n\n    String serializedSource = Base64Serializer.serializeUnchecked(source);\n    Dataset<Row> rowDataset =\n        sparkSession\n            .readStream()\n            .format(sourceProviderClass)\n            .option(BEAM_SOURCE_OPTION, serializedSource)\n            .option(\n                DEFAULT_PARALLELISM,\n                String.valueOf(context.getSparkSession().sparkContext().defaultParallelism()))\n            .option(\n                PIPELINE_OPTIONS,\n                context.getSerializableOptions().toString())\n            .load();\n\n    \r\n    WindowedValue.FullWindowedValueCoder<T> windowedValueCoder =\n        WindowedValue.FullWindowedValueCoder.of(\n            source.getOutputCoder(), GlobalWindow.Coder.INSTANCE);\n    Dataset<WindowedValue<T>> dataset =\n        rowDataset.map(\n            RowHelpers.extractWindowedValueFromRowMapFunction(windowedValueCoder),\n            EncoderHelpers.fromBeamCoder(windowedValueCoder));\n\n    PCollection<T> output = (PCollection<T>) context.getOutput();\n    context.putDataset(output, dataset);\n  }\n","date":"2021-08-05 17:59:06","endLine":88,"groupId":"10141","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"translateTransform","params":"(PTransform<PBegin@PCollection<T>>transform@AbstractTranslationContextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-beam-10-0.7/blobInfo/CC_OUT/blobs/23/b1d97968b1166060c6ee871f06e19bc19cc5f8.src","preCode":"  public void translateTransform(\n      PTransform<PBegin, PCollection<T>> transform, TranslationContext context) {\n    AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>> rootTransform =\n        (AppliedPTransform<PBegin, PCollection<T>, PTransform<PBegin, PCollection<T>>>)\n            context.getCurrentTransform();\n\n    UnboundedSource<T, UnboundedSource.CheckpointMark> source;\n    try {\n      source = ReadTranslation.unboundedSourceFromTransform(rootTransform);\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    SparkSession sparkSession = context.getSparkSession();\n\n    String serializedSource = Base64Serializer.serializeUnchecked(source);\n    Dataset<Row> rowDataset =\n        sparkSession\n            .readStream()\n            .format(sourceProviderClass)\n            .option(DatasetSourceStreaming.BEAM_SOURCE_OPTION, serializedSource)\n            .option(\n                DatasetSourceStreaming.DEFAULT_PARALLELISM,\n                String.valueOf(context.getSparkSession().sparkContext().defaultParallelism()))\n            .option(\n                DatasetSourceStreaming.PIPELINE_OPTIONS,\n                context.getSerializableOptions().toString())\n            .load();\n\n    \r\n    WindowedValue.FullWindowedValueCoder<T> windowedValueCoder =\n        WindowedValue.FullWindowedValueCoder.of(\n            source.getOutputCoder(), GlobalWindow.Coder.INSTANCE);\n    Dataset<WindowedValue<T>> dataset =\n        rowDataset.map(\n            RowHelpers.extractWindowedValueFromRowMapFunction(windowedValueCoder),\n            EncoderHelpers.fromBeamCoder(windowedValueCoder));\n\n    PCollection<T> output = (PCollection<T>) context.getOutput();\n    context.putDataset(output, dataset);\n  }\n","realPath":"runners/spark/src/main/java/org/apache/beam/runners/spark/structuredstreaming/translation/streaming/ReadSourceTranslatorStreaming.java","repoName":"beam","snippetEndLine":0,"snippetStartLine":0,"startLine":49,"status":"M"}],"commitId":"2144cab210d4608d10a0855eeef45ce4e113e074","commitMessage":"@@@Merge pull request #15218 from echauchot/BEAM-7093-spark3-fix-for-SS-runner\n\nMerge pull request #15218: [BEAM-7093] Migrate spark structured streaming runner to spark 3","date":"2021-08-05 17:59:06","modifiedFileCount":"3","status":"M","submitter":"Etienne Chauchot"}]
