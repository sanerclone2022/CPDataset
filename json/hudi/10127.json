[{"authorTime":"2020-12-25 22:43:34","codes":[{"authorDate":"2021-01-06 23:07:24","commitOrder":3,"curCode":"  public void testGlobalFailure() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[0];\n\n    String instantTime = \"001\";\n    HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), STRUCT_TYPE);\n\n    int size = 10 + RANDOM.nextInt(100);\n    int totalFailures = 5;\n    \r\n    Dataset<Row> inputRows = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    List<InternalRow> internalRows = toInternalRows(inputRows, ENCODER);\n\n    \r\n    for (int i = 0; i < totalFailures; i++) {\n      internalRows.add(getInternalRowWithError(partitionPath));\n    }\n\n    \r\n    Dataset<Row> inputRows2 = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    internalRows.addAll(toInternalRows(inputRows2, ENCODER));\n\n    \r\n    try {\n      for (InternalRow internalRow : internalRows) {\n        writer.write(internalRow);\n      }\n      fail(\"Should have failed\");\n    } catch (Throwable e) {\n      \r\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n\n    Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n    Option<List<String>> fileNames = Option.of(new ArrayList<>());\n    \r\n    assertWriteStatuses(commitMetadata.getWriteStatuses(), 1, size / 2, fileAbsPaths, fileNames);\n\n    \r\n    Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n    assertOutput(inputRows, result, instantTime, fileNames);\n  }\n","date":"2021-01-06 23:07:24","endLine":140,"groupId":"2274","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testGlobalFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ff/b649bd3970ad5f72990825371a4af8a99afd03.src","preCode":"  public void testGlobalFailure() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[0];\n\n    String instantTime = \"001\";\n    HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), STRUCT_TYPE);\n\n    int size = 10 + RANDOM.nextInt(100);\n    int totalFailures = 5;\n    \r\n    Dataset<Row> inputRows = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    List<InternalRow> internalRows = toInternalRows(inputRows, ENCODER);\n\n    \r\n    for (int i = 0; i < totalFailures; i++) {\n      internalRows.add(getInternalRowWithError(partitionPath));\n    }\n\n    \r\n    Dataset<Row> inputRows2 = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    internalRows.addAll(toInternalRows(inputRows2, ENCODER));\n\n    \r\n    try {\n      for (InternalRow internalRow : internalRows) {\n        writer.write(internalRow);\n      }\n      fail(\"Should have failed\");\n    } catch (Throwable e) {\n      \r\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n\n    Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n    Option<List<String>> fileNames = Option.of(new ArrayList<>());\n    \r\n    assertWriteStatuses(commitMetadata.getWriteStatuses(), 1, size / 2, fileAbsPaths, fileNames);\n\n    \r\n    Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n    assertOutput(inputRows, result, instantTime, fileNames);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieBulkInsertDataInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":96,"status":"B"},{"authorDate":"2020-12-25 22:43:34","commitOrder":3,"curCode":"  public void testGlobalFailure() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[0];\n\n    String instantTime = \"001\";\n    HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), RANDOM.nextLong(), STRUCT_TYPE);\n\n    int size = 10 + RANDOM.nextInt(100);\n    int totalFailures = 5;\n    \r\n    Dataset<Row> inputRows = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    List<InternalRow> internalRows = toInternalRows(inputRows, ENCODER);\n\n    \r\n    for (int i = 0; i < totalFailures; i++) {\n      internalRows.add(getInternalRowWithError(partitionPath));\n    }\n\n    \r\n    Dataset<Row> inputRows2 = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    internalRows.addAll(toInternalRows(inputRows2, ENCODER));\n\n    \r\n    try {\n      for (InternalRow internalRow : internalRows) {\n        writer.write(internalRow);\n      }\n      fail(\"Should have failed\");\n    } catch (Throwable e) {\n      \r\n    }\n\n    BaseWriterCommitMessage commitMetadata = (BaseWriterCommitMessage) writer.commit();\n\n    Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n    Option<List<String>> fileNames = Option.of(new ArrayList<>());\n    \r\n    assertWriteStatuses(commitMetadata.getWriteStatuses(), 1, size / 2, fileAbsPaths, fileNames);\n\n    \r\n    Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n    assertOutput(inputRows, result, instantTime, fileNames);\n  }\n","date":"2020-12-25 22:43:34","endLine":139,"groupId":"2274","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testGlobalFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/0b/021abeb3b5fe2deced8f07df8f6bac397d52b6.src","preCode":"  public void testGlobalFailure() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[0];\n\n    String instantTime = \"001\";\n    HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), RANDOM.nextLong(), STRUCT_TYPE);\n\n    int size = 10 + RANDOM.nextInt(100);\n    int totalFailures = 5;\n    \r\n    Dataset<Row> inputRows = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    List<InternalRow> internalRows = toInternalRows(inputRows, ENCODER);\n\n    \r\n    for (int i = 0; i < totalFailures; i++) {\n      internalRows.add(getInternalRowWithError(partitionPath));\n    }\n\n    \r\n    Dataset<Row> inputRows2 = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    internalRows.addAll(toInternalRows(inputRows2, ENCODER));\n\n    \r\n    try {\n      for (InternalRow internalRow : internalRows) {\n        writer.write(internalRow);\n      }\n      fail(\"Should have failed\");\n    } catch (Throwable e) {\n      \r\n    }\n\n    BaseWriterCommitMessage commitMetadata = (BaseWriterCommitMessage) writer.commit();\n\n    Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n    Option<List<String>> fileNames = Option.of(new ArrayList<>());\n    \r\n    assertWriteStatuses(commitMetadata.getWriteStatuses(), 1, size / 2, fileAbsPaths, fileNames);\n\n    \r\n    Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n    assertOutput(inputRows, result, instantTime, fileNames);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieBulkInsertDataInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":95,"status":"NB"}],"commitId":"b593f1062931a4d017ae8bd7dd42e47a8873a39f","commitMessage":"@@@[MINOR] Rename unit test package of hudi-spark3 from scala to java (#2411)\n\n","date":"2021-01-06 23:07:24","modifiedFileCount":"0","status":"M","submitter":"wangxianghu"},{"authorTime":"2021-07-07 23:15:25","codes":[{"authorDate":"2021-07-07 23:15:25","commitOrder":4,"curCode":"  public void testGlobalFailure() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[0];\n\n    String instantTime = \"001\";\n    HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), STRUCT_TYPE, false);\n\n    int size = 10 + RANDOM.nextInt(100);\n    int totalFailures = 5;\n    \r\n    Dataset<Row> inputRows = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    List<InternalRow> internalRows = toInternalRows(inputRows, ENCODER);\n\n    \r\n    for (int i = 0; i < totalFailures; i++) {\n      internalRows.add(getInternalRowWithError(partitionPath));\n    }\n\n    \r\n    Dataset<Row> inputRows2 = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    internalRows.addAll(toInternalRows(inputRows2, ENCODER));\n\n    \r\n    try {\n      for (InternalRow internalRow : internalRows) {\n        writer.write(internalRow);\n      }\n      fail(\"Should have failed\");\n    } catch (Throwable e) {\n      \r\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n\n    Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n    Option<List<String>> fileNames = Option.of(new ArrayList<>());\n    \r\n    assertWriteStatuses(commitMetadata.getWriteStatuses(), 1, size / 2, fileAbsPaths, fileNames);\n\n    \r\n    Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n    assertOutput(inputRows, result, instantTime, fileNames);\n  }\n","date":"2021-07-07 23:15:25","endLine":154,"groupId":"2274","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testGlobalFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/26/685fd8fc567b17298946a48330a93fd8f57ccd.src","preCode":"  public void testGlobalFailure() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[0];\n\n    String instantTime = \"001\";\n    HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), STRUCT_TYPE);\n\n    int size = 10 + RANDOM.nextInt(100);\n    int totalFailures = 5;\n    \r\n    Dataset<Row> inputRows = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    List<InternalRow> internalRows = toInternalRows(inputRows, ENCODER);\n\n    \r\n    for (int i = 0; i < totalFailures; i++) {\n      internalRows.add(getInternalRowWithError(partitionPath));\n    }\n\n    \r\n    Dataset<Row> inputRows2 = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    internalRows.addAll(toInternalRows(inputRows2, ENCODER));\n\n    \r\n    try {\n      for (InternalRow internalRow : internalRows) {\n        writer.write(internalRow);\n      }\n      fail(\"Should have failed\");\n    } catch (Throwable e) {\n      \r\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n\n    Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n    Option<List<String>> fileNames = Option.of(new ArrayList<>());\n    \r\n    assertWriteStatuses(commitMetadata.getWriteStatuses(), 1, size / 2, fileAbsPaths, fileNames);\n\n    \r\n    Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n    assertOutput(inputRows, result, instantTime, fileNames);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieBulkInsertDataInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":110,"status":"M"},{"authorDate":"2021-07-07 23:15:25","commitOrder":4,"curCode":"  public void testGlobalFailure() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[0];\n\n    String instantTime = \"001\";\n    HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), RANDOM.nextLong(), STRUCT_TYPE,\n        false);\n\n    int size = 10 + RANDOM.nextInt(100);\n    int totalFailures = 5;\n    \r\n    Dataset<Row> inputRows = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    List<InternalRow> internalRows = toInternalRows(inputRows, ENCODER);\n\n    \r\n    for (int i = 0; i < totalFailures; i++) {\n      internalRows.add(getInternalRowWithError(partitionPath));\n    }\n\n    \r\n    Dataset<Row> inputRows2 = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    internalRows.addAll(toInternalRows(inputRows2, ENCODER));\n\n    \r\n    try {\n      for (InternalRow internalRow : internalRows) {\n        writer.write(internalRow);\n      }\n      fail(\"Should have failed\");\n    } catch (Throwable e) {\n      \r\n    }\n\n    BaseWriterCommitMessage commitMetadata = (BaseWriterCommitMessage) writer.commit();\n\n    Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n    Option<List<String>> fileNames = Option.of(new ArrayList<>());\n    \r\n    assertWriteStatuses(commitMetadata.getWriteStatuses(), 1, size / 2, false, fileAbsPaths, fileNames);\n\n    \r\n    Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n    assertOutput(inputRows, result, instantTime, fileNames);\n  }\n","date":"2021-07-07 23:15:25","endLine":154,"groupId":"2274","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testGlobalFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/f7/7763f85d941ab01045bebe123f8d897cd18868.src","preCode":"  public void testGlobalFailure() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[0];\n\n    String instantTime = \"001\";\n    HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), RANDOM.nextLong(), STRUCT_TYPE);\n\n    int size = 10 + RANDOM.nextInt(100);\n    int totalFailures = 5;\n    \r\n    Dataset<Row> inputRows = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    List<InternalRow> internalRows = toInternalRows(inputRows, ENCODER);\n\n    \r\n    for (int i = 0; i < totalFailures; i++) {\n      internalRows.add(getInternalRowWithError(partitionPath));\n    }\n\n    \r\n    Dataset<Row> inputRows2 = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    internalRows.addAll(toInternalRows(inputRows2, ENCODER));\n\n    \r\n    try {\n      for (InternalRow internalRow : internalRows) {\n        writer.write(internalRow);\n      }\n      fail(\"Should have failed\");\n    } catch (Throwable e) {\n      \r\n    }\n\n    BaseWriterCommitMessage commitMetadata = (BaseWriterCommitMessage) writer.commit();\n\n    Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n    Option<List<String>> fileNames = Option.of(new ArrayList<>());\n    \r\n    assertWriteStatuses(commitMetadata.getWriteStatuses(), 1, size / 2, fileAbsPaths, fileNames);\n\n    \r\n    Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n    assertOutput(inputRows, result, instantTime, fileNames);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieBulkInsertDataInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":109,"status":"M"}],"commitId":"ea9e5d0e8b7557ef82631ac173d67f15bad13690","commitMessage":"@@@[HUDI-1104] Adding support for UserDefinedPartitioners and SortModes to BulkInsert with Rows (#3149)\n\n","date":"2021-07-07 23:15:25","modifiedFileCount":"21","status":"M","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-07-20 08:43:48","codes":[{"authorDate":"2021-07-20 08:43:48","commitOrder":5,"curCode":"  public void testGlobalFailure() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(true);\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[0];\n\n    String instantTime = \"001\";\n    HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000),\n        RANDOM.nextLong(), STRUCT_TYPE, true, false);\n\n    int size = 10 + RANDOM.nextInt(100);\n    int totalFailures = 5;\n    \r\n    Dataset<Row> inputRows = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    List<InternalRow> internalRows = toInternalRows(inputRows, ENCODER);\n\n    \r\n    for (int i = 0; i < totalFailures; i++) {\n      internalRows.add(getInternalRowWithError(partitionPath));\n    }\n\n    \r\n    Dataset<Row> inputRows2 = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    internalRows.addAll(toInternalRows(inputRows2, ENCODER));\n\n    \r\n    try {\n      for (InternalRow internalRow : internalRows) {\n        writer.write(internalRow);\n      }\n      fail(\"Should have failed\");\n    } catch (Throwable e) {\n      \r\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n\n    Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n    Option<List<String>> fileNames = Option.of(new ArrayList<>());\n    \r\n    assertWriteStatuses(commitMetadata.getWriteStatuses(), 1, size / 2, fileAbsPaths, fileNames);\n\n    \r\n    Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n    assertOutput(inputRows, result, instantTime, fileNames, true);\n  }\n","date":"2021-07-20 08:43:48","endLine":164,"groupId":"10127","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testGlobalFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/3b/7fb97ffa36657b4dd871d0576e1304fa94bd1c.src","preCode":"  public void testGlobalFailure() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[0];\n\n    String instantTime = \"001\";\n    HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), STRUCT_TYPE, false);\n\n    int size = 10 + RANDOM.nextInt(100);\n    int totalFailures = 5;\n    \r\n    Dataset<Row> inputRows = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    List<InternalRow> internalRows = toInternalRows(inputRows, ENCODER);\n\n    \r\n    for (int i = 0; i < totalFailures; i++) {\n      internalRows.add(getInternalRowWithError(partitionPath));\n    }\n\n    \r\n    Dataset<Row> inputRows2 = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    internalRows.addAll(toInternalRows(inputRows2, ENCODER));\n\n    \r\n    try {\n      for (InternalRow internalRow : internalRows) {\n        writer.write(internalRow);\n      }\n      fail(\"Should have failed\");\n    } catch (Throwable e) {\n      \r\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n\n    Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n    Option<List<String>> fileNames = Option.of(new ArrayList<>());\n    \r\n    assertWriteStatuses(commitMetadata.getWriteStatuses(), 1, size / 2, fileAbsPaths, fileNames);\n\n    \r\n    Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n    assertOutput(inputRows, result, instantTime, fileNames);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieBulkInsertDataInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":119,"status":"M"},{"authorDate":"2021-07-20 08:43:48","commitOrder":5,"curCode":"  public void testGlobalFailure() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(true);\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[0];\n\n    String instantTime = \"001\";\n    HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), RANDOM.nextLong(),\n        STRUCT_TYPE, true, false);\n\n    int size = 10 + RANDOM.nextInt(100);\n    int totalFailures = 5;\n    \r\n    Dataset<Row> inputRows = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    List<InternalRow> internalRows = toInternalRows(inputRows, ENCODER);\n\n    \r\n    for (int i = 0; i < totalFailures; i++) {\n      internalRows.add(getInternalRowWithError(partitionPath));\n    }\n\n    \r\n    Dataset<Row> inputRows2 = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    internalRows.addAll(toInternalRows(inputRows2, ENCODER));\n\n    \r\n    try {\n      for (InternalRow internalRow : internalRows) {\n        writer.write(internalRow);\n      }\n      fail(\"Should have failed\");\n    } catch (Throwable e) {\n      \r\n    }\n\n    BaseWriterCommitMessage commitMetadata = (BaseWriterCommitMessage) writer.commit();\n\n    Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n    Option<List<String>> fileNames = Option.of(new ArrayList<>());\n    \r\n    assertWriteStatuses(commitMetadata.getWriteStatuses(), 1, size / 2, false, fileAbsPaths, fileNames);\n\n    \r\n    Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n    assertOutput(inputRows, result, instantTime, fileNames, true);\n  }\n","date":"2021-07-20 08:43:48","endLine":162,"groupId":"10127","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testGlobalFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/97/35379d45f2f18faae7e02ce8e6f12fef9308cc.src","preCode":"  public void testGlobalFailure() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[0];\n\n    String instantTime = \"001\";\n    HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), RANDOM.nextLong(), STRUCT_TYPE,\n        false);\n\n    int size = 10 + RANDOM.nextInt(100);\n    int totalFailures = 5;\n    \r\n    Dataset<Row> inputRows = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    List<InternalRow> internalRows = toInternalRows(inputRows, ENCODER);\n\n    \r\n    for (int i = 0; i < totalFailures; i++) {\n      internalRows.add(getInternalRowWithError(partitionPath));\n    }\n\n    \r\n    Dataset<Row> inputRows2 = getRandomRows(sqlContext, size / 2, partitionPath, false);\n    internalRows.addAll(toInternalRows(inputRows2, ENCODER));\n\n    \r\n    try {\n      for (InternalRow internalRow : internalRows) {\n        writer.write(internalRow);\n      }\n      fail(\"Should have failed\");\n    } catch (Throwable e) {\n      \r\n    }\n\n    BaseWriterCommitMessage commitMetadata = (BaseWriterCommitMessage) writer.commit();\n\n    Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n    Option<List<String>> fileNames = Option.of(new ArrayList<>());\n    \r\n    assertWriteStatuses(commitMetadata.getWriteStatuses(), 1, size / 2, false, fileAbsPaths, fileNames);\n\n    \r\n    Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n    assertOutput(inputRows, result, instantTime, fileNames);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieBulkInsertDataInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":117,"status":"M"}],"commitId":"d5026e9a24850bdcce9f6df3686bf2235d7d01c4","commitMessage":"@@@[HUDI-2161] Adding support to disable meta columns with bulk insert operation (#3247)\n\n","date":"2021-07-20 08:43:48","modifiedFileCount":"39","status":"M","submitter":"Sivabalan Narayanan"}]
