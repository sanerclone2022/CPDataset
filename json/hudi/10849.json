[{"authorTime":"2021-01-28 02:09:51","codes":[{"authorDate":"2020-12-20 11:19:42","commitOrder":5,"curCode":"  public void write(GenericRecord oldRecord) {\n    String key = oldRecord.get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n    boolean copyOldRecord = true;\n    if (keyToNewRecords.containsKey(key)) {\n      \r\n      \r\n      HoodieRecord<T> hoodieRecord = new HoodieRecord<>(keyToNewRecords.get(key));\n      try {\n        Option<IndexedRecord> combinedAvroRecord =\n            hoodieRecord.getData().combineAndGetUpdateValue(oldRecord, useWriterSchema ? writerSchemaWithMetafields : writerSchema,\n                config.getPayloadConfig().getProps());\n        if (writeUpdateRecord(hoodieRecord, combinedAvroRecord)) {\n          \r\n\r\n\r\n\r\n\n          copyOldRecord = false;\n        }\n        writtenRecordKeys.add(key);\n      } catch (Exception e) {\n        throw new HoodieUpsertException(\"Failed to combine/merge new record with old value in storage, for new record {\"\n            + keyToNewRecords.get(key) + \"}, old value {\" + oldRecord + \"}\", e);\n      }\n    }\n\n    if (copyOldRecord) {\n      \r\n      try {\n        fileWriter.writeAvro(key, oldRecord);\n      } catch (IOException | RuntimeException e) {\n        String errMsg = String.format(\"Failed to merge old record into new file for key %s from old file %s to new file %s with writerSchema %s\",\n                key, getOldFilePath(), newFilePath, writerSchemaWithMetafields.toString(true));\n        LOG.debug(\"Old record is \" + oldRecord);\n        throw new HoodieUpsertException(errMsg, e);\n      }\n      recordsWritten++;\n    }\n  }\n","date":"2020-12-20 11:19:42","endLine":258,"groupId":"3371","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"write","params":"(GenericRecordoldRecord)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/1b/98de4398a8eb77d8615e7d1d64de3234a0532b.src","preCode":"  public void write(GenericRecord oldRecord) {\n    String key = oldRecord.get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n    boolean copyOldRecord = true;\n    if (keyToNewRecords.containsKey(key)) {\n      \r\n      \r\n      HoodieRecord<T> hoodieRecord = new HoodieRecord<>(keyToNewRecords.get(key));\n      try {\n        Option<IndexedRecord> combinedAvroRecord =\n            hoodieRecord.getData().combineAndGetUpdateValue(oldRecord, useWriterSchema ? writerSchemaWithMetafields : writerSchema,\n                config.getPayloadConfig().getProps());\n        if (writeUpdateRecord(hoodieRecord, combinedAvroRecord)) {\n          \r\n\r\n\r\n\r\n\n          copyOldRecord = false;\n        }\n        writtenRecordKeys.add(key);\n      } catch (Exception e) {\n        throw new HoodieUpsertException(\"Failed to combine/merge new record with old value in storage, for new record {\"\n            + keyToNewRecords.get(key) + \"}, old value {\" + oldRecord + \"}\", e);\n      }\n    }\n\n    if (copyOldRecord) {\n      \r\n      try {\n        fileWriter.writeAvro(key, oldRecord);\n      } catch (IOException | RuntimeException e) {\n        String errMsg = String.format(\"Failed to merge old record into new file for key %s from old file %s to new file %s with writerSchema %s\",\n                key, getOldFilePath(), newFilePath, writerSchemaWithMetafields.toString(true));\n        LOG.debug(\"Old record is \" + oldRecord);\n        throw new HoodieUpsertException(errMsg, e);\n      }\n      recordsWritten++;\n    }\n  }\n","realPath":"hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":219,"status":"NB"},{"authorDate":"2021-01-28 02:09:51","commitOrder":5,"curCode":"  public void write(GenericRecord oldRecord) {\n    String key = oldRecord.get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n    try {\n      fileWriter.writeAvro(key, oldRecord);\n    } catch (IOException | RuntimeException e) {\n      String errMsg = String.format(\"Failed to write old record into new file for key %s from old file %s to new file %s with writerSchema %s\",\n          key, getOldFilePath(), newFilePath, writerSchemaWithMetafields.toString(true));\n      LOG.debug(\"Old record is \" + oldRecord);\n      throw new HoodieUpsertException(errMsg, e);\n    }\n    recordsWritten++;\n  }\n","date":"2021-01-28 02:09:51","endLine":93,"groupId":"3373","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"write","params":"(GenericRecordoldRecord)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ea/56689b5364f98f3e0c09d07bf4fdc50a6eacfe.src","preCode":"  public void write(GenericRecord oldRecord) {\n    String key = oldRecord.get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n    try {\n      fileWriter.writeAvro(key, oldRecord);\n    } catch (IOException | RuntimeException e) {\n      String errMsg = String.format(\"Failed to write old record into new file for key %s from old file %s to new file %s with writerSchema %s\",\n          key, getOldFilePath(), newFilePath, writerSchemaWithMetafields.toString(true));\n      LOG.debug(\"Old record is \" + oldRecord);\n      throw new HoodieUpsertException(errMsg, e);\n    }\n    recordsWritten++;\n  }\n","realPath":"hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/storage/HoodieConcatHandle.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":82,"status":"B"}],"commitId":"2ee1c3fb0cb9740dc6d924f5e6e638aec19ed9b3","commitMessage":"@@@[HUDI-1234] Insert new records to data files without merging for \"Insert\" operation.  (#2111)\n\n* Added HoodieConcatHandle to skip merging for \"insert\" operation when the corresponding config is set\n\nCo-authored-by: Sivabalan Narayanan <sivabala@uber.com>","date":"2021-01-28 02:09:51","modifiedFileCount":"6","status":"M","submitter":"SteNicholas"},{"authorTime":"2021-06-08 14:24:32","codes":[{"authorDate":"2021-06-08 14:24:32","commitOrder":6,"curCode":"  public void write(GenericRecord oldRecord) {\n    String key = oldRecord.get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n    boolean copyOldRecord = true;\n    if (keyToNewRecords.containsKey(key)) {\n      \r\n      \r\n      HoodieRecord<T> hoodieRecord = new HoodieRecord<>(keyToNewRecords.get(key));\n      try {\n        Option<IndexedRecord> combinedAvroRecord =\n            hoodieRecord.getData().combineAndGetUpdateValue(oldRecord,\n              useWriterSchema ? tableSchemaWithMetaFields : tableSchema,\n                config.getPayloadConfig().getProps());\n\n        if (combinedAvroRecord.isPresent() && combinedAvroRecord.get().equals(IGNORE_RECORD)) {\n          \r\n          copyOldRecord = true;\n        } else if (writeUpdateRecord(hoodieRecord, combinedAvroRecord)) {\n          \r\n\r\n\r\n\r\n\r\n\n          copyOldRecord = false;\n        }\n        writtenRecordKeys.add(key);\n      } catch (Exception e) {\n        throw new HoodieUpsertException(\"Failed to combine/merge new record with old value in storage, for new record {\"\n            + keyToNewRecords.get(key) + \"}, old value {\" + oldRecord + \"}\", e);\n      }\n    }\n\n    if (copyOldRecord) {\n      \r\n      try {\n        fileWriter.writeAvro(key, oldRecord);\n      } catch (IOException | RuntimeException e) {\n        String errMsg = String.format(\"Failed to merge old record into new file for key %s from old file %s to new file %s with writerSchema %s\",\n                key, getOldFilePath(), newFilePath, writeSchemaWithMetaFields.toString(true));\n        LOG.debug(\"Old record is \" + oldRecord);\n        throw new HoodieUpsertException(errMsg, e);\n      }\n      recordsWritten++;\n    }\n  }\n","date":"2021-06-08 14:24:32","endLine":324,"groupId":"2680","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"write","params":"(GenericRecordoldRecord)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/95/421d98e920a86f6aa80535b883f57a78397475.src","preCode":"  public void write(GenericRecord oldRecord) {\n    String key = oldRecord.get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n    boolean copyOldRecord = true;\n    if (keyToNewRecords.containsKey(key)) {\n      \r\n      \r\n      HoodieRecord<T> hoodieRecord = new HoodieRecord<>(keyToNewRecords.get(key));\n      try {\n        Option<IndexedRecord> combinedAvroRecord =\n            hoodieRecord.getData().combineAndGetUpdateValue(oldRecord, useWriterSchema ? writerSchemaWithMetafields : writerSchema,\n                config.getPayloadConfig().getProps());\n        if (writeUpdateRecord(hoodieRecord, combinedAvroRecord)) {\n          \r\n\r\n\r\n\r\n\n          copyOldRecord = false;\n        }\n        writtenRecordKeys.add(key);\n      } catch (Exception e) {\n        throw new HoodieUpsertException(\"Failed to combine/merge new record with old value in storage, for new record {\"\n            + keyToNewRecords.get(key) + \"}, old value {\" + oldRecord + \"}\", e);\n      }\n    }\n\n    if (copyOldRecord) {\n      \r\n      try {\n        fileWriter.writeAvro(key, oldRecord);\n      } catch (IOException | RuntimeException e) {\n        String errMsg = String.format(\"Failed to merge old record into new file for key %s from old file %s to new file %s with writerSchema %s\",\n                key, getOldFilePath(), newFilePath, writerSchemaWithMetafields.toString(true));\n        LOG.debug(\"Old record is \" + oldRecord);\n        throw new HoodieUpsertException(errMsg, e);\n      }\n      recordsWritten++;\n    }\n  }\n","realPath":"hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":279,"status":"M"},{"authorDate":"2021-06-08 14:24:32","commitOrder":6,"curCode":"  public void write(GenericRecord oldRecord) {\n    String key = oldRecord.get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n    try {\n      fileWriter.writeAvro(key, oldRecord);\n    } catch (IOException | RuntimeException e) {\n      String errMsg = String.format(\"Failed to write old record into new file for key %s from old file %s to new file %s with writerSchema %s\",\n          key, getOldFilePath(), newFilePath, writeSchemaWithMetaFields.toString(true));\n      LOG.debug(\"Old record is \" + oldRecord);\n      throw new HoodieUpsertException(errMsg, e);\n    }\n    recordsWritten++;\n  }\n","date":"2021-06-08 14:24:32","endLine":93,"groupId":"3373","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"write","params":"(GenericRecordoldRecord)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/4d/21403136522ccc134baf32f18181bf303b153e.src","preCode":"  public void write(GenericRecord oldRecord) {\n    String key = oldRecord.get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n    try {\n      fileWriter.writeAvro(key, oldRecord);\n    } catch (IOException | RuntimeException e) {\n      String errMsg = String.format(\"Failed to write old record into new file for key %s from old file %s to new file %s with writerSchema %s\",\n          key, getOldFilePath(), newFilePath, writerSchemaWithMetafields.toString(true));\n      LOG.debug(\"Old record is \" + oldRecord);\n      throw new HoodieUpsertException(errMsg, e);\n    }\n    recordsWritten++;\n  }\n","realPath":"hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/storage/HoodieConcatHandle.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":82,"status":"M"}],"commitId":"f760ec543ec9ea23b7d4c9f61c76a283bd737f27","commitMessage":"@@@[HUDI-1659] Basic Implement Of Spark Sql Support For Hoodie (#2645)\n\nMain functions:\nSupport create table for hoodie.\nSupport CTAS.\nSupport Insert for hoodie. Including dynamic partition and static partition insert.\nSupport MergeInto for hoodie.\nSupport DELETE\nSupport UPDATE\nBoth support spark2 & spark3 based on DataSourceV1.\n\nMain changes:\nAdd sql parser for spark2.\nAdd HoodieAnalysis for sql resolve and logical plan rewrite.\nAdd commands implementation for CREATE TABLE?INSERT?MERGE INTO & CTAS.\nIn order to push down the update&insert logical to the HoodieRecordPayload for MergeInto.  I make same change to the\nHoodieWriteHandler and other related classes.\n1?Add the inputSchema for parser the incoming record. This is because the inputSchema for MergeInto is different from writeSchema as there are some transforms in the update& insert expression.\n2?Add WRITE_SCHEMA to HoodieWriteConfig to pass the write schema for merge into.\n3?Pass properties to HoodieRecordPayload#getInsertValue to pass the insert expression and table schema.\n\n\nVerify this pull request\nAdd TestCreateTable for test create hoodie tables and CTAS.\nAdd TestInsertTable for test insert hoodie tables.\nAdd TestMergeIntoTable for test merge hoodie tables.\nAdd TestUpdateTable for test update hoodie tables.\nAdd TestDeleteTable for test delete hoodie tables.\nAdd TestSqlStatement for test supported ddl/dml currently.","date":"2021-06-08 14:24:32","modifiedFileCount":"28","status":"M","submitter":"pengzhiwei"},{"authorTime":"2021-07-27 05:21:04","codes":[{"authorDate":"2021-07-27 05:21:04","commitOrder":7,"curCode":"  public void write(GenericRecord oldRecord) {\n    String key = KeyGenUtils.getRecordKeyFromGenericRecord(oldRecord, keyGeneratorOpt);\n    boolean copyOldRecord = true;\n    if (keyToNewRecords.containsKey(key)) {\n      \r\n      \r\n      HoodieRecord<T> hoodieRecord = new HoodieRecord<>(keyToNewRecords.get(key));\n      try {\n        Option<IndexedRecord> combinedAvroRecord =\n            hoodieRecord.getData().combineAndGetUpdateValue(oldRecord,\n              useWriterSchema ? tableSchemaWithMetaFields : tableSchema,\n                config.getPayloadConfig().getProps());\n\n        if (combinedAvroRecord.isPresent() && combinedAvroRecord.get().equals(IGNORE_RECORD)) {\n          \r\n          copyOldRecord = true;\n        } else if (writeUpdateRecord(hoodieRecord, combinedAvroRecord)) {\n          \r\n\r\n\r\n\r\n\r\n\n          copyOldRecord = false;\n        }\n        writtenRecordKeys.add(key);\n      } catch (Exception e) {\n        throw new HoodieUpsertException(\"Failed to combine/merge new record with old value in storage, for new record {\"\n            + keyToNewRecords.get(key) + \"}, old value {\" + oldRecord + \"}\", e);\n      }\n    }\n\n    if (copyOldRecord) {\n      \r\n      try {\n        fileWriter.writeAvro(key, oldRecord);\n      } catch (IOException | RuntimeException e) {\n        String errMsg = String.format(\"Failed to merge old record into new file for key %s from old file %s to new file %s with writerSchema %s\",\n                key, getOldFilePath(), newFilePath, writeSchemaWithMetaFields.toString(true));\n        LOG.debug(\"Old record is \" + oldRecord);\n        throw new HoodieUpsertException(errMsg, e);\n      }\n      recordsWritten++;\n    }\n  }\n","date":"2021-07-27 05:21:04","endLine":336,"groupId":"10849","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"write","params":"(GenericRecordoldRecord)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/84/141f33d7422d7297a0c739ab32fe462b3abc4e.src","preCode":"  public void write(GenericRecord oldRecord) {\n    String key = oldRecord.get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n    boolean copyOldRecord = true;\n    if (keyToNewRecords.containsKey(key)) {\n      \r\n      \r\n      HoodieRecord<T> hoodieRecord = new HoodieRecord<>(keyToNewRecords.get(key));\n      try {\n        Option<IndexedRecord> combinedAvroRecord =\n            hoodieRecord.getData().combineAndGetUpdateValue(oldRecord,\n              useWriterSchema ? tableSchemaWithMetaFields : tableSchema,\n                config.getPayloadConfig().getProps());\n\n        if (combinedAvroRecord.isPresent() && combinedAvroRecord.get().equals(IGNORE_RECORD)) {\n          \r\n          copyOldRecord = true;\n        } else if (writeUpdateRecord(hoodieRecord, combinedAvroRecord)) {\n          \r\n\r\n\r\n\r\n\r\n\n          copyOldRecord = false;\n        }\n        writtenRecordKeys.add(key);\n      } catch (Exception e) {\n        throw new HoodieUpsertException(\"Failed to combine/merge new record with old value in storage, for new record {\"\n            + keyToNewRecords.get(key) + \"}, old value {\" + oldRecord + \"}\", e);\n      }\n    }\n\n    if (copyOldRecord) {\n      \r\n      try {\n        fileWriter.writeAvro(key, oldRecord);\n      } catch (IOException | RuntimeException e) {\n        String errMsg = String.format(\"Failed to merge old record into new file for key %s from old file %s to new file %s with writerSchema %s\",\n                key, getOldFilePath(), newFilePath, writeSchemaWithMetaFields.toString(true));\n        LOG.debug(\"Old record is \" + oldRecord);\n        throw new HoodieUpsertException(errMsg, e);\n      }\n      recordsWritten++;\n    }\n  }\n","realPath":"hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":291,"status":"M"},{"authorDate":"2021-07-27 05:21:04","commitOrder":7,"curCode":"  public void write(GenericRecord oldRecord) {\n    String key = KeyGenUtils.getRecordKeyFromGenericRecord(oldRecord, keyGeneratorOpt);\n    try {\n      fileWriter.writeAvro(key, oldRecord);\n    } catch (IOException | RuntimeException e) {\n      String errMsg = String.format(\"Failed to write old record into new file for key %s from old file %s to new file %s with writerSchema %s\",\n          key, getOldFilePath(), newFilePath, writeSchemaWithMetaFields.toString(true));\n      LOG.debug(\"Old record is \" + oldRecord);\n      throw new HoodieUpsertException(errMsg, e);\n    }\n    recordsWritten++;\n  }\n","date":"2021-07-27 05:21:04","endLine":96,"groupId":"10849","id":6,"instanceNumber":2,"isCurCommit":1,"methodName":"write","params":"(GenericRecordoldRecord)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/04/00608860496ec6868d3180e554af10ccdbdf76.src","preCode":"  public void write(GenericRecord oldRecord) {\n    String key = oldRecord.get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n    try {\n      fileWriter.writeAvro(key, oldRecord);\n    } catch (IOException | RuntimeException e) {\n      String errMsg = String.format(\"Failed to write old record into new file for key %s from old file %s to new file %s with writerSchema %s\",\n          key, getOldFilePath(), newFilePath, writeSchemaWithMetaFields.toString(true));\n      LOG.debug(\"Old record is \" + oldRecord);\n      throw new HoodieUpsertException(errMsg, e);\n    }\n    recordsWritten++;\n  }\n","realPath":"hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/storage/HoodieConcatHandle.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":85,"status":"M"}],"commitId":"61148c1c43c9ff5ba04b6c174e9e2a006db9e7b3","commitMessage":"@@@[HUDI-2176.  2178.  2179] Adding virtual key support to COW table (#3306)\n\n","date":"2021-07-27 05:21:04","modifiedFileCount":"42","status":"M","submitter":"Sivabalan Narayanan"}]
