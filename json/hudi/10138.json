[{"authorTime":"2021-08-05 07:08:50","codes":[{"authorDate":"2021-08-05 07:08:50","commitOrder":1,"curCode":"  public void testBulkInsertHelper() {\n    HoodieWriteConfig config = getConfigBuilder(schemaStr).withProps(getPropsAllSet()).combineInput(false, false).build();\n    List<Row> rows = DataSourceTestUtils.generateRandomRows(10);\n    Dataset<Row> dataset = sqlContext.createDataFrame(rows, structType);\n    Dataset<Row> result = HoodieDatasetBulkInsertHelper.prepareHoodieDatasetForBulkInsert(sqlContext, config, dataset, \"testStructName\",\n        \"testNamespace\", new NonSortPartitionerWithRows(), false);\n    StructType resultSchema = result.schema();\n\n    assertEquals(result.count(), 10);\n    assertEquals(resultSchema.fieldNames().length, structType.fieldNames().length + HoodieRecord.HOODIE_META_COLUMNS.size());\n\n    for (Map.Entry<String, Integer> entry : HoodieRecord.HOODIE_META_COLUMNS_NAME_TO_POS.entrySet()) {\n      assertTrue(resultSchema.fieldIndex(entry.getKey()) == entry.getValue());\n    }\n\n    result.toJavaRDD().foreach(entry -> {\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.RECORD_KEY_METADATA_FIELD)).equals(entry.getAs(\"_row_key\")));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.PARTITION_PATH_METADATA_FIELD)).equals(entry.getAs(\"partition\")));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.COMMIT_TIME_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.FILENAME_METADATA_FIELD)).equals(\"\"));\n    });\n\n    Dataset<Row> trimmedOutput = result.drop(HoodieRecord.PARTITION_PATH_METADATA_FIELD).drop(HoodieRecord.RECORD_KEY_METADATA_FIELD)\n        .drop(HoodieRecord.FILENAME_METADATA_FIELD).drop(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD).drop(HoodieRecord.COMMIT_TIME_METADATA_FIELD);\n    assertTrue(dataset.except(trimmedOutput).count() == 0);\n  }\n","date":"2021-08-05 07:08:50","endLine":119,"groupId":"3044","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testBulkInsertHelper","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/69/836c70abe1b64077962aad4075bbf4eb7de378.src","preCode":"  public void testBulkInsertHelper() {\n    HoodieWriteConfig config = getConfigBuilder(schemaStr).withProps(getPropsAllSet()).combineInput(false, false).build();\n    List<Row> rows = DataSourceTestUtils.generateRandomRows(10);\n    Dataset<Row> dataset = sqlContext.createDataFrame(rows, structType);\n    Dataset<Row> result = HoodieDatasetBulkInsertHelper.prepareHoodieDatasetForBulkInsert(sqlContext, config, dataset, \"testStructName\",\n        \"testNamespace\", new NonSortPartitionerWithRows(), false);\n    StructType resultSchema = result.schema();\n\n    assertEquals(result.count(), 10);\n    assertEquals(resultSchema.fieldNames().length, structType.fieldNames().length + HoodieRecord.HOODIE_META_COLUMNS.size());\n\n    for (Map.Entry<String, Integer> entry : HoodieRecord.HOODIE_META_COLUMNS_NAME_TO_POS.entrySet()) {\n      assertTrue(resultSchema.fieldIndex(entry.getKey()) == entry.getValue());\n    }\n\n    result.toJavaRDD().foreach(entry -> {\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.RECORD_KEY_METADATA_FIELD)).equals(entry.getAs(\"_row_key\")));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.PARTITION_PATH_METADATA_FIELD)).equals(entry.getAs(\"partition\")));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.COMMIT_TIME_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.FILENAME_METADATA_FIELD)).equals(\"\"));\n    });\n\n    Dataset<Row> trimmedOutput = result.drop(HoodieRecord.PARTITION_PATH_METADATA_FIELD).drop(HoodieRecord.RECORD_KEY_METADATA_FIELD)\n        .drop(HoodieRecord.FILENAME_METADATA_FIELD).drop(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD).drop(HoodieRecord.COMMIT_TIME_METADATA_FIELD);\n    assertTrue(dataset.except(trimmedOutput).count() == 0);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestHoodieDatasetBulkInsertHelper.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":93,"status":"B"},{"authorDate":"2021-08-05 07:08:50","commitOrder":1,"curCode":"  public void testBulkInsertHelperNoMetaFields() {\n    List<Row> rows = DataSourceTestUtils.generateRandomRows(10);\n    Dataset<Row> dataset = sqlContext.createDataFrame(rows, structType);\n    Dataset<Row> result = HoodieDatasetBulkInsertHelper.prepareHoodieDatasetForBulkInsertWithoutMetaFields(dataset);\n    StructType resultSchema = result.schema();\n\n    assertEquals(result.count(), 10);\n    assertEquals(resultSchema.fieldNames().length, structType.fieldNames().length + HoodieRecord.HOODIE_META_COLUMNS.size());\n\n    for (Map.Entry<String, Integer> entry : HoodieRecord.HOODIE_META_COLUMNS_NAME_TO_POS.entrySet()) {\n      assertTrue(resultSchema.fieldIndex(entry.getKey()) == entry.getValue());\n    }\n\n    result.toJavaRDD().foreach(entry -> {\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.RECORD_KEY_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.PARTITION_PATH_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.COMMIT_TIME_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.FILENAME_METADATA_FIELD)).equals(\"\"));\n    });\n\n    Dataset<Row> trimmedOutput = result.drop(HoodieRecord.PARTITION_PATH_METADATA_FIELD).drop(HoodieRecord.RECORD_KEY_METADATA_FIELD)\n        .drop(HoodieRecord.FILENAME_METADATA_FIELD).drop(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD).drop(HoodieRecord.COMMIT_TIME_METADATA_FIELD);\n    assertTrue(dataset.except(trimmedOutput).count() == 0);\n  }\n","date":"2021-08-05 07:08:50","endLine":146,"groupId":"3241","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testBulkInsertHelperNoMetaFields","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/69/836c70abe1b64077962aad4075bbf4eb7de378.src","preCode":"  public void testBulkInsertHelperNoMetaFields() {\n    List<Row> rows = DataSourceTestUtils.generateRandomRows(10);\n    Dataset<Row> dataset = sqlContext.createDataFrame(rows, structType);\n    Dataset<Row> result = HoodieDatasetBulkInsertHelper.prepareHoodieDatasetForBulkInsertWithoutMetaFields(dataset);\n    StructType resultSchema = result.schema();\n\n    assertEquals(result.count(), 10);\n    assertEquals(resultSchema.fieldNames().length, structType.fieldNames().length + HoodieRecord.HOODIE_META_COLUMNS.size());\n\n    for (Map.Entry<String, Integer> entry : HoodieRecord.HOODIE_META_COLUMNS_NAME_TO_POS.entrySet()) {\n      assertTrue(resultSchema.fieldIndex(entry.getKey()) == entry.getValue());\n    }\n\n    result.toJavaRDD().foreach(entry -> {\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.RECORD_KEY_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.PARTITION_PATH_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.COMMIT_TIME_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.FILENAME_METADATA_FIELD)).equals(\"\"));\n    });\n\n    Dataset<Row> trimmedOutput = result.drop(HoodieRecord.PARTITION_PATH_METADATA_FIELD).drop(HoodieRecord.RECORD_KEY_METADATA_FIELD)\n        .drop(HoodieRecord.FILENAME_METADATA_FIELD).drop(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD).drop(HoodieRecord.COMMIT_TIME_METADATA_FIELD);\n    assertTrue(dataset.except(trimmedOutput).count() == 0);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestHoodieDatasetBulkInsertHelper.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":122,"status":"B"}],"commitId":"1df5ded433eced3bbb66137bc741e616beab9a70","commitMessage":"@@@[HUDI-2273] Migrating some long running tests to functional test profile (#3398)\n\n","date":"2021-08-05 07:08:50","modifiedFileCount":"1","status":"B","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-08-05 07:08:50","codes":[{"authorDate":"2021-08-14 01:01:26","commitOrder":2,"curCode":"  public void testBulkInsertHelper() {\n    HoodieWriteConfig config = getConfigBuilder(schemaStr).withProps(getPropsAllSet()).combineInput(false, false).build();\n    List<Row> rows = DataSourceTestUtils.generateRandomRows(10);\n    Dataset<Row> dataset = sqlContext.createDataFrame(rows, structType);\n    Dataset<Row> result = HoodieDatasetBulkInsertHelper.prepareHoodieDatasetForBulkInsert(sqlContext, config, dataset, \"testStructName\",\n        \"testNamespace\", new NonSortPartitionerWithRows(), false, false);\n    StructType resultSchema = result.schema();\n\n    assertEquals(result.count(), 10);\n    assertEquals(resultSchema.fieldNames().length, structType.fieldNames().length + HoodieRecord.HOODIE_META_COLUMNS.size());\n\n    for (Map.Entry<String, Integer> entry : HoodieRecord.HOODIE_META_COLUMNS_NAME_TO_POS.entrySet()) {\n      assertTrue(resultSchema.fieldIndex(entry.getKey()) == entry.getValue());\n    }\n\n    result.toJavaRDD().foreach(entry -> {\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.RECORD_KEY_METADATA_FIELD)).equals(entry.getAs(\"_row_key\")));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.PARTITION_PATH_METADATA_FIELD)).equals(entry.getAs(\"partition\")));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.COMMIT_TIME_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.FILENAME_METADATA_FIELD)).equals(\"\"));\n    });\n\n    Dataset<Row> trimmedOutput = result.drop(HoodieRecord.PARTITION_PATH_METADATA_FIELD).drop(HoodieRecord.RECORD_KEY_METADATA_FIELD)\n        .drop(HoodieRecord.FILENAME_METADATA_FIELD).drop(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD).drop(HoodieRecord.COMMIT_TIME_METADATA_FIELD);\n    assertTrue(dataset.except(trimmedOutput).count() == 0);\n  }\n","date":"2021-08-14 01:01:26","endLine":119,"groupId":"10138","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testBulkInsertHelper","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/07/1a12cbcf954b32bb53d91a0f3243c756081f8c.src","preCode":"  public void testBulkInsertHelper() {\n    HoodieWriteConfig config = getConfigBuilder(schemaStr).withProps(getPropsAllSet()).combineInput(false, false).build();\n    List<Row> rows = DataSourceTestUtils.generateRandomRows(10);\n    Dataset<Row> dataset = sqlContext.createDataFrame(rows, structType);\n    Dataset<Row> result = HoodieDatasetBulkInsertHelper.prepareHoodieDatasetForBulkInsert(sqlContext, config, dataset, \"testStructName\",\n        \"testNamespace\", new NonSortPartitionerWithRows(), false);\n    StructType resultSchema = result.schema();\n\n    assertEquals(result.count(), 10);\n    assertEquals(resultSchema.fieldNames().length, structType.fieldNames().length + HoodieRecord.HOODIE_META_COLUMNS.size());\n\n    for (Map.Entry<String, Integer> entry : HoodieRecord.HOODIE_META_COLUMNS_NAME_TO_POS.entrySet()) {\n      assertTrue(resultSchema.fieldIndex(entry.getKey()) == entry.getValue());\n    }\n\n    result.toJavaRDD().foreach(entry -> {\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.RECORD_KEY_METADATA_FIELD)).equals(entry.getAs(\"_row_key\")));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.PARTITION_PATH_METADATA_FIELD)).equals(entry.getAs(\"partition\")));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.COMMIT_TIME_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.FILENAME_METADATA_FIELD)).equals(\"\"));\n    });\n\n    Dataset<Row> trimmedOutput = result.drop(HoodieRecord.PARTITION_PATH_METADATA_FIELD).drop(HoodieRecord.RECORD_KEY_METADATA_FIELD)\n        .drop(HoodieRecord.FILENAME_METADATA_FIELD).drop(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD).drop(HoodieRecord.COMMIT_TIME_METADATA_FIELD);\n    assertTrue(dataset.except(trimmedOutput).count() == 0);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestHoodieDatasetBulkInsertHelper.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":93,"status":"M"},{"authorDate":"2021-08-05 07:08:50","commitOrder":2,"curCode":"  public void testBulkInsertHelperNoMetaFields() {\n    List<Row> rows = DataSourceTestUtils.generateRandomRows(10);\n    Dataset<Row> dataset = sqlContext.createDataFrame(rows, structType);\n    Dataset<Row> result = HoodieDatasetBulkInsertHelper.prepareHoodieDatasetForBulkInsertWithoutMetaFields(dataset);\n    StructType resultSchema = result.schema();\n\n    assertEquals(result.count(), 10);\n    assertEquals(resultSchema.fieldNames().length, structType.fieldNames().length + HoodieRecord.HOODIE_META_COLUMNS.size());\n\n    for (Map.Entry<String, Integer> entry : HoodieRecord.HOODIE_META_COLUMNS_NAME_TO_POS.entrySet()) {\n      assertTrue(resultSchema.fieldIndex(entry.getKey()) == entry.getValue());\n    }\n\n    result.toJavaRDD().foreach(entry -> {\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.RECORD_KEY_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.PARTITION_PATH_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.COMMIT_TIME_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.FILENAME_METADATA_FIELD)).equals(\"\"));\n    });\n\n    Dataset<Row> trimmedOutput = result.drop(HoodieRecord.PARTITION_PATH_METADATA_FIELD).drop(HoodieRecord.RECORD_KEY_METADATA_FIELD)\n        .drop(HoodieRecord.FILENAME_METADATA_FIELD).drop(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD).drop(HoodieRecord.COMMIT_TIME_METADATA_FIELD);\n    assertTrue(dataset.except(trimmedOutput).count() == 0);\n  }\n","date":"2021-08-05 07:08:50","endLine":146,"groupId":"10138","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testBulkInsertHelperNoMetaFields","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/69/836c70abe1b64077962aad4075bbf4eb7de378.src","preCode":"  public void testBulkInsertHelperNoMetaFields() {\n    List<Row> rows = DataSourceTestUtils.generateRandomRows(10);\n    Dataset<Row> dataset = sqlContext.createDataFrame(rows, structType);\n    Dataset<Row> result = HoodieDatasetBulkInsertHelper.prepareHoodieDatasetForBulkInsertWithoutMetaFields(dataset);\n    StructType resultSchema = result.schema();\n\n    assertEquals(result.count(), 10);\n    assertEquals(resultSchema.fieldNames().length, structType.fieldNames().length + HoodieRecord.HOODIE_META_COLUMNS.size());\n\n    for (Map.Entry<String, Integer> entry : HoodieRecord.HOODIE_META_COLUMNS_NAME_TO_POS.entrySet()) {\n      assertTrue(resultSchema.fieldIndex(entry.getKey()) == entry.getValue());\n    }\n\n    result.toJavaRDD().foreach(entry -> {\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.RECORD_KEY_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.PARTITION_PATH_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.COMMIT_TIME_METADATA_FIELD)).equals(\"\"));\n      assertTrue(entry.get(resultSchema.fieldIndex(HoodieRecord.FILENAME_METADATA_FIELD)).equals(\"\"));\n    });\n\n    Dataset<Row> trimmedOutput = result.drop(HoodieRecord.PARTITION_PATH_METADATA_FIELD).drop(HoodieRecord.RECORD_KEY_METADATA_FIELD)\n        .drop(HoodieRecord.FILENAME_METADATA_FIELD).drop(HoodieRecord.COMMIT_SEQNO_METADATA_FIELD).drop(HoodieRecord.COMMIT_TIME_METADATA_FIELD);\n    assertTrue(dataset.except(trimmedOutput).count() == 0);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark/src/test/java/org/apache/hudi/functional/TestHoodieDatasetBulkInsertHelper.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":122,"status":"N"}],"commitId":"968927801470953f137368cf146778a7f01aa63f","commitMessage":"@@@[HUDI-1363] Provide option to drop partition columns (#3465)\n\n- Co-authored-by: Sivabalan Narayanan <n.siva.b@gmail.com>","date":"2021-08-14 01:01:26","modifiedFileCount":"3","status":"M","submitter":"Sagar Sumit"}]
