[{"authorTime":"2020-05-27 16:28:17","codes":[{"authorDate":"2020-05-27 16:28:17","commitOrder":1,"curCode":"  public void testBasicAppendAndReadInReverse(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords2 = records2.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    dataBlock = new HoodieAvroDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords3 = records3.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    dataBlock = new HoodieAvroDataBlock(records3, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    HoodieLogFileReader reader = new HoodieLogFileReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema(),\n        bufferSize, readBlocksLazily, true);\n\n    assertTrue(reader.hasPrev(), \"Last block should be available\");\n    HoodieLogBlock prevBlock = reader.prev();\n    HoodieAvroDataBlock dataBlockRead = (HoodieAvroDataBlock) prevBlock;\n\n    assertEquals(copyOfRecords3.size(), dataBlockRead.getRecords().size(),\n        \"Third records size should be equal to the written records size\");\n    assertEquals(copyOfRecords3, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertTrue(reader.hasPrev(), \"Second block should be available\");\n    prevBlock = reader.prev();\n    dataBlockRead = (HoodieAvroDataBlock) prevBlock;\n    assertEquals(copyOfRecords2.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords2, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertTrue(reader.hasPrev(), \"First block should be available\");\n    prevBlock = reader.prev();\n    dataBlockRead = (HoodieAvroDataBlock) prevBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertFalse(reader.hasPrev());\n    reader.close();\n  }\n","date":"2020-05-27 16:28:17","endLine":1285,"groupId":"2905","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testBasicAppendAndReadInReverse","params":"(booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e9/f0ef7b167a811f253b4412a326c85966d5382c.src","preCode":"  public void testBasicAppendAndReadInReverse(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords2 = records2.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    dataBlock = new HoodieAvroDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords3 = records3.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    dataBlock = new HoodieAvroDataBlock(records3, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    HoodieLogFileReader reader = new HoodieLogFileReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema(),\n        bufferSize, readBlocksLazily, true);\n\n    assertTrue(reader.hasPrev(), \"Last block should be available\");\n    HoodieLogBlock prevBlock = reader.prev();\n    HoodieAvroDataBlock dataBlockRead = (HoodieAvroDataBlock) prevBlock;\n\n    assertEquals(copyOfRecords3.size(), dataBlockRead.getRecords().size(),\n        \"Third records size should be equal to the written records size\");\n    assertEquals(copyOfRecords3, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertTrue(reader.hasPrev(), \"Second block should be available\");\n    prevBlock = reader.prev();\n    dataBlockRead = (HoodieAvroDataBlock) prevBlock;\n    assertEquals(copyOfRecords2.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords2, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertTrue(reader.hasPrev(), \"First block should be available\");\n    prevBlock = reader.prev();\n    dataBlockRead = (HoodieAvroDataBlock) prevBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertFalse(reader.hasPrev());\n    reader.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":1218,"status":"B"},{"authorDate":"2020-05-27 16:28:17","commitOrder":1,"curCode":"  public void testBasicAppendAndTraverseInReverse(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    dataBlock = new HoodieAvroDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    dataBlock = new HoodieAvroDataBlock(records3, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    HoodieLogFileReader reader = new HoodieLogFileReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema(),\n        bufferSize, readBlocksLazily, true);\n\n    assertTrue(reader.hasPrev(), \"Third block should be available\");\n    reader.moveToPrev();\n\n    assertTrue(reader.hasPrev(), \"Second block should be available\");\n    reader.moveToPrev();\n\n    \r\n    assertTrue(reader.hasPrev(), \"First block should be available\");\n    HoodieLogBlock prevBlock = reader.prev();\n    HoodieAvroDataBlock dataBlockRead = (HoodieAvroDataBlock) prevBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertFalse(reader.hasPrev());\n    reader.close();\n  }\n","date":"2020-05-27 16:28:17","endLine":1399,"groupId":"4543","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testBasicAppendAndTraverseInReverse","params":"(booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e9/f0ef7b167a811f253b4412a326c85966d5382c.src","preCode":"  public void testBasicAppendAndTraverseInReverse(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    dataBlock = new HoodieAvroDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    dataBlock = new HoodieAvroDataBlock(records3, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    HoodieLogFileReader reader = new HoodieLogFileReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema(),\n        bufferSize, readBlocksLazily, true);\n\n    assertTrue(reader.hasPrev(), \"Third block should be available\");\n    reader.moveToPrev();\n\n    assertTrue(reader.hasPrev(), \"Second block should be available\");\n    reader.moveToPrev();\n\n    \r\n    assertTrue(reader.hasPrev(), \"First block should be available\");\n    HoodieLogBlock prevBlock = reader.prev();\n    HoodieAvroDataBlock dataBlockRead = (HoodieAvroDataBlock) prevBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertFalse(reader.hasPrev());\n    reader.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":1346,"status":"B"}],"commitId":"03f136361a5fed594855992ab10bee8bb5060c5b","commitMessage":"@@@[HUDI-811] Restructure test packages in hudi-common (#1644)\n\n* [HUDI-811] Restructure test packages in hudi-common","date":"2020-05-27 16:28:17","modifiedFileCount":"53","status":"B","submitter":"Raymond Xu"},{"authorTime":"2020-06-26 14:46:55","codes":[{"authorDate":"2020-06-26 14:46:55","commitOrder":2,"curCode":"  public void testBasicAppendAndReadInReverse(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords2 = records2.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    dataBlock = getDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords3 = records3.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    dataBlock = getDataBlock(records3, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    HoodieLogFileReader reader = new HoodieLogFileReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema(),\n        bufferSize, readBlocksLazily, true);\n\n    assertTrue(reader.hasPrev(), \"Last block should be available\");\n    HoodieLogBlock prevBlock = reader.prev();\n    HoodieDataBlock dataBlockRead = (HoodieDataBlock) prevBlock;\n\n    assertEquals(copyOfRecords3.size(), dataBlockRead.getRecords().size(),\n        \"Third records size should be equal to the written records size\");\n    assertEquals(copyOfRecords3, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertTrue(reader.hasPrev(), \"Second block should be available\");\n    prevBlock = reader.prev();\n    dataBlockRead = (HoodieDataBlock) prevBlock;\n    assertEquals(copyOfRecords2.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords2, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertTrue(reader.hasPrev(), \"First block should be available\");\n    prevBlock = reader.prev();\n    dataBlockRead = (HoodieDataBlock) prevBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertFalse(reader.hasPrev());\n    reader.close();\n  }\n","date":"2020-06-26 14:46:55","endLine":1294,"groupId":"3552","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testBasicAppendAndReadInReverse","params":"(booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/62/14af1af36313e2c739a9c15553b5c32d81dd94.src","preCode":"  public void testBasicAppendAndReadInReverse(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords2 = records2.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    dataBlock = new HoodieAvroDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords3 = records3.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    dataBlock = new HoodieAvroDataBlock(records3, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    HoodieLogFileReader reader = new HoodieLogFileReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema(),\n        bufferSize, readBlocksLazily, true);\n\n    assertTrue(reader.hasPrev(), \"Last block should be available\");\n    HoodieLogBlock prevBlock = reader.prev();\n    HoodieAvroDataBlock dataBlockRead = (HoodieAvroDataBlock) prevBlock;\n\n    assertEquals(copyOfRecords3.size(), dataBlockRead.getRecords().size(),\n        \"Third records size should be equal to the written records size\");\n    assertEquals(copyOfRecords3, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertTrue(reader.hasPrev(), \"Second block should be available\");\n    prevBlock = reader.prev();\n    dataBlockRead = (HoodieAvroDataBlock) prevBlock;\n    assertEquals(copyOfRecords2.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords2, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertTrue(reader.hasPrev(), \"First block should be available\");\n    prevBlock = reader.prev();\n    dataBlockRead = (HoodieAvroDataBlock) prevBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertFalse(reader.hasPrev());\n    reader.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":1227,"status":"M"},{"authorDate":"2020-06-26 14:46:55","commitOrder":2,"curCode":"  public void testBasicAppendAndTraverseInReverse(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    dataBlock = getDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    dataBlock = getDataBlock(records3, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    HoodieLogFileReader reader = new HoodieLogFileReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema(),\n        bufferSize, readBlocksLazily, true);\n\n    assertTrue(reader.hasPrev(), \"Third block should be available\");\n    reader.moveToPrev();\n\n    assertTrue(reader.hasPrev(), \"Second block should be available\");\n    reader.moveToPrev();\n\n    \r\n    assertTrue(reader.hasPrev(), \"First block should be available\");\n    HoodieLogBlock prevBlock = reader.prev();\n    HoodieDataBlock dataBlockRead = (HoodieDataBlock) prevBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertFalse(reader.hasPrev());\n    reader.close();\n  }\n","date":"2020-06-26 14:46:55","endLine":1408,"groupId":"3687","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testBasicAppendAndTraverseInReverse","params":"(booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/62/14af1af36313e2c739a9c15553b5c32d81dd94.src","preCode":"  public void testBasicAppendAndTraverseInReverse(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    dataBlock = new HoodieAvroDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    dataBlock = new HoodieAvroDataBlock(records3, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    HoodieLogFileReader reader = new HoodieLogFileReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema(),\n        bufferSize, readBlocksLazily, true);\n\n    assertTrue(reader.hasPrev(), \"Third block should be available\");\n    reader.moveToPrev();\n\n    assertTrue(reader.hasPrev(), \"Second block should be available\");\n    reader.moveToPrev();\n\n    \r\n    assertTrue(reader.hasPrev(), \"First block should be available\");\n    HoodieLogBlock prevBlock = reader.prev();\n    HoodieAvroDataBlock dataBlockRead = (HoodieAvroDataBlock) prevBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertFalse(reader.hasPrev());\n    reader.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":1355,"status":"M"}],"commitId":"2603cfb33e272632d7f36a53e1b13fe86dbb8627","commitMessage":"@@@[HUDI-684] Introduced abstraction for writing and reading different types of base file formats. (#1687)\n\nNotable changes:\n    1. HoodieFileWriter and HoodieFileReader abstractions for writer/reader side of a base file format\n    2. HoodieDataBlock abstraction for creation specific data blocks for base file formats. (e.g. Parquet has HoodieAvroDataBlock)\n    3. All hardocded references to Parquet / Parquet based classes have been abstracted to call methods which accept a base file format\n    4. HiveSyncTool accepts the base file format as a CLI parameter\n    5. HoodieDeltaStreamer accepts the base file format as a CLI parameter\n    6. HoodieSparkSqlWriter accepts the base file format as a parameter","date":"2020-06-26 14:46:55","modifiedFileCount":"42","status":"M","submitter":"Prashant Wason"},{"authorTime":"2020-12-31 06:22:15","codes":[{"authorDate":"2020-12-31 06:22:15","commitOrder":3,"curCode":"  public void testBasicAppendAndReadInReverse(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords2 = records2.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    dataBlock = getDataBlock(records2, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords3 = records3.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    dataBlock = getDataBlock(records3, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    HoodieLogFileReader reader = new HoodieLogFileReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema(),\n        bufferSize, readBlocksLazily, true);\n\n    assertTrue(reader.hasPrev(), \"Last block should be available\");\n    HoodieLogBlock prevBlock = reader.prev();\n    HoodieDataBlock dataBlockRead = (HoodieDataBlock) prevBlock;\n\n    assertEquals(copyOfRecords3.size(), dataBlockRead.getRecords().size(),\n        \"Third records size should be equal to the written records size\");\n    assertEquals(copyOfRecords3, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertTrue(reader.hasPrev(), \"Second block should be available\");\n    prevBlock = reader.prev();\n    dataBlockRead = (HoodieDataBlock) prevBlock;\n    assertEquals(copyOfRecords2.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords2, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertTrue(reader.hasPrev(), \"First block should be available\");\n    prevBlock = reader.prev();\n    dataBlockRead = (HoodieDataBlock) prevBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertFalse(reader.hasPrev());\n    reader.close();\n  }\n","date":"2020-12-31 06:22:15","endLine":1431,"groupId":"3552","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testBasicAppendAndReadInReverse","params":"(booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/57/e814c33909030385442385ccab9edfb802bcf5.src","preCode":"  public void testBasicAppendAndReadInReverse(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords2 = records2.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    dataBlock = getDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords3 = records3.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    dataBlock = getDataBlock(records3, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    HoodieLogFileReader reader = new HoodieLogFileReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema(),\n        bufferSize, readBlocksLazily, true);\n\n    assertTrue(reader.hasPrev(), \"Last block should be available\");\n    HoodieLogBlock prevBlock = reader.prev();\n    HoodieDataBlock dataBlockRead = (HoodieDataBlock) prevBlock;\n\n    assertEquals(copyOfRecords3.size(), dataBlockRead.getRecords().size(),\n        \"Third records size should be equal to the written records size\");\n    assertEquals(copyOfRecords3, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertTrue(reader.hasPrev(), \"Second block should be available\");\n    prevBlock = reader.prev();\n    dataBlockRead = (HoodieDataBlock) prevBlock;\n    assertEquals(copyOfRecords2.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords2, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertTrue(reader.hasPrev(), \"First block should be available\");\n    prevBlock = reader.prev();\n    dataBlockRead = (HoodieDataBlock) prevBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertFalse(reader.hasPrev());\n    reader.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":1364,"status":"M"},{"authorDate":"2020-12-31 06:22:15","commitOrder":3,"curCode":"  public void testBasicAppendAndTraverseInReverse(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    dataBlock = getDataBlock(records2, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    dataBlock = getDataBlock(records3, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    HoodieLogFileReader reader = new HoodieLogFileReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema(),\n        bufferSize, readBlocksLazily, true);\n\n    assertTrue(reader.hasPrev(), \"Third block should be available\");\n    reader.moveToPrev();\n\n    assertTrue(reader.hasPrev(), \"Second block should be available\");\n    reader.moveToPrev();\n\n    \r\n    assertTrue(reader.hasPrev(), \"First block should be available\");\n    HoodieLogBlock prevBlock = reader.prev();\n    HoodieDataBlock dataBlockRead = (HoodieDataBlock) prevBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertFalse(reader.hasPrev());\n    reader.close();\n  }\n","date":"2020-12-31 06:22:15","endLine":1545,"groupId":"3964","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testBasicAppendAndTraverseInReverse","params":"(booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/57/e814c33909030385442385ccab9edfb802bcf5.src","preCode":"  public void testBasicAppendAndTraverseInReverse(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    dataBlock = getDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    dataBlock = getDataBlock(records3, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    HoodieLogFileReader reader = new HoodieLogFileReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema(),\n        bufferSize, readBlocksLazily, true);\n\n    assertTrue(reader.hasPrev(), \"Third block should be available\");\n    reader.moveToPrev();\n\n    assertTrue(reader.hasPrev(), \"Second block should be available\");\n    reader.moveToPrev();\n\n    \r\n    assertTrue(reader.hasPrev(), \"First block should be available\");\n    HoodieLogBlock prevBlock = reader.prev();\n    HoodieDataBlock dataBlockRead = (HoodieDataBlock) prevBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertFalse(reader.hasPrev());\n    reader.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":1492,"status":"M"}],"commitId":"605b617cfa9a25bff48e618beccfaa7b4eaeee56","commitMessage":"@@@[HUDI-1434] fix incorrect log file path in HoodieWriteStat (#2300)\n\n* [HUDI-1434] fix incorrect log file path in HoodieWriteStat\n\n* HoodieWriteHandle#close() returns a list of WriteStatus objs\n\n* Handle rolled-over log files and return a WriteStatus per log file written\n\n - Combined data and delete block logging into a single call\n - Lazily initialize and manage write status based on returned AppendResult\n - Use FSUtils.getFileSize() to set final file size.  consistent with other handles\n - Added tests around returned values in AppendResult\n - Added validation of the file sizes returned in write stat\n\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2020-12-31 06:22:15","modifiedFileCount":"27","status":"M","submitter":"Gary Li"},{"authorTime":"2021-02-20 12:12:22","codes":[{"authorDate":"2021-02-20 12:12:22","commitOrder":4,"curCode":"  public void testBasicAppendAndReadInReverse(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords2 = records2.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    dataBlock = getDataBlock(records2, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords3 = records3.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    dataBlock = getDataBlock(records3, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    FileCreateUtils.createDeltaCommit(basePath, \"100\", fs);\n\n    HoodieLogFileReader reader = new HoodieLogFileReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema(),\n        bufferSize, readBlocksLazily, true);\n\n    assertTrue(reader.hasPrev(), \"Last block should be available\");\n    HoodieLogBlock prevBlock = reader.prev();\n    HoodieDataBlock dataBlockRead = (HoodieDataBlock) prevBlock;\n\n    assertEquals(copyOfRecords3.size(), dataBlockRead.getRecords().size(),\n        \"Third records size should be equal to the written records size\");\n    assertEquals(copyOfRecords3, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertTrue(reader.hasPrev(), \"Second block should be available\");\n    prevBlock = reader.prev();\n    dataBlockRead = (HoodieDataBlock) prevBlock;\n    assertEquals(copyOfRecords2.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords2, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertTrue(reader.hasPrev(), \"First block should be available\");\n    prevBlock = reader.prev();\n    dataBlockRead = (HoodieDataBlock) prevBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertFalse(reader.hasPrev());\n    reader.close();\n  }\n","date":"2021-02-20 12:12:22","endLine":1454,"groupId":"10946","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testBasicAppendAndReadInReverse","params":"(booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/69/fc60f6a3e429215cf49530f53411f54fd686cd.src","preCode":"  public void testBasicAppendAndReadInReverse(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords2 = records2.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    dataBlock = getDataBlock(records2, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords3 = records3.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    dataBlock = getDataBlock(records3, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    HoodieLogFileReader reader = new HoodieLogFileReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema(),\n        bufferSize, readBlocksLazily, true);\n\n    assertTrue(reader.hasPrev(), \"Last block should be available\");\n    HoodieLogBlock prevBlock = reader.prev();\n    HoodieDataBlock dataBlockRead = (HoodieDataBlock) prevBlock;\n\n    assertEquals(copyOfRecords3.size(), dataBlockRead.getRecords().size(),\n        \"Third records size should be equal to the written records size\");\n    assertEquals(copyOfRecords3, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertTrue(reader.hasPrev(), \"Second block should be available\");\n    prevBlock = reader.prev();\n    dataBlockRead = (HoodieDataBlock) prevBlock;\n    assertEquals(copyOfRecords2.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords2, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertTrue(reader.hasPrev(), \"First block should be available\");\n    prevBlock = reader.prev();\n    dataBlockRead = (HoodieDataBlock) prevBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertFalse(reader.hasPrev());\n    reader.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":1385,"status":"M"},{"authorDate":"2021-02-20 12:12:22","commitOrder":4,"curCode":"  public void testBasicAppendAndTraverseInReverse(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    dataBlock = getDataBlock(records2, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    dataBlock = getDataBlock(records3, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    FileCreateUtils.createDeltaCommit(basePath, \"100\", fs);\n\n    HoodieLogFileReader reader = new HoodieLogFileReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema(),\n        bufferSize, readBlocksLazily, true);\n\n    assertTrue(reader.hasPrev(), \"Third block should be available\");\n    reader.moveToPrev();\n\n    assertTrue(reader.hasPrev(), \"Second block should be available\");\n    reader.moveToPrev();\n\n    \r\n    assertTrue(reader.hasPrev(), \"First block should be available\");\n    HoodieLogBlock prevBlock = reader.prev();\n    HoodieDataBlock dataBlockRead = (HoodieDataBlock) prevBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertFalse(reader.hasPrev());\n    reader.close();\n  }\n","date":"2021-02-20 12:12:22","endLine":1572,"groupId":"10946","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testBasicAppendAndTraverseInReverse","params":"(booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/69/fc60f6a3e429215cf49530f53411f54fd686cd.src","preCode":"  public void testBasicAppendAndTraverseInReverse(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    dataBlock = getDataBlock(records2, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    dataBlock = getDataBlock(records3, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    HoodieLogFileReader reader = new HoodieLogFileReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema(),\n        bufferSize, readBlocksLazily, true);\n\n    assertTrue(reader.hasPrev(), \"Third block should be available\");\n    reader.moveToPrev();\n\n    assertTrue(reader.hasPrev(), \"Second block should be available\");\n    reader.moveToPrev();\n\n    \r\n    assertTrue(reader.hasPrev(), \"First block should be available\");\n    HoodieLogBlock prevBlock = reader.prev();\n    HoodieDataBlock dataBlockRead = (HoodieDataBlock) prevBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    assertFalse(reader.hasPrev());\n    reader.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":1517,"status":"M"}],"commitId":"ffcfb58bacab377bc72d20041baa54a3fd8fc812","commitMessage":"@@@[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)\n\n1. Refactor rollback and move cleaning failed commits logic into cleaner\n2. Introduce hoodie heartbeat to ascertain failed commits\n3. Fix test cases","date":"2021-02-20 12:12:22","modifiedFileCount":"56","status":"M","submitter":"n3nash"}]
