[{"authorTime":"2019-08-12 08:48:17","codes":[{"authorDate":"2019-08-12 08:48:17","commitOrder":1,"curCode":"  private static void doCompactUnschedule(JavaSparkContext jsc, String basePath, String compactionInstant,\n      String outputPath, int parallelism, String sparkMaster, String sparkMemory, boolean skipValidation,\n      boolean dryRun) throws Exception {\n    HoodieCompactionAdminTool.Config cfg = new HoodieCompactionAdminTool.Config();\n    cfg.basePath = basePath;\n    cfg.operation = Operation.UNSCHEDULE_PLAN;\n    cfg.outputPath = outputPath;\n    cfg.compactionInstantTime = compactionInstant;\n    cfg.parallelism = parallelism;\n    cfg.dryRun = dryRun;\n    cfg.skipValidation = skipValidation;\n    if ((null != sparkMaster) && (!sparkMaster.isEmpty())) {\n      jsc.getConf().setMaster(sparkMaster);\n    }\n    jsc.getConf().set(\"spark.executor.memory\", sparkMemory);\n    new HoodieCompactionAdminTool(cfg).run(jsc);\n  }\n","date":"2019-08-12 08:48:17","endLine":181,"groupId":"3061","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"doCompactUnschedule","params":"(JavaSparkContextjsc@StringbasePath@StringcompactionInstant@StringoutputPath@intparallelism@StringsparkMaster@StringsparkMemory@booleanskipValidation@booleandryRun)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/2a/11fef105b788d740403fcae3b8af716e834e12.src","preCode":"  private static void doCompactUnschedule(JavaSparkContext jsc, String basePath, String compactionInstant,\n      String outputPath, int parallelism, String sparkMaster, String sparkMemory, boolean skipValidation,\n      boolean dryRun) throws Exception {\n    HoodieCompactionAdminTool.Config cfg = new HoodieCompactionAdminTool.Config();\n    cfg.basePath = basePath;\n    cfg.operation = Operation.UNSCHEDULE_PLAN;\n    cfg.outputPath = outputPath;\n    cfg.compactionInstantTime = compactionInstant;\n    cfg.parallelism = parallelism;\n    cfg.dryRun = dryRun;\n    cfg.skipValidation = skipValidation;\n    if ((null != sparkMaster) && (!sparkMaster.isEmpty())) {\n      jsc.getConf().setMaster(sparkMaster);\n    }\n    jsc.getConf().set(\"spark.executor.memory\", sparkMemory);\n    new HoodieCompactionAdminTool(cfg).run(jsc);\n  }\n","realPath":"hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":165,"status":"B"},{"authorDate":"2019-08-12 08:48:17","commitOrder":1,"curCode":"  private static void doCompactUnscheduleFile(JavaSparkContext jsc, String basePath, String fileId,\n      String outputPath, int parallelism, String sparkMaster, String sparkMemory, boolean skipValidation,\n      boolean dryRun) throws Exception {\n    HoodieCompactionAdminTool.Config cfg = new HoodieCompactionAdminTool.Config();\n    cfg.basePath = basePath;\n    cfg.operation = Operation.UNSCHEDULE_FILE;\n    cfg.outputPath = outputPath;\n    cfg.fileId = fileId;\n    cfg.parallelism = parallelism;\n    cfg.dryRun = dryRun;\n    cfg.skipValidation = skipValidation;\n    if ((null != sparkMaster) && (!sparkMaster.isEmpty())) {\n      jsc.getConf().setMaster(sparkMaster);\n    }\n    jsc.getConf().set(\"spark.executor.memory\", sparkMemory);\n    new HoodieCompactionAdminTool(cfg).run(jsc);\n  }\n","date":"2019-08-12 08:48:17","endLine":199,"groupId":"3888","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"doCompactUnscheduleFile","params":"(JavaSparkContextjsc@StringbasePath@StringfileId@StringoutputPath@intparallelism@StringsparkMaster@StringsparkMemory@booleanskipValidation@booleandryRun)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/2a/11fef105b788d740403fcae3b8af716e834e12.src","preCode":"  private static void doCompactUnscheduleFile(JavaSparkContext jsc, String basePath, String fileId,\n      String outputPath, int parallelism, String sparkMaster, String sparkMemory, boolean skipValidation,\n      boolean dryRun) throws Exception {\n    HoodieCompactionAdminTool.Config cfg = new HoodieCompactionAdminTool.Config();\n    cfg.basePath = basePath;\n    cfg.operation = Operation.UNSCHEDULE_FILE;\n    cfg.outputPath = outputPath;\n    cfg.fileId = fileId;\n    cfg.parallelism = parallelism;\n    cfg.dryRun = dryRun;\n    cfg.skipValidation = skipValidation;\n    if ((null != sparkMaster) && (!sparkMaster.isEmpty())) {\n      jsc.getConf().setMaster(sparkMaster);\n    }\n    jsc.getConf().set(\"spark.executor.memory\", sparkMemory);\n    new HoodieCompactionAdminTool(cfg).run(jsc);\n  }\n","realPath":"hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":183,"status":"B"}],"commitId":"a4f9d7575f39bb79089714049ffea12ba5f25ec8","commitMessage":"@@@HUDI-123 Rename code packages/constants to org.apache.hudi (#830)\n\n- Rename com.uber.hoodie to org.apache.hudi\n- Flag to pass com.uber.hoodie Input formats for hoodie-sync\n- Works with HUDI demo. \n- Also tested for backwards compatibility with datasets built by com.uber.hoodie packages\n- Migration guide : https://cwiki.apache.org/confluence/display/HUDI/Migration+Guide+From+com.uber.hoodie+to+org.apache.hudi","date":"2019-08-12 08:48:17","modifiedFileCount":"0","status":"B","submitter":"Balaji Varadarajan"},{"authorTime":"2020-04-08 21:33:15","codes":[{"authorDate":"2020-04-08 21:33:15","commitOrder":2,"curCode":"  private static void doCompactUnschedule(JavaSparkContext jsc, String basePath, String compactionInstant,\n      String outputPath, int parallelism, boolean skipValidation, boolean dryRun) throws Exception {\n    HoodieCompactionAdminTool.Config cfg = new HoodieCompactionAdminTool.Config();\n    cfg.basePath = basePath;\n    cfg.operation = Operation.UNSCHEDULE_PLAN;\n    cfg.outputPath = outputPath;\n    cfg.compactionInstantTime = compactionInstant;\n    cfg.parallelism = parallelism;\n    cfg.dryRun = dryRun;\n    cfg.skipValidation = skipValidation;\n    new HoodieCompactionAdminTool(cfg).run(jsc);\n  }\n","date":"2020-04-08 21:33:15","endLine":231,"groupId":"3061","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"doCompactUnschedule","params":"(JavaSparkContextjsc@StringbasePath@StringcompactionInstant@StringoutputPath@intparallelism@booleanskipValidation@booleandryRun)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e8/f4c6aff0ac794e29d817b7716e6f42cbe7bb92.src","preCode":"  private static void doCompactUnschedule(JavaSparkContext jsc, String basePath, String compactionInstant,\n      String outputPath, int parallelism, String sparkMaster, String sparkMemory, boolean skipValidation,\n      boolean dryRun) throws Exception {\n    HoodieCompactionAdminTool.Config cfg = new HoodieCompactionAdminTool.Config();\n    cfg.basePath = basePath;\n    cfg.operation = Operation.UNSCHEDULE_PLAN;\n    cfg.outputPath = outputPath;\n    cfg.compactionInstantTime = compactionInstant;\n    cfg.parallelism = parallelism;\n    cfg.dryRun = dryRun;\n    cfg.skipValidation = skipValidation;\n    if ((null != sparkMaster) && (!sparkMaster.isEmpty())) {\n      jsc.getConf().setMaster(sparkMaster);\n    }\n    jsc.getConf().set(\"spark.executor.memory\", sparkMemory);\n    new HoodieCompactionAdminTool(cfg).run(jsc);\n  }\n","realPath":"hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":220,"status":"M"},{"authorDate":"2020-04-08 21:33:15","commitOrder":2,"curCode":"  private static void doCompactUnscheduleFile(JavaSparkContext jsc, String basePath, String fileId, String outputPath,\n      int parallelism, boolean skipValidation, boolean dryRun)\n      throws Exception {\n    HoodieCompactionAdminTool.Config cfg = new HoodieCompactionAdminTool.Config();\n    cfg.basePath = basePath;\n    cfg.operation = Operation.UNSCHEDULE_FILE;\n    cfg.outputPath = outputPath;\n    cfg.fileId = fileId;\n    cfg.parallelism = parallelism;\n    cfg.dryRun = dryRun;\n    cfg.skipValidation = skipValidation;\n    new HoodieCompactionAdminTool(cfg).run(jsc);\n  }\n","date":"2020-04-08 21:33:15","endLine":245,"groupId":"3888","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"doCompactUnscheduleFile","params":"(JavaSparkContextjsc@StringbasePath@StringfileId@StringoutputPath@intparallelism@booleanskipValidation@booleandryRun)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e8/f4c6aff0ac794e29d817b7716e6f42cbe7bb92.src","preCode":"  private static void doCompactUnscheduleFile(JavaSparkContext jsc, String basePath, String fileId, String outputPath,\n      int parallelism, String sparkMaster, String sparkMemory, boolean skipValidation, boolean dryRun)\n      throws Exception {\n    HoodieCompactionAdminTool.Config cfg = new HoodieCompactionAdminTool.Config();\n    cfg.basePath = basePath;\n    cfg.operation = Operation.UNSCHEDULE_FILE;\n    cfg.outputPath = outputPath;\n    cfg.fileId = fileId;\n    cfg.parallelism = parallelism;\n    cfg.dryRun = dryRun;\n    cfg.skipValidation = skipValidation;\n    if ((null != sparkMaster) && (!sparkMaster.isEmpty())) {\n      jsc.getConf().setMaster(sparkMaster);\n    }\n    jsc.getConf().set(\"spark.executor.memory\", sparkMemory);\n    new HoodieCompactionAdminTool(cfg).run(jsc);\n  }\n","realPath":"hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":233,"status":"M"}],"commitId":"4e5c8671ef3213ffa5c40f09aae27aacfa20f907","commitMessage":"@@@[HUDI-740]Fix can not specify the sparkMaster and code clean for SparkUtil (#1452)\n\n","date":"2020-04-08 21:33:15","modifiedFileCount":"4","status":"M","submitter":"hongdd"},{"authorTime":"2021-04-08 15:35:33","codes":[{"authorDate":"2020-04-08 21:33:15","commitOrder":3,"curCode":"  private static void doCompactUnschedule(JavaSparkContext jsc, String basePath, String compactionInstant,\n      String outputPath, int parallelism, boolean skipValidation, boolean dryRun) throws Exception {\n    HoodieCompactionAdminTool.Config cfg = new HoodieCompactionAdminTool.Config();\n    cfg.basePath = basePath;\n    cfg.operation = Operation.UNSCHEDULE_PLAN;\n    cfg.outputPath = outputPath;\n    cfg.compactionInstantTime = compactionInstant;\n    cfg.parallelism = parallelism;\n    cfg.dryRun = dryRun;\n    cfg.skipValidation = skipValidation;\n    new HoodieCompactionAdminTool(cfg).run(jsc);\n  }\n","date":"2020-04-08 21:33:15","endLine":231,"groupId":"10109","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"doCompactUnschedule","params":"(JavaSparkContextjsc@StringbasePath@StringcompactionInstant@StringoutputPath@intparallelism@booleanskipValidation@booleandryRun)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e8/f4c6aff0ac794e29d817b7716e6f42cbe7bb92.src","preCode":"  private static void doCompactUnschedule(JavaSparkContext jsc, String basePath, String compactionInstant,\n      String outputPath, int parallelism, boolean skipValidation, boolean dryRun) throws Exception {\n    HoodieCompactionAdminTool.Config cfg = new HoodieCompactionAdminTool.Config();\n    cfg.basePath = basePath;\n    cfg.operation = Operation.UNSCHEDULE_PLAN;\n    cfg.outputPath = outputPath;\n    cfg.compactionInstantTime = compactionInstant;\n    cfg.parallelism = parallelism;\n    cfg.dryRun = dryRun;\n    cfg.skipValidation = skipValidation;\n    new HoodieCompactionAdminTool(cfg).run(jsc);\n  }\n","realPath":"hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":220,"status":"N"},{"authorDate":"2021-04-08 15:35:33","commitOrder":3,"curCode":"  private static void doCompactUnscheduleFile(JavaSparkContext jsc, String basePath, String fileId, String partitionPath,\n      String outputPath, int parallelism, boolean skipValidation, boolean dryRun)\n      throws Exception {\n    HoodieCompactionAdminTool.Config cfg = new HoodieCompactionAdminTool.Config();\n    cfg.basePath = basePath;\n    cfg.operation = Operation.UNSCHEDULE_FILE;\n    cfg.outputPath = outputPath;\n    cfg.partitionPath = partitionPath;\n    cfg.fileId = fileId;\n    cfg.parallelism = parallelism;\n    cfg.dryRun = dryRun;\n    cfg.skipValidation = skipValidation;\n    new HoodieCompactionAdminTool(cfg).run(jsc);\n  }\n","date":"2021-04-08 15:35:33","endLine":286,"groupId":"10109","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"doCompactUnscheduleFile","params":"(JavaSparkContextjsc@StringbasePath@StringfileId@StringpartitionPath@StringoutputPath@intparallelism@booleanskipValidation@booleandryRun)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/af/b22fd7e19488864c7990ca16fa18afcb570a75.src","preCode":"  private static void doCompactUnscheduleFile(JavaSparkContext jsc, String basePath, String fileId, String outputPath,\n      int parallelism, boolean skipValidation, boolean dryRun)\n      throws Exception {\n    HoodieCompactionAdminTool.Config cfg = new HoodieCompactionAdminTool.Config();\n    cfg.basePath = basePath;\n    cfg.operation = Operation.UNSCHEDULE_FILE;\n    cfg.outputPath = outputPath;\n    cfg.fileId = fileId;\n    cfg.parallelism = parallelism;\n    cfg.dryRun = dryRun;\n    cfg.skipValidation = skipValidation;\n    new HoodieCompactionAdminTool(cfg).run(jsc);\n  }\n","realPath":"hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":273,"status":"M"}],"commitId":"ecdbd2517fd8157d9e96f3d9abf195a589e191ae","commitMessage":"@@@[HUDI-699] Fix CompactionCommand and add unit test for CompactionCommand (#2325)\n\n","date":"2021-04-08 15:35:33","modifiedFileCount":"9","status":"M","submitter":"hongdd"}]
