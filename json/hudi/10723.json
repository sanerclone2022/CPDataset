[{"authorTime":"2020-11-18 17:57:11","codes":[{"authorDate":"2020-10-02 05:25:29","commitOrder":2,"curCode":"  List<HoodieCleanStat> clean(HoodieEngineContext context, HoodieCleanerPlan cleanerPlan) {\n    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n    int cleanerParallelism = Math.min(\n        (int) (cleanerPlan.getFilePathsToBeDeletedPerPartition().values().stream().mapToInt(List::size).count()),\n        config.getCleanerParallelism());\n    LOG.info(\"Using cleanerParallelism: \" + cleanerParallelism);\n\n    context.setJobStatus(this.getClass().getSimpleName(), \"Perform cleaning of partitions\");\n    List<Tuple2<String, PartitionCleanStat>> partitionCleanStats = jsc\n        .parallelize(cleanerPlan.getFilePathsToBeDeletedPerPartition().entrySet().stream()\n            .flatMap(x -> x.getValue().stream().map(y -> new Tuple2<>(x.getKey(),\n                new CleanFileInfo(y.getFilePath(), y.getIsBootstrapBaseFile()))))\n            .collect(Collectors.toList()), cleanerParallelism)\n        .mapPartitionsToPair(deleteFilesFunc(table))\n        .reduceByKey(PartitionCleanStat::merge).collect();\n\n    Map<String, PartitionCleanStat> partitionCleanStatsMap = partitionCleanStats.stream()\n        .collect(Collectors.toMap(Tuple2::_1, Tuple2::_2));\n\n    \r\n    return cleanerPlan.getFilePathsToBeDeletedPerPartition().keySet().stream().map(partitionPath -> {\n      PartitionCleanStat partitionCleanStat = partitionCleanStatsMap.containsKey(partitionPath)\n          ? partitionCleanStatsMap.get(partitionPath)\n          : new PartitionCleanStat(partitionPath);\n      HoodieActionInstant actionInstant = cleanerPlan.getEarliestInstantToRetain();\n      return HoodieCleanStat.newBuilder().withPolicy(config.getCleanerPolicy()).withPartitionPath(partitionPath)\n          .withEarliestCommitRetained(Option.ofNullable(\n              actionInstant != null\n                  ? new HoodieInstant(HoodieInstant.State.valueOf(actionInstant.getState()),\n                  actionInstant.getAction(), actionInstant.getTimestamp())\n                  : null))\n          .withDeletePathPattern(partitionCleanStat.deletePathPatterns())\n          .withSuccessfulDeletes(partitionCleanStat.successDeleteFiles())\n          .withFailedDeletes(partitionCleanStat.failedDeleteFiles())\n          .withDeleteBootstrapBasePathPatterns(partitionCleanStat.getDeleteBootstrapBasePathPatterns())\n          .withSuccessfulDeleteBootstrapBaseFiles(partitionCleanStat.getSuccessfulDeleteBootstrapBaseFiles())\n          .withFailedDeleteBootstrapBaseFiles(partitionCleanStat.getFailedDeleteBootstrapBaseFiles())\n          .build();\n    }).collect(Collectors.toList());\n  }\n","date":"2020-10-02 05:25:29","endLine":133,"groupId":"885","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"clean","params":"(HoodieEngineContextcontext@HoodieCleanerPlancleanerPlan)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/bb/d5c1fb0e4ccdf8b135c1d2b48ad65fb281351b.src","preCode":"  List<HoodieCleanStat> clean(HoodieEngineContext context, HoodieCleanerPlan cleanerPlan) {\n    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n    int cleanerParallelism = Math.min(\n        (int) (cleanerPlan.getFilePathsToBeDeletedPerPartition().values().stream().mapToInt(List::size).count()),\n        config.getCleanerParallelism());\n    LOG.info(\"Using cleanerParallelism: \" + cleanerParallelism);\n\n    context.setJobStatus(this.getClass().getSimpleName(), \"Perform cleaning of partitions\");\n    List<Tuple2<String, PartitionCleanStat>> partitionCleanStats = jsc\n        .parallelize(cleanerPlan.getFilePathsToBeDeletedPerPartition().entrySet().stream()\n            .flatMap(x -> x.getValue().stream().map(y -> new Tuple2<>(x.getKey(),\n                new CleanFileInfo(y.getFilePath(), y.getIsBootstrapBaseFile()))))\n            .collect(Collectors.toList()), cleanerParallelism)\n        .mapPartitionsToPair(deleteFilesFunc(table))\n        .reduceByKey(PartitionCleanStat::merge).collect();\n\n    Map<String, PartitionCleanStat> partitionCleanStatsMap = partitionCleanStats.stream()\n        .collect(Collectors.toMap(Tuple2::_1, Tuple2::_2));\n\n    \r\n    return cleanerPlan.getFilePathsToBeDeletedPerPartition().keySet().stream().map(partitionPath -> {\n      PartitionCleanStat partitionCleanStat = partitionCleanStatsMap.containsKey(partitionPath)\n          ? partitionCleanStatsMap.get(partitionPath)\n          : new PartitionCleanStat(partitionPath);\n      HoodieActionInstant actionInstant = cleanerPlan.getEarliestInstantToRetain();\n      return HoodieCleanStat.newBuilder().withPolicy(config.getCleanerPolicy()).withPartitionPath(partitionPath)\n          .withEarliestCommitRetained(Option.ofNullable(\n              actionInstant != null\n                  ? new HoodieInstant(HoodieInstant.State.valueOf(actionInstant.getState()),\n                  actionInstant.getAction(), actionInstant.getTimestamp())\n                  : null))\n          .withDeletePathPattern(partitionCleanStat.deletePathPatterns())\n          .withSuccessfulDeletes(partitionCleanStat.successDeleteFiles())\n          .withFailedDeletes(partitionCleanStat.failedDeleteFiles())\n          .withDeleteBootstrapBasePathPatterns(partitionCleanStat.getDeleteBootstrapBasePathPatterns())\n          .withSuccessfulDeleteBootstrapBaseFiles(partitionCleanStat.getSuccessfulDeleteBootstrapBaseFiles())\n          .withFailedDeleteBootstrapBaseFiles(partitionCleanStat.getFailedDeleteBootstrapBaseFiles())\n          .build();\n    }).collect(Collectors.toList());\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/clean/SparkCleanActionExecutor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":94,"status":"NB"},{"authorDate":"2020-11-18 17:57:11","commitOrder":2,"curCode":"  List<HoodieCleanStat> clean(HoodieEngineContext context, HoodieCleanerPlan cleanerPlan) {\n\n    Iterator<Tuple2<String, CleanFileInfo>> filesToBeDeletedPerPartition = cleanerPlan.getFilePathsToBeDeletedPerPartition().entrySet().stream()\n        .flatMap(x -> x.getValue().stream().map(y -> new Tuple2<>(x.getKey(), new CleanFileInfo(y.getFilePath(), y.getIsBootstrapBaseFile())))).iterator();\n\n    Stream<Tuple2<String, PartitionCleanStat>> partitionCleanStats =\n        deleteFilesFunc(filesToBeDeletedPerPartition, table)\n            .collect(Collectors.groupingBy(Pair::getLeft))\n            .entrySet().stream()\n            .map(x -> new Tuple2(x.getKey(), x.getValue().stream().map(y -> y.getRight()).reduce(PartitionCleanStat::merge).get()));\n\n    Map<String, PartitionCleanStat> partitionCleanStatsMap = partitionCleanStats\n        .collect(Collectors.toMap(Tuple2::_1, Tuple2::_2));\n\n    \r\n    return cleanerPlan.getFilePathsToBeDeletedPerPartition().keySet().stream().map(partitionPath -> {\n      PartitionCleanStat partitionCleanStat = partitionCleanStatsMap.containsKey(partitionPath)\n          ? partitionCleanStatsMap.get(partitionPath)\n          : new PartitionCleanStat(partitionPath);\n      HoodieActionInstant actionInstant = cleanerPlan.getEarliestInstantToRetain();\n      return HoodieCleanStat.newBuilder().withPolicy(config.getCleanerPolicy()).withPartitionPath(partitionPath)\n          .withEarliestCommitRetained(Option.ofNullable(\n              actionInstant != null\n                  ? new HoodieInstant(HoodieInstant.State.valueOf(actionInstant.getState()),\n                  actionInstant.getAction(), actionInstant.getTimestamp())\n                  : null))\n          .withDeletePathPattern(partitionCleanStat.deletePathPatterns())\n          .withSuccessfulDeletes(partitionCleanStat.successDeleteFiles())\n          .withFailedDeletes(partitionCleanStat.failedDeleteFiles())\n          .withDeleteBootstrapBasePathPatterns(partitionCleanStat.getDeleteBootstrapBasePathPatterns())\n          .withSuccessfulDeleteBootstrapBaseFiles(partitionCleanStat.getSuccessfulDeleteBootstrapBaseFiles())\n          .withFailedDeleteBootstrapBaseFiles(partitionCleanStat.getFailedDeleteBootstrapBaseFiles())\n          .build();\n    }).collect(Collectors.toList());\n  }\n","date":"2020-11-18 17:57:11","endLine":98,"groupId":"2168","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"clean","params":"(HoodieEngineContextcontext@HoodieCleanerPlancleanerPlan)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/01/0e2a16af4a7cb736fbf30ea19ffe90f274d95a.src","preCode":"  List<HoodieCleanStat> clean(HoodieEngineContext context, HoodieCleanerPlan cleanerPlan) {\n\n    Iterator<Tuple2<String, CleanFileInfo>> filesToBeDeletedPerPartition = cleanerPlan.getFilePathsToBeDeletedPerPartition().entrySet().stream()\n        .flatMap(x -> x.getValue().stream().map(y -> new Tuple2<>(x.getKey(), new CleanFileInfo(y.getFilePath(), y.getIsBootstrapBaseFile())))).iterator();\n\n    Stream<Tuple2<String, PartitionCleanStat>> partitionCleanStats =\n        deleteFilesFunc(filesToBeDeletedPerPartition, table)\n            .collect(Collectors.groupingBy(Pair::getLeft))\n            .entrySet().stream()\n            .map(x -> new Tuple2(x.getKey(), x.getValue().stream().map(y -> y.getRight()).reduce(PartitionCleanStat::merge).get()));\n\n    Map<String, PartitionCleanStat> partitionCleanStatsMap = partitionCleanStats\n        .collect(Collectors.toMap(Tuple2::_1, Tuple2::_2));\n\n    \r\n    return cleanerPlan.getFilePathsToBeDeletedPerPartition().keySet().stream().map(partitionPath -> {\n      PartitionCleanStat partitionCleanStat = partitionCleanStatsMap.containsKey(partitionPath)\n          ? partitionCleanStatsMap.get(partitionPath)\n          : new PartitionCleanStat(partitionPath);\n      HoodieActionInstant actionInstant = cleanerPlan.getEarliestInstantToRetain();\n      return HoodieCleanStat.newBuilder().withPolicy(config.getCleanerPolicy()).withPartitionPath(partitionPath)\n          .withEarliestCommitRetained(Option.ofNullable(\n              actionInstant != null\n                  ? new HoodieInstant(HoodieInstant.State.valueOf(actionInstant.getState()),\n                  actionInstant.getAction(), actionInstant.getTimestamp())\n                  : null))\n          .withDeletePathPattern(partitionCleanStat.deletePathPatterns())\n          .withSuccessfulDeletes(partitionCleanStat.successDeleteFiles())\n          .withFailedDeletes(partitionCleanStat.failedDeleteFiles())\n          .withDeleteBootstrapBasePathPatterns(partitionCleanStat.getDeleteBootstrapBasePathPatterns())\n          .withSuccessfulDeleteBootstrapBaseFiles(partitionCleanStat.getSuccessfulDeleteBootstrapBaseFiles())\n          .withFailedDeleteBootstrapBaseFiles(partitionCleanStat.getFailedDeleteBootstrapBaseFiles())\n          .build();\n    }).collect(Collectors.toList());\n  }\n","realPath":"hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/clean/FlinkCleanActionExecutor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":64,"status":"B"}],"commitId":"4d05680038752077ceaebef261b66a5afc761e10","commitMessage":"@@@[HUDI-1327] Introduce base implemetation of hudi-flink-client (#2176)\n\n","date":"2020-11-18 17:57:11","modifiedFileCount":"6","status":"M","submitter":"wangxianghu"},{"authorTime":"2021-07-22 13:34:15","codes":[{"authorDate":"2020-10-02 05:25:29","commitOrder":3,"curCode":"  List<HoodieCleanStat> clean(HoodieEngineContext context, HoodieCleanerPlan cleanerPlan) {\n    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n    int cleanerParallelism = Math.min(\n        (int) (cleanerPlan.getFilePathsToBeDeletedPerPartition().values().stream().mapToInt(List::size).count()),\n        config.getCleanerParallelism());\n    LOG.info(\"Using cleanerParallelism: \" + cleanerParallelism);\n\n    context.setJobStatus(this.getClass().getSimpleName(), \"Perform cleaning of partitions\");\n    List<Tuple2<String, PartitionCleanStat>> partitionCleanStats = jsc\n        .parallelize(cleanerPlan.getFilePathsToBeDeletedPerPartition().entrySet().stream()\n            .flatMap(x -> x.getValue().stream().map(y -> new Tuple2<>(x.getKey(),\n                new CleanFileInfo(y.getFilePath(), y.getIsBootstrapBaseFile()))))\n            .collect(Collectors.toList()), cleanerParallelism)\n        .mapPartitionsToPair(deleteFilesFunc(table))\n        .reduceByKey(PartitionCleanStat::merge).collect();\n\n    Map<String, PartitionCleanStat> partitionCleanStatsMap = partitionCleanStats.stream()\n        .collect(Collectors.toMap(Tuple2::_1, Tuple2::_2));\n\n    \r\n    return cleanerPlan.getFilePathsToBeDeletedPerPartition().keySet().stream().map(partitionPath -> {\n      PartitionCleanStat partitionCleanStat = partitionCleanStatsMap.containsKey(partitionPath)\n          ? partitionCleanStatsMap.get(partitionPath)\n          : new PartitionCleanStat(partitionPath);\n      HoodieActionInstant actionInstant = cleanerPlan.getEarliestInstantToRetain();\n      return HoodieCleanStat.newBuilder().withPolicy(config.getCleanerPolicy()).withPartitionPath(partitionPath)\n          .withEarliestCommitRetained(Option.ofNullable(\n              actionInstant != null\n                  ? new HoodieInstant(HoodieInstant.State.valueOf(actionInstant.getState()),\n                  actionInstant.getAction(), actionInstant.getTimestamp())\n                  : null))\n          .withDeletePathPattern(partitionCleanStat.deletePathPatterns())\n          .withSuccessfulDeletes(partitionCleanStat.successDeleteFiles())\n          .withFailedDeletes(partitionCleanStat.failedDeleteFiles())\n          .withDeleteBootstrapBasePathPatterns(partitionCleanStat.getDeleteBootstrapBasePathPatterns())\n          .withSuccessfulDeleteBootstrapBaseFiles(partitionCleanStat.getSuccessfulDeleteBootstrapBaseFiles())\n          .withFailedDeleteBootstrapBaseFiles(partitionCleanStat.getFailedDeleteBootstrapBaseFiles())\n          .build();\n    }).collect(Collectors.toList());\n  }\n","date":"2020-10-02 05:25:29","endLine":133,"groupId":"10723","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"clean","params":"(HoodieEngineContextcontext@HoodieCleanerPlancleanerPlan)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/bb/d5c1fb0e4ccdf8b135c1d2b48ad65fb281351b.src","preCode":"  List<HoodieCleanStat> clean(HoodieEngineContext context, HoodieCleanerPlan cleanerPlan) {\n    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n    int cleanerParallelism = Math.min(\n        (int) (cleanerPlan.getFilePathsToBeDeletedPerPartition().values().stream().mapToInt(List::size).count()),\n        config.getCleanerParallelism());\n    LOG.info(\"Using cleanerParallelism: \" + cleanerParallelism);\n\n    context.setJobStatus(this.getClass().getSimpleName(), \"Perform cleaning of partitions\");\n    List<Tuple2<String, PartitionCleanStat>> partitionCleanStats = jsc\n        .parallelize(cleanerPlan.getFilePathsToBeDeletedPerPartition().entrySet().stream()\n            .flatMap(x -> x.getValue().stream().map(y -> new Tuple2<>(x.getKey(),\n                new CleanFileInfo(y.getFilePath(), y.getIsBootstrapBaseFile()))))\n            .collect(Collectors.toList()), cleanerParallelism)\n        .mapPartitionsToPair(deleteFilesFunc(table))\n        .reduceByKey(PartitionCleanStat::merge).collect();\n\n    Map<String, PartitionCleanStat> partitionCleanStatsMap = partitionCleanStats.stream()\n        .collect(Collectors.toMap(Tuple2::_1, Tuple2::_2));\n\n    \r\n    return cleanerPlan.getFilePathsToBeDeletedPerPartition().keySet().stream().map(partitionPath -> {\n      PartitionCleanStat partitionCleanStat = partitionCleanStatsMap.containsKey(partitionPath)\n          ? partitionCleanStatsMap.get(partitionPath)\n          : new PartitionCleanStat(partitionPath);\n      HoodieActionInstant actionInstant = cleanerPlan.getEarliestInstantToRetain();\n      return HoodieCleanStat.newBuilder().withPolicy(config.getCleanerPolicy()).withPartitionPath(partitionPath)\n          .withEarliestCommitRetained(Option.ofNullable(\n              actionInstant != null\n                  ? new HoodieInstant(HoodieInstant.State.valueOf(actionInstant.getState()),\n                  actionInstant.getAction(), actionInstant.getTimestamp())\n                  : null))\n          .withDeletePathPattern(partitionCleanStat.deletePathPatterns())\n          .withSuccessfulDeletes(partitionCleanStat.successDeleteFiles())\n          .withFailedDeletes(partitionCleanStat.failedDeleteFiles())\n          .withDeleteBootstrapBasePathPatterns(partitionCleanStat.getDeleteBootstrapBasePathPatterns())\n          .withSuccessfulDeleteBootstrapBaseFiles(partitionCleanStat.getSuccessfulDeleteBootstrapBaseFiles())\n          .withFailedDeleteBootstrapBaseFiles(partitionCleanStat.getFailedDeleteBootstrapBaseFiles())\n          .build();\n    }).collect(Collectors.toList());\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/clean/SparkCleanActionExecutor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":94,"status":"N"},{"authorDate":"2021-07-22 13:34:15","commitOrder":3,"curCode":"  List<HoodieCleanStat> clean(HoodieEngineContext context, HoodieCleanerPlan cleanerPlan) {\n    Stream<Tuple2<String, CleanFileInfo>> filesToBeDeletedPerPartition = cleanerPlan.getFilePathsToBeDeletedPerPartition().entrySet().stream()\n        .flatMap(x -> x.getValue().stream().map(y -> new Tuple2<>(x.getKey(), new CleanFileInfo(y.getFilePath(), y.getIsBootstrapBaseFile()))));\n\n    Stream<Tuple2<String, PartitionCleanStat>> partitionCleanStats =\n        deleteFilesFunc(filesToBeDeletedPerPartition, table)\n            .collect(Collectors.groupingBy(Pair::getLeft))\n            .entrySet().stream()\n            .map(x -> new Tuple2(x.getKey(), x.getValue().stream().map(y -> y.getRight()).reduce(PartitionCleanStat::merge).get()));\n\n    Map<String, PartitionCleanStat> partitionCleanStatsMap = partitionCleanStats\n        .collect(Collectors.toMap(Tuple2::_1, Tuple2::_2));\n\n    \r\n    return cleanerPlan.getFilePathsToBeDeletedPerPartition().keySet().stream().map(partitionPath -> {\n      PartitionCleanStat partitionCleanStat = partitionCleanStatsMap.containsKey(partitionPath)\n          ? partitionCleanStatsMap.get(partitionPath)\n          : new PartitionCleanStat(partitionPath);\n      HoodieActionInstant actionInstant = cleanerPlan.getEarliestInstantToRetain();\n      return HoodieCleanStat.newBuilder().withPolicy(config.getCleanerPolicy()).withPartitionPath(partitionPath)\n          .withEarliestCommitRetained(Option.ofNullable(\n              actionInstant != null\n                  ? new HoodieInstant(HoodieInstant.State.valueOf(actionInstant.getState()),\n                  actionInstant.getAction(), actionInstant.getTimestamp())\n                  : null))\n          .withDeletePathPattern(partitionCleanStat.deletePathPatterns())\n          .withSuccessfulDeletes(partitionCleanStat.successDeleteFiles())\n          .withFailedDeletes(partitionCleanStat.failedDeleteFiles())\n          .withDeleteBootstrapBasePathPatterns(partitionCleanStat.getDeleteBootstrapBasePathPatterns())\n          .withSuccessfulDeleteBootstrapBaseFiles(partitionCleanStat.getSuccessfulDeleteBootstrapBaseFiles())\n          .withFailedDeleteBootstrapBaseFiles(partitionCleanStat.getFailedDeleteBootstrapBaseFiles())\n          .build();\n    }).collect(Collectors.toList());\n  }\n","date":"2021-07-22 13:34:15","endLine":96,"groupId":"10723","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"clean","params":"(HoodieEngineContextcontext@HoodieCleanerPlancleanerPlan)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/93/78cb2304b7908addef5d2c363ece313d28348c.src","preCode":"  List<HoodieCleanStat> clean(HoodieEngineContext context, HoodieCleanerPlan cleanerPlan) {\n\n    Iterator<Tuple2<String, CleanFileInfo>> filesToBeDeletedPerPartition = cleanerPlan.getFilePathsToBeDeletedPerPartition().entrySet().stream()\n        .flatMap(x -> x.getValue().stream().map(y -> new Tuple2<>(x.getKey(), new CleanFileInfo(y.getFilePath(), y.getIsBootstrapBaseFile())))).iterator();\n\n    Stream<Tuple2<String, PartitionCleanStat>> partitionCleanStats =\n        deleteFilesFunc(filesToBeDeletedPerPartition, table)\n            .collect(Collectors.groupingBy(Pair::getLeft))\n            .entrySet().stream()\n            .map(x -> new Tuple2(x.getKey(), x.getValue().stream().map(y -> y.getRight()).reduce(PartitionCleanStat::merge).get()));\n\n    Map<String, PartitionCleanStat> partitionCleanStatsMap = partitionCleanStats\n        .collect(Collectors.toMap(Tuple2::_1, Tuple2::_2));\n\n    \r\n    return cleanerPlan.getFilePathsToBeDeletedPerPartition().keySet().stream().map(partitionPath -> {\n      PartitionCleanStat partitionCleanStat = partitionCleanStatsMap.containsKey(partitionPath)\n          ? partitionCleanStatsMap.get(partitionPath)\n          : new PartitionCleanStat(partitionPath);\n      HoodieActionInstant actionInstant = cleanerPlan.getEarliestInstantToRetain();\n      return HoodieCleanStat.newBuilder().withPolicy(config.getCleanerPolicy()).withPartitionPath(partitionPath)\n          .withEarliestCommitRetained(Option.ofNullable(\n              actionInstant != null\n                  ? new HoodieInstant(HoodieInstant.State.valueOf(actionInstant.getState()),\n                  actionInstant.getAction(), actionInstant.getTimestamp())\n                  : null))\n          .withDeletePathPattern(partitionCleanStat.deletePathPatterns())\n          .withSuccessfulDeletes(partitionCleanStat.successDeleteFiles())\n          .withFailedDeletes(partitionCleanStat.failedDeleteFiles())\n          .withDeleteBootstrapBasePathPatterns(partitionCleanStat.getDeleteBootstrapBasePathPatterns())\n          .withSuccessfulDeleteBootstrapBaseFiles(partitionCleanStat.getSuccessfulDeleteBootstrapBaseFiles())\n          .withFailedDeleteBootstrapBaseFiles(partitionCleanStat.getFailedDeleteBootstrapBaseFiles())\n          .build();\n    }).collect(Collectors.toList());\n  }\n","realPath":"hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/clean/FlinkCleanActionExecutor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":63,"status":"M"}],"commitId":"2370a9facbe4418f994f29c426e9b2a255e3abb0","commitMessage":"@@@[HUDI-2204] Add marker files for flink writer (#3316)\n\n","date":"2021-07-22 13:34:15","modifiedFileCount":"6","status":"M","submitter":"Danny Chan"}]
