[{"authorTime":"2021-08-28 20:16:54","codes":[{"authorDate":"2021-07-30 14:25:05","commitOrder":4,"curCode":"  public void testConsumeFromLatestCommit() throws Exception {\n    \r\n    TestData.writeData(TestData.DATA_SET_INSERT, conf);\n    TestData.writeData(TestData.DATA_SET_UPDATE_INSERT, conf);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n      String latestCommit = TestUtils.getLatestCommit(tempFile.getAbsolutePath());\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getLatestCommit().equals(latestCommit)),\n          \"All the splits should be with latestCommit instant time\");\n\n      \r\n      function.close();\n    }\n  }\n","date":"2021-07-30 14:25:05","endLine":99,"groupId":"4687","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testConsumeFromLatestCommit","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/f1/45744465330a165b6cccdd63e902234c53ae10.src","preCode":"  public void testConsumeFromLatestCommit() throws Exception {\n    \r\n    TestData.writeData(TestData.DATA_SET_INSERT, conf);\n    TestData.writeData(TestData.DATA_SET_UPDATE_INSERT, conf);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n      String latestCommit = TestUtils.getLatestCommit(tempFile.getAbsolutePath());\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getLatestCommit().equals(latestCommit)),\n          \"All the splits should be with latestCommit instant time\");\n\n      \r\n      function.close();\n    }\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/source/TestStreamReadMonitoringFunction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":72,"status":"NB"},{"authorDate":"2021-08-28 20:16:54","commitOrder":4,"curCode":"  public void testConsumeFromEarliestCommit() throws Exception {\n    \r\n    \r\n    TestData.writeData(TestData.DATA_SET_INSERT, conf);\n    TestData.writeData(TestData.DATA_SET_UPDATE_INSERT, conf);\n    String specifiedCommit = TestUtils.getLatestCommit(tempFile.getAbsolutePath());\n    conf.setString(FlinkOptions.READ_STREAMING_START_COMMIT, FlinkOptions.START_COMMIT_EARLIEST);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().noneMatch(split -> split.getInstantRange().isPresent()),\n          \"No instants should have range limit\");\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getLatestCommit().equals(specifiedCommit)),\n          \"All the splits should be with specified instant time\");\n\n      \r\n      function.close();\n    }\n  }\n","date":"2021-08-28 20:16:54","endLine":200,"groupId":"1544","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testConsumeFromEarliestCommit","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/d1/3f68319d9b41c016f8bbae3dad85c10cadd6e1.src","preCode":"  public void testConsumeFromEarliestCommit() throws Exception {\n    \r\n    \r\n    TestData.writeData(TestData.DATA_SET_INSERT, conf);\n    TestData.writeData(TestData.DATA_SET_UPDATE_INSERT, conf);\n    String specifiedCommit = TestUtils.getLatestCommit(tempFile.getAbsolutePath());\n    conf.setString(FlinkOptions.READ_STREAMING_START_COMMIT, FlinkOptions.START_COMMIT_EARLIEST);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().noneMatch(split -> split.getInstantRange().isPresent()),\n          \"No instants should have range limit\");\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getLatestCommit().equals(specifiedCommit)),\n          \"All the splits should be with specified instant time\");\n\n      \r\n      function.close();\n    }\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/source/TestStreamReadMonitoringFunction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":172,"status":"B"}],"commitId":"57668d02a0aa723dd4b2245dc7659fe18113eb59","commitMessage":"@@@[HUDI-2371] Improvement flink streaming reader (#3552)\n\n- Support reading empty table\n- Fix filtering by partition path\n- Support reading from earliest commit","date":"2021-08-28 20:16:54","modifiedFileCount":"9","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-09-19 09:06:46","codes":[{"authorDate":"2021-07-30 14:25:05","commitOrder":5,"curCode":"  public void testConsumeFromLatestCommit() throws Exception {\n    \r\n    TestData.writeData(TestData.DATA_SET_INSERT, conf);\n    TestData.writeData(TestData.DATA_SET_UPDATE_INSERT, conf);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n      String latestCommit = TestUtils.getLatestCommit(tempFile.getAbsolutePath());\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getLatestCommit().equals(latestCommit)),\n          \"All the splits should be with latestCommit instant time\");\n\n      \r\n      function.close();\n    }\n  }\n","date":"2021-07-30 14:25:05","endLine":99,"groupId":"10434","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testConsumeFromLatestCommit","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/f1/45744465330a165b6cccdd63e902234c53ae10.src","preCode":"  public void testConsumeFromLatestCommit() throws Exception {\n    \r\n    TestData.writeData(TestData.DATA_SET_INSERT, conf);\n    TestData.writeData(TestData.DATA_SET_UPDATE_INSERT, conf);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n      String latestCommit = TestUtils.getLatestCommit(tempFile.getAbsolutePath());\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getLatestCommit().equals(latestCommit)),\n          \"All the splits should be with latestCommit instant time\");\n\n      \r\n      function.close();\n    }\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/source/TestStreamReadMonitoringFunction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":72,"status":"N"},{"authorDate":"2021-09-19 09:06:46","commitOrder":5,"curCode":"  public void testConsumeFromEarliestCommit() throws Exception {\n    \r\n    \r\n    TestData.writeData(TestData.DATA_SET_INSERT, conf);\n    TestData.writeData(TestData.DATA_SET_UPDATE_INSERT, conf);\n    String specifiedCommit = TestUtils.getLatestCommit(tempFile.getAbsolutePath());\n    conf.setString(FlinkOptions.READ_START_COMMIT, FlinkOptions.START_COMMIT_EARLIEST);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().noneMatch(split -> split.getInstantRange().isPresent()),\n          \"No instants should have range limit\");\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getLatestCommit().equals(specifiedCommit)),\n          \"All the splits should be with specified instant time\");\n\n      \r\n      function.close();\n    }\n  }\n","date":"2021-09-19 09:06:46","endLine":200,"groupId":"10434","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"testConsumeFromEarliestCommit","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/36/87e9d7cee4d3ac79c5c4ac11d56784e8365478.src","preCode":"  public void testConsumeFromEarliestCommit() throws Exception {\n    \r\n    \r\n    TestData.writeData(TestData.DATA_SET_INSERT, conf);\n    TestData.writeData(TestData.DATA_SET_UPDATE_INSERT, conf);\n    String specifiedCommit = TestUtils.getLatestCommit(tempFile.getAbsolutePath());\n    conf.setString(FlinkOptions.READ_STREAMING_START_COMMIT, FlinkOptions.START_COMMIT_EARLIEST);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().noneMatch(split -> split.getInstantRange().isPresent()),\n          \"No instants should have range limit\");\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getLatestCommit().equals(specifiedCommit)),\n          \"All the splits should be with specified instant time\");\n\n      \r\n      function.close();\n    }\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/source/TestStreamReadMonitoringFunction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":172,"status":"M"}],"commitId":"3354fac42f9a2c4dbc8ac73ca4749160e9b9459b","commitMessage":"@@@[HUDI-2449] Incremental read for Flink (#3686)\n\n","date":"2021-09-19 09:06:46","modifiedFileCount":"15","status":"M","submitter":"Danny Chan"}]
