[{"authorTime":"2020-10-02 05:25:29","codes":[{"authorDate":"2020-10-02 05:25:29","commitOrder":1,"curCode":"  public void testInflightCompaction() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfig(true);\n    try (SparkRDDWriteClient client = getHoodieWriteClient(cfg, true);) {\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      String firstInstantTime = \"001\";\n      String secondInstantTime = \"004\";\n      String compactionInstantTime = \"005\";\n      String thirdInstantTime = \"006\";\n      String fourthInstantTime = \"007\";\n\n      int numRecs = 2000;\n\n      List<HoodieRecord> records = dataGen.generateInserts(firstInstantTime, numRecs);\n      records = runNextDeltaCommits(client, readClient, Arrays.asList(firstInstantTime, secondInstantTime), records, cfg, true,\n          new ArrayList<>());\n\n      \r\n      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n      HoodieTable hoodieTable = getHoodieTable(metaClient, cfg);\n      scheduleCompaction(compactionInstantTime, client, cfg);\n      moveCompactionFromRequestedToInflight(compactionInstantTime, cfg);\n\n      \r\n      runNextDeltaCommits(client, readClient, Arrays.asList(thirdInstantTime, fourthInstantTime), records, cfg, false,\n          Arrays.asList(compactionInstantTime));\n\n      \r\n      executeCompaction(compactionInstantTime, client, hoodieTable, cfg, numRecs, true);\n    }\n  }\n","date":"2020-10-02 05:25:29","endLine":187,"groupId":"5431","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testInflightCompaction","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/8d/a1f3ddc6755d52a2573d745184c65ad0172c30.src","preCode":"  public void testInflightCompaction() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfig(true);\n    try (SparkRDDWriteClient client = getHoodieWriteClient(cfg, true);) {\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      String firstInstantTime = \"001\";\n      String secondInstantTime = \"004\";\n      String compactionInstantTime = \"005\";\n      String thirdInstantTime = \"006\";\n      String fourthInstantTime = \"007\";\n\n      int numRecs = 2000;\n\n      List<HoodieRecord> records = dataGen.generateInserts(firstInstantTime, numRecs);\n      records = runNextDeltaCommits(client, readClient, Arrays.asList(firstInstantTime, secondInstantTime), records, cfg, true,\n          new ArrayList<>());\n\n      \r\n      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n      HoodieTable hoodieTable = getHoodieTable(metaClient, cfg);\n      scheduleCompaction(compactionInstantTime, client, cfg);\n      moveCompactionFromRequestedToInflight(compactionInstantTime, cfg);\n\n      \r\n      runNextDeltaCommits(client, readClient, Arrays.asList(thirdInstantTime, fourthInstantTime), records, cfg, false,\n          Arrays.asList(compactionInstantTime));\n\n      \r\n      executeCompaction(compactionInstantTime, client, hoodieTable, cfg, numRecs, true);\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestAsyncCompaction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":157,"status":"B"},{"authorDate":"2020-10-02 05:25:29","commitOrder":1,"curCode":"  public void testInterleavedCompaction() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfig(true);\n    try (SparkRDDWriteClient client = getHoodieWriteClient(cfg, true);) {\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      String firstInstantTime = \"001\";\n      String secondInstantTime = \"004\";\n      String compactionInstantTime = \"005\";\n      String thirdInstantTime = \"006\";\n      String fourthInstantTime = \"007\";\n\n      int numRecs = 2000;\n\n      List<HoodieRecord> records = dataGen.generateInserts(firstInstantTime, numRecs);\n      records = runNextDeltaCommits(client, readClient, Arrays.asList(firstInstantTime, secondInstantTime), records, cfg, true,\n          new ArrayList<>());\n\n      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n      HoodieTable hoodieTable = getHoodieTable(metaClient, cfg);\n      scheduleCompaction(compactionInstantTime, client, cfg);\n\n      runNextDeltaCommits(client, readClient, Arrays.asList(thirdInstantTime, fourthInstantTime), records, cfg, false,\n          Arrays.asList(compactionInstantTime));\n      executeCompaction(compactionInstantTime, client, hoodieTable, cfg, numRecs, true);\n    }\n  }\n","date":"2020-10-02 05:25:29","endLine":334,"groupId":"5431","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testInterleavedCompaction","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/8d/a1f3ddc6755d52a2573d745184c65ad0172c30.src","preCode":"  public void testInterleavedCompaction() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfig(true);\n    try (SparkRDDWriteClient client = getHoodieWriteClient(cfg, true);) {\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      String firstInstantTime = \"001\";\n      String secondInstantTime = \"004\";\n      String compactionInstantTime = \"005\";\n      String thirdInstantTime = \"006\";\n      String fourthInstantTime = \"007\";\n\n      int numRecs = 2000;\n\n      List<HoodieRecord> records = dataGen.generateInserts(firstInstantTime, numRecs);\n      records = runNextDeltaCommits(client, readClient, Arrays.asList(firstInstantTime, secondInstantTime), records, cfg, true,\n          new ArrayList<>());\n\n      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n      HoodieTable hoodieTable = getHoodieTable(metaClient, cfg);\n      scheduleCompaction(compactionInstantTime, client, cfg);\n\n      runNextDeltaCommits(client, readClient, Arrays.asList(thirdInstantTime, fourthInstantTime), records, cfg, false,\n          Arrays.asList(compactionInstantTime));\n      executeCompaction(compactionInstantTime, client, hoodieTable, cfg, numRecs, true);\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestAsyncCompaction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":309,"status":"B"}],"commitId":"1f7add92916c37b05be270d9c75a9042134ec506","commitMessage":"@@@[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)\n\n- This change breaks `hudi-client` into `hudi-client-common` and `hudi-spark-client` modules \n- Simple usages of Spark using jsc.parallelize() has been redone using EngineContext#map.  EngineContext#flatMap etc\n- Code changes in the PR.  break classes into `BaseXYZ` parent classes with no spark dependencies living in `hudi-client-common`\n- Classes on `hudi-spark-client` are named `SparkXYZ` extending the parent classes with all the Spark dependencies\n- To simplify/cleanup.  HoodieIndex#fetchRecordLocation has been removed and its usages in tests replaced with alternatives\n\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2020-10-02 05:25:29","modifiedFileCount":"31","status":"B","submitter":"Mathieu"},{"authorTime":"2021-02-20 09:54:26","codes":[{"authorDate":"2021-02-20 09:54:26","commitOrder":2,"curCode":"  public void testInflightCompaction() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfig(true);\n    try (SparkRDDWriteClient client = getHoodieWriteClient(cfg, true);) {\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      String firstInstantTime = \"001\";\n      String secondInstantTime = \"004\";\n      String compactionInstantTime = \"005\";\n      String thirdInstantTime = \"006\";\n      String fourthInstantTime = \"007\";\n\n      int numRecs = 2000;\n\n      List<HoodieRecord> records = dataGen.generateInserts(firstInstantTime, numRecs);\n      records = runNextDeltaCommits(client, readClient, Arrays.asList(firstInstantTime, secondInstantTime), records, cfg, true,\n          new ArrayList<>());\n\n      \r\n      HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n      HoodieTable hoodieTable = getHoodieTable(metaClient, cfg);\n      scheduleCompaction(compactionInstantTime, client, cfg);\n      moveCompactionFromRequestedToInflight(compactionInstantTime, cfg);\n\n      \r\n      runNextDeltaCommits(client, readClient, Arrays.asList(thirdInstantTime, fourthInstantTime), records, cfg, false,\n          Arrays.asList(compactionInstantTime));\n\n      \r\n      executeCompaction(compactionInstantTime, client, hoodieTable, cfg, numRecs, true);\n    }\n  }\n","date":"2021-02-20 09:54:26","endLine":192,"groupId":"5949","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testInflightCompaction","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/6e/8326c6ad0a3d0664ceacbae401ee5a9b8cdde4.src","preCode":"  public void testInflightCompaction() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfig(true);\n    try (SparkRDDWriteClient client = getHoodieWriteClient(cfg, true);) {\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      String firstInstantTime = \"001\";\n      String secondInstantTime = \"004\";\n      String compactionInstantTime = \"005\";\n      String thirdInstantTime = \"006\";\n      String fourthInstantTime = \"007\";\n\n      int numRecs = 2000;\n\n      List<HoodieRecord> records = dataGen.generateInserts(firstInstantTime, numRecs);\n      records = runNextDeltaCommits(client, readClient, Arrays.asList(firstInstantTime, secondInstantTime), records, cfg, true,\n          new ArrayList<>());\n\n      \r\n      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n      HoodieTable hoodieTable = getHoodieTable(metaClient, cfg);\n      scheduleCompaction(compactionInstantTime, client, cfg);\n      moveCompactionFromRequestedToInflight(compactionInstantTime, cfg);\n\n      \r\n      runNextDeltaCommits(client, readClient, Arrays.asList(thirdInstantTime, fourthInstantTime), records, cfg, false,\n          Arrays.asList(compactionInstantTime));\n\n      \r\n      executeCompaction(compactionInstantTime, client, hoodieTable, cfg, numRecs, true);\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestAsyncCompaction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":162,"status":"M"},{"authorDate":"2021-02-20 09:54:26","commitOrder":2,"curCode":"  public void testInterleavedCompaction() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfig(true);\n    try (SparkRDDWriteClient client = getHoodieWriteClient(cfg, true);) {\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      String firstInstantTime = \"001\";\n      String secondInstantTime = \"004\";\n      String compactionInstantTime = \"005\";\n      String thirdInstantTime = \"006\";\n      String fourthInstantTime = \"007\";\n\n      int numRecs = 2000;\n\n      List<HoodieRecord> records = dataGen.generateInserts(firstInstantTime, numRecs);\n      records = runNextDeltaCommits(client, readClient, Arrays.asList(firstInstantTime, secondInstantTime), records, cfg, true,\n          new ArrayList<>());\n\n      HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n      HoodieTable hoodieTable = getHoodieTable(metaClient, cfg);\n      scheduleCompaction(compactionInstantTime, client, cfg);\n\n      runNextDeltaCommits(client, readClient, Arrays.asList(thirdInstantTime, fourthInstantTime), records, cfg, false,\n          Arrays.asList(compactionInstantTime));\n      executeCompaction(compactionInstantTime, client, hoodieTable, cfg, numRecs, true);\n    }\n  }\n","date":"2021-02-20 09:54:26","endLine":339,"groupId":"5949","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testInterleavedCompaction","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/6e/8326c6ad0a3d0664ceacbae401ee5a9b8cdde4.src","preCode":"  public void testInterleavedCompaction() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfig(true);\n    try (SparkRDDWriteClient client = getHoodieWriteClient(cfg, true);) {\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      String firstInstantTime = \"001\";\n      String secondInstantTime = \"004\";\n      String compactionInstantTime = \"005\";\n      String thirdInstantTime = \"006\";\n      String fourthInstantTime = \"007\";\n\n      int numRecs = 2000;\n\n      List<HoodieRecord> records = dataGen.generateInserts(firstInstantTime, numRecs);\n      records = runNextDeltaCommits(client, readClient, Arrays.asList(firstInstantTime, secondInstantTime), records, cfg, true,\n          new ArrayList<>());\n\n      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n      HoodieTable hoodieTable = getHoodieTable(metaClient, cfg);\n      scheduleCompaction(compactionInstantTime, client, cfg);\n\n      runNextDeltaCommits(client, readClient, Arrays.asList(thirdInstantTime, fourthInstantTime), records, cfg, false,\n          Arrays.asList(compactionInstantTime));\n      executeCompaction(compactionInstantTime, client, hoodieTable, cfg, numRecs, true);\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestAsyncCompaction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":314,"status":"M"}],"commitId":"c9fcf964b2bae56a54cb72951c8d8999eb323ed6","commitMessage":"@@@[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)\n\n","date":"2021-02-20 09:54:26","modifiedFileCount":"57","status":"M","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-02-20 12:12:22","codes":[{"authorDate":"2021-02-20 12:12:22","commitOrder":3,"curCode":"  public void testInflightCompaction() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfig(true);\n    try (SparkRDDWriteClient client = getHoodieWriteClient(cfg);) {\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      String firstInstantTime = \"001\";\n      String secondInstantTime = \"004\";\n      String compactionInstantTime = \"005\";\n      String thirdInstantTime = \"006\";\n      String fourthInstantTime = \"007\";\n\n      int numRecs = 2000;\n\n      List<HoodieRecord> records = dataGen.generateInserts(firstInstantTime, numRecs);\n      records = runNextDeltaCommits(client, readClient, Arrays.asList(firstInstantTime, secondInstantTime), records, cfg, true,\n          new ArrayList<>());\n\n      \r\n      HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n      HoodieTable hoodieTable = getHoodieTable(metaClient, cfg);\n      scheduleCompaction(compactionInstantTime, client, cfg);\n      moveCompactionFromRequestedToInflight(compactionInstantTime, cfg);\n\n      \r\n      runNextDeltaCommits(client, readClient, Arrays.asList(thirdInstantTime, fourthInstantTime), records, cfg, false,\n          Arrays.asList(compactionInstantTime));\n\n      \r\n      executeCompaction(compactionInstantTime, client, hoodieTable, cfg, numRecs, true);\n    }\n  }\n","date":"2021-02-20 12:12:22","endLine":192,"groupId":"10578","id":5,"instanceNumber":1,"isCurCommit":1,"methodName":"testInflightCompaction","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/79/c415a4bc268a7990f3398e14469d180261999d.src","preCode":"  public void testInflightCompaction() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfig(true);\n    try (SparkRDDWriteClient client = getHoodieWriteClient(cfg, true);) {\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      String firstInstantTime = \"001\";\n      String secondInstantTime = \"004\";\n      String compactionInstantTime = \"005\";\n      String thirdInstantTime = \"006\";\n      String fourthInstantTime = \"007\";\n\n      int numRecs = 2000;\n\n      List<HoodieRecord> records = dataGen.generateInserts(firstInstantTime, numRecs);\n      records = runNextDeltaCommits(client, readClient, Arrays.asList(firstInstantTime, secondInstantTime), records, cfg, true,\n          new ArrayList<>());\n\n      \r\n      HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n      HoodieTable hoodieTable = getHoodieTable(metaClient, cfg);\n      scheduleCompaction(compactionInstantTime, client, cfg);\n      moveCompactionFromRequestedToInflight(compactionInstantTime, cfg);\n\n      \r\n      runNextDeltaCommits(client, readClient, Arrays.asList(thirdInstantTime, fourthInstantTime), records, cfg, false,\n          Arrays.asList(compactionInstantTime));\n\n      \r\n      executeCompaction(compactionInstantTime, client, hoodieTable, cfg, numRecs, true);\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestAsyncCompaction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":162,"status":"M"},{"authorDate":"2021-02-20 12:12:22","commitOrder":3,"curCode":"  public void testInterleavedCompaction() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfig(true);\n    try (SparkRDDWriteClient client = getHoodieWriteClient(cfg);) {\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      String firstInstantTime = \"001\";\n      String secondInstantTime = \"004\";\n      String compactionInstantTime = \"005\";\n      String thirdInstantTime = \"006\";\n      String fourthInstantTime = \"007\";\n\n      int numRecs = 2000;\n\n      List<HoodieRecord> records = dataGen.generateInserts(firstInstantTime, numRecs);\n      records = runNextDeltaCommits(client, readClient, Arrays.asList(firstInstantTime, secondInstantTime), records, cfg, true,\n          new ArrayList<>());\n\n      HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n      HoodieTable hoodieTable = getHoodieTable(metaClient, cfg);\n      scheduleCompaction(compactionInstantTime, client, cfg);\n\n      runNextDeltaCommits(client, readClient, Arrays.asList(thirdInstantTime, fourthInstantTime), records, cfg, false,\n          Arrays.asList(compactionInstantTime));\n      executeCompaction(compactionInstantTime, client, hoodieTable, cfg, numRecs, true);\n    }\n  }\n","date":"2021-02-20 12:12:22","endLine":339,"groupId":"10578","id":6,"instanceNumber":2,"isCurCommit":1,"methodName":"testInterleavedCompaction","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/79/c415a4bc268a7990f3398e14469d180261999d.src","preCode":"  public void testInterleavedCompaction() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfig(true);\n    try (SparkRDDWriteClient client = getHoodieWriteClient(cfg, true);) {\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      String firstInstantTime = \"001\";\n      String secondInstantTime = \"004\";\n      String compactionInstantTime = \"005\";\n      String thirdInstantTime = \"006\";\n      String fourthInstantTime = \"007\";\n\n      int numRecs = 2000;\n\n      List<HoodieRecord> records = dataGen.generateInserts(firstInstantTime, numRecs);\n      records = runNextDeltaCommits(client, readClient, Arrays.asList(firstInstantTime, secondInstantTime), records, cfg, true,\n          new ArrayList<>());\n\n      HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n      HoodieTable hoodieTable = getHoodieTable(metaClient, cfg);\n      scheduleCompaction(compactionInstantTime, client, cfg);\n\n      runNextDeltaCommits(client, readClient, Arrays.asList(thirdInstantTime, fourthInstantTime), records, cfg, false,\n          Arrays.asList(compactionInstantTime));\n      executeCompaction(compactionInstantTime, client, hoodieTable, cfg, numRecs, true);\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestAsyncCompaction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":314,"status":"M"}],"commitId":"ffcfb58bacab377bc72d20041baa54a3fd8fc812","commitMessage":"@@@[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)\n\n1. Refactor rollback and move cleaning failed commits logic into cleaner\n2. Introduce hoodie heartbeat to ascertain failed commits\n3. Fix test cases","date":"2021-02-20 12:12:22","modifiedFileCount":"56","status":"M","submitter":"n3nash"}]
