[{"authorTime":"2020-05-14 06:37:03","codes":[{"authorDate":"2020-05-14 06:37:03","commitOrder":1,"curCode":"  private List<GenericRecord> createInsertRecords(Path srcFolder) throws ParseException, IOException {\n    Path srcFile = new Path(srcFolder.toString(), \"file1.parquet\");\n    long startTime = HoodieActiveTimeline.COMMIT_FORMATTER.parse(\"20170203000000\").getTime() / 1000;\n    List<GenericRecord> records = new ArrayList<GenericRecord>();\n    for (long recordNum = 0; recordNum < 96; recordNum++) {\n      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-\" + recordNum,\n          \"driver-\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n    }\n    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(srcFile)\n        .withSchema(HoodieTestDataGenerator.AVRO_SCHEMA).withConf(HoodieTestUtils.getDefaultHadoopConf()).build()) {\n      for (GenericRecord record : records) {\n        writer.write(record);\n      }\n    }\n    return records;\n  }\n","date":"2020-05-14 06:37:03","endLine":282,"groupId":"702","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"createInsertRecords","params":"(PathsrcFolder)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/f7/38cdfdece690b977ec31850ebdf81d1d20a181.src","preCode":"  private List<GenericRecord> createInsertRecords(Path srcFolder) throws ParseException, IOException {\n    Path srcFile = new Path(srcFolder.toString(), \"file1.parquet\");\n    long startTime = HoodieActiveTimeline.COMMIT_FORMATTER.parse(\"20170203000000\").getTime() / 1000;\n    List<GenericRecord> records = new ArrayList<GenericRecord>();\n    for (long recordNum = 0; recordNum < 96; recordNum++) {\n      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-\" + recordNum,\n          \"driver-\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n    }\n    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(srcFile)\n        .withSchema(HoodieTestDataGenerator.AVRO_SCHEMA).withConf(HoodieTestUtils.getDefaultHadoopConf()).build()) {\n      for (GenericRecord record : records) {\n        writer.write(record);\n      }\n    }\n    return records;\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":267,"status":"B"},{"authorDate":"2020-05-14 06:37:03","commitOrder":1,"curCode":"  private List<GenericRecord> createUpsertRecords(Path srcFolder) throws ParseException, IOException {\n    Path srcFile = new Path(srcFolder.toString(), \"file1.parquet\");\n    long startTime = HoodieActiveTimeline.COMMIT_FORMATTER.parse(\"20170203000000\").getTime() / 1000;\n    List<GenericRecord> records = new ArrayList<GenericRecord>();\n    \r\n    for (long recordNum = 0; recordNum < 11; recordNum++) {\n      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-upsert-\" + recordNum,\n          \"driver-upsert\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n    }\n    \r\n    for (long recordNum = 96; recordNum < 100; recordNum++) {\n      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-upsert-\" + recordNum,\n          \"driver-upsert\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n    }\n    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(srcFile)\n        .withSchema(HoodieTestDataGenerator.AVRO_SCHEMA).withConf(HoodieTestUtils.getDefaultHadoopConf()).build()) {\n      for (GenericRecord record : records) {\n        writer.write(record);\n      }\n    }\n    return records;\n  }\n","date":"2020-05-14 06:37:03","endLine":305,"groupId":"928","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"createUpsertRecords","params":"(PathsrcFolder)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/f7/38cdfdece690b977ec31850ebdf81d1d20a181.src","preCode":"  private List<GenericRecord> createUpsertRecords(Path srcFolder) throws ParseException, IOException {\n    Path srcFile = new Path(srcFolder.toString(), \"file1.parquet\");\n    long startTime = HoodieActiveTimeline.COMMIT_FORMATTER.parse(\"20170203000000\").getTime() / 1000;\n    List<GenericRecord> records = new ArrayList<GenericRecord>();\n    \r\n    for (long recordNum = 0; recordNum < 11; recordNum++) {\n      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-upsert-\" + recordNum,\n          \"driver-upsert\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n    }\n    \r\n    for (long recordNum = 96; recordNum < 100; recordNum++) {\n      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-upsert-\" + recordNum,\n          \"driver-upsert\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n    }\n    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(srcFile)\n        .withSchema(HoodieTestDataGenerator.AVRO_SCHEMA).withConf(HoodieTestUtils.getDefaultHadoopConf()).build()) {\n      for (GenericRecord record : records) {\n        writer.write(record);\n      }\n    }\n    return records;\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":284,"status":"B"}],"commitId":"0d4848b68b625a17d05b38864a84a6cc71189bfa","commitMessage":"@@@[HUDI-811] Restructure test packages (#1607)\n\n* restructure hudi-spark tests\n* restructure hudi-timeline-service tests\n* restructure hudi-hadoop-mr hudi-utilities tests\n* restructure hudi-hive-sync tests","date":"2020-05-14 06:37:03","modifiedFileCount":"11","status":"B","submitter":"Raymond Xu"},{"authorTime":"2020-05-14 19:15:49","codes":[{"authorDate":"2020-05-14 19:15:49","commitOrder":2,"curCode":"  public List<GenericRecord> createInsertRecords(Path srcFolder) throws ParseException, IOException {\n    Path srcFile = new Path(srcFolder.toString(), \"file1.parquet\");\n    long startTime = HoodieActiveTimeline.COMMIT_FORMATTER.parse(\"20170203000000\").getTime() / 1000;\n    List<GenericRecord> records = new ArrayList<GenericRecord>();\n    for (long recordNum = 0; recordNum < 96; recordNum++) {\n      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-\" + recordNum,\n          \"driver-\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n    }\n    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(srcFile)\n        .withSchema(HoodieTestDataGenerator.AVRO_SCHEMA).withConf(HoodieTestUtils.getDefaultHadoopConf()).build()) {\n      for (GenericRecord record : records) {\n        writer.write(record);\n      }\n    }\n    return records;\n  }\n","date":"2020-05-14 19:15:49","endLine":282,"groupId":"702","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"createInsertRecords","params":"(PathsrcFolder)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/a8/474f670aec9287d2fb774ab52b609cb92ec63d.src","preCode":"  private List<GenericRecord> createInsertRecords(Path srcFolder) throws ParseException, IOException {\n    Path srcFile = new Path(srcFolder.toString(), \"file1.parquet\");\n    long startTime = HoodieActiveTimeline.COMMIT_FORMATTER.parse(\"20170203000000\").getTime() / 1000;\n    List<GenericRecord> records = new ArrayList<GenericRecord>();\n    for (long recordNum = 0; recordNum < 96; recordNum++) {\n      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-\" + recordNum,\n          \"driver-\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n    }\n    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(srcFile)\n        .withSchema(HoodieTestDataGenerator.AVRO_SCHEMA).withConf(HoodieTestUtils.getDefaultHadoopConf()).build()) {\n      for (GenericRecord record : records) {\n        writer.write(record);\n      }\n    }\n    return records;\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":267,"status":"M"},{"authorDate":"2020-05-14 19:15:49","commitOrder":2,"curCode":"  public List<GenericRecord> createUpsertRecords(Path srcFolder) throws ParseException, IOException {\n    Path srcFile = new Path(srcFolder.toString(), \"file1.parquet\");\n    long startTime = HoodieActiveTimeline.COMMIT_FORMATTER.parse(\"20170203000000\").getTime() / 1000;\n    List<GenericRecord> records = new ArrayList<GenericRecord>();\n    \r\n    for (long recordNum = 0; recordNum < 11; recordNum++) {\n      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-upsert-\" + recordNum,\n          \"driver-upsert\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n    }\n    \r\n    for (long recordNum = 96; recordNum < 100; recordNum++) {\n      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-upsert-\" + recordNum,\n          \"driver-upsert\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n    }\n    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(srcFile)\n        .withSchema(HoodieTestDataGenerator.AVRO_SCHEMA).withConf(HoodieTestUtils.getDefaultHadoopConf()).build()) {\n      for (GenericRecord record : records) {\n        writer.write(record);\n      }\n    }\n    return records;\n  }\n","date":"2020-05-14 19:15:49","endLine":305,"groupId":"928","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"createUpsertRecords","params":"(PathsrcFolder)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/a8/474f670aec9287d2fb774ab52b609cb92ec63d.src","preCode":"  private List<GenericRecord> createUpsertRecords(Path srcFolder) throws ParseException, IOException {\n    Path srcFile = new Path(srcFolder.toString(), \"file1.parquet\");\n    long startTime = HoodieActiveTimeline.COMMIT_FORMATTER.parse(\"20170203000000\").getTime() / 1000;\n    List<GenericRecord> records = new ArrayList<GenericRecord>();\n    \r\n    for (long recordNum = 0; recordNum < 11; recordNum++) {\n      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-upsert-\" + recordNum,\n          \"driver-upsert\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n    }\n    \r\n    for (long recordNum = 96; recordNum < 100; recordNum++) {\n      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-upsert-\" + recordNum,\n          \"driver-upsert\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n    }\n    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(srcFile)\n        .withSchema(HoodieTestDataGenerator.AVRO_SCHEMA).withConf(HoodieTestUtils.getDefaultHadoopConf()).build()) {\n      for (GenericRecord record : records) {\n        writer.write(record);\n      }\n    }\n    return records;\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":284,"status":"M"}],"commitId":"3a2fe13fcb7c168f8ff023e3bdb6ae482b400316","commitMessage":"@@@[HUDI-701] Add unit test for HDFSParquetImportCommand (#1574)\n\n","date":"2020-05-14 19:15:49","modifiedFileCount":"3","status":"M","submitter":"hongdd"},{"authorTime":"2021-07-27 05:21:04","codes":[{"authorDate":"2021-07-27 05:21:04","commitOrder":3,"curCode":"  public List<GenericRecord> createInsertRecords(Path srcFolder) throws ParseException, IOException {\n    Path srcFile = new Path(srcFolder.toString(), \"file1.parquet\");\n    long startTime = HoodieActiveTimeline.COMMIT_FORMATTER.parse(\"20170203000000\").getTime() / 1000;\n    List<GenericRecord> records = new ArrayList<GenericRecord>();\n    for (long recordNum = 0; recordNum < 96; recordNum++) {\n      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"0\", \"rider-\" + recordNum,\n          \"driver-\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n    }\n    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(srcFile)\n        .withSchema(HoodieTestDataGenerator.AVRO_SCHEMA).withConf(HoodieTestUtils.getDefaultHadoopConf()).build()) {\n      for (GenericRecord record : records) {\n        writer.write(record);\n      }\n    }\n    return records;\n  }\n","date":"2021-07-27 05:21:04","endLine":244,"groupId":"10278","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"createInsertRecords","params":"(PathsrcFolder)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/c7/2cf721ccacbf23c54155f20fe628d794d81b02.src","preCode":"  public List<GenericRecord> createInsertRecords(Path srcFolder) throws ParseException, IOException {\n    Path srcFile = new Path(srcFolder.toString(), \"file1.parquet\");\n    long startTime = HoodieActiveTimeline.COMMIT_FORMATTER.parse(\"20170203000000\").getTime() / 1000;\n    List<GenericRecord> records = new ArrayList<GenericRecord>();\n    for (long recordNum = 0; recordNum < 96; recordNum++) {\n      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-\" + recordNum,\n          \"driver-\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n    }\n    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(srcFile)\n        .withSchema(HoodieTestDataGenerator.AVRO_SCHEMA).withConf(HoodieTestUtils.getDefaultHadoopConf()).build()) {\n      for (GenericRecord record : records) {\n        writer.write(record);\n      }\n    }\n    return records;\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":229,"status":"M"},{"authorDate":"2021-07-27 05:21:04","commitOrder":3,"curCode":"  public List<GenericRecord> createUpsertRecords(Path srcFolder) throws ParseException, IOException {\n    Path srcFile = new Path(srcFolder.toString(), \"file1.parquet\");\n    long startTime = HoodieActiveTimeline.COMMIT_FORMATTER.parse(\"20170203000000\").getTime() / 1000;\n    List<GenericRecord> records = new ArrayList<GenericRecord>();\n    \r\n    for (long recordNum = 0; recordNum < 11; recordNum++) {\n      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"0\", \"rider-upsert-\" + recordNum,\n          \"driver-upsert\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n    }\n    \r\n    for (long recordNum = 96; recordNum < 100; recordNum++) {\n      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"0\", \"rider-upsert-\" + recordNum,\n          \"driver-upsert\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n    }\n    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(srcFile)\n        .withSchema(HoodieTestDataGenerator.AVRO_SCHEMA).withConf(HoodieTestUtils.getDefaultHadoopConf()).build()) {\n      for (GenericRecord record : records) {\n        writer.write(record);\n      }\n    }\n    return records;\n  }\n","date":"2021-07-27 05:21:04","endLine":267,"groupId":"10278","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"createUpsertRecords","params":"(PathsrcFolder)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/c7/2cf721ccacbf23c54155f20fe628d794d81b02.src","preCode":"  public List<GenericRecord> createUpsertRecords(Path srcFolder) throws ParseException, IOException {\n    Path srcFile = new Path(srcFolder.toString(), \"file1.parquet\");\n    long startTime = HoodieActiveTimeline.COMMIT_FORMATTER.parse(\"20170203000000\").getTime() / 1000;\n    List<GenericRecord> records = new ArrayList<GenericRecord>();\n    \r\n    for (long recordNum = 0; recordNum < 11; recordNum++) {\n      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-upsert-\" + recordNum,\n          \"driver-upsert\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n    }\n    \r\n    for (long recordNum = 96; recordNum < 100; recordNum++) {\n      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-upsert-\" + recordNum,\n          \"driver-upsert\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n    }\n    try (ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(srcFile)\n        .withSchema(HoodieTestDataGenerator.AVRO_SCHEMA).withConf(HoodieTestUtils.getDefaultHadoopConf()).build()) {\n      for (GenericRecord record : records) {\n        writer.write(record);\n      }\n    }\n    return records;\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":246,"status":"M"}],"commitId":"61148c1c43c9ff5ba04b6c174e9e2a006db9e7b3","commitMessage":"@@@[HUDI-2176.  2178.  2179] Adding virtual key support to COW table (#3306)\n\n","date":"2021-07-27 05:21:04","modifiedFileCount":"42","status":"M","submitter":"Sivabalan Narayanan"}]
