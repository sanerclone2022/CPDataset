[{"authorTime":"2021-09-06 12:07:55","codes":[{"authorDate":"2021-09-06 12:07:55","commitOrder":1,"curCode":"  public void testSimpleTagLocationAndUpdate(HoodieTableType tableType) throws Exception {\n    metaClient = HoodieTestUtils.init(hadoopConf, basePath(), tableType);\n\n    final String newCommitTime = \"001\";\n    final int numRecords = 10;\n    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n    JavaRDD<HoodieRecord> writeRecords = jsc().parallelize(records, 1);\n\n    \r\n    HoodieWriteConfig config = getConfig();\n    SparkHoodieHBaseIndex index = new SparkHoodieHBaseIndex(config);\n    try (SparkRDDWriteClient writeClient = getHoodieWriteClient(config);) {\n      metaClient = HoodieTableMetaClient.reload(metaClient);\n      HoodieTable hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n\n      \r\n      JavaRDD<HoodieRecord> records1 = index.tagLocation(writeRecords, context(), hoodieTable);\n      assertEquals(0, records1.filter(record -> record.isCurrentLocationKnown()).count());\n\n      \r\n      writeClient.startCommitWithTime(newCommitTime);\n      JavaRDD<WriteStatus> writeStatues = writeClient.upsert(writeRecords, newCommitTime);\n      assertNoWriteErrors(writeStatues.collect());\n\n      \r\n      JavaRDD<HoodieRecord> records2 = index.tagLocation(writeRecords, context(), hoodieTable);\n      assertEquals(0, records2.filter(record -> record.isCurrentLocationKnown()).count());\n\n      \r\n      writeClient.commit(newCommitTime, writeStatues);\n      \r\n      metaClient = HoodieTableMetaClient.reload(metaClient);\n      hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n      List<HoodieRecord> records3 = index.tagLocation(writeRecords, context(), hoodieTable).collect();\n      assertEquals(numRecords, records3.stream().filter(record -> record.isCurrentLocationKnown()).count());\n      assertEquals(numRecords, records3.stream().map(record -> record.getKey().getRecordKey()).distinct().count());\n      assertEquals(numRecords, records3.stream().filter(record -> (record.getCurrentLocation() != null\n          && record.getCurrentLocation().getInstantTime().equals(newCommitTime))).distinct().count());\n    }\n  }\n","date":"2021-09-06 12:07:55","endLine":182,"groupId":"5547","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testSimpleTagLocationAndUpdate","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ef/aad254f796bfbd9fd527d9adec3b3faa7e020f.src","preCode":"  public void testSimpleTagLocationAndUpdate(HoodieTableType tableType) throws Exception {\n    metaClient = HoodieTestUtils.init(hadoopConf, basePath(), tableType);\n\n    final String newCommitTime = \"001\";\n    final int numRecords = 10;\n    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n    JavaRDD<HoodieRecord> writeRecords = jsc().parallelize(records, 1);\n\n    \r\n    HoodieWriteConfig config = getConfig();\n    SparkHoodieHBaseIndex index = new SparkHoodieHBaseIndex(config);\n    try (SparkRDDWriteClient writeClient = getHoodieWriteClient(config);) {\n      metaClient = HoodieTableMetaClient.reload(metaClient);\n      HoodieTable hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n\n      \r\n      JavaRDD<HoodieRecord> records1 = index.tagLocation(writeRecords, context(), hoodieTable);\n      assertEquals(0, records1.filter(record -> record.isCurrentLocationKnown()).count());\n\n      \r\n      writeClient.startCommitWithTime(newCommitTime);\n      JavaRDD<WriteStatus> writeStatues = writeClient.upsert(writeRecords, newCommitTime);\n      assertNoWriteErrors(writeStatues.collect());\n\n      \r\n      JavaRDD<HoodieRecord> records2 = index.tagLocation(writeRecords, context(), hoodieTable);\n      assertEquals(0, records2.filter(record -> record.isCurrentLocationKnown()).count());\n\n      \r\n      writeClient.commit(newCommitTime, writeStatues);\n      \r\n      metaClient = HoodieTableMetaClient.reload(metaClient);\n      hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n      List<HoodieRecord> records3 = index.tagLocation(writeRecords, context(), hoodieTable).collect();\n      assertEquals(numRecords, records3.stream().filter(record -> record.isCurrentLocationKnown()).count());\n      assertEquals(numRecords, records3.stream().map(record -> record.getKey().getRecordKey()).distinct().count());\n      assertEquals(numRecords, records3.stream().filter(record -> (record.getCurrentLocation() != null\n          && record.getCurrentLocation().getInstantTime().equals(newCommitTime))).distinct().count());\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHBaseIndex.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":143,"status":"B"},{"authorDate":"2021-09-06 12:07:55","commitOrder":1,"curCode":"  public void testSmallBatchSize() throws Exception {\n    final String newCommitTime = \"001\";\n    final int numRecords = 10;\n    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n    JavaRDD<HoodieRecord> writeRecords = jsc().parallelize(records, 1);\n\n    \r\n    HoodieWriteConfig config = getConfig(2);\n    SparkHoodieHBaseIndex index = new SparkHoodieHBaseIndex(config);\n    try (SparkRDDWriteClient writeClient = getHoodieWriteClient(config);) {\n      metaClient = HoodieTableMetaClient.reload(metaClient);\n      HoodieTable hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n\n      \r\n      JavaRDD<HoodieRecord> records1 = index.tagLocation(writeRecords, context(), hoodieTable);\n      assertEquals(0, records1.filter(record -> record.isCurrentLocationKnown()).count());\n      \r\n      writeClient.startCommitWithTime(newCommitTime);\n      JavaRDD<WriteStatus> writeStatues = writeClient.upsert(writeRecords, newCommitTime);\n      assertNoWriteErrors(writeStatues.collect());\n\n      \r\n      \r\n      JavaRDD<HoodieRecord> records2 = index.tagLocation(writeRecords, context(), hoodieTable);\n      assertEquals(0, records2.filter(record -> record.isCurrentLocationKnown()).count());\n\n      \r\n      writeClient.commit(newCommitTime, writeStatues);\n      \r\n      metaClient = HoodieTableMetaClient.reload(metaClient);\n      hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n      List<HoodieRecord> records3 = index.tagLocation(writeRecords, context(), hoodieTable).collect();\n      assertEquals(numRecords, records3.stream().filter(record -> record.isCurrentLocationKnown()).count());\n      assertEquals(numRecords, records3.stream().map(record -> record.getKey().getRecordKey()).distinct().count());\n      assertEquals(numRecords, records3.stream().filter(record -> (record.getCurrentLocation() != null\n          && record.getCurrentLocation().getInstantTime().equals(newCommitTime))).distinct().count());\n    }\n  }\n","date":"2021-09-06 12:07:55","endLine":720,"groupId":"5547","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testSmallBatchSize","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ef/aad254f796bfbd9fd527d9adec3b3faa7e020f.src","preCode":"  public void testSmallBatchSize() throws Exception {\n    final String newCommitTime = \"001\";\n    final int numRecords = 10;\n    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n    JavaRDD<HoodieRecord> writeRecords = jsc().parallelize(records, 1);\n\n    \r\n    HoodieWriteConfig config = getConfig(2);\n    SparkHoodieHBaseIndex index = new SparkHoodieHBaseIndex(config);\n    try (SparkRDDWriteClient writeClient = getHoodieWriteClient(config);) {\n      metaClient = HoodieTableMetaClient.reload(metaClient);\n      HoodieTable hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n\n      \r\n      JavaRDD<HoodieRecord> records1 = index.tagLocation(writeRecords, context(), hoodieTable);\n      assertEquals(0, records1.filter(record -> record.isCurrentLocationKnown()).count());\n      \r\n      writeClient.startCommitWithTime(newCommitTime);\n      JavaRDD<WriteStatus> writeStatues = writeClient.upsert(writeRecords, newCommitTime);\n      assertNoWriteErrors(writeStatues.collect());\n\n      \r\n      \r\n      JavaRDD<HoodieRecord> records2 = index.tagLocation(writeRecords, context(), hoodieTable);\n      assertEquals(0, records2.filter(record -> record.isCurrentLocationKnown()).count());\n\n      \r\n      writeClient.commit(newCommitTime, writeStatues);\n      \r\n      metaClient = HoodieTableMetaClient.reload(metaClient);\n      hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n      List<HoodieRecord> records3 = index.tagLocation(writeRecords, context(), hoodieTable).collect();\n      assertEquals(numRecords, records3.stream().filter(record -> record.isCurrentLocationKnown()).count());\n      assertEquals(numRecords, records3.stream().map(record -> record.getKey().getRecordKey()).distinct().count());\n      assertEquals(numRecords, records3.stream().filter(record -> (record.getCurrentLocation() != null\n          && record.getCurrentLocation().getInstantTime().equals(newCommitTime))).distinct().count());\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHBaseIndex.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":683,"status":"B"}],"commitId":"f218693f5d20491802c48bb02b50a32e573100db","commitMessage":"@@@[MINOR] Fixing some functional tests by moving to right packages (#3596)\n\n","date":"2021-09-06 12:07:55","modifiedFileCount":"0","status":"B","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-09-06 12:07:55","codes":[{"authorDate":"2021-09-09 23:29:04","commitOrder":2,"curCode":"  public void testSimpleTagLocationAndUpdate(HoodieTableType tableType) throws Exception {\n    metaClient = HoodieTestUtils.init(hadoopConf, basePath, tableType);\n\n    final String newCommitTime = \"001\";\n    final int numRecords = 10;\n    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n    JavaRDD<HoodieRecord> writeRecords = jsc().parallelize(records, 1);\n\n    \r\n    HoodieWriteConfig config = getConfig();\n    SparkHoodieHBaseIndex index = new SparkHoodieHBaseIndex(config);\n    try (SparkRDDWriteClient writeClient = getHoodieWriteClient(config);) {\n      metaClient = HoodieTableMetaClient.reload(metaClient);\n      HoodieTable hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n\n      \r\n      JavaRDD<HoodieRecord> records1 = index.tagLocation(writeRecords, context(), hoodieTable);\n      assertEquals(0, records1.filter(record -> record.isCurrentLocationKnown()).count());\n\n      \r\n      writeClient.startCommitWithTime(newCommitTime);\n      JavaRDD<WriteStatus> writeStatues = writeClient.upsert(writeRecords, newCommitTime);\n      assertNoWriteErrors(writeStatues.collect());\n\n      \r\n      JavaRDD<HoodieRecord> records2 = index.tagLocation(writeRecords, context(), hoodieTable);\n      assertEquals(0, records2.filter(record -> record.isCurrentLocationKnown()).count());\n\n      \r\n      writeClient.commit(newCommitTime, writeStatues);\n      \r\n      metaClient = HoodieTableMetaClient.reload(metaClient);\n      hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n      List<HoodieRecord> records3 = index.tagLocation(writeRecords, context(), hoodieTable).collect();\n      assertEquals(numRecords, records3.stream().filter(record -> record.isCurrentLocationKnown()).count());\n      assertEquals(numRecords, records3.stream().map(record -> record.getKey().getRecordKey()).distinct().count());\n      assertEquals(numRecords, records3.stream().filter(record -> (record.getCurrentLocation() != null\n          && record.getCurrentLocation().getInstantTime().equals(newCommitTime))).distinct().count());\n    }\n  }\n","date":"2021-09-09 23:29:04","endLine":181,"groupId":"10666","id":3,"instanceNumber":1,"isCurCommit":1,"methodName":"testSimpleTagLocationAndUpdate","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/3a/d777475f4966d94642cdfff0b71feeb0648789.src","preCode":"  public void testSimpleTagLocationAndUpdate(HoodieTableType tableType) throws Exception {\n    metaClient = HoodieTestUtils.init(hadoopConf, basePath(), tableType);\n\n    final String newCommitTime = \"001\";\n    final int numRecords = 10;\n    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n    JavaRDD<HoodieRecord> writeRecords = jsc().parallelize(records, 1);\n\n    \r\n    HoodieWriteConfig config = getConfig();\n    SparkHoodieHBaseIndex index = new SparkHoodieHBaseIndex(config);\n    try (SparkRDDWriteClient writeClient = getHoodieWriteClient(config);) {\n      metaClient = HoodieTableMetaClient.reload(metaClient);\n      HoodieTable hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n\n      \r\n      JavaRDD<HoodieRecord> records1 = index.tagLocation(writeRecords, context(), hoodieTable);\n      assertEquals(0, records1.filter(record -> record.isCurrentLocationKnown()).count());\n\n      \r\n      writeClient.startCommitWithTime(newCommitTime);\n      JavaRDD<WriteStatus> writeStatues = writeClient.upsert(writeRecords, newCommitTime);\n      assertNoWriteErrors(writeStatues.collect());\n\n      \r\n      JavaRDD<HoodieRecord> records2 = index.tagLocation(writeRecords, context(), hoodieTable);\n      assertEquals(0, records2.filter(record -> record.isCurrentLocationKnown()).count());\n\n      \r\n      writeClient.commit(newCommitTime, writeStatues);\n      \r\n      metaClient = HoodieTableMetaClient.reload(metaClient);\n      hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n      List<HoodieRecord> records3 = index.tagLocation(writeRecords, context(), hoodieTable).collect();\n      assertEquals(numRecords, records3.stream().filter(record -> record.isCurrentLocationKnown()).count());\n      assertEquals(numRecords, records3.stream().map(record -> record.getKey().getRecordKey()).distinct().count());\n      assertEquals(numRecords, records3.stream().filter(record -> (record.getCurrentLocation() != null\n          && record.getCurrentLocation().getInstantTime().equals(newCommitTime))).distinct().count());\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHBaseIndex.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":142,"status":"M"},{"authorDate":"2021-09-06 12:07:55","commitOrder":2,"curCode":"  public void testSmallBatchSize() throws Exception {\n    final String newCommitTime = \"001\";\n    final int numRecords = 10;\n    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n    JavaRDD<HoodieRecord> writeRecords = jsc().parallelize(records, 1);\n\n    \r\n    HoodieWriteConfig config = getConfig(2);\n    SparkHoodieHBaseIndex index = new SparkHoodieHBaseIndex(config);\n    try (SparkRDDWriteClient writeClient = getHoodieWriteClient(config);) {\n      metaClient = HoodieTableMetaClient.reload(metaClient);\n      HoodieTable hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n\n      \r\n      JavaRDD<HoodieRecord> records1 = index.tagLocation(writeRecords, context(), hoodieTable);\n      assertEquals(0, records1.filter(record -> record.isCurrentLocationKnown()).count());\n      \r\n      writeClient.startCommitWithTime(newCommitTime);\n      JavaRDD<WriteStatus> writeStatues = writeClient.upsert(writeRecords, newCommitTime);\n      assertNoWriteErrors(writeStatues.collect());\n\n      \r\n      \r\n      JavaRDD<HoodieRecord> records2 = index.tagLocation(writeRecords, context(), hoodieTable);\n      assertEquals(0, records2.filter(record -> record.isCurrentLocationKnown()).count());\n\n      \r\n      writeClient.commit(newCommitTime, writeStatues);\n      \r\n      metaClient = HoodieTableMetaClient.reload(metaClient);\n      hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n      List<HoodieRecord> records3 = index.tagLocation(writeRecords, context(), hoodieTable).collect();\n      assertEquals(numRecords, records3.stream().filter(record -> record.isCurrentLocationKnown()).count());\n      assertEquals(numRecords, records3.stream().map(record -> record.getKey().getRecordKey()).distinct().count());\n      assertEquals(numRecords, records3.stream().filter(record -> (record.getCurrentLocation() != null\n          && record.getCurrentLocation().getInstantTime().equals(newCommitTime))).distinct().count());\n    }\n  }\n","date":"2021-09-06 12:07:55","endLine":720,"groupId":"10666","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testSmallBatchSize","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ef/aad254f796bfbd9fd527d9adec3b3faa7e020f.src","preCode":"  public void testSmallBatchSize() throws Exception {\n    final String newCommitTime = \"001\";\n    final int numRecords = 10;\n    List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, numRecords);\n    JavaRDD<HoodieRecord> writeRecords = jsc().parallelize(records, 1);\n\n    \r\n    HoodieWriteConfig config = getConfig(2);\n    SparkHoodieHBaseIndex index = new SparkHoodieHBaseIndex(config);\n    try (SparkRDDWriteClient writeClient = getHoodieWriteClient(config);) {\n      metaClient = HoodieTableMetaClient.reload(metaClient);\n      HoodieTable hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n\n      \r\n      JavaRDD<HoodieRecord> records1 = index.tagLocation(writeRecords, context(), hoodieTable);\n      assertEquals(0, records1.filter(record -> record.isCurrentLocationKnown()).count());\n      \r\n      writeClient.startCommitWithTime(newCommitTime);\n      JavaRDD<WriteStatus> writeStatues = writeClient.upsert(writeRecords, newCommitTime);\n      assertNoWriteErrors(writeStatues.collect());\n\n      \r\n      \r\n      JavaRDD<HoodieRecord> records2 = index.tagLocation(writeRecords, context(), hoodieTable);\n      assertEquals(0, records2.filter(record -> record.isCurrentLocationKnown()).count());\n\n      \r\n      writeClient.commit(newCommitTime, writeStatues);\n      \r\n      metaClient = HoodieTableMetaClient.reload(metaClient);\n      hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n      List<HoodieRecord> records3 = index.tagLocation(writeRecords, context(), hoodieTable).collect();\n      assertEquals(numRecords, records3.stream().filter(record -> record.isCurrentLocationKnown()).count());\n      assertEquals(numRecords, records3.stream().map(record -> record.getKey().getRecordKey()).distinct().count());\n      assertEquals(numRecords, records3.stream().filter(record -> (record.getCurrentLocation() != null\n          && record.getCurrentLocation().getInstantTime().equals(newCommitTime))).distinct().count());\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHBaseIndex.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":683,"status":"N"}],"commitId":"57c8113ee1941615a03f0efc2e3d46b634e940eb","commitMessage":"@@@[HUDI-2408] Deprecate FunctionalTestHarness to avoid init DFS (#3628)\n\n","date":"2021-09-09 23:29:04","modifiedFileCount":"9","status":"M","submitter":"Raymond Xu"}]
