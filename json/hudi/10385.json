[{"authorTime":"2021-08-19 23:21:20","codes":[{"authorDate":"2021-08-19 23:21:20","commitOrder":2,"curCode":"  void testBatchModeUpsertWithoutPartition(HoodieTableType tableType) {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.TABLE_NAME, tableType.name())\n        .withPartition(false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    execInsertSql(tableEnv, TestSQL.INSERT_T1);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(tableEnv, TestSQL.UPDATE_INSERT_T1);\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result2, TestData.DATA_SET_SOURCE_MERGED);\n  }\n","date":"2021-08-19 23:21:20","endLine":349,"groupId":"4111","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testBatchModeUpsertWithoutPartition","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9e/ffdcc8c67f59ad51cfd9a0e5ed56ecd41b530e.src","preCode":"  void testBatchModeUpsertWithoutPartition(HoodieTableType tableType) {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.TABLE_NAME, tableType.name())\n        .withPartition(false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    execInsertSql(tableEnv, TestSQL.INSERT_T1);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(tableEnv, TestSQL.UPDATE_INSERT_T1);\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result2, TestData.DATA_SET_SOURCE_MERGED);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":329,"status":"MB"},{"authorDate":"2021-08-19 23:21:20","commitOrder":2,"curCode":"  void testBatchModeUpsert(HoodieTableType tableType, boolean hiveStylePartitioning) {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.TABLE_NAME, tableType.name())\n        .option(FlinkOptions.HIVE_STYLE_PARTITIONING, hiveStylePartitioning)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    execInsertSql(tableEnv, TestSQL.INSERT_T1);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(tableEnv, TestSQL.UPDATE_INSERT_T1);\n\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result2, TestData.DATA_SET_SOURCE_MERGED);\n  }\n","date":"2021-08-19 23:21:20","endLine":374,"groupId":"4111","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testBatchModeUpsert","params":"(HoodieTableTypetableType@booleanhiveStylePartitioning)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9e/ffdcc8c67f59ad51cfd9a0e5ed56ecd41b530e.src","preCode":"  void testBatchModeUpsert(HoodieTableType tableType, boolean hiveStylePartitioning) {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.TABLE_NAME, tableType.name())\n        .option(FlinkOptions.HIVE_STYLE_PARTITIONING, hiveStylePartitioning)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    execInsertSql(tableEnv, TestSQL.INSERT_T1);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(tableEnv, TestSQL.UPDATE_INSERT_T1);\n\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result2, TestData.DATA_SET_SOURCE_MERGED);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":353,"status":"MB"}],"commitId":"9762e4c08c0ff953cc62e72b1295db4fd4c002c5","commitMessage":"@@@[MINOR] Some cosmetic changes for Flink (#3503)\n\n","date":"2021-08-19 23:21:20","modifiedFileCount":"6","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-09-11 13:17:16","codes":[{"authorDate":"2021-08-19 23:21:20","commitOrder":3,"curCode":"  void testBatchModeUpsertWithoutPartition(HoodieTableType tableType) {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.TABLE_NAME, tableType.name())\n        .withPartition(false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    execInsertSql(tableEnv, TestSQL.INSERT_T1);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(tableEnv, TestSQL.UPDATE_INSERT_T1);\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result2, TestData.DATA_SET_SOURCE_MERGED);\n  }\n","date":"2021-08-19 23:21:20","endLine":349,"groupId":"4111","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testBatchModeUpsertWithoutPartition","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9e/ffdcc8c67f59ad51cfd9a0e5ed56ecd41b530e.src","preCode":"  void testBatchModeUpsertWithoutPartition(HoodieTableType tableType) {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.TABLE_NAME, tableType.name())\n        .withPartition(false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    execInsertSql(tableEnv, TestSQL.INSERT_T1);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(tableEnv, TestSQL.UPDATE_INSERT_T1);\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result2, TestData.DATA_SET_SOURCE_MERGED);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":329,"status":"N"},{"authorDate":"2021-09-11 13:17:16","commitOrder":3,"curCode":"  void testBatchModeUpsert(HoodieTableType tableType, boolean hiveStylePartitioning) {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.TABLE_NAME, tableType)\n        .option(FlinkOptions.HIVE_STYLE_PARTITIONING, hiveStylePartitioning)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    execInsertSql(tableEnv, TestSQL.INSERT_T1);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(tableEnv, TestSQL.UPDATE_INSERT_T1);\n\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result2, TestData.DATA_SET_SOURCE_MERGED);\n  }\n","date":"2021-09-11 13:17:16","endLine":442,"groupId":"4111","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testBatchModeUpsert","params":"(HoodieTableTypetableType@booleanhiveStylePartitioning)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/5b/e603f7838e5195aa26e8cf17398d597ca9ee2f.src","preCode":"  void testBatchModeUpsert(HoodieTableType tableType, boolean hiveStylePartitioning) {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.TABLE_NAME, tableType.name())\n        .option(FlinkOptions.HIVE_STYLE_PARTITIONING, hiveStylePartitioning)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    execInsertSql(tableEnv, TestSQL.INSERT_T1);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(tableEnv, TestSQL.UPDATE_INSERT_T1);\n\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result2, TestData.DATA_SET_SOURCE_MERGED);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":421,"status":"M"}],"commitId":"b30c5bdaef77aee9f564ac24f80f5c364014bb17","commitMessage":"@@@[HUDI-2412] Add timestamp based partitioning for flink writer (#3638)\n\n","date":"2021-09-11 13:17:16","modifiedFileCount":"11","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-09-11 13:17:16","codes":[{"authorDate":"2021-09-15 12:04:46","commitOrder":4,"curCode":"  void testBatchModeUpsertWithoutPartition(HoodieTableType tableType) {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.TABLE_NAME, tableType.name())\n        .noPartition()\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    execInsertSql(tableEnv, TestSQL.INSERT_T1);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(tableEnv, TestSQL.UPDATE_INSERT_T1);\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result2, TestData.DATA_SET_SOURCE_MERGED);\n  }\n","date":"2021-09-15 12:04:46","endLine":417,"groupId":"10385","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testBatchModeUpsertWithoutPartition","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9d/0bcabac6aaad159803c08004869e0ff2db0462.src","preCode":"  void testBatchModeUpsertWithoutPartition(HoodieTableType tableType) {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.TABLE_NAME, tableType.name())\n        .withPartition(false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    execInsertSql(tableEnv, TestSQL.INSERT_T1);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(tableEnv, TestSQL.UPDATE_INSERT_T1);\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result2, TestData.DATA_SET_SOURCE_MERGED);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":397,"status":"M"},{"authorDate":"2021-09-11 13:17:16","commitOrder":4,"curCode":"  void testBatchModeUpsert(HoodieTableType tableType, boolean hiveStylePartitioning) {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.TABLE_NAME, tableType)\n        .option(FlinkOptions.HIVE_STYLE_PARTITIONING, hiveStylePartitioning)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    execInsertSql(tableEnv, TestSQL.INSERT_T1);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(tableEnv, TestSQL.UPDATE_INSERT_T1);\n\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result2, TestData.DATA_SET_SOURCE_MERGED);\n  }\n","date":"2021-09-11 13:17:16","endLine":442,"groupId":"10385","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testBatchModeUpsert","params":"(HoodieTableTypetableType@booleanhiveStylePartitioning)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/5b/e603f7838e5195aa26e8cf17398d597ca9ee2f.src","preCode":"  void testBatchModeUpsert(HoodieTableType tableType, boolean hiveStylePartitioning) {\n    TableEnvironment tableEnv = batchTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.TABLE_NAME, tableType)\n        .option(FlinkOptions.HIVE_STYLE_PARTITIONING, hiveStylePartitioning)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    execInsertSql(tableEnv, TestSQL.INSERT_T1);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(tableEnv, TestSQL.UPDATE_INSERT_T1);\n\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result2, TestData.DATA_SET_SOURCE_MERGED);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":421,"status":"N"}],"commitId":"627f20f9c54a87dded3350b73a6327f5b95632f6","commitMessage":"@@@[HUDI-2430] Make decimal compatible with hudi for flink writer (#3658)\n\n","date":"2021-09-15 12:04:46","modifiedFileCount":"5","status":"M","submitter":"Danny Chan"}]
