[{"authorTime":"2020-12-10 07:52:23","codes":[{"authorDate":"2020-12-10 07:52:23","commitOrder":1,"curCode":"  public void stream(Dataset<Row> streamingInput, String operationType, String checkpointLocation) throws Exception {\n\n    DataStreamWriter<Row> writer = streamingInput.writeStream().format(\"org.apache.hudi\")\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\").option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        .option(\"hoodie.delete.shuffle.parallelism\", \"2\")\n        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), operationType)\n        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY(), tableType)\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\")\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"partition\")\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), \"timestamp\")\n        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP, \"1\")\n        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_OPT_KEY(), \"true\")\n        .option(HoodieWriteConfig.TABLE_NAME, tableName).option(\"checkpointLocation\", checkpointLocation)\n        .outputMode(OutputMode.Append());\n\n    updateHiveSyncConfig(writer);\n    StreamingQuery query = writer.trigger(Trigger.ProcessingTime(500)).start(tablePath);\n    query.awaitTermination(streamingDurationInMs);\n  }\n","date":"2020-12-10 07:52:23","endLine":371,"groupId":"2770","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"stream","params":"(Dataset<Row>streamingInput@StringoperationType@StringcheckpointLocation)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/1d/f12a35032abec9ca330082f779dc037269690e.src","preCode":"  public void stream(Dataset<Row> streamingInput, String operationType, String checkpointLocation) throws Exception {\n\n    DataStreamWriter<Row> writer = streamingInput.writeStream().format(\"org.apache.hudi\")\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\").option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        .option(\"hoodie.delete.shuffle.parallelism\", \"2\")\n        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), operationType)\n        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY(), tableType)\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\")\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"partition\")\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), \"timestamp\")\n        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP, \"1\")\n        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_OPT_KEY(), \"true\")\n        .option(HoodieWriteConfig.TABLE_NAME, tableName).option(\"checkpointLocation\", checkpointLocation)\n        .outputMode(OutputMode.Append());\n\n    updateHiveSyncConfig(writer);\n    StreamingQuery query = writer.trigger(Trigger.ProcessingTime(500)).start(tablePath);\n    query.awaitTermination(streamingDurationInMs);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":353,"status":"B"},{"authorDate":"2020-12-10 07:52:23","commitOrder":1,"curCode":"  private void insert(SparkSession spark) throws IOException {\n    HoodieTestDataGenerator dataGen = getDataGenerate();\n\n    JavaSparkContext jssc = new JavaSparkContext(spark.sparkContext());\n\n    \r\n    String instantTime = HoodieActiveTimeline.createNewInstantTime();\n    List<HoodieRecord> recordsSoFar = new ArrayList<>(dataGen.generateInserts(instantTime, 100));\n    List<String> records1 = recordsToStrings(recordsSoFar);\n    Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));\n\n    \r\n    \r\n    DataFrameWriter<Row> writer = inputDF1.write().format(\"org.apache.hudi\")\n        \r\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\")\n        \r\n        .option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        \r\n        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY(), tableType)\n        \r\n        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())\n        \r\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\")\n        \r\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"partition\")\n        \r\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), \"timestamp\")\n        \r\n        .option(HoodieWriteConfig.TABLE_NAME, tableName)\n        \r\n        .option(DataSourceWriteOptions.KEYGENERATOR_CLASS_OPT_KEY(),\n            nonPartitionedTable ? NonpartitionedKeyGenerator.class.getCanonicalName()\n                : SimpleKeyGenerator.class.getCanonicalName())\n        .mode(commitType);\n\n    updateHiveSyncConfig(writer);\n    \r\n    writer.save(tablePath); \r\n    FileSystem fs = FileSystem.get(jssc.hadoopConfiguration());\n    String commitInstantTime1 = HoodieDataSourceHelpers.latestCommit(fs, tablePath);\n    LOG.info(\"Commit at instant time :\" + commitInstantTime1);\n  }\n","date":"2020-12-10 07:52:23","endLine":191,"groupId":"2576","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"insert","params":"(SparkSessionspark)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/01/2134fdb7e6dd9fcaf9e307cdc75e53e94411a5.src","preCode":"  private void insert(SparkSession spark) throws IOException {\n    HoodieTestDataGenerator dataGen = getDataGenerate();\n\n    JavaSparkContext jssc = new JavaSparkContext(spark.sparkContext());\n\n    \r\n    String instantTime = HoodieActiveTimeline.createNewInstantTime();\n    List<HoodieRecord> recordsSoFar = new ArrayList<>(dataGen.generateInserts(instantTime, 100));\n    List<String> records1 = recordsToStrings(recordsSoFar);\n    Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));\n\n    \r\n    \r\n    DataFrameWriter<Row> writer = inputDF1.write().format(\"org.apache.hudi\")\n        \r\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\")\n        \r\n        .option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        \r\n        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY(), tableType)\n        \r\n        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())\n        \r\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\")\n        \r\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"partition\")\n        \r\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), \"timestamp\")\n        \r\n        .option(HoodieWriteConfig.TABLE_NAME, tableName)\n        \r\n        .option(DataSourceWriteOptions.KEYGENERATOR_CLASS_OPT_KEY(),\n            nonPartitionedTable ? NonpartitionedKeyGenerator.class.getCanonicalName()\n                : SimpleKeyGenerator.class.getCanonicalName())\n        .mode(commitType);\n\n    updateHiveSyncConfig(writer);\n    \r\n    writer.save(tablePath); \r\n    FileSystem fs = FileSystem.get(jssc.hadoopConfiguration());\n    String commitInstantTime1 = HoodieDataSourceHelpers.latestCommit(fs, tablePath);\n    LOG.info(\"Commit at instant time :\" + commitInstantTime1);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":149,"status":"B"}],"commitId":"fce1453fa608fcff5df4d5aca8c88107d4151b09","commitMessage":"@@@[HUDI-1040] Make Hudi support Spark 3 (#2208)\n\n* Fix flaky MOR unit test\n\n* Update Spark APIs to make it be compatible with both spark2 & spark3\n\n* Refactor bulk insert v2 part to make Hudi be able to compile with Spark3\n\n* Add spark3 profile to handle fasterxml & spark version\n\n* Create hudi-spark-common module & refactor hudi-spark related modules\n\nCo-authored-by: Wenning Ding <wenningd@amazon.com>","date":"2020-12-10 07:52:23","modifiedFileCount":"10","status":"B","submitter":"wenningd"},{"authorTime":"2021-07-01 05:26:30","codes":[{"authorDate":"2021-07-01 05:26:30","commitOrder":2,"curCode":"  public void stream(Dataset<Row> streamingInput, String operationType, String checkpointLocation) throws Exception {\n\n    DataStreamWriter<Row> writer = streamingInput.writeStream().format(\"org.apache.hudi\")\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\").option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        .option(\"hoodie.delete.shuffle.parallelism\", \"2\")\n        .option(DataSourceWriteOptions.OPERATION_OPT_KEY().key(), operationType)\n        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY().key(), tableType)\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY().key(), \"_row_key\")\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY().key(), \"partition\")\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY().key(), \"timestamp\")\n        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP.key(), \"1\")\n        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_OPT_KEY().key(), \"true\")\n        .option(HoodieWriteConfig.TABLE_NAME.key(), tableName).option(\"checkpointLocation\", checkpointLocation)\n        .outputMode(OutputMode.Append());\n\n    updateHiveSyncConfig(writer);\n    StreamingQuery query = writer.trigger(Trigger.ProcessingTime(500)).start(tablePath);\n    query.awaitTermination(streamingDurationInMs);\n  }\n","date":"2021-07-01 05:26:30","endLine":371,"groupId":"5088","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"stream","params":"(Dataset<Row>streamingInput@StringoperationType@StringcheckpointLocation)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/b0/f063103388220d4ae4183ea814539fa00cb612.src","preCode":"  public void stream(Dataset<Row> streamingInput, String operationType, String checkpointLocation) throws Exception {\n\n    DataStreamWriter<Row> writer = streamingInput.writeStream().format(\"org.apache.hudi\")\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\").option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        .option(\"hoodie.delete.shuffle.parallelism\", \"2\")\n        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), operationType)\n        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY(), tableType)\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\")\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"partition\")\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), \"timestamp\")\n        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP, \"1\")\n        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_OPT_KEY(), \"true\")\n        .option(HoodieWriteConfig.TABLE_NAME, tableName).option(\"checkpointLocation\", checkpointLocation)\n        .outputMode(OutputMode.Append());\n\n    updateHiveSyncConfig(writer);\n    StreamingQuery query = writer.trigger(Trigger.ProcessingTime(500)).start(tablePath);\n    query.awaitTermination(streamingDurationInMs);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":353,"status":"M"},{"authorDate":"2021-07-01 05:26:30","commitOrder":2,"curCode":"  private void insert(SparkSession spark) throws IOException {\n    HoodieTestDataGenerator dataGen = getDataGenerate();\n\n    JavaSparkContext jssc = new JavaSparkContext(spark.sparkContext());\n\n    \r\n    String instantTime = HoodieActiveTimeline.createNewInstantTime();\n    List<HoodieRecord> recordsSoFar = new ArrayList<>(dataGen.generateInserts(instantTime, 100));\n    List<String> records1 = recordsToStrings(recordsSoFar);\n    Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));\n\n    \r\n    \r\n    DataFrameWriter<Row> writer = inputDF1.write().format(\"org.apache.hudi\")\n        \r\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\")\n        \r\n        .option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        \r\n        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY().key(), tableType)\n        \r\n        .option(DataSourceWriteOptions.OPERATION_OPT_KEY().key(), DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())\n        \r\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY().key(), \"_row_key\")\n        \r\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY().key(), \"partition\")\n        \r\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY().key(), \"timestamp\")\n        \r\n        .option(HoodieWriteConfig.TABLE_NAME.key(), tableName)\n        \r\n        .option(DataSourceWriteOptions.KEYGENERATOR_CLASS_OPT_KEY().key(),\n            nonPartitionedTable ? NonpartitionedKeyGenerator.class.getCanonicalName()\n                : SimpleKeyGenerator.class.getCanonicalName())\n        .mode(commitType);\n\n    updateHiveSyncConfig(writer);\n    \r\n    writer.save(tablePath); \r\n    FileSystem fs = FileSystem.get(jssc.hadoopConfiguration());\n    String commitInstantTime1 = HoodieDataSourceHelpers.latestCommit(fs, tablePath);\n    LOG.info(\"Commit at instant time :\" + commitInstantTime1);\n  }\n","date":"2021-07-01 05:26:30","endLine":191,"groupId":"0","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"insert","params":"(SparkSessionspark)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e9/07cd7cc4e91a4762e27310818d24578028d511.src","preCode":"  private void insert(SparkSession spark) throws IOException {\n    HoodieTestDataGenerator dataGen = getDataGenerate();\n\n    JavaSparkContext jssc = new JavaSparkContext(spark.sparkContext());\n\n    \r\n    String instantTime = HoodieActiveTimeline.createNewInstantTime();\n    List<HoodieRecord> recordsSoFar = new ArrayList<>(dataGen.generateInserts(instantTime, 100));\n    List<String> records1 = recordsToStrings(recordsSoFar);\n    Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));\n\n    \r\n    \r\n    DataFrameWriter<Row> writer = inputDF1.write().format(\"org.apache.hudi\")\n        \r\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\")\n        \r\n        .option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        \r\n        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY(), tableType)\n        \r\n        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())\n        \r\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\")\n        \r\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"partition\")\n        \r\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), \"timestamp\")\n        \r\n        .option(HoodieWriteConfig.TABLE_NAME, tableName)\n        \r\n        .option(DataSourceWriteOptions.KEYGENERATOR_CLASS_OPT_KEY(),\n            nonPartitionedTable ? NonpartitionedKeyGenerator.class.getCanonicalName()\n                : SimpleKeyGenerator.class.getCanonicalName())\n        .mode(commitType);\n\n    updateHiveSyncConfig(writer);\n    \r\n    writer.save(tablePath); \r\n    FileSystem fs = FileSystem.get(jssc.hadoopConfiguration());\n    String commitInstantTime1 = HoodieDataSourceHelpers.latestCommit(fs, tablePath);\n    LOG.info(\"Commit at instant time :\" + commitInstantTime1);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":149,"status":"M"}],"commitId":"d412fb2fe642417460532044cac162bb68f4bec4","commitMessage":"@@@[HUDI-89] Add configOption & refactor all configs based on that (#2833)\n\nCo-authored-by: Wenning Ding <wenningd@amazon.com>","date":"2021-07-01 05:26:30","modifiedFileCount":"138","status":"M","submitter":"wenningd"},{"authorTime":"2021-07-01 05:26:30","codes":[{"authorDate":"2021-07-12 02:43:38","commitOrder":3,"curCode":"  public void stream(Dataset<Row> streamingInput, String operationType, String checkpointLocation) throws Exception {\n\n    DataStreamWriter<Row> writer = streamingInput.writeStream().format(\"org.apache.hudi\")\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\").option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        .option(\"hoodie.delete.shuffle.parallelism\", \"2\")\n        .option(DataSourceWriteOptions.OPERATION_OPT_KEY().key(), operationType)\n        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY().key(), tableType)\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY().key(), \"_row_key\")\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY().key(), \"partition\")\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY().key(), \"timestamp\")\n        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP.key(), \"1\")\n        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_OPT_KEY().key(), \"true\")\n        .option(DataSourceWriteOptions.ASYNC_CLUSTERING_ENABLE_OPT_KEY().key(), \"true\")\n        .option(HoodieWriteConfig.TABLE_NAME.key(), tableName).option(\"checkpointLocation\", checkpointLocation)\n        .outputMode(OutputMode.Append());\n\n    updateHiveSyncConfig(writer);\n    StreamingQuery query = writer.trigger(Trigger.ProcessingTime(500)).start(tablePath);\n    query.awaitTermination(streamingDurationInMs);\n  }\n","date":"2021-07-12 02:43:38","endLine":372,"groupId":"5088","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"stream","params":"(Dataset<Row>streamingInput@StringoperationType@StringcheckpointLocation)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/75/fa91ebabfdc9165896815852ab4a95839674aa.src","preCode":"  public void stream(Dataset<Row> streamingInput, String operationType, String checkpointLocation) throws Exception {\n\n    DataStreamWriter<Row> writer = streamingInput.writeStream().format(\"org.apache.hudi\")\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\").option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        .option(\"hoodie.delete.shuffle.parallelism\", \"2\")\n        .option(DataSourceWriteOptions.OPERATION_OPT_KEY().key(), operationType)\n        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY().key(), tableType)\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY().key(), \"_row_key\")\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY().key(), \"partition\")\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY().key(), \"timestamp\")\n        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP.key(), \"1\")\n        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_OPT_KEY().key(), \"true\")\n        .option(HoodieWriteConfig.TABLE_NAME.key(), tableName).option(\"checkpointLocation\", checkpointLocation)\n        .outputMode(OutputMode.Append());\n\n    updateHiveSyncConfig(writer);\n    StreamingQuery query = writer.trigger(Trigger.ProcessingTime(500)).start(tablePath);\n    query.awaitTermination(streamingDurationInMs);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":353,"status":"M"},{"authorDate":"2021-07-01 05:26:30","commitOrder":3,"curCode":"  private void insert(SparkSession spark) throws IOException {\n    HoodieTestDataGenerator dataGen = getDataGenerate();\n\n    JavaSparkContext jssc = new JavaSparkContext(spark.sparkContext());\n\n    \r\n    String instantTime = HoodieActiveTimeline.createNewInstantTime();\n    List<HoodieRecord> recordsSoFar = new ArrayList<>(dataGen.generateInserts(instantTime, 100));\n    List<String> records1 = recordsToStrings(recordsSoFar);\n    Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));\n\n    \r\n    \r\n    DataFrameWriter<Row> writer = inputDF1.write().format(\"org.apache.hudi\")\n        \r\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\")\n        \r\n        .option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        \r\n        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY().key(), tableType)\n        \r\n        .option(DataSourceWriteOptions.OPERATION_OPT_KEY().key(), DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())\n        \r\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY().key(), \"_row_key\")\n        \r\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY().key(), \"partition\")\n        \r\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY().key(), \"timestamp\")\n        \r\n        .option(HoodieWriteConfig.TABLE_NAME.key(), tableName)\n        \r\n        .option(DataSourceWriteOptions.KEYGENERATOR_CLASS_OPT_KEY().key(),\n            nonPartitionedTable ? NonpartitionedKeyGenerator.class.getCanonicalName()\n                : SimpleKeyGenerator.class.getCanonicalName())\n        .mode(commitType);\n\n    updateHiveSyncConfig(writer);\n    \r\n    writer.save(tablePath); \r\n    FileSystem fs = FileSystem.get(jssc.hadoopConfiguration());\n    String commitInstantTime1 = HoodieDataSourceHelpers.latestCommit(fs, tablePath);\n    LOG.info(\"Commit at instant time :\" + commitInstantTime1);\n  }\n","date":"2021-07-01 05:26:30","endLine":191,"groupId":"0","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"insert","params":"(SparkSessionspark)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e9/07cd7cc4e91a4762e27310818d24578028d511.src","preCode":"  private void insert(SparkSession spark) throws IOException {\n    HoodieTestDataGenerator dataGen = getDataGenerate();\n\n    JavaSparkContext jssc = new JavaSparkContext(spark.sparkContext());\n\n    \r\n    String instantTime = HoodieActiveTimeline.createNewInstantTime();\n    List<HoodieRecord> recordsSoFar = new ArrayList<>(dataGen.generateInserts(instantTime, 100));\n    List<String> records1 = recordsToStrings(recordsSoFar);\n    Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));\n\n    \r\n    \r\n    DataFrameWriter<Row> writer = inputDF1.write().format(\"org.apache.hudi\")\n        \r\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\")\n        \r\n        .option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        \r\n        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY().key(), tableType)\n        \r\n        .option(DataSourceWriteOptions.OPERATION_OPT_KEY().key(), DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())\n        \r\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY().key(), \"_row_key\")\n        \r\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY().key(), \"partition\")\n        \r\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY().key(), \"timestamp\")\n        \r\n        .option(HoodieWriteConfig.TABLE_NAME.key(), tableName)\n        \r\n        .option(DataSourceWriteOptions.KEYGENERATOR_CLASS_OPT_KEY().key(),\n            nonPartitionedTable ? NonpartitionedKeyGenerator.class.getCanonicalName()\n                : SimpleKeyGenerator.class.getCanonicalName())\n        .mode(commitType);\n\n    updateHiveSyncConfig(writer);\n    \r\n    writer.save(tablePath); \r\n    FileSystem fs = FileSystem.get(jssc.hadoopConfiguration());\n    String commitInstantTime1 = HoodieDataSourceHelpers.latestCommit(fs, tablePath);\n    LOG.info(\"Commit at instant time :\" + commitInstantTime1);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":149,"status":"N"}],"commitId":"5804ad8e32ae05758ebc5e47f5d4fb4db371ab52","commitMessage":"@@@[HUDI-1483] Support async clustering for deltastreamer and Spark streaming (#3142)\n\n- Integrate async clustering service with HoodieDeltaStreamer and HoodieStreamingSink\n- Added methods in HoodieAsyncService to reuse code\n","date":"2021-07-12 02:43:38","modifiedFileCount":"13","status":"M","submitter":"Sagar Sumit"},{"authorTime":"2021-08-04 08:50:30","codes":[{"authorDate":"2021-08-04 08:50:30","commitOrder":4,"curCode":"  public void stream(Dataset<Row> streamingInput, String operationType, String checkpointLocation) throws Exception {\n\n    DataStreamWriter<Row> writer = streamingInput.writeStream().format(\"org.apache.hudi\")\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\").option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        .option(\"hoodie.delete.shuffle.parallelism\", \"2\")\n        .option(DataSourceWriteOptions.OPERATION().key(), operationType)\n        .option(DataSourceWriteOptions.TABLE_TYPE().key(), tableType)\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD().key(), \"_row_key\")\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD().key(), \"partition\")\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD().key(), \"timestamp\")\n        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP.key(), \"1\")\n        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE().key(), \"true\")\n        .option(DataSourceWriteOptions.ASYNC_CLUSTERING_ENABLE().key(), \"true\")\n        .option(HoodieWriteConfig.TABLE_NAME.key(), tableName).option(\"checkpointLocation\", checkpointLocation)\n        .outputMode(OutputMode.Append());\n\n    updateHiveSyncConfig(writer);\n    StreamingQuery query = writer.trigger(Trigger.ProcessingTime(500)).start(tablePath);\n    query.awaitTermination(streamingDurationInMs);\n  }\n","date":"2021-08-04 08:50:30","endLine":372,"groupId":"4791","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"stream","params":"(Dataset<Row>streamingInput@StringoperationType@StringcheckpointLocation)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/df/aa887cb5c4832f982d9a852ba0ae2de0515e38.src","preCode":"  public void stream(Dataset<Row> streamingInput, String operationType, String checkpointLocation) throws Exception {\n\n    DataStreamWriter<Row> writer = streamingInput.writeStream().format(\"org.apache.hudi\")\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\").option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        .option(\"hoodie.delete.shuffle.parallelism\", \"2\")\n        .option(DataSourceWriteOptions.OPERATION_OPT_KEY().key(), operationType)\n        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY().key(), tableType)\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY().key(), \"_row_key\")\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY().key(), \"partition\")\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY().key(), \"timestamp\")\n        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP.key(), \"1\")\n        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_OPT_KEY().key(), \"true\")\n        .option(DataSourceWriteOptions.ASYNC_CLUSTERING_ENABLE_OPT_KEY().key(), \"true\")\n        .option(HoodieWriteConfig.TABLE_NAME.key(), tableName).option(\"checkpointLocation\", checkpointLocation)\n        .outputMode(OutputMode.Append());\n\n    updateHiveSyncConfig(writer);\n    StreamingQuery query = writer.trigger(Trigger.ProcessingTime(500)).start(tablePath);\n    query.awaitTermination(streamingDurationInMs);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":353,"status":"M"},{"authorDate":"2021-08-04 08:50:30","commitOrder":4,"curCode":"  private void insert(SparkSession spark) throws IOException {\n    HoodieTestDataGenerator dataGen = getDataGenerate();\n\n    JavaSparkContext jssc = new JavaSparkContext(spark.sparkContext());\n\n    \r\n    String instantTime = HoodieActiveTimeline.createNewInstantTime();\n    List<HoodieRecord> recordsSoFar = new ArrayList<>(dataGen.generateInserts(instantTime, 100));\n    List<String> records1 = recordsToStrings(recordsSoFar);\n    Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));\n\n    \r\n    \r\n    DataFrameWriter<Row> writer = inputDF1.write().format(\"org.apache.hudi\")\n        \r\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\")\n        \r\n        .option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        \r\n        .option(DataSourceWriteOptions.TABLE_TYPE().key(), tableType)\n        \r\n        .option(DataSourceWriteOptions.OPERATION().key(), DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())\n        \r\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD().key(), \"_row_key\")\n        \r\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD().key(), \"partition\")\n        \r\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD().key(), \"timestamp\")\n        \r\n        .option(HoodieWriteConfig.TABLE_NAME.key(), tableName)\n        \r\n        .option(DataSourceWriteOptions.KEYGENERATOR_CLASS().key(),\n            nonPartitionedTable ? NonpartitionedKeyGenerator.class.getCanonicalName()\n                : SimpleKeyGenerator.class.getCanonicalName())\n        .mode(commitType);\n\n    updateHiveSyncConfig(writer);\n    \r\n    writer.save(tablePath); \r\n    FileSystem fs = FileSystem.get(jssc.hadoopConfiguration());\n    String commitInstantTime1 = HoodieDataSourceHelpers.latestCommit(fs, tablePath);\n    LOG.info(\"Commit at instant time :\" + commitInstantTime1);\n  }\n","date":"2021-08-04 08:50:30","endLine":191,"groupId":"1598","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"insert","params":"(SparkSessionspark)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/43/e0b20d3505fdd84ba495b85f5fcf0bc0c539e6.src","preCode":"  private void insert(SparkSession spark) throws IOException {\n    HoodieTestDataGenerator dataGen = getDataGenerate();\n\n    JavaSparkContext jssc = new JavaSparkContext(spark.sparkContext());\n\n    \r\n    String instantTime = HoodieActiveTimeline.createNewInstantTime();\n    List<HoodieRecord> recordsSoFar = new ArrayList<>(dataGen.generateInserts(instantTime, 100));\n    List<String> records1 = recordsToStrings(recordsSoFar);\n    Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));\n\n    \r\n    \r\n    DataFrameWriter<Row> writer = inputDF1.write().format(\"org.apache.hudi\")\n        \r\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\")\n        \r\n        .option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        \r\n        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY().key(), tableType)\n        \r\n        .option(DataSourceWriteOptions.OPERATION_OPT_KEY().key(), DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())\n        \r\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY().key(), \"_row_key\")\n        \r\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY().key(), \"partition\")\n        \r\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY().key(), \"timestamp\")\n        \r\n        .option(HoodieWriteConfig.TABLE_NAME.key(), tableName)\n        \r\n        .option(DataSourceWriteOptions.KEYGENERATOR_CLASS_OPT_KEY().key(),\n            nonPartitionedTable ? NonpartitionedKeyGenerator.class.getCanonicalName()\n                : SimpleKeyGenerator.class.getCanonicalName())\n        .mode(commitType);\n\n    updateHiveSyncConfig(writer);\n    \r\n    writer.save(tablePath); \r\n    FileSystem fs = FileSystem.get(jssc.hadoopConfiguration());\n    String commitInstantTime1 = HoodieDataSourceHelpers.latestCommit(fs, tablePath);\n    LOG.info(\"Commit at instant time :\" + commitInstantTime1);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":149,"status":"M"}],"commitId":"91bb0d13184c57ec08f02db3337e734bc20739c4","commitMessage":"@@@[HUDI-2255] Refactor Datasource options (#3373)\n\nCo-authored-by: Wenning Ding <wenningd@amazon.com>","date":"2021-08-04 08:50:30","modifiedFileCount":"50","status":"M","submitter":"wenningd"},{"authorTime":"2021-08-04 08:50:30","codes":[{"authorDate":"2021-08-13 11:31:04","commitOrder":5,"curCode":"  public void stream(Dataset<Row> streamingInput, String operationType, String checkpointLocation) throws Exception {\n\n    DataStreamWriter<Row> writer = streamingInput.writeStream().format(\"org.apache.hudi\")\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\").option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        .option(\"hoodie.delete.shuffle.parallelism\", \"2\")\n        .option(DataSourceWriteOptions.OPERATION().key(), operationType)\n        .option(DataSourceWriteOptions.TABLE_TYPE().key(), tableType)\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD().key(), \"_row_key\")\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD().key(), \"partition\")\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD().key(), \"timestamp\")\n        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS.key(), \"1\")\n        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE().key(), \"true\")\n        .option(DataSourceWriteOptions.ASYNC_CLUSTERING_ENABLE().key(), \"true\")\n        .option(HoodieWriteConfig.TABLE_NAME.key(), tableName).option(\"checkpointLocation\", checkpointLocation)\n        .outputMode(OutputMode.Append());\n\n    updateHiveSyncConfig(writer);\n    StreamingQuery query = writer.trigger(Trigger.ProcessingTime(500)).start(tablePath);\n    query.awaitTermination(streamingDurationInMs);\n  }\n","date":"2021-08-13 11:31:04","endLine":372,"groupId":"4791","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"stream","params":"(Dataset<Row>streamingInput@StringoperationType@StringcheckpointLocation)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ea/71e55f042d24b6375249a6469e0770aa2c05ee.src","preCode":"  public void stream(Dataset<Row> streamingInput, String operationType, String checkpointLocation) throws Exception {\n\n    DataStreamWriter<Row> writer = streamingInput.writeStream().format(\"org.apache.hudi\")\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\").option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        .option(\"hoodie.delete.shuffle.parallelism\", \"2\")\n        .option(DataSourceWriteOptions.OPERATION().key(), operationType)\n        .option(DataSourceWriteOptions.TABLE_TYPE().key(), tableType)\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD().key(), \"_row_key\")\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD().key(), \"partition\")\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD().key(), \"timestamp\")\n        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP.key(), \"1\")\n        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE().key(), \"true\")\n        .option(DataSourceWriteOptions.ASYNC_CLUSTERING_ENABLE().key(), \"true\")\n        .option(HoodieWriteConfig.TABLE_NAME.key(), tableName).option(\"checkpointLocation\", checkpointLocation)\n        .outputMode(OutputMode.Append());\n\n    updateHiveSyncConfig(writer);\n    StreamingQuery query = writer.trigger(Trigger.ProcessingTime(500)).start(tablePath);\n    query.awaitTermination(streamingDurationInMs);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":353,"status":"M"},{"authorDate":"2021-08-04 08:50:30","commitOrder":5,"curCode":"  private void insert(SparkSession spark) throws IOException {\n    HoodieTestDataGenerator dataGen = getDataGenerate();\n\n    JavaSparkContext jssc = new JavaSparkContext(spark.sparkContext());\n\n    \r\n    String instantTime = HoodieActiveTimeline.createNewInstantTime();\n    List<HoodieRecord> recordsSoFar = new ArrayList<>(dataGen.generateInserts(instantTime, 100));\n    List<String> records1 = recordsToStrings(recordsSoFar);\n    Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));\n\n    \r\n    \r\n    DataFrameWriter<Row> writer = inputDF1.write().format(\"org.apache.hudi\")\n        \r\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\")\n        \r\n        .option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        \r\n        .option(DataSourceWriteOptions.TABLE_TYPE().key(), tableType)\n        \r\n        .option(DataSourceWriteOptions.OPERATION().key(), DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())\n        \r\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD().key(), \"_row_key\")\n        \r\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD().key(), \"partition\")\n        \r\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD().key(), \"timestamp\")\n        \r\n        .option(HoodieWriteConfig.TABLE_NAME.key(), tableName)\n        \r\n        .option(DataSourceWriteOptions.KEYGENERATOR_CLASS().key(),\n            nonPartitionedTable ? NonpartitionedKeyGenerator.class.getCanonicalName()\n                : SimpleKeyGenerator.class.getCanonicalName())\n        .mode(commitType);\n\n    updateHiveSyncConfig(writer);\n    \r\n    writer.save(tablePath); \r\n    FileSystem fs = FileSystem.get(jssc.hadoopConfiguration());\n    String commitInstantTime1 = HoodieDataSourceHelpers.latestCommit(fs, tablePath);\n    LOG.info(\"Commit at instant time :\" + commitInstantTime1);\n  }\n","date":"2021-08-04 08:50:30","endLine":191,"groupId":"1598","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"insert","params":"(SparkSessionspark)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/43/e0b20d3505fdd84ba495b85f5fcf0bc0c539e6.src","preCode":"  private void insert(SparkSession spark) throws IOException {\n    HoodieTestDataGenerator dataGen = getDataGenerate();\n\n    JavaSparkContext jssc = new JavaSparkContext(spark.sparkContext());\n\n    \r\n    String instantTime = HoodieActiveTimeline.createNewInstantTime();\n    List<HoodieRecord> recordsSoFar = new ArrayList<>(dataGen.generateInserts(instantTime, 100));\n    List<String> records1 = recordsToStrings(recordsSoFar);\n    Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));\n\n    \r\n    \r\n    DataFrameWriter<Row> writer = inputDF1.write().format(\"org.apache.hudi\")\n        \r\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\")\n        \r\n        .option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        \r\n        .option(DataSourceWriteOptions.TABLE_TYPE().key(), tableType)\n        \r\n        .option(DataSourceWriteOptions.OPERATION().key(), DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())\n        \r\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD().key(), \"_row_key\")\n        \r\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD().key(), \"partition\")\n        \r\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD().key(), \"timestamp\")\n        \r\n        .option(HoodieWriteConfig.TABLE_NAME.key(), tableName)\n        \r\n        .option(DataSourceWriteOptions.KEYGENERATOR_CLASS().key(),\n            nonPartitionedTable ? NonpartitionedKeyGenerator.class.getCanonicalName()\n                : SimpleKeyGenerator.class.getCanonicalName())\n        .mode(commitType);\n\n    updateHiveSyncConfig(writer);\n    \r\n    writer.save(tablePath); \r\n    FileSystem fs = FileSystem.get(jssc.hadoopConfiguration());\n    String commitInstantTime1 = HoodieDataSourceHelpers.latestCommit(fs, tablePath);\n    LOG.info(\"Commit at instant time :\" + commitInstantTime1);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":149,"status":"N"}],"commitId":"0544d70d8f4204f4e5edfe9144c17f1ed221eb7c","commitMessage":"@@@[MINOR] Deprecate older configs (#3464)\n\nRename and deprecate props in HoodieWriteConfig\n\nRename and deprecate older props","date":"2021-08-13 11:31:04","modifiedFileCount":"38","status":"M","submitter":"Sagar Sumit"},{"authorTime":"2021-08-20 04:36:40","codes":[{"authorDate":"2021-08-20 04:36:40","commitOrder":6,"curCode":"  public void stream(Dataset<Row> streamingInput, String operationType, String checkpointLocation) throws Exception {\n\n    DataStreamWriter<Row> writer = streamingInput.writeStream().format(\"org.apache.hudi\")\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\").option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        .option(\"hoodie.delete.shuffle.parallelism\", \"2\")\n        .option(DataSourceWriteOptions.OPERATION().key(), operationType)\n        .option(DataSourceWriteOptions.TABLE_TYPE().key(), tableType)\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD().key(), \"_row_key\")\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD().key(), \"partition\")\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD().key(), \"timestamp\")\n        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS.key(), \"1\")\n        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE().key(), \"true\")\n        .option(DataSourceWriteOptions.ASYNC_CLUSTERING_ENABLE().key(), \"true\")\n        .option(HoodieWriteConfig.TBL_NAME.key(), tableName).option(\"checkpointLocation\", checkpointLocation)\n        .outputMode(OutputMode.Append());\n\n    updateHiveSyncConfig(writer);\n    StreamingQuery query = writer.trigger(Trigger.ProcessingTime(500)).start(tablePath);\n    query.awaitTermination(streamingDurationInMs);\n  }\n","date":"2021-08-20 04:36:40","endLine":372,"groupId":"10132","id":11,"instanceNumber":1,"isCurCommit":1,"methodName":"stream","params":"(Dataset<Row>streamingInput@StringoperationType@StringcheckpointLocation)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/c0/47ef19677cfb2ff13ff186c9c4ed879231feb2.src","preCode":"  public void stream(Dataset<Row> streamingInput, String operationType, String checkpointLocation) throws Exception {\n\n    DataStreamWriter<Row> writer = streamingInput.writeStream().format(\"org.apache.hudi\")\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\").option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        .option(\"hoodie.delete.shuffle.parallelism\", \"2\")\n        .option(DataSourceWriteOptions.OPERATION().key(), operationType)\n        .option(DataSourceWriteOptions.TABLE_TYPE().key(), tableType)\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD().key(), \"_row_key\")\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD().key(), \"partition\")\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD().key(), \"timestamp\")\n        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS.key(), \"1\")\n        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE().key(), \"true\")\n        .option(DataSourceWriteOptions.ASYNC_CLUSTERING_ENABLE().key(), \"true\")\n        .option(HoodieWriteConfig.TABLE_NAME.key(), tableName).option(\"checkpointLocation\", checkpointLocation)\n        .outputMode(OutputMode.Append());\n\n    updateHiveSyncConfig(writer);\n    StreamingQuery query = writer.trigger(Trigger.ProcessingTime(500)).start(tablePath);\n    query.awaitTermination(streamingDurationInMs);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaStreamingApp.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":353,"status":"M"},{"authorDate":"2021-08-20 04:36:40","commitOrder":6,"curCode":"  private void insert(SparkSession spark) throws IOException {\n    HoodieTestDataGenerator dataGen = getDataGenerate();\n\n    JavaSparkContext jssc = new JavaSparkContext(spark.sparkContext());\n\n    \r\n    String instantTime = HoodieActiveTimeline.createNewInstantTime();\n    List<HoodieRecord> recordsSoFar = new ArrayList<>(dataGen.generateInserts(instantTime, 100));\n    List<String> records1 = recordsToStrings(recordsSoFar);\n    Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));\n\n    \r\n    \r\n    DataFrameWriter<Row> writer = inputDF1.write().format(\"org.apache.hudi\")\n        \r\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\")\n        \r\n        .option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        \r\n        .option(DataSourceWriteOptions.TABLE_TYPE().key(), tableType)\n        \r\n        .option(DataSourceWriteOptions.OPERATION().key(), DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())\n        \r\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD().key(), \"_row_key\")\n        \r\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD().key(), \"partition\")\n        \r\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD().key(), \"timestamp\")\n        \r\n        .option(HoodieWriteConfig.TBL_NAME.key(), tableName)\n        \r\n        .option(DataSourceWriteOptions.KEYGENERATOR_CLASS_NAME().key(),\n            nonPartitionedTable ? NonpartitionedKeyGenerator.class.getCanonicalName()\n                : SimpleKeyGenerator.class.getCanonicalName())\n        .mode(commitType);\n\n    updateHiveSyncConfig(writer);\n    \r\n    writer.save(tablePath); \r\n    FileSystem fs = FileSystem.get(jssc.hadoopConfiguration());\n    String commitInstantTime1 = HoodieDataSourceHelpers.latestCommit(fs, tablePath);\n    LOG.info(\"Commit at instant time :\" + commitInstantTime1);\n  }\n","date":"2021-08-20 04:36:40","endLine":191,"groupId":"10132","id":12,"instanceNumber":2,"isCurCommit":1,"methodName":"insert","params":"(SparkSessionspark)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/cf/86ba7ba3959449871570a024d8b7d4c98eb717.src","preCode":"  private void insert(SparkSession spark) throws IOException {\n    HoodieTestDataGenerator dataGen = getDataGenerate();\n\n    JavaSparkContext jssc = new JavaSparkContext(spark.sparkContext());\n\n    \r\n    String instantTime = HoodieActiveTimeline.createNewInstantTime();\n    List<HoodieRecord> recordsSoFar = new ArrayList<>(dataGen.generateInserts(instantTime, 100));\n    List<String> records1 = recordsToStrings(recordsSoFar);\n    Dataset<Row> inputDF1 = spark.read().json(jssc.parallelize(records1, 2));\n\n    \r\n    \r\n    DataFrameWriter<Row> writer = inputDF1.write().format(\"org.apache.hudi\")\n        \r\n        .option(\"hoodie.insert.shuffle.parallelism\", \"2\")\n        \r\n        .option(\"hoodie.upsert.shuffle.parallelism\", \"2\")\n        \r\n        .option(DataSourceWriteOptions.TABLE_TYPE().key(), tableType)\n        \r\n        .option(DataSourceWriteOptions.OPERATION().key(), DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())\n        \r\n        .option(DataSourceWriteOptions.RECORDKEY_FIELD().key(), \"_row_key\")\n        \r\n        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD().key(), \"partition\")\n        \r\n        .option(DataSourceWriteOptions.PRECOMBINE_FIELD().key(), \"timestamp\")\n        \r\n        .option(HoodieWriteConfig.TABLE_NAME.key(), tableName)\n        \r\n        .option(DataSourceWriteOptions.KEYGENERATOR_CLASS().key(),\n            nonPartitionedTable ? NonpartitionedKeyGenerator.class.getCanonicalName()\n                : SimpleKeyGenerator.class.getCanonicalName())\n        .mode(commitType);\n\n    updateHiveSyncConfig(writer);\n    \r\n    writer.save(tablePath); \r\n    FileSystem fs = FileSystem.get(jssc.hadoopConfiguration());\n    String commitInstantTime1 = HoodieDataSourceHelpers.latestCommit(fs, tablePath);\n    LOG.info(\"Commit at instant time :\" + commitInstantTime1);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark/src/test/java/HoodieJavaGenerateApp.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":149,"status":"M"}],"commitId":"c350d05dd3301f14fa9d688746c9de2416db3f11","commitMessage":"@@@Restore 0.8.0 config keys with deprecated annotation (#3506)\n\nCo-authored-by: Sagar Sumit <sagarsumit09@gmail.com>\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2021-08-20 04:36:40","modifiedFileCount":"109","status":"M","submitter":"Udit Mehrotra"}]
