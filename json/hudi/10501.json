[{"authorTime":"2021-02-06 22:03:52","codes":[{"authorDate":"2021-03-31 11:06:04","commitOrder":7,"curCode":"  private void assignInserts(WorkloadProfile profile, HoodieEngineContext context) {\n    \r\n    Set<String> partitionPaths = profile.getPartitionPaths();\n    long averageRecordSize =\n        averageBytesPerRecord(table.getMetaClient().getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n            config);\n    LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n\n    Map<String, List<SmallFile>> partitionSmallFilesMap =\n        getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n\n    for (String partitionPath : partitionPaths) {\n      WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n      if (pStat.getNumInserts() > 0) {\n\n        List<SmallFile> smallFiles = partitionSmallFilesMap.get(partitionPath);\n        this.smallFiles.addAll(smallFiles);\n\n        LOG.info(\"For partitionPath : \" + partitionPath + \" Small Files => \" + smallFiles);\n\n        long totalUnassignedInserts = pStat.getNumInserts();\n        List<Integer> bucketNumbers = new ArrayList<>();\n        List<Long> recordsPerBucket = new ArrayList<>();\n\n        \r\n        for (SmallFile smallFile : smallFiles) {\n          long recordsToAppend = Math.min((config.getParquetMaxFileSize() - smallFile.sizeBytes) / averageRecordSize,\n              totalUnassignedInserts);\n          if (recordsToAppend > 0) {\n            \r\n            int bucket;\n            if (updateLocationToBucket.containsKey(smallFile.location.getFileId())) {\n              bucket = updateLocationToBucket.get(smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to existing update bucket \" + bucket);\n            } else {\n              bucket = addUpdateBucket(partitionPath, smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to new update bucket \" + bucket);\n            }\n            bucketNumbers.add(bucket);\n            recordsPerBucket.add(recordsToAppend);\n            totalUnassignedInserts -= recordsToAppend;\n          }\n        }\n\n        \r\n        if (totalUnassignedInserts > 0) {\n          long insertRecordsPerBucket = config.getCopyOnWriteInsertSplitSize();\n          if (config.shouldAutoTuneInsertSplits()) {\n            insertRecordsPerBucket = config.getParquetMaxFileSize() / averageRecordSize;\n          }\n\n          int insertBuckets = (int) Math.ceil((1.0 * totalUnassignedInserts) / insertRecordsPerBucket);\n          LOG.info(\"After small file assignment: unassignedInserts => \" + totalUnassignedInserts\n              + \", totalInsertBuckets => \" + insertBuckets + \", recordsPerBucket => \" + insertRecordsPerBucket);\n          for (int b = 0; b < insertBuckets; b++) {\n            bucketNumbers.add(totalBuckets);\n            if (b < insertBuckets - 1) {\n              recordsPerBucket.add(insertRecordsPerBucket);\n            } else {\n              recordsPerBucket.add(totalUnassignedInserts - (insertBuckets - 1) * insertRecordsPerBucket);\n            }\n            BucketInfo bucketInfo = new BucketInfo(BucketType.INSERT, FSUtils.createNewFileIdPfx(), partitionPath);\n            bucketInfoMap.put(totalBuckets, bucketInfo);\n            totalBuckets++;\n          }\n        }\n\n        \r\n        List<InsertBucketCumulativeWeightPair> insertBuckets = new ArrayList<>();\n        double curentCumulativeWeight = 0;\n        for (int i = 0; i < bucketNumbers.size(); i++) {\n          InsertBucket bkt = new InsertBucket();\n          bkt.bucketNumber = bucketNumbers.get(i);\n          bkt.weight = (1.0 * recordsPerBucket.get(i)) / pStat.getNumInserts();\n          curentCumulativeWeight += bkt.weight;\n          insertBuckets.add(new InsertBucketCumulativeWeightPair(bkt, curentCumulativeWeight));\n        }\n        LOG.info(\"Total insert buckets for partition path \" + partitionPath + \" => \" + insertBuckets);\n        partitionPathToInsertBucketInfos.put(partitionPath, insertBuckets);\n      }\n    }\n  }\n","date":"2021-03-31 11:06:04","endLine":204,"groupId":"4517","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"assignInserts","params":"(WorkloadProfileprofile@HoodieEngineContextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/6b/5cb2997759c00feea98a6ae0aa6e2be4ba5c1c.src","preCode":"  private void assignInserts(WorkloadProfile profile, HoodieEngineContext context) {\n    \r\n    Set<String> partitionPaths = profile.getPartitionPaths();\n    long averageRecordSize =\n        averageBytesPerRecord(table.getMetaClient().getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n            config);\n    LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n\n    Map<String, List<SmallFile>> partitionSmallFilesMap =\n        getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n\n    for (String partitionPath : partitionPaths) {\n      WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n      if (pStat.getNumInserts() > 0) {\n\n        List<SmallFile> smallFiles = partitionSmallFilesMap.get(partitionPath);\n        this.smallFiles.addAll(smallFiles);\n\n        LOG.info(\"For partitionPath : \" + partitionPath + \" Small Files => \" + smallFiles);\n\n        long totalUnassignedInserts = pStat.getNumInserts();\n        List<Integer> bucketNumbers = new ArrayList<>();\n        List<Long> recordsPerBucket = new ArrayList<>();\n\n        \r\n        for (SmallFile smallFile : smallFiles) {\n          long recordsToAppend = Math.min((config.getParquetMaxFileSize() - smallFile.sizeBytes) / averageRecordSize,\n              totalUnassignedInserts);\n          if (recordsToAppend > 0) {\n            \r\n            int bucket;\n            if (updateLocationToBucket.containsKey(smallFile.location.getFileId())) {\n              bucket = updateLocationToBucket.get(smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to existing update bucket \" + bucket);\n            } else {\n              bucket = addUpdateBucket(partitionPath, smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to new update bucket \" + bucket);\n            }\n            bucketNumbers.add(bucket);\n            recordsPerBucket.add(recordsToAppend);\n            totalUnassignedInserts -= recordsToAppend;\n          }\n        }\n\n        \r\n        if (totalUnassignedInserts > 0) {\n          long insertRecordsPerBucket = config.getCopyOnWriteInsertSplitSize();\n          if (config.shouldAutoTuneInsertSplits()) {\n            insertRecordsPerBucket = config.getParquetMaxFileSize() / averageRecordSize;\n          }\n\n          int insertBuckets = (int) Math.ceil((1.0 * totalUnassignedInserts) / insertRecordsPerBucket);\n          LOG.info(\"After small file assignment: unassignedInserts => \" + totalUnassignedInserts\n              + \", totalInsertBuckets => \" + insertBuckets + \", recordsPerBucket => \" + insertRecordsPerBucket);\n          for (int b = 0; b < insertBuckets; b++) {\n            bucketNumbers.add(totalBuckets);\n            if (b < insertBuckets - 1) {\n              recordsPerBucket.add(insertRecordsPerBucket);\n            } else {\n              recordsPerBucket.add(totalUnassignedInserts - (insertBuckets - 1) * insertRecordsPerBucket);\n            }\n            BucketInfo bucketInfo = new BucketInfo(BucketType.INSERT, FSUtils.createNewFileIdPfx(), partitionPath);\n            bucketInfoMap.put(totalBuckets, bucketInfo);\n            totalBuckets++;\n          }\n        }\n\n        \r\n        List<InsertBucketCumulativeWeightPair> insertBuckets = new ArrayList<>();\n        double curentCumulativeWeight = 0;\n        for (int i = 0; i < bucketNumbers.size(); i++) {\n          InsertBucket bkt = new InsertBucket();\n          bkt.bucketNumber = bucketNumbers.get(i);\n          bkt.weight = (1.0 * recordsPerBucket.get(i)) / pStat.getNumInserts();\n          curentCumulativeWeight += bkt.weight;\n          insertBuckets.add(new InsertBucketCumulativeWeightPair(bkt, curentCumulativeWeight));\n        }\n        LOG.info(\"Total insert buckets for partition path \" + partitionPath + \" => \" + insertBuckets);\n        partitionPathToInsertBucketInfos.put(partitionPath, insertBuckets);\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertPartitioner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":123,"status":"B"},{"authorDate":"2021-02-06 22:03:52","commitOrder":7,"curCode":"  private void assignInserts(WorkloadProfile profile, HoodieEngineContext context) {\n    \r\n    Set<String> partitionPaths = profile.getPartitionPaths();\n    long averageRecordSize =\n        averageBytesPerRecord(table.getMetaClient().getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n            config);\n    LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n\n    Map<String, List<SmallFile>> partitionSmallFilesMap =\n        getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n\n    Map<String, Set<String>> partitionPathToPendingClusteringFileGroupsId = getPartitionPathToPendingClusteringFileGroupsId();\n\n    for (String partitionPath : partitionPaths) {\n      WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n      if (pStat.getNumInserts() > 0) {\n\n        List<SmallFile> smallFiles =\n            filterSmallFilesInClustering(partitionPathToPendingClusteringFileGroupsId.getOrDefault(partitionPath, Collections.emptySet()),\n                partitionSmallFilesMap.get(partitionPath));\n\n        this.smallFiles.addAll(smallFiles);\n\n        LOG.info(\"For partitionPath : \" + partitionPath + \" Small Files => \" + smallFiles);\n\n        long totalUnassignedInserts = pStat.getNumInserts();\n        List<Integer> bucketNumbers = new ArrayList<>();\n        List<Long> recordsPerBucket = new ArrayList<>();\n\n        \r\n        for (SmallFile smallFile : smallFiles) {\n          long recordsToAppend = Math.min((config.getParquetMaxFileSize() - smallFile.sizeBytes) / averageRecordSize,\n              totalUnassignedInserts);\n          if (recordsToAppend > 0 && totalUnassignedInserts > 0) {\n            \r\n            int bucket;\n            if (updateLocationToBucket.containsKey(smallFile.location.getFileId())) {\n              bucket = updateLocationToBucket.get(smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to existing update bucket \" + bucket);\n            } else {\n              bucket = addUpdateBucket(partitionPath, smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to new update bucket \" + bucket);\n            }\n            bucketNumbers.add(bucket);\n            recordsPerBucket.add(recordsToAppend);\n            totalUnassignedInserts -= recordsToAppend;\n          }\n        }\n\n        \r\n        if (totalUnassignedInserts > 0) {\n          long insertRecordsPerBucket = config.getCopyOnWriteInsertSplitSize();\n          if (config.shouldAutoTuneInsertSplits()) {\n            insertRecordsPerBucket = config.getParquetMaxFileSize() / averageRecordSize;\n          }\n\n          int insertBuckets = (int) Math.ceil((1.0 * totalUnassignedInserts) / insertRecordsPerBucket);\n          LOG.info(\"After small file assignment: unassignedInserts => \" + totalUnassignedInserts\n              + \", totalInsertBuckets => \" + insertBuckets + \", recordsPerBucket => \" + insertRecordsPerBucket);\n          for (int b = 0; b < insertBuckets; b++) {\n            bucketNumbers.add(totalBuckets);\n            if (b < insertBuckets - 1) {\n              recordsPerBucket.add(insertRecordsPerBucket);\n            } else {\n              recordsPerBucket.add(totalUnassignedInserts - (insertBuckets - 1) * insertRecordsPerBucket);\n            }\n            BucketInfo bucketInfo = new BucketInfo(BucketType.INSERT, FSUtils.createNewFileIdPfx(), partitionPath);\n            bucketInfoMap.put(totalBuckets, bucketInfo);\n            totalBuckets++;\n          }\n        }\n\n        \r\n        List<InsertBucketCumulativeWeightPair> insertBuckets = new ArrayList<>();\n        double curentCumulativeWeight = 0;\n        for (int i = 0; i < bucketNumbers.size(); i++) {\n          InsertBucket bkt = new InsertBucket();\n          bkt.bucketNumber = bucketNumbers.get(i);\n          bkt.weight = (1.0 * recordsPerBucket.get(i)) / pStat.getNumInserts();\n          curentCumulativeWeight += bkt.weight;\n          insertBuckets.add(new InsertBucketCumulativeWeightPair(bkt, curentCumulativeWeight));\n        }\n        LOG.info(\"Total insert buckets for partition path \" + partitionPath + \" => \" + insertBuckets);\n        partitionPathToInsertBucketInfos.put(partitionPath, insertBuckets);\n      }\n    }\n  }\n","date":"2021-02-06 22:03:52","endLine":243,"groupId":"2332","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"assignInserts","params":"(WorkloadProfileprofile@HoodieEngineContextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9d/60cde69e5725060c7fb955b7a1d621aaf41206.src","preCode":"  private void assignInserts(WorkloadProfile profile, HoodieEngineContext context) {\n    \r\n    Set<String> partitionPaths = profile.getPartitionPaths();\n    long averageRecordSize =\n        averageBytesPerRecord(table.getMetaClient().getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n            config);\n    LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n\n    Map<String, List<SmallFile>> partitionSmallFilesMap =\n        getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n\n    Map<String, Set<String>> partitionPathToPendingClusteringFileGroupsId = getPartitionPathToPendingClusteringFileGroupsId();\n\n    for (String partitionPath : partitionPaths) {\n      WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n      if (pStat.getNumInserts() > 0) {\n\n        List<SmallFile> smallFiles =\n            filterSmallFilesInClustering(partitionPathToPendingClusteringFileGroupsId.getOrDefault(partitionPath, Collections.emptySet()),\n                partitionSmallFilesMap.get(partitionPath));\n\n        this.smallFiles.addAll(smallFiles);\n\n        LOG.info(\"For partitionPath : \" + partitionPath + \" Small Files => \" + smallFiles);\n\n        long totalUnassignedInserts = pStat.getNumInserts();\n        List<Integer> bucketNumbers = new ArrayList<>();\n        List<Long> recordsPerBucket = new ArrayList<>();\n\n        \r\n        for (SmallFile smallFile : smallFiles) {\n          long recordsToAppend = Math.min((config.getParquetMaxFileSize() - smallFile.sizeBytes) / averageRecordSize,\n              totalUnassignedInserts);\n          if (recordsToAppend > 0 && totalUnassignedInserts > 0) {\n            \r\n            int bucket;\n            if (updateLocationToBucket.containsKey(smallFile.location.getFileId())) {\n              bucket = updateLocationToBucket.get(smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to existing update bucket \" + bucket);\n            } else {\n              bucket = addUpdateBucket(partitionPath, smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to new update bucket \" + bucket);\n            }\n            bucketNumbers.add(bucket);\n            recordsPerBucket.add(recordsToAppend);\n            totalUnassignedInserts -= recordsToAppend;\n          }\n        }\n\n        \r\n        if (totalUnassignedInserts > 0) {\n          long insertRecordsPerBucket = config.getCopyOnWriteInsertSplitSize();\n          if (config.shouldAutoTuneInsertSplits()) {\n            insertRecordsPerBucket = config.getParquetMaxFileSize() / averageRecordSize;\n          }\n\n          int insertBuckets = (int) Math.ceil((1.0 * totalUnassignedInserts) / insertRecordsPerBucket);\n          LOG.info(\"After small file assignment: unassignedInserts => \" + totalUnassignedInserts\n              + \", totalInsertBuckets => \" + insertBuckets + \", recordsPerBucket => \" + insertRecordsPerBucket);\n          for (int b = 0; b < insertBuckets; b++) {\n            bucketNumbers.add(totalBuckets);\n            if (b < insertBuckets - 1) {\n              recordsPerBucket.add(insertRecordsPerBucket);\n            } else {\n              recordsPerBucket.add(totalUnassignedInserts - (insertBuckets - 1) * insertRecordsPerBucket);\n            }\n            BucketInfo bucketInfo = new BucketInfo(BucketType.INSERT, FSUtils.createNewFileIdPfx(), partitionPath);\n            bucketInfoMap.put(totalBuckets, bucketInfo);\n            totalBuckets++;\n          }\n        }\n\n        \r\n        List<InsertBucketCumulativeWeightPair> insertBuckets = new ArrayList<>();\n        double curentCumulativeWeight = 0;\n        for (int i = 0; i < bucketNumbers.size(); i++) {\n          InsertBucket bkt = new InsertBucket();\n          bkt.bucketNumber = bucketNumbers.get(i);\n          bkt.weight = (1.0 * recordsPerBucket.get(i)) / pStat.getNumInserts();\n          curentCumulativeWeight += bkt.weight;\n          insertBuckets.add(new InsertBucketCumulativeWeightPair(bkt, curentCumulativeWeight));\n        }\n        LOG.info(\"Total insert buckets for partition path \" + partitionPath + \" => \" + insertBuckets);\n        partitionPathToInsertBucketInfos.put(partitionPath, insertBuckets);\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":157,"status":"NB"}],"commitId":"8bc65b93189e249be43fe6cba2387ca83babc2c8","commitMessage":"@@@[HUDI-1731] Rename UpsertPartitioner in hudi-java-client (#2734)\n\nCo-authored-by: lei.zhu <lei.zhu@envisioncn.com>","date":"2021-03-31 11:06:04","modifiedFileCount":"1","status":"M","submitter":"leo-Iamok"},{"authorTime":"2021-07-06 00:30:57","codes":[{"authorDate":"2021-03-31 11:06:04","commitOrder":8,"curCode":"  private void assignInserts(WorkloadProfile profile, HoodieEngineContext context) {\n    \r\n    Set<String> partitionPaths = profile.getPartitionPaths();\n    long averageRecordSize =\n        averageBytesPerRecord(table.getMetaClient().getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n            config);\n    LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n\n    Map<String, List<SmallFile>> partitionSmallFilesMap =\n        getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n\n    for (String partitionPath : partitionPaths) {\n      WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n      if (pStat.getNumInserts() > 0) {\n\n        List<SmallFile> smallFiles = partitionSmallFilesMap.get(partitionPath);\n        this.smallFiles.addAll(smallFiles);\n\n        LOG.info(\"For partitionPath : \" + partitionPath + \" Small Files => \" + smallFiles);\n\n        long totalUnassignedInserts = pStat.getNumInserts();\n        List<Integer> bucketNumbers = new ArrayList<>();\n        List<Long> recordsPerBucket = new ArrayList<>();\n\n        \r\n        for (SmallFile smallFile : smallFiles) {\n          long recordsToAppend = Math.min((config.getParquetMaxFileSize() - smallFile.sizeBytes) / averageRecordSize,\n              totalUnassignedInserts);\n          if (recordsToAppend > 0) {\n            \r\n            int bucket;\n            if (updateLocationToBucket.containsKey(smallFile.location.getFileId())) {\n              bucket = updateLocationToBucket.get(smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to existing update bucket \" + bucket);\n            } else {\n              bucket = addUpdateBucket(partitionPath, smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to new update bucket \" + bucket);\n            }\n            bucketNumbers.add(bucket);\n            recordsPerBucket.add(recordsToAppend);\n            totalUnassignedInserts -= recordsToAppend;\n          }\n        }\n\n        \r\n        if (totalUnassignedInserts > 0) {\n          long insertRecordsPerBucket = config.getCopyOnWriteInsertSplitSize();\n          if (config.shouldAutoTuneInsertSplits()) {\n            insertRecordsPerBucket = config.getParquetMaxFileSize() / averageRecordSize;\n          }\n\n          int insertBuckets = (int) Math.ceil((1.0 * totalUnassignedInserts) / insertRecordsPerBucket);\n          LOG.info(\"After small file assignment: unassignedInserts => \" + totalUnassignedInserts\n              + \", totalInsertBuckets => \" + insertBuckets + \", recordsPerBucket => \" + insertRecordsPerBucket);\n          for (int b = 0; b < insertBuckets; b++) {\n            bucketNumbers.add(totalBuckets);\n            if (b < insertBuckets - 1) {\n              recordsPerBucket.add(insertRecordsPerBucket);\n            } else {\n              recordsPerBucket.add(totalUnassignedInserts - (insertBuckets - 1) * insertRecordsPerBucket);\n            }\n            BucketInfo bucketInfo = new BucketInfo(BucketType.INSERT, FSUtils.createNewFileIdPfx(), partitionPath);\n            bucketInfoMap.put(totalBuckets, bucketInfo);\n            totalBuckets++;\n          }\n        }\n\n        \r\n        List<InsertBucketCumulativeWeightPair> insertBuckets = new ArrayList<>();\n        double curentCumulativeWeight = 0;\n        for (int i = 0; i < bucketNumbers.size(); i++) {\n          InsertBucket bkt = new InsertBucket();\n          bkt.bucketNumber = bucketNumbers.get(i);\n          bkt.weight = (1.0 * recordsPerBucket.get(i)) / pStat.getNumInserts();\n          curentCumulativeWeight += bkt.weight;\n          insertBuckets.add(new InsertBucketCumulativeWeightPair(bkt, curentCumulativeWeight));\n        }\n        LOG.info(\"Total insert buckets for partition path \" + partitionPath + \" => \" + insertBuckets);\n        partitionPathToInsertBucketInfos.put(partitionPath, insertBuckets);\n      }\n    }\n  }\n","date":"2021-03-31 11:06:04","endLine":204,"groupId":"4517","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"assignInserts","params":"(WorkloadProfileprofile@HoodieEngineContextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/6b/5cb2997759c00feea98a6ae0aa6e2be4ba5c1c.src","preCode":"  private void assignInserts(WorkloadProfile profile, HoodieEngineContext context) {\n    \r\n    Set<String> partitionPaths = profile.getPartitionPaths();\n    long averageRecordSize =\n        averageBytesPerRecord(table.getMetaClient().getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n            config);\n    LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n\n    Map<String, List<SmallFile>> partitionSmallFilesMap =\n        getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n\n    for (String partitionPath : partitionPaths) {\n      WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n      if (pStat.getNumInserts() > 0) {\n\n        List<SmallFile> smallFiles = partitionSmallFilesMap.get(partitionPath);\n        this.smallFiles.addAll(smallFiles);\n\n        LOG.info(\"For partitionPath : \" + partitionPath + \" Small Files => \" + smallFiles);\n\n        long totalUnassignedInserts = pStat.getNumInserts();\n        List<Integer> bucketNumbers = new ArrayList<>();\n        List<Long> recordsPerBucket = new ArrayList<>();\n\n        \r\n        for (SmallFile smallFile : smallFiles) {\n          long recordsToAppend = Math.min((config.getParquetMaxFileSize() - smallFile.sizeBytes) / averageRecordSize,\n              totalUnassignedInserts);\n          if (recordsToAppend > 0) {\n            \r\n            int bucket;\n            if (updateLocationToBucket.containsKey(smallFile.location.getFileId())) {\n              bucket = updateLocationToBucket.get(smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to existing update bucket \" + bucket);\n            } else {\n              bucket = addUpdateBucket(partitionPath, smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to new update bucket \" + bucket);\n            }\n            bucketNumbers.add(bucket);\n            recordsPerBucket.add(recordsToAppend);\n            totalUnassignedInserts -= recordsToAppend;\n          }\n        }\n\n        \r\n        if (totalUnassignedInserts > 0) {\n          long insertRecordsPerBucket = config.getCopyOnWriteInsertSplitSize();\n          if (config.shouldAutoTuneInsertSplits()) {\n            insertRecordsPerBucket = config.getParquetMaxFileSize() / averageRecordSize;\n          }\n\n          int insertBuckets = (int) Math.ceil((1.0 * totalUnassignedInserts) / insertRecordsPerBucket);\n          LOG.info(\"After small file assignment: unassignedInserts => \" + totalUnassignedInserts\n              + \", totalInsertBuckets => \" + insertBuckets + \", recordsPerBucket => \" + insertRecordsPerBucket);\n          for (int b = 0; b < insertBuckets; b++) {\n            bucketNumbers.add(totalBuckets);\n            if (b < insertBuckets - 1) {\n              recordsPerBucket.add(insertRecordsPerBucket);\n            } else {\n              recordsPerBucket.add(totalUnassignedInserts - (insertBuckets - 1) * insertRecordsPerBucket);\n            }\n            BucketInfo bucketInfo = new BucketInfo(BucketType.INSERT, FSUtils.createNewFileIdPfx(), partitionPath);\n            bucketInfoMap.put(totalBuckets, bucketInfo);\n            totalBuckets++;\n          }\n        }\n\n        \r\n        List<InsertBucketCumulativeWeightPair> insertBuckets = new ArrayList<>();\n        double curentCumulativeWeight = 0;\n        for (int i = 0; i < bucketNumbers.size(); i++) {\n          InsertBucket bkt = new InsertBucket();\n          bkt.bucketNumber = bucketNumbers.get(i);\n          bkt.weight = (1.0 * recordsPerBucket.get(i)) / pStat.getNumInserts();\n          curentCumulativeWeight += bkt.weight;\n          insertBuckets.add(new InsertBucketCumulativeWeightPair(bkt, curentCumulativeWeight));\n        }\n        LOG.info(\"Total insert buckets for partition path \" + partitionPath + \" => \" + insertBuckets);\n        partitionPathToInsertBucketInfos.put(partitionPath, insertBuckets);\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertPartitioner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":123,"status":"N"},{"authorDate":"2021-07-06 00:30:57","commitOrder":8,"curCode":"  private void assignInserts(WorkloadProfile profile, HoodieEngineContext context) {\n    \r\n    Set<String> partitionPaths = profile.getPartitionPaths();\n    long averageRecordSize =\n        averageBytesPerRecord(table.getMetaClient().getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n            config);\n    LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n\n    Map<String, List<SmallFile>> partitionSmallFilesMap =\n        getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n\n    Map<String, Set<String>> partitionPathToPendingClusteringFileGroupsId = getPartitionPathToPendingClusteringFileGroupsId();\n\n    for (String partitionPath : partitionPaths) {\n      WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n      if (pStat.getNumInserts() > 0) {\n\n        List<SmallFile> smallFiles =\n            filterSmallFilesInClustering(partitionPathToPendingClusteringFileGroupsId.getOrDefault(partitionPath, Collections.emptySet()),\n                partitionSmallFilesMap.get(partitionPath));\n\n        this.smallFiles.addAll(smallFiles);\n\n        LOG.info(\"For partitionPath : \" + partitionPath + \" Small Files => \" + smallFiles);\n\n        long totalUnassignedInserts = pStat.getNumInserts();\n        List<Integer> bucketNumbers = new ArrayList<>();\n        List<Long> recordsPerBucket = new ArrayList<>();\n\n        \r\n        for (SmallFile smallFile : smallFiles) {\n          long recordsToAppend = Math.min((config.getParquetMaxFileSize() - smallFile.sizeBytes) / averageRecordSize,\n              totalUnassignedInserts);\n          if (recordsToAppend > 0) {\n            \r\n            int bucket;\n            if (updateLocationToBucket.containsKey(smallFile.location.getFileId())) {\n              bucket = updateLocationToBucket.get(smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to existing update bucket \" + bucket);\n            } else {\n              bucket = addUpdateBucket(partitionPath, smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to new update bucket \" + bucket);\n            }\n            bucketNumbers.add(bucket);\n            recordsPerBucket.add(recordsToAppend);\n            totalUnassignedInserts -= recordsToAppend;\n            if (totalUnassignedInserts <= 0) {\n              \r\n              break;\n            }\n          }\n        }\n\n        \r\n        if (totalUnassignedInserts > 0) {\n          long insertRecordsPerBucket = config.getCopyOnWriteInsertSplitSize();\n          if (config.shouldAutoTuneInsertSplits()) {\n            insertRecordsPerBucket = config.getParquetMaxFileSize() / averageRecordSize;\n          }\n\n          int insertBuckets = (int) Math.ceil((1.0 * totalUnassignedInserts) / insertRecordsPerBucket);\n          LOG.info(\"After small file assignment: unassignedInserts => \" + totalUnassignedInserts\n              + \", totalInsertBuckets => \" + insertBuckets + \", recordsPerBucket => \" + insertRecordsPerBucket);\n          for (int b = 0; b < insertBuckets; b++) {\n            bucketNumbers.add(totalBuckets);\n            if (b < insertBuckets - 1) {\n              recordsPerBucket.add(insertRecordsPerBucket);\n            } else {\n              recordsPerBucket.add(totalUnassignedInserts - (insertBuckets - 1) * insertRecordsPerBucket);\n            }\n            BucketInfo bucketInfo = new BucketInfo(BucketType.INSERT, FSUtils.createNewFileIdPfx(), partitionPath);\n            bucketInfoMap.put(totalBuckets, bucketInfo);\n            totalBuckets++;\n          }\n        }\n\n        \r\n        List<InsertBucketCumulativeWeightPair> insertBuckets = new ArrayList<>();\n        double curentCumulativeWeight = 0;\n        for (int i = 0; i < bucketNumbers.size(); i++) {\n          InsertBucket bkt = new InsertBucket();\n          bkt.bucketNumber = bucketNumbers.get(i);\n          bkt.weight = (1.0 * recordsPerBucket.get(i)) / pStat.getNumInserts();\n          curentCumulativeWeight += bkt.weight;\n          insertBuckets.add(new InsertBucketCumulativeWeightPair(bkt, curentCumulativeWeight));\n        }\n        LOG.info(\"Total insert buckets for partition path \" + partitionPath + \" => \" + insertBuckets);\n        partitionPathToInsertBucketInfos.put(partitionPath, insertBuckets);\n      }\n    }\n  }\n","date":"2021-07-06 00:30:57","endLine":247,"groupId":"3424","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"assignInserts","params":"(WorkloadProfileprofile@HoodieEngineContextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/3a/c81510b5a425d82bcd79dd50d4e3ec3e5dfc23.src","preCode":"  private void assignInserts(WorkloadProfile profile, HoodieEngineContext context) {\n    \r\n    Set<String> partitionPaths = profile.getPartitionPaths();\n    long averageRecordSize =\n        averageBytesPerRecord(table.getMetaClient().getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n            config);\n    LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n\n    Map<String, List<SmallFile>> partitionSmallFilesMap =\n        getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n\n    Map<String, Set<String>> partitionPathToPendingClusteringFileGroupsId = getPartitionPathToPendingClusteringFileGroupsId();\n\n    for (String partitionPath : partitionPaths) {\n      WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n      if (pStat.getNumInserts() > 0) {\n\n        List<SmallFile> smallFiles =\n            filterSmallFilesInClustering(partitionPathToPendingClusteringFileGroupsId.getOrDefault(partitionPath, Collections.emptySet()),\n                partitionSmallFilesMap.get(partitionPath));\n\n        this.smallFiles.addAll(smallFiles);\n\n        LOG.info(\"For partitionPath : \" + partitionPath + \" Small Files => \" + smallFiles);\n\n        long totalUnassignedInserts = pStat.getNumInserts();\n        List<Integer> bucketNumbers = new ArrayList<>();\n        List<Long> recordsPerBucket = new ArrayList<>();\n\n        \r\n        for (SmallFile smallFile : smallFiles) {\n          long recordsToAppend = Math.min((config.getParquetMaxFileSize() - smallFile.sizeBytes) / averageRecordSize,\n              totalUnassignedInserts);\n          if (recordsToAppend > 0 && totalUnassignedInserts > 0) {\n            \r\n            int bucket;\n            if (updateLocationToBucket.containsKey(smallFile.location.getFileId())) {\n              bucket = updateLocationToBucket.get(smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to existing update bucket \" + bucket);\n            } else {\n              bucket = addUpdateBucket(partitionPath, smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to new update bucket \" + bucket);\n            }\n            bucketNumbers.add(bucket);\n            recordsPerBucket.add(recordsToAppend);\n            totalUnassignedInserts -= recordsToAppend;\n          }\n        }\n\n        \r\n        if (totalUnassignedInserts > 0) {\n          long insertRecordsPerBucket = config.getCopyOnWriteInsertSplitSize();\n          if (config.shouldAutoTuneInsertSplits()) {\n            insertRecordsPerBucket = config.getParquetMaxFileSize() / averageRecordSize;\n          }\n\n          int insertBuckets = (int) Math.ceil((1.0 * totalUnassignedInserts) / insertRecordsPerBucket);\n          LOG.info(\"After small file assignment: unassignedInserts => \" + totalUnassignedInserts\n              + \", totalInsertBuckets => \" + insertBuckets + \", recordsPerBucket => \" + insertRecordsPerBucket);\n          for (int b = 0; b < insertBuckets; b++) {\n            bucketNumbers.add(totalBuckets);\n            if (b < insertBuckets - 1) {\n              recordsPerBucket.add(insertRecordsPerBucket);\n            } else {\n              recordsPerBucket.add(totalUnassignedInserts - (insertBuckets - 1) * insertRecordsPerBucket);\n            }\n            BucketInfo bucketInfo = new BucketInfo(BucketType.INSERT, FSUtils.createNewFileIdPfx(), partitionPath);\n            bucketInfoMap.put(totalBuckets, bucketInfo);\n            totalBuckets++;\n          }\n        }\n\n        \r\n        List<InsertBucketCumulativeWeightPair> insertBuckets = new ArrayList<>();\n        double curentCumulativeWeight = 0;\n        for (int i = 0; i < bucketNumbers.size(); i++) {\n          InsertBucket bkt = new InsertBucket();\n          bkt.bucketNumber = bucketNumbers.get(i);\n          bkt.weight = (1.0 * recordsPerBucket.get(i)) / pStat.getNumInserts();\n          curentCumulativeWeight += bkt.weight;\n          insertBuckets.add(new InsertBucketCumulativeWeightPair(bkt, curentCumulativeWeight));\n        }\n        LOG.info(\"Total insert buckets for partition path \" + partitionPath + \" => \" + insertBuckets);\n        partitionPathToInsertBucketInfos.put(partitionPath, insertBuckets);\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":157,"status":"M"}],"commitId":"650c4455c600b0346fed8b5b6aa4cc0bf3452e8c","commitMessage":"@@@[HUDI-2122] Improvement in packaging insert into smallfiles (#3213)\n\n","date":"2021-07-06 00:30:57","modifiedFileCount":"1","status":"M","submitter":"wangxianghu"},{"authorTime":"2021-09-11 15:45:49","codes":[{"authorDate":"2021-09-11 15:45:49","commitOrder":9,"curCode":"  private void assignInserts(WorkloadProfile profile, HoodieEngineContext context) {\n    \r\n    Set<String> partitionPaths = profile.getPartitionPaths();\n    long averageRecordSize =\n        averageBytesPerRecord(table.getMetaClient().getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n            config);\n    LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n\n    Map<String, List<SmallFile>> partitionSmallFilesMap =\n        getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n\n    for (String partitionPath : partitionPaths) {\n      WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n      if (pStat.getNumInserts() > 0) {\n\n        List<SmallFile> smallFiles = partitionSmallFilesMap.get(partitionPath);\n        this.smallFiles.addAll(smallFiles);\n\n        LOG.info(\"For partitionPath : \" + partitionPath + \" Small Files => \" + smallFiles);\n\n        long totalUnassignedInserts = pStat.getNumInserts();\n        List<Integer> bucketNumbers = new ArrayList<>();\n        List<Long> recordsPerBucket = new ArrayList<>();\n\n        \r\n        for (SmallFile smallFile : smallFiles) {\n          long recordsToAppend = Math.min((config.getParquetMaxFileSize() - smallFile.sizeBytes) / averageRecordSize,\n              totalUnassignedInserts);\n          if (recordsToAppend > 0) {\n            \r\n            int bucket;\n            if (updateLocationToBucket.containsKey(smallFile.location.getFileId())) {\n              bucket = updateLocationToBucket.get(smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to existing update bucket \" + bucket);\n            } else {\n              bucket = addUpdateBucket(partitionPath, smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to new update bucket \" + bucket);\n            }\n            bucketNumbers.add(bucket);\n            recordsPerBucket.add(recordsToAppend);\n            totalUnassignedInserts -= recordsToAppend;\n          }\n        }\n\n        \r\n        if (totalUnassignedInserts > 0) {\n          long insertRecordsPerBucket = config.getCopyOnWriteInsertSplitSize();\n          if (config.shouldAutoTuneInsertSplits()) {\n            insertRecordsPerBucket = config.getParquetMaxFileSize() / averageRecordSize;\n          }\n\n          int insertBuckets = (int) Math.ceil((1.0 * totalUnassignedInserts) / insertRecordsPerBucket);\n          LOG.info(\"After small file assignment: unassignedInserts => \" + totalUnassignedInserts\n              + \", totalInsertBuckets => \" + insertBuckets + \", recordsPerBucket => \" + insertRecordsPerBucket);\n          for (int b = 0; b < insertBuckets; b++) {\n            bucketNumbers.add(totalBuckets);\n            if (b < insertBuckets - 1) {\n              recordsPerBucket.add(insertRecordsPerBucket);\n            } else {\n              recordsPerBucket.add(totalUnassignedInserts - (insertBuckets - 1) * insertRecordsPerBucket);\n            }\n            BucketInfo bucketInfo = new BucketInfo(BucketType.INSERT, FSUtils.createNewFileIdPfx(), partitionPath);\n            bucketInfoMap.put(totalBuckets, bucketInfo);\n            totalBuckets++;\n          }\n        }\n\n        \r\n        List<InsertBucketCumulativeWeightPair> insertBuckets = new ArrayList<>();\n        double currentCumulativeWeight = 0;\n        for (int i = 0; i < bucketNumbers.size(); i++) {\n          InsertBucket bkt = new InsertBucket();\n          bkt.bucketNumber = bucketNumbers.get(i);\n          bkt.weight = (1.0 * recordsPerBucket.get(i)) / pStat.getNumInserts();\n          currentCumulativeWeight += bkt.weight;\n          insertBuckets.add(new InsertBucketCumulativeWeightPair(bkt, currentCumulativeWeight));\n        }\n        LOG.info(\"Total insert buckets for partition path \" + partitionPath + \" => \" + insertBuckets);\n        partitionPathToInsertBucketInfos.put(partitionPath, insertBuckets);\n      }\n    }\n  }\n","date":"2021-09-11 15:45:49","endLine":204,"groupId":"10501","id":5,"instanceNumber":1,"isCurCommit":1,"methodName":"assignInserts","params":"(WorkloadProfileprofile@HoodieEngineContextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/33/f59f4406f39f3aac08e54e0df0ffb253492fd9.src","preCode":"  private void assignInserts(WorkloadProfile profile, HoodieEngineContext context) {\n    \r\n    Set<String> partitionPaths = profile.getPartitionPaths();\n    long averageRecordSize =\n        averageBytesPerRecord(table.getMetaClient().getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n            config);\n    LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n\n    Map<String, List<SmallFile>> partitionSmallFilesMap =\n        getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n\n    for (String partitionPath : partitionPaths) {\n      WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n      if (pStat.getNumInserts() > 0) {\n\n        List<SmallFile> smallFiles = partitionSmallFilesMap.get(partitionPath);\n        this.smallFiles.addAll(smallFiles);\n\n        LOG.info(\"For partitionPath : \" + partitionPath + \" Small Files => \" + smallFiles);\n\n        long totalUnassignedInserts = pStat.getNumInserts();\n        List<Integer> bucketNumbers = new ArrayList<>();\n        List<Long> recordsPerBucket = new ArrayList<>();\n\n        \r\n        for (SmallFile smallFile : smallFiles) {\n          long recordsToAppend = Math.min((config.getParquetMaxFileSize() - smallFile.sizeBytes) / averageRecordSize,\n              totalUnassignedInserts);\n          if (recordsToAppend > 0) {\n            \r\n            int bucket;\n            if (updateLocationToBucket.containsKey(smallFile.location.getFileId())) {\n              bucket = updateLocationToBucket.get(smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to existing update bucket \" + bucket);\n            } else {\n              bucket = addUpdateBucket(partitionPath, smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to new update bucket \" + bucket);\n            }\n            bucketNumbers.add(bucket);\n            recordsPerBucket.add(recordsToAppend);\n            totalUnassignedInserts -= recordsToAppend;\n          }\n        }\n\n        \r\n        if (totalUnassignedInserts > 0) {\n          long insertRecordsPerBucket = config.getCopyOnWriteInsertSplitSize();\n          if (config.shouldAutoTuneInsertSplits()) {\n            insertRecordsPerBucket = config.getParquetMaxFileSize() / averageRecordSize;\n          }\n\n          int insertBuckets = (int) Math.ceil((1.0 * totalUnassignedInserts) / insertRecordsPerBucket);\n          LOG.info(\"After small file assignment: unassignedInserts => \" + totalUnassignedInserts\n              + \", totalInsertBuckets => \" + insertBuckets + \", recordsPerBucket => \" + insertRecordsPerBucket);\n          for (int b = 0; b < insertBuckets; b++) {\n            bucketNumbers.add(totalBuckets);\n            if (b < insertBuckets - 1) {\n              recordsPerBucket.add(insertRecordsPerBucket);\n            } else {\n              recordsPerBucket.add(totalUnassignedInserts - (insertBuckets - 1) * insertRecordsPerBucket);\n            }\n            BucketInfo bucketInfo = new BucketInfo(BucketType.INSERT, FSUtils.createNewFileIdPfx(), partitionPath);\n            bucketInfoMap.put(totalBuckets, bucketInfo);\n            totalBuckets++;\n          }\n        }\n\n        \r\n        List<InsertBucketCumulativeWeightPair> insertBuckets = new ArrayList<>();\n        double curentCumulativeWeight = 0;\n        for (int i = 0; i < bucketNumbers.size(); i++) {\n          InsertBucket bkt = new InsertBucket();\n          bkt.bucketNumber = bucketNumbers.get(i);\n          bkt.weight = (1.0 * recordsPerBucket.get(i)) / pStat.getNumInserts();\n          curentCumulativeWeight += bkt.weight;\n          insertBuckets.add(new InsertBucketCumulativeWeightPair(bkt, curentCumulativeWeight));\n        }\n        LOG.info(\"Total insert buckets for partition path \" + partitionPath + \" => \" + insertBuckets);\n        partitionPathToInsertBucketInfos.put(partitionPath, insertBuckets);\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-java-client/src/main/java/org/apache/hudi/table/action/commit/JavaUpsertPartitioner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":123,"status":"M"},{"authorDate":"2021-09-11 15:45:49","commitOrder":9,"curCode":"  private void assignInserts(WorkloadProfile profile, HoodieEngineContext context) {\n    \r\n    Set<String> partitionPaths = profile.getPartitionPaths();\n    long averageRecordSize =\n        averageBytesPerRecord(table.getMetaClient().getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n            config);\n    LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n\n    Map<String, List<SmallFile>> partitionSmallFilesMap =\n        getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n\n    Map<String, Set<String>> partitionPathToPendingClusteringFileGroupsId = getPartitionPathToPendingClusteringFileGroupsId();\n\n    for (String partitionPath : partitionPaths) {\n      WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n      if (pStat.getNumInserts() > 0) {\n\n        List<SmallFile> smallFiles =\n            filterSmallFilesInClustering(partitionPathToPendingClusteringFileGroupsId.getOrDefault(partitionPath, Collections.emptySet()),\n                partitionSmallFilesMap.get(partitionPath));\n\n        this.smallFiles.addAll(smallFiles);\n\n        LOG.info(\"For partitionPath : \" + partitionPath + \" Small Files => \" + smallFiles);\n\n        long totalUnassignedInserts = pStat.getNumInserts();\n        List<Integer> bucketNumbers = new ArrayList<>();\n        List<Long> recordsPerBucket = new ArrayList<>();\n\n        \r\n        for (SmallFile smallFile : smallFiles) {\n          long recordsToAppend = Math.min((config.getParquetMaxFileSize() - smallFile.sizeBytes) / averageRecordSize,\n              totalUnassignedInserts);\n          if (recordsToAppend > 0) {\n            \r\n            int bucket;\n            if (updateLocationToBucket.containsKey(smallFile.location.getFileId())) {\n              bucket = updateLocationToBucket.get(smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to existing update bucket \" + bucket);\n            } else {\n              bucket = addUpdateBucket(partitionPath, smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to new update bucket \" + bucket);\n            }\n            bucketNumbers.add(bucket);\n            recordsPerBucket.add(recordsToAppend);\n            totalUnassignedInserts -= recordsToAppend;\n            if (totalUnassignedInserts <= 0) {\n              \r\n              break;\n            }\n          }\n        }\n\n        \r\n        if (totalUnassignedInserts > 0) {\n          long insertRecordsPerBucket = config.getCopyOnWriteInsertSplitSize();\n          if (config.shouldAutoTuneInsertSplits()) {\n            insertRecordsPerBucket = config.getParquetMaxFileSize() / averageRecordSize;\n          }\n\n          int insertBuckets = (int) Math.ceil((1.0 * totalUnassignedInserts) / insertRecordsPerBucket);\n          LOG.info(\"After small file assignment: unassignedInserts => \" + totalUnassignedInserts\n              + \", totalInsertBuckets => \" + insertBuckets + \", recordsPerBucket => \" + insertRecordsPerBucket);\n          for (int b = 0; b < insertBuckets; b++) {\n            bucketNumbers.add(totalBuckets);\n            if (b < insertBuckets - 1) {\n              recordsPerBucket.add(insertRecordsPerBucket);\n            } else {\n              recordsPerBucket.add(totalUnassignedInserts - (insertBuckets - 1) * insertRecordsPerBucket);\n            }\n            BucketInfo bucketInfo = new BucketInfo(BucketType.INSERT, FSUtils.createNewFileIdPfx(), partitionPath);\n            bucketInfoMap.put(totalBuckets, bucketInfo);\n            totalBuckets++;\n          }\n        }\n\n        \r\n        List<InsertBucketCumulativeWeightPair> insertBuckets = new ArrayList<>();\n        double currentCumulativeWeight = 0;\n        for (int i = 0; i < bucketNumbers.size(); i++) {\n          InsertBucket bkt = new InsertBucket();\n          bkt.bucketNumber = bucketNumbers.get(i);\n          bkt.weight = (1.0 * recordsPerBucket.get(i)) / pStat.getNumInserts();\n          currentCumulativeWeight += bkt.weight;\n          insertBuckets.add(new InsertBucketCumulativeWeightPair(bkt, currentCumulativeWeight));\n        }\n        LOG.info(\"Total insert buckets for partition path \" + partitionPath + \" => \" + insertBuckets);\n        partitionPathToInsertBucketInfos.put(partitionPath, insertBuckets);\n      }\n    }\n  }\n","date":"2021-09-11 15:45:49","endLine":247,"groupId":"10501","id":6,"instanceNumber":2,"isCurCommit":1,"methodName":"assignInserts","params":"(WorkloadProfileprofile@HoodieEngineContextcontext)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/35/a8bddf94fbd1edf387ba43e3ce6236c44089aa.src","preCode":"  private void assignInserts(WorkloadProfile profile, HoodieEngineContext context) {\n    \r\n    Set<String> partitionPaths = profile.getPartitionPaths();\n    long averageRecordSize =\n        averageBytesPerRecord(table.getMetaClient().getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n            config);\n    LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n\n    Map<String, List<SmallFile>> partitionSmallFilesMap =\n        getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n\n    Map<String, Set<String>> partitionPathToPendingClusteringFileGroupsId = getPartitionPathToPendingClusteringFileGroupsId();\n\n    for (String partitionPath : partitionPaths) {\n      WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n      if (pStat.getNumInserts() > 0) {\n\n        List<SmallFile> smallFiles =\n            filterSmallFilesInClustering(partitionPathToPendingClusteringFileGroupsId.getOrDefault(partitionPath, Collections.emptySet()),\n                partitionSmallFilesMap.get(partitionPath));\n\n        this.smallFiles.addAll(smallFiles);\n\n        LOG.info(\"For partitionPath : \" + partitionPath + \" Small Files => \" + smallFiles);\n\n        long totalUnassignedInserts = pStat.getNumInserts();\n        List<Integer> bucketNumbers = new ArrayList<>();\n        List<Long> recordsPerBucket = new ArrayList<>();\n\n        \r\n        for (SmallFile smallFile : smallFiles) {\n          long recordsToAppend = Math.min((config.getParquetMaxFileSize() - smallFile.sizeBytes) / averageRecordSize,\n              totalUnassignedInserts);\n          if (recordsToAppend > 0) {\n            \r\n            int bucket;\n            if (updateLocationToBucket.containsKey(smallFile.location.getFileId())) {\n              bucket = updateLocationToBucket.get(smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to existing update bucket \" + bucket);\n            } else {\n              bucket = addUpdateBucket(partitionPath, smallFile.location.getFileId());\n              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to new update bucket \" + bucket);\n            }\n            bucketNumbers.add(bucket);\n            recordsPerBucket.add(recordsToAppend);\n            totalUnassignedInserts -= recordsToAppend;\n            if (totalUnassignedInserts <= 0) {\n              \r\n              break;\n            }\n          }\n        }\n\n        \r\n        if (totalUnassignedInserts > 0) {\n          long insertRecordsPerBucket = config.getCopyOnWriteInsertSplitSize();\n          if (config.shouldAutoTuneInsertSplits()) {\n            insertRecordsPerBucket = config.getParquetMaxFileSize() / averageRecordSize;\n          }\n\n          int insertBuckets = (int) Math.ceil((1.0 * totalUnassignedInserts) / insertRecordsPerBucket);\n          LOG.info(\"After small file assignment: unassignedInserts => \" + totalUnassignedInserts\n              + \", totalInsertBuckets => \" + insertBuckets + \", recordsPerBucket => \" + insertRecordsPerBucket);\n          for (int b = 0; b < insertBuckets; b++) {\n            bucketNumbers.add(totalBuckets);\n            if (b < insertBuckets - 1) {\n              recordsPerBucket.add(insertRecordsPerBucket);\n            } else {\n              recordsPerBucket.add(totalUnassignedInserts - (insertBuckets - 1) * insertRecordsPerBucket);\n            }\n            BucketInfo bucketInfo = new BucketInfo(BucketType.INSERT, FSUtils.createNewFileIdPfx(), partitionPath);\n            bucketInfoMap.put(totalBuckets, bucketInfo);\n            totalBuckets++;\n          }\n        }\n\n        \r\n        List<InsertBucketCumulativeWeightPair> insertBuckets = new ArrayList<>();\n        double curentCumulativeWeight = 0;\n        for (int i = 0; i < bucketNumbers.size(); i++) {\n          InsertBucket bkt = new InsertBucket();\n          bkt.bucketNumber = bucketNumbers.get(i);\n          bkt.weight = (1.0 * recordsPerBucket.get(i)) / pStat.getNumInserts();\n          curentCumulativeWeight += bkt.weight;\n          insertBuckets.add(new InsertBucketCumulativeWeightPair(bkt, curentCumulativeWeight));\n        }\n        LOG.info(\"Total insert buckets for partition path \" + partitionPath + \" => \" + insertBuckets);\n        partitionPathToInsertBucketInfos.put(partitionPath, insertBuckets);\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":157,"status":"M"}],"commitId":"dbcf60f370e93ab490cf82e677387a07ea743cda","commitMessage":"@@@[MINOR] fix typo (#3640)\n\n","date":"2021-09-11 15:45:49","modifiedFileCount":"2","status":"M","submitter":"???"}]
