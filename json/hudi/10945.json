[{"authorTime":"2020-05-27 16:28:17","codes":[{"authorDate":"2020-05-27 16:28:17","commitOrder":1,"curCode":"  public void testAvroLogRecordReaderWithFailedRollbacks(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    List<IndexedRecord> records2 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    dataBlock = new HoodieAvroDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer = writer.appendBlock(deleteBlock);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    try {\n      writer = writer.appendBlock(commandBlock);\n      \r\n      throw new Exception(\"simulating failure\");\n    } catch (Exception e) {\n      \r\n    }\n    \r\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    \r\n    HoodieMergedLogRecordScanner scanner = new HoodieMergedLogRecordScanner(fs, basePath, allLogFiles, schema, \"100\",\n        10240L, readBlocksLazily, false, bufferSize, BASE_OUTPUT_PATH);\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would have scanned 0 records because of rollback\");\n\n    final List<String> readKeys = new ArrayList<>();\n    scanner.forEach(s -> readKeys.add(s.getKey().getRecordKey()));\n    assertEquals(0, readKeys.size(), \"Stream collect should return all 0 records\");\n  }\n","date":"2020-05-27 16:28:17","endLine":897,"groupId":"4539","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testAvroLogRecordReaderWithFailedRollbacks","params":"(booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e9/f0ef7b167a811f253b4412a326c85966d5382c.src","preCode":"  public void testAvroLogRecordReaderWithFailedRollbacks(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    List<IndexedRecord> records2 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    dataBlock = new HoodieAvroDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer = writer.appendBlock(deleteBlock);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    try {\n      writer = writer.appendBlock(commandBlock);\n      \r\n      throw new Exception(\"simulating failure\");\n    } catch (Exception e) {\n      \r\n    }\n    \r\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    \r\n    HoodieMergedLogRecordScanner scanner = new HoodieMergedLogRecordScanner(fs, basePath, allLogFiles, schema, \"100\",\n        10240L, readBlocksLazily, false, bufferSize, BASE_OUTPUT_PATH);\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would have scanned 0 records because of rollback\");\n\n    final List<String> readKeys = new ArrayList<>();\n    scanner.forEach(s -> readKeys.add(s.getKey().getRecordKey()));\n    assertEquals(0, readKeys.size(), \"Stream collect should return all 0 records\");\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":833,"status":"B"},{"authorDate":"2020-05-27 16:28:17","commitOrder":1,"curCode":"  public void testAvroLogRecordReaderWithInsertDeleteAndRollback(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer = writer.appendBlock(deleteBlock);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    writer = writer.appendBlock(commandBlock);\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    HoodieMergedLogRecordScanner scanner = new HoodieMergedLogRecordScanner(fs, basePath, allLogFiles, schema, \"100\",\n        10240L, readBlocksLazily, false, bufferSize, BASE_OUTPUT_PATH);\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would read 0 records\");\n  }\n","date":"2020-05-27 16:28:17","endLine":944,"groupId":"4404","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testAvroLogRecordReaderWithInsertDeleteAndRollback","params":"(booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e9/f0ef7b167a811f253b4412a326c85966d5382c.src","preCode":"  public void testAvroLogRecordReaderWithInsertDeleteAndRollback(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer = writer.appendBlock(deleteBlock);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    writer = writer.appendBlock(commandBlock);\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    HoodieMergedLogRecordScanner scanner = new HoodieMergedLogRecordScanner(fs, basePath, allLogFiles, schema, \"100\",\n        10240L, readBlocksLazily, false, bufferSize, BASE_OUTPUT_PATH);\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would read 0 records\");\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":901,"status":"B"}],"commitId":"03f136361a5fed594855992ab10bee8bb5060c5b","commitMessage":"@@@[HUDI-811] Restructure test packages in hudi-common (#1644)\n\n* [HUDI-811] Restructure test packages in hudi-common","date":"2020-05-27 16:28:17","modifiedFileCount":"53","status":"B","submitter":"Raymond Xu"},{"authorTime":"2020-06-26 14:46:55","codes":[{"authorDate":"2020-06-26 14:46:55","commitOrder":2,"curCode":"  public void testAvroLogRecordReaderWithFailedRollbacks(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    List<IndexedRecord> records2 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    dataBlock = getDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer = writer.appendBlock(deleteBlock);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    try {\n      writer = writer.appendBlock(commandBlock);\n      \r\n      throw new Exception(\"simulating failure\");\n    } catch (Exception e) {\n      \r\n    }\n    \r\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    \r\n    HoodieMergedLogRecordScanner scanner = new HoodieMergedLogRecordScanner(fs, basePath, allLogFiles, schema, \"100\",\n        10240L, readBlocksLazily, false, bufferSize, BASE_OUTPUT_PATH);\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would have scanned 0 records because of rollback\");\n\n    final List<String> readKeys = new ArrayList<>();\n    scanner.forEach(s -> readKeys.add(s.getKey().getRecordKey()));\n    assertEquals(0, readKeys.size(), \"Stream collect should return all 0 records\");\n  }\n","date":"2020-06-26 14:46:55","endLine":906,"groupId":"3682","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testAvroLogRecordReaderWithFailedRollbacks","params":"(booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/62/14af1af36313e2c739a9c15553b5c32d81dd94.src","preCode":"  public void testAvroLogRecordReaderWithFailedRollbacks(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    List<IndexedRecord> records2 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    dataBlock = new HoodieAvroDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer = writer.appendBlock(deleteBlock);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    try {\n      writer = writer.appendBlock(commandBlock);\n      \r\n      throw new Exception(\"simulating failure\");\n    } catch (Exception e) {\n      \r\n    }\n    \r\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    \r\n    HoodieMergedLogRecordScanner scanner = new HoodieMergedLogRecordScanner(fs, basePath, allLogFiles, schema, \"100\",\n        10240L, readBlocksLazily, false, bufferSize, BASE_OUTPUT_PATH);\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would have scanned 0 records because of rollback\");\n\n    final List<String> readKeys = new ArrayList<>();\n    scanner.forEach(s -> readKeys.add(s.getKey().getRecordKey()));\n    assertEquals(0, readKeys.size(), \"Stream collect should return all 0 records\");\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":842,"status":"M"},{"authorDate":"2020-06-26 14:46:55","commitOrder":2,"curCode":"  public void testAvroLogRecordReaderWithInsertDeleteAndRollback(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer = writer.appendBlock(deleteBlock);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    writer = writer.appendBlock(commandBlock);\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    HoodieMergedLogRecordScanner scanner = new HoodieMergedLogRecordScanner(fs, basePath, allLogFiles, schema, \"100\",\n        10240L, readBlocksLazily, false, bufferSize, BASE_OUTPUT_PATH);\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would read 0 records\");\n  }\n","date":"2020-06-26 14:46:55","endLine":953,"groupId":"3683","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testAvroLogRecordReaderWithInsertDeleteAndRollback","params":"(booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/62/14af1af36313e2c739a9c15553b5c32d81dd94.src","preCode":"  public void testAvroLogRecordReaderWithInsertDeleteAndRollback(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer = writer.appendBlock(deleteBlock);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    writer = writer.appendBlock(commandBlock);\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    HoodieMergedLogRecordScanner scanner = new HoodieMergedLogRecordScanner(fs, basePath, allLogFiles, schema, \"100\",\n        10240L, readBlocksLazily, false, bufferSize, BASE_OUTPUT_PATH);\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would read 0 records\");\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":910,"status":"M"}],"commitId":"2603cfb33e272632d7f36a53e1b13fe86dbb8627","commitMessage":"@@@[HUDI-684] Introduced abstraction for writing and reading different types of base file formats. (#1687)\n\nNotable changes:\n    1. HoodieFileWriter and HoodieFileReader abstractions for writer/reader side of a base file format\n    2. HoodieDataBlock abstraction for creation specific data blocks for base file formats. (e.g. Parquet has HoodieAvroDataBlock)\n    3. All hardocded references to Parquet / Parquet based classes have been abstracted to call methods which accept a base file format\n    4. HiveSyncTool accepts the base file format as a CLI parameter\n    5. HoodieDeltaStreamer accepts the base file format as a CLI parameter\n    6. HoodieSparkSqlWriter accepts the base file format as a parameter","date":"2020-06-26 14:46:55","modifiedFileCount":"42","status":"M","submitter":"Prashant Wason"},{"authorTime":"2020-12-10 20:02:02","codes":[{"authorDate":"2020-12-10 20:02:02","commitOrder":3,"curCode":"  public void testAvroLogRecordReaderWithFailedRollbacks(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    List<IndexedRecord> records2 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    dataBlock = getDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer = writer.appendBlock(deleteBlock);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    try {\n      writer = writer.appendBlock(commandBlock);\n      \r\n      throw new Exception(\"simulating failure\");\n    } catch (Exception e) {\n      \r\n    }\n    \r\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    \r\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(basePath)\n        .withLogFilePaths(allLogFiles)\n        .withReaderSchema(schema)\n        .withLatestInstantTime(\"100\")\n        .withMaxMemorySizeInBytes(10240L)\n        .withReadBlocksLazily(readBlocksLazily)\n        .withReverseReader(false)\n        .withBufferSize(bufferSize)\n        .withSpillableMapBasePath(BASE_OUTPUT_PATH)\n        .build();\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would have scanned 0 records because of rollback\");\n\n    final List<String> readKeys = new ArrayList<>();\n    scanner.forEach(s -> readKeys.add(s.getKey().getRecordKey()));\n    assertEquals(0, readKeys.size(), \"Stream collect should return all 0 records\");\n  }\n","date":"2020-12-10 20:02:02","endLine":979,"groupId":"3682","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testAvroLogRecordReaderWithFailedRollbacks","params":"(booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/98/ece7309ea7d11421abfb1207539c5505345ca3.src","preCode":"  public void testAvroLogRecordReaderWithFailedRollbacks(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    List<IndexedRecord> records2 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    dataBlock = getDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer = writer.appendBlock(deleteBlock);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    try {\n      writer = writer.appendBlock(commandBlock);\n      \r\n      throw new Exception(\"simulating failure\");\n    } catch (Exception e) {\n      \r\n    }\n    \r\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    \r\n    HoodieMergedLogRecordScanner scanner = new HoodieMergedLogRecordScanner(fs, basePath, allLogFiles, schema, \"100\",\n        10240L, readBlocksLazily, false, bufferSize, BASE_OUTPUT_PATH);\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would have scanned 0 records because of rollback\");\n\n    final List<String> readKeys = new ArrayList<>();\n    scanner.forEach(s -> readKeys.add(s.getKey().getRecordKey()));\n    assertEquals(0, readKeys.size(), \"Stream collect should return all 0 records\");\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":905,"status":"M"},{"authorDate":"2020-12-10 20:02:02","commitOrder":3,"curCode":"  public void testAvroLogRecordReaderWithInsertDeleteAndRollback(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer = writer.appendBlock(deleteBlock);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    writer = writer.appendBlock(commandBlock);\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(basePath)\n        .withLogFilePaths(allLogFiles)\n        .withReaderSchema(schema)\n        .withLatestInstantTime(\"100\")\n        .withMaxMemorySizeInBytes(10240L)\n        .withReadBlocksLazily(readBlocksLazily)\n        .withReverseReader(false)\n        .withBufferSize(bufferSize)\n        .withSpillableMapBasePath(BASE_OUTPUT_PATH)\n        .build();\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would read 0 records\");\n  }\n","date":"2020-12-10 20:02:02","endLine":1036,"groupId":"3683","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testAvroLogRecordReaderWithInsertDeleteAndRollback","params":"(booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/98/ece7309ea7d11421abfb1207539c5505345ca3.src","preCode":"  public void testAvroLogRecordReaderWithInsertDeleteAndRollback(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer = writer.appendBlock(deleteBlock);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    writer = writer.appendBlock(commandBlock);\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    HoodieMergedLogRecordScanner scanner = new HoodieMergedLogRecordScanner(fs, basePath, allLogFiles, schema, \"100\",\n        10240L, readBlocksLazily, false, bufferSize, BASE_OUTPUT_PATH);\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would read 0 records\");\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":983,"status":"M"}],"commitId":"4bc45a391afc14738a42fc731fe958b2af5e3ee8","commitMessage":"@@@[HUDI-1445] Refactor AbstractHoodieLogRecordScanner to use Builder (#2313)\n\n","date":"2020-12-10 20:02:02","modifiedFileCount":"10","status":"M","submitter":"Danny Chan"},{"authorTime":"2020-12-31 06:22:15","codes":[{"authorDate":"2020-12-31 06:22:15","commitOrder":4,"curCode":"  public void testAvroLogRecordReaderWithFailedRollbacks(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer.appendBlock(dataBlock);\n\n    \r\n    List<IndexedRecord> records2 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    dataBlock = getDataBlock(records2, header);\n    writer.appendBlock(dataBlock);\n\n    \r\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer.appendBlock(deleteBlock);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    try {\n      writer.appendBlock(commandBlock);\n      \r\n      throw new Exception(\"simulating failure\");\n    } catch (Exception e) {\n      \r\n    }\n    \r\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    \r\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(basePath)\n        .withLogFilePaths(allLogFiles)\n        .withReaderSchema(schema)\n        .withLatestInstantTime(\"100\")\n        .withMaxMemorySizeInBytes(10240L)\n        .withReadBlocksLazily(readBlocksLazily)\n        .withReverseReader(false)\n        .withBufferSize(bufferSize)\n        .withSpillableMapBasePath(BASE_OUTPUT_PATH)\n        .build();\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would have scanned 0 records because of rollback\");\n\n    final List<String> readKeys = new ArrayList<>();\n    scanner.forEach(s -> readKeys.add(s.getKey().getRecordKey()));\n    assertEquals(0, readKeys.size(), \"Stream collect should return all 0 records\");\n  }\n","date":"2020-12-31 06:22:15","endLine":993,"groupId":"3956","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testAvroLogRecordReaderWithFailedRollbacks","params":"(booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/57/e814c33909030385442385ccab9edfb802bcf5.src","preCode":"  public void testAvroLogRecordReaderWithFailedRollbacks(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    List<IndexedRecord> records2 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    dataBlock = getDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer = writer.appendBlock(deleteBlock);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    try {\n      writer = writer.appendBlock(commandBlock);\n      \r\n      throw new Exception(\"simulating failure\");\n    } catch (Exception e) {\n      \r\n    }\n    \r\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    \r\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(basePath)\n        .withLogFilePaths(allLogFiles)\n        .withReaderSchema(schema)\n        .withLatestInstantTime(\"100\")\n        .withMaxMemorySizeInBytes(10240L)\n        .withReadBlocksLazily(readBlocksLazily)\n        .withReverseReader(false)\n        .withBufferSize(bufferSize)\n        .withSpillableMapBasePath(BASE_OUTPUT_PATH)\n        .build();\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would have scanned 0 records because of rollback\");\n\n    final List<String> readKeys = new ArrayList<>();\n    scanner.forEach(s -> readKeys.add(s.getKey().getRecordKey()));\n    assertEquals(0, readKeys.size(), \"Stream collect should return all 0 records\");\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":919,"status":"M"},{"authorDate":"2020-12-31 06:22:15","commitOrder":4,"curCode":"  public void testAvroLogRecordReaderWithInsertDeleteAndRollback(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer.appendBlock(dataBlock);\n\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer.appendBlock(deleteBlock);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    writer.appendBlock(commandBlock);\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(basePath)\n        .withLogFilePaths(allLogFiles)\n        .withReaderSchema(schema)\n        .withLatestInstantTime(\"100\")\n        .withMaxMemorySizeInBytes(10240L)\n        .withReadBlocksLazily(readBlocksLazily)\n        .withReverseReader(false)\n        .withBufferSize(bufferSize)\n        .withSpillableMapBasePath(BASE_OUTPUT_PATH)\n        .build();\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would read 0 records\");\n  }\n","date":"2020-12-31 06:22:15","endLine":1050,"groupId":"3957","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testAvroLogRecordReaderWithInsertDeleteAndRollback","params":"(booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/57/e814c33909030385442385ccab9edfb802bcf5.src","preCode":"  public void testAvroLogRecordReaderWithInsertDeleteAndRollback(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer = writer.appendBlock(deleteBlock);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    writer = writer.appendBlock(commandBlock);\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(basePath)\n        .withLogFilePaths(allLogFiles)\n        .withReaderSchema(schema)\n        .withLatestInstantTime(\"100\")\n        .withMaxMemorySizeInBytes(10240L)\n        .withReadBlocksLazily(readBlocksLazily)\n        .withReverseReader(false)\n        .withBufferSize(bufferSize)\n        .withSpillableMapBasePath(BASE_OUTPUT_PATH)\n        .build();\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would read 0 records\");\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":997,"status":"M"}],"commitId":"605b617cfa9a25bff48e618beccfaa7b4eaeee56","commitMessage":"@@@[HUDI-1434] fix incorrect log file path in HoodieWriteStat (#2300)\n\n* [HUDI-1434] fix incorrect log file path in HoodieWriteStat\n\n* HoodieWriteHandle#close() returns a list of WriteStatus objs\n\n* Handle rolled-over log files and return a WriteStatus per log file written\n\n - Combined data and delete block logging into a single call\n - Lazily initialize and manage write status based on returned AppendResult\n - Use FSUtils.getFileSize() to set final file size.  consistent with other handles\n - Added tests around returned values in AppendResult\n - Added validation of the file sizes returned in write stat\n\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2020-12-31 06:22:15","modifiedFileCount":"27","status":"M","submitter":"Gary Li"},{"authorTime":"2021-02-20 12:12:22","codes":[{"authorDate":"2021-02-20 12:12:22","commitOrder":5,"curCode":"  public void testAvroLogRecordReaderWithFailedRollbacks(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer.appendBlock(dataBlock);\n\n    \r\n    List<IndexedRecord> records2 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    dataBlock = getDataBlock(records2, header);\n    writer.appendBlock(dataBlock);\n\n    \r\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer.appendBlock(deleteBlock);\n\n    FileCreateUtils.createDeltaCommit(basePath, \"100\", fs);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    try {\n      writer.appendBlock(commandBlock);\n      \r\n      throw new Exception(\"simulating failure\");\n    } catch (Exception e) {\n      \r\n    }\n    \r\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    \r\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(basePath)\n        .withLogFilePaths(allLogFiles)\n        .withReaderSchema(schema)\n        .withLatestInstantTime(\"100\")\n        .withMaxMemorySizeInBytes(10240L)\n        .withReadBlocksLazily(readBlocksLazily)\n        .withReverseReader(false)\n        .withBufferSize(bufferSize)\n        .withSpillableMapBasePath(BASE_OUTPUT_PATH)\n        .build();\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would have scanned 0 records because of rollback\");\n\n    final List<String> readKeys = new ArrayList<>();\n    scanner.forEach(s -> readKeys.add(s.getKey().getRecordKey()));\n    assertEquals(0, readKeys.size(), \"Stream collect should return all 0 records\");\n    FileCreateUtils.deleteDeltaCommit(basePath, \"100\", fs);\n  }\n","date":"2021-02-20 12:12:22","endLine":1001,"groupId":"3956","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testAvroLogRecordReaderWithFailedRollbacks","params":"(booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/69/fc60f6a3e429215cf49530f53411f54fd686cd.src","preCode":"  public void testAvroLogRecordReaderWithFailedRollbacks(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer.appendBlock(dataBlock);\n\n    \r\n    List<IndexedRecord> records2 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    dataBlock = getDataBlock(records2, header);\n    writer.appendBlock(dataBlock);\n\n    \r\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer.appendBlock(deleteBlock);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    try {\n      writer.appendBlock(commandBlock);\n      \r\n      throw new Exception(\"simulating failure\");\n    } catch (Exception e) {\n      \r\n    }\n    \r\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    \r\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(basePath)\n        .withLogFilePaths(allLogFiles)\n        .withReaderSchema(schema)\n        .withLatestInstantTime(\"100\")\n        .withMaxMemorySizeInBytes(10240L)\n        .withReadBlocksLazily(readBlocksLazily)\n        .withReverseReader(false)\n        .withBufferSize(bufferSize)\n        .withSpillableMapBasePath(BASE_OUTPUT_PATH)\n        .build();\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would have scanned 0 records because of rollback\");\n\n    final List<String> readKeys = new ArrayList<>();\n    scanner.forEach(s -> readKeys.add(s.getKey().getRecordKey()));\n    assertEquals(0, readKeys.size(), \"Stream collect should return all 0 records\");\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":924,"status":"M"},{"authorDate":"2021-02-20 12:12:22","commitOrder":5,"curCode":"  public void testAvroLogRecordReaderWithInsertDeleteAndRollback(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer.appendBlock(dataBlock);\n\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer.appendBlock(deleteBlock);\n\n    FileCreateUtils.createDeltaCommit(basePath, \"100\", fs);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    writer.appendBlock(commandBlock);\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(basePath)\n        .withLogFilePaths(allLogFiles)\n        .withReaderSchema(schema)\n        .withLatestInstantTime(\"100\")\n        .withMaxMemorySizeInBytes(10240L)\n        .withReadBlocksLazily(readBlocksLazily)\n        .withReverseReader(false)\n        .withBufferSize(bufferSize)\n        .withSpillableMapBasePath(BASE_OUTPUT_PATH)\n        .build();\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would read 0 records\");\n    FileCreateUtils.deleteDeltaCommit(basePath, \"100\", fs);\n  }\n","date":"2021-02-20 12:12:22","endLine":1061,"groupId":"3957","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testAvroLogRecordReaderWithInsertDeleteAndRollback","params":"(booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/69/fc60f6a3e429215cf49530f53411f54fd686cd.src","preCode":"  public void testAvroLogRecordReaderWithInsertDeleteAndRollback(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer.appendBlock(dataBlock);\n\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer.appendBlock(deleteBlock);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    writer.appendBlock(commandBlock);\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(basePath)\n        .withLogFilePaths(allLogFiles)\n        .withReaderSchema(schema)\n        .withLatestInstantTime(\"100\")\n        .withMaxMemorySizeInBytes(10240L)\n        .withReadBlocksLazily(readBlocksLazily)\n        .withReverseReader(false)\n        .withBufferSize(bufferSize)\n        .withSpillableMapBasePath(BASE_OUTPUT_PATH)\n        .build();\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would read 0 records\");\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":1005,"status":"M"}],"commitId":"ffcfb58bacab377bc72d20041baa54a3fd8fc812","commitMessage":"@@@[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)\n\n1. Refactor rollback and move cleaning failed commits logic into cleaner\n2. Introduce hoodie heartbeat to ascertain failed commits\n3. Fix test cases","date":"2021-02-20 12:12:22","modifiedFileCount":"56","status":"M","submitter":"n3nash"},{"authorTime":"2021-07-28 13:31:03","codes":[{"authorDate":"2021-07-28 13:31:03","commitOrder":6,"curCode":"  public void testAvroLogRecordReaderWithFailedRollbacks(ExternalSpillableMap.DiskMapType diskMapType,\n                                                         boolean isCompressionEnabled,\n                                                         boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer.appendBlock(dataBlock);\n\n    \r\n    List<IndexedRecord> records2 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    dataBlock = getDataBlock(records2, header);\n    writer.appendBlock(dataBlock);\n\n    \r\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer.appendBlock(deleteBlock);\n\n    FileCreateUtils.createDeltaCommit(basePath, \"100\", fs);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    try {\n      writer.appendBlock(commandBlock);\n      \r\n      throw new Exception(\"simulating failure\");\n    } catch (Exception e) {\n      \r\n    }\n    \r\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    \r\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(basePath)\n        .withLogFilePaths(allLogFiles)\n        .withReaderSchema(schema)\n        .withLatestInstantTime(\"100\")\n        .withMaxMemorySizeInBytes(10240L)\n        .withReadBlocksLazily(readBlocksLazily)\n        .withReverseReader(false)\n        .withBufferSize(bufferSize)\n        .withSpillableMapBasePath(BASE_OUTPUT_PATH)\n        .withDiskMapType(diskMapType)\n        .withBitCaskDiskMapCompressionEnabled(isCompressionEnabled)\n        .build();\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would have scanned 0 records because of rollback\");\n\n    final List<String> readKeys = new ArrayList<>();\n    scanner.forEach(s -> readKeys.add(s.getKey().getRecordKey()));\n    assertEquals(0, readKeys.size(), \"Stream collect should return all 0 records\");\n    FileCreateUtils.deleteDeltaCommit(basePath, \"100\", fs);\n  }\n","date":"2021-07-28 13:31:03","endLine":1033,"groupId":"10945","id":11,"instanceNumber":1,"isCurCommit":1,"methodName":"testAvroLogRecordReaderWithFailedRollbacks","params":"(ExternalSpillableMap.DiskMapTypediskMapType@booleanisCompressionEnabled@booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/33/68c17c7bd1002a2c9c7c24c28a6747d5440d61.src","preCode":"  public void testAvroLogRecordReaderWithFailedRollbacks(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer.appendBlock(dataBlock);\n\n    \r\n    List<IndexedRecord> records2 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    dataBlock = getDataBlock(records2, header);\n    writer.appendBlock(dataBlock);\n\n    \r\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer.appendBlock(deleteBlock);\n\n    FileCreateUtils.createDeltaCommit(basePath, \"100\", fs);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    try {\n      writer.appendBlock(commandBlock);\n      \r\n      throw new Exception(\"simulating failure\");\n    } catch (Exception e) {\n      \r\n    }\n    \r\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    \r\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(basePath)\n        .withLogFilePaths(allLogFiles)\n        .withReaderSchema(schema)\n        .withLatestInstantTime(\"100\")\n        .withMaxMemorySizeInBytes(10240L)\n        .withReadBlocksLazily(readBlocksLazily)\n        .withReverseReader(false)\n        .withBufferSize(bufferSize)\n        .withSpillableMapBasePath(BASE_OUTPUT_PATH)\n        .build();\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would have scanned 0 records because of rollback\");\n\n    final List<String> readKeys = new ArrayList<>();\n    scanner.forEach(s -> readKeys.add(s.getKey().getRecordKey()));\n    assertEquals(0, readKeys.size(), \"Stream collect should return all 0 records\");\n    FileCreateUtils.deleteDeltaCommit(basePath, \"100\", fs);\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":952,"status":"M"},{"authorDate":"2021-07-28 13:31:03","commitOrder":6,"curCode":"  public void testAvroLogRecordReaderWithInsertDeleteAndRollback(ExternalSpillableMap.DiskMapType diskMapType,\n                                                                 boolean isCompressionEnabled,\n                                                                 boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer.appendBlock(dataBlock);\n\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer.appendBlock(deleteBlock);\n\n    FileCreateUtils.createDeltaCommit(basePath, \"100\", fs);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    writer.appendBlock(commandBlock);\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(basePath)\n        .withLogFilePaths(allLogFiles)\n        .withReaderSchema(schema)\n        .withLatestInstantTime(\"100\")\n        .withMaxMemorySizeInBytes(10240L)\n        .withReadBlocksLazily(readBlocksLazily)\n        .withReverseReader(false)\n        .withBufferSize(bufferSize)\n        .withSpillableMapBasePath(BASE_OUTPUT_PATH)\n        .withDiskMapType(diskMapType)\n        .withBitCaskDiskMapCompressionEnabled(isCompressionEnabled)\n        .build();\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would read 0 records\");\n    FileCreateUtils.deleteDeltaCommit(basePath, \"100\", fs);\n  }\n","date":"2021-07-28 13:31:03","endLine":1097,"groupId":"10945","id":12,"instanceNumber":2,"isCurCommit":1,"methodName":"testAvroLogRecordReaderWithInsertDeleteAndRollback","params":"(ExternalSpillableMap.DiskMapTypediskMapType@booleanisCompressionEnabled@booleanreadBlocksLazily)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/33/68c17c7bd1002a2c9c7c24c28a6747d5440d61.src","preCode":"  public void testAvroLogRecordReaderWithInsertDeleteAndRollback(boolean readBlocksLazily)\n      throws IOException, URISyntaxException, InterruptedException {\n\n    \r\n    Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());\n    \r\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n\n    \r\n    List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer.appendBlock(dataBlock);\n\n    \r\n    List<HoodieKey> deletedKeys = copyOfRecords1.stream()\n        .map(s -> (new HoodieKey(((GenericRecord) s).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString(),\n            ((GenericRecord) s).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString())))\n        .collect(Collectors.toList()).subList(0, 50);\n    HoodieDeleteBlock deleteBlock = new HoodieDeleteBlock(deletedKeys.toArray(new HoodieKey[50]), header);\n    writer.appendBlock(deleteBlock);\n\n    FileCreateUtils.createDeltaCommit(basePath, \"100\", fs);\n\n    \r\n    header.put(HoodieLogBlock.HeaderMetadataType.COMMAND_BLOCK_TYPE,\n        String.valueOf(HoodieCommandBlock.HoodieCommandBlockTypeEnum.ROLLBACK_PREVIOUS_BLOCK.ordinal()));\n    HoodieCommandBlock commandBlock = new HoodieCommandBlock(header);\n    writer.appendBlock(commandBlock);\n    writer.appendBlock(commandBlock);\n\n    List<String> allLogFiles =\n        FSUtils.getAllLogFiles(fs, partitionPath, \"test-fileid1\", HoodieLogFile.DELTA_EXTENSION, \"100\")\n            .map(s -> s.getPath().toString()).collect(Collectors.toList());\n\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(basePath)\n        .withLogFilePaths(allLogFiles)\n        .withReaderSchema(schema)\n        .withLatestInstantTime(\"100\")\n        .withMaxMemorySizeInBytes(10240L)\n        .withReadBlocksLazily(readBlocksLazily)\n        .withReverseReader(false)\n        .withBufferSize(bufferSize)\n        .withSpillableMapBasePath(BASE_OUTPUT_PATH)\n        .build();\n    assertEquals(0, scanner.getTotalLogRecords(), \"We would read 0 records\");\n    FileCreateUtils.deleteDeltaCommit(basePath, \"100\", fs);\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":1037,"status":"M"}],"commitId":"8fef50e237b2342ea3366be32950a2b87a9608c4","commitMessage":"@@@[HUDI-2044] Integrate consumers with rocksDB and compression within External Spillable Map (#3318)\n\n","date":"2021-07-28 13:31:03","modifiedFileCount":"26","status":"M","submitter":"rmahindra123"}]
