[{"authorTime":"2020-10-02 05:25:29","codes":[{"authorDate":"2020-10-02 05:25:29","commitOrder":1,"curCode":"  protected Pair<HashMap<String, WorkloadStat>, WorkloadStat> buildProfile(JavaRDD<HoodieRecord> inputRecordsRDD) {\n    HashMap<String, WorkloadStat> partitionPathStatMap = new HashMap<>();\n    WorkloadStat globalStat = new WorkloadStat();\n\n    \r\n    \r\n    Map<Tuple2<String, Option<HoodieRecordLocation>>, Long> partitionLocationCounts = inputRecordsRDD\n        .mapToPair(record -> new Tuple2<>(\n            new Tuple2<>(record.getPartitionPath(), Option.ofNullable(record.getCurrentLocation())), record))\n        .countByKey();\n\n    \r\n    for (Map.Entry<Tuple2<String, Option<HoodieRecordLocation>>, Long> e : partitionLocationCounts.entrySet()) {\n      String partitionPath = e.getKey()._1();\n      Long count = e.getValue();\n      Option<HoodieRecordLocation> locOption = e.getKey()._2();\n\n      if (!partitionPathStatMap.containsKey(partitionPath)) {\n        partitionPathStatMap.put(partitionPath, new WorkloadStat());\n      }\n\n      if (locOption.isPresent()) {\n        \r\n        partitionPathStatMap.get(partitionPath).addUpdates(locOption.get(), count);\n        globalStat.addUpdates(locOption.get(), count);\n      } else {\n        \r\n        partitionPathStatMap.get(partitionPath).addInserts(count);\n        globalStat.addInserts(count);\n      }\n    }\n    return Pair.of(partitionPathStatMap, globalStat);\n  }\n","date":"2020-10-02 05:25:29","endLine":410,"groupId":"2070","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"buildProfile","params":"(JavaRDD<HoodieRecord>inputRecordsRDD)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/b1/0781e3b8ec9dac667537fcea8e44ee8c577868.src","preCode":"  protected Pair<HashMap<String, WorkloadStat>, WorkloadStat> buildProfile(JavaRDD<HoodieRecord> inputRecordsRDD) {\n    HashMap<String, WorkloadStat> partitionPathStatMap = new HashMap<>();\n    WorkloadStat globalStat = new WorkloadStat();\n\n    \r\n    \r\n    Map<Tuple2<String, Option<HoodieRecordLocation>>, Long> partitionLocationCounts = inputRecordsRDD\n        .mapToPair(record -> new Tuple2<>(\n            new Tuple2<>(record.getPartitionPath(), Option.ofNullable(record.getCurrentLocation())), record))\n        .countByKey();\n\n    \r\n    for (Map.Entry<Tuple2<String, Option<HoodieRecordLocation>>, Long> e : partitionLocationCounts.entrySet()) {\n      String partitionPath = e.getKey()._1();\n      Long count = e.getValue();\n      Option<HoodieRecordLocation> locOption = e.getKey()._2();\n\n      if (!partitionPathStatMap.containsKey(partitionPath)) {\n        partitionPathStatMap.put(partitionPath, new WorkloadStat());\n      }\n\n      if (locOption.isPresent()) {\n        \r\n        partitionPathStatMap.get(partitionPath).addUpdates(locOption.get(), count);\n        globalStat.addUpdates(locOption.get(), count);\n      } else {\n        \r\n        partitionPathStatMap.get(partitionPath).addInserts(count);\n        globalStat.addInserts(count);\n      }\n    }\n    return Pair.of(partitionPathStatMap, globalStat);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestHarness.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":378,"status":"B"},{"authorDate":"2020-10-02 05:25:29","commitOrder":1,"curCode":"  private Pair<HashMap<String, WorkloadStat>, WorkloadStat> buildProfile(JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n    HashMap<String, WorkloadStat> partitionPathStatMap = new HashMap<>();\n    WorkloadStat globalStat = new WorkloadStat();\n\n    \r\n    \r\n    Map<Tuple2<String, Option<HoodieRecordLocation>>, Long> partitionLocationCounts = inputRecordsRDD\n        .mapToPair(record -> new Tuple2<>(\n            new Tuple2<>(record.getPartitionPath(), Option.ofNullable(record.getCurrentLocation())), record))\n        .countByKey();\n\n    \r\n    for (Map.Entry<Tuple2<String, Option<HoodieRecordLocation>>, Long> e : partitionLocationCounts.entrySet()) {\n      String partitionPath = e.getKey()._1();\n      Long count = e.getValue();\n      Option<HoodieRecordLocation> locOption = e.getKey()._2();\n\n      if (!partitionPathStatMap.containsKey(partitionPath)) {\n        partitionPathStatMap.put(partitionPath, new WorkloadStat());\n      }\n\n      if (locOption.isPresent()) {\n        \r\n        partitionPathStatMap.get(partitionPath).addUpdates(locOption.get(), count);\n        globalStat.addUpdates(locOption.get(), count);\n      } else {\n        \r\n        partitionPathStatMap.get(partitionPath).addInserts(count);\n        globalStat.addInserts(count);\n      }\n    }\n    return Pair.of(partitionPathStatMap, globalStat);\n  }\n","date":"2020-10-02 05:25:29","endLine":156,"groupId":"2070","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"buildProfile","params":"(JavaRDD<HoodieRecord<T>>inputRecordsRDD)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/36/cca8cd1403c5fe6ced6d0b42aa6684081520d7.src","preCode":"  private Pair<HashMap<String, WorkloadStat>, WorkloadStat> buildProfile(JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n    HashMap<String, WorkloadStat> partitionPathStatMap = new HashMap<>();\n    WorkloadStat globalStat = new WorkloadStat();\n\n    \r\n    \r\n    Map<Tuple2<String, Option<HoodieRecordLocation>>, Long> partitionLocationCounts = inputRecordsRDD\n        .mapToPair(record -> new Tuple2<>(\n            new Tuple2<>(record.getPartitionPath(), Option.ofNullable(record.getCurrentLocation())), record))\n        .countByKey();\n\n    \r\n    for (Map.Entry<Tuple2<String, Option<HoodieRecordLocation>>, Long> e : partitionLocationCounts.entrySet()) {\n      String partitionPath = e.getKey()._1();\n      Long count = e.getValue();\n      Option<HoodieRecordLocation> locOption = e.getKey()._2();\n\n      if (!partitionPathStatMap.containsKey(partitionPath)) {\n        partitionPathStatMap.put(partitionPath, new WorkloadStat());\n      }\n\n      if (locOption.isPresent()) {\n        \r\n        partitionPathStatMap.get(partitionPath).addUpdates(locOption.get(), count);\n        globalStat.addUpdates(locOption.get(), count);\n      } else {\n        \r\n        partitionPathStatMap.get(partitionPath).addInserts(count);\n        globalStat.addInserts(count);\n      }\n    }\n    return Pair.of(partitionPathStatMap, globalStat);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":124,"status":"B"}],"commitId":"1f7add92916c37b05be270d9c75a9042134ec506","commitMessage":"@@@[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)\n\n- This change breaks `hudi-client` into `hudi-client-common` and `hudi-spark-client` modules \n- Simple usages of Spark using jsc.parallelize() has been redone using EngineContext#map.  EngineContext#flatMap etc\n- Code changes in the PR.  break classes into `BaseXYZ` parent classes with no spark dependencies living in `hudi-client-common`\n- Classes on `hudi-spark-client` are named `SparkXYZ` extending the parent classes with all the Spark dependencies\n- To simplify/cleanup.  HoodieIndex#fetchRecordLocation has been removed and its usages in tests replaced with alternatives\n\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2020-10-02 05:25:29","modifiedFileCount":"31","status":"B","submitter":"Mathieu"},{"authorTime":"2020-10-02 05:25:29","codes":[{"authorDate":"2021-09-04 04:17:17","commitOrder":2,"curCode":"  public static Pair<HashMap<String, WorkloadStat>, WorkloadStat> buildProfile(JavaRDD<HoodieRecord> inputRecordsRDD) {\n    HashMap<String, WorkloadStat> partitionPathStatMap = new HashMap<>();\n    WorkloadStat globalStat = new WorkloadStat();\n\n    \r\n    \r\n    Map<Tuple2<String, Option<HoodieRecordLocation>>, Long> partitionLocationCounts = inputRecordsRDD\n        .mapToPair(record -> new Tuple2<>(\n            new Tuple2<>(record.getPartitionPath(), Option.ofNullable(record.getCurrentLocation())), record))\n        .countByKey();\n\n    \r\n    for (Map.Entry<Tuple2<String, Option<HoodieRecordLocation>>, Long> e : partitionLocationCounts.entrySet()) {\n      String partitionPath = e.getKey()._1();\n      Long count = e.getValue();\n      Option<HoodieRecordLocation> locOption = e.getKey()._2();\n\n      if (!partitionPathStatMap.containsKey(partitionPath)) {\n        partitionPathStatMap.put(partitionPath, new WorkloadStat());\n      }\n\n      if (locOption.isPresent()) {\n        \r\n        partitionPathStatMap.get(partitionPath).addUpdates(locOption.get(), count);\n        globalStat.addUpdates(locOption.get(), count);\n      } else {\n        \r\n        partitionPathStatMap.get(partitionPath).addInserts(count);\n        globalStat.addInserts(count);\n      }\n    }\n    return Pair.of(partitionPathStatMap, globalStat);\n  }\n","date":"2021-09-04 04:17:17","endLine":420,"groupId":"10623","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"buildProfile","params":"(JavaRDD<HoodieRecord>inputRecordsRDD)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/1e/52e449453df8dccaa3651436d880f8436df68a.src","preCode":"  protected Pair<HashMap<String, WorkloadStat>, WorkloadStat> buildProfile(JavaRDD<HoodieRecord> inputRecordsRDD) {\n    HashMap<String, WorkloadStat> partitionPathStatMap = new HashMap<>();\n    WorkloadStat globalStat = new WorkloadStat();\n\n    \r\n    \r\n    Map<Tuple2<String, Option<HoodieRecordLocation>>, Long> partitionLocationCounts = inputRecordsRDD\n        .mapToPair(record -> new Tuple2<>(\n            new Tuple2<>(record.getPartitionPath(), Option.ofNullable(record.getCurrentLocation())), record))\n        .countByKey();\n\n    \r\n    for (Map.Entry<Tuple2<String, Option<HoodieRecordLocation>>, Long> e : partitionLocationCounts.entrySet()) {\n      String partitionPath = e.getKey()._1();\n      Long count = e.getValue();\n      Option<HoodieRecordLocation> locOption = e.getKey()._2();\n\n      if (!partitionPathStatMap.containsKey(partitionPath)) {\n        partitionPathStatMap.put(partitionPath, new WorkloadStat());\n      }\n\n      if (locOption.isPresent()) {\n        \r\n        partitionPathStatMap.get(partitionPath).addUpdates(locOption.get(), count);\n        globalStat.addUpdates(locOption.get(), count);\n      } else {\n        \r\n        partitionPathStatMap.get(partitionPath).addInserts(count);\n        globalStat.addInserts(count);\n      }\n    }\n    return Pair.of(partitionPathStatMap, globalStat);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestHarness.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":388,"status":"M"},{"authorDate":"2020-10-02 05:25:29","commitOrder":2,"curCode":"  private Pair<HashMap<String, WorkloadStat>, WorkloadStat> buildProfile(JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n    HashMap<String, WorkloadStat> partitionPathStatMap = new HashMap<>();\n    WorkloadStat globalStat = new WorkloadStat();\n\n    \r\n    \r\n    Map<Tuple2<String, Option<HoodieRecordLocation>>, Long> partitionLocationCounts = inputRecordsRDD\n        .mapToPair(record -> new Tuple2<>(\n            new Tuple2<>(record.getPartitionPath(), Option.ofNullable(record.getCurrentLocation())), record))\n        .countByKey();\n\n    \r\n    for (Map.Entry<Tuple2<String, Option<HoodieRecordLocation>>, Long> e : partitionLocationCounts.entrySet()) {\n      String partitionPath = e.getKey()._1();\n      Long count = e.getValue();\n      Option<HoodieRecordLocation> locOption = e.getKey()._2();\n\n      if (!partitionPathStatMap.containsKey(partitionPath)) {\n        partitionPathStatMap.put(partitionPath, new WorkloadStat());\n      }\n\n      if (locOption.isPresent()) {\n        \r\n        partitionPathStatMap.get(partitionPath).addUpdates(locOption.get(), count);\n        globalStat.addUpdates(locOption.get(), count);\n      } else {\n        \r\n        partitionPathStatMap.get(partitionPath).addInserts(count);\n        globalStat.addInserts(count);\n      }\n    }\n    return Pair.of(partitionPathStatMap, globalStat);\n  }\n","date":"2020-10-02 05:25:29","endLine":156,"groupId":"10623","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"buildProfile","params":"(JavaRDD<HoodieRecord<T>>inputRecordsRDD)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/36/cca8cd1403c5fe6ced6d0b42aa6684081520d7.src","preCode":"  private Pair<HashMap<String, WorkloadStat>, WorkloadStat> buildProfile(JavaRDD<HoodieRecord<T>> inputRecordsRDD) {\n    HashMap<String, WorkloadStat> partitionPathStatMap = new HashMap<>();\n    WorkloadStat globalStat = new WorkloadStat();\n\n    \r\n    \r\n    Map<Tuple2<String, Option<HoodieRecordLocation>>, Long> partitionLocationCounts = inputRecordsRDD\n        .mapToPair(record -> new Tuple2<>(\n            new Tuple2<>(record.getPartitionPath(), Option.ofNullable(record.getCurrentLocation())), record))\n        .countByKey();\n\n    \r\n    for (Map.Entry<Tuple2<String, Option<HoodieRecordLocation>>, Long> e : partitionLocationCounts.entrySet()) {\n      String partitionPath = e.getKey()._1();\n      Long count = e.getValue();\n      Option<HoodieRecordLocation> locOption = e.getKey()._2();\n\n      if (!partitionPathStatMap.containsKey(partitionPath)) {\n        partitionPathStatMap.put(partitionPath, new WorkloadStat());\n      }\n\n      if (locOption.isPresent()) {\n        \r\n        partitionPathStatMap.get(partitionPath).addUpdates(locOption.get(), count);\n        globalStat.addUpdates(locOption.get(), count);\n      } else {\n        \r\n        partitionPathStatMap.get(partitionPath).addInserts(count);\n        globalStat.addInserts(count);\n      }\n    }\n    return Pair.of(partitionPathStatMap, globalStat);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":124,"status":"N"}],"commitId":"6bd3ca98d69679630ce5c35e7a0dabcc0cfa6c5d","commitMessage":"@@@[HUDI-1989] Fix flakiness in TestHoodieMergeOnReadTable (#3574)\n\n* [HUDI-1989] Refactor clustering tests for MoR table\n\n* refactor assertion helper\n\n* add CheckedFunction\n\n* SparkClientFunctionalTestHarness.java\n\n* put back original test case\n\n* move testcases out from TestHoodieMergeOnReadTable.java\n\n* add TestHoodieSparkMergeOnReadTableRollback.java\n\n* use SparkClientFunctionalTestHarness\n\n* add tag","date":"2021-09-04 04:17:17","modifiedFileCount":"3","status":"M","submitter":"Raymond Xu"}]
