[{"authorTime":"2021-07-09 16:06:32","codes":[{"authorDate":"2021-03-15 16:02:05","commitOrder":2,"curCode":"  public static void checkWrittenData(\n      File baseFile,\n      Map<String, String> expected,\n      int partitions) throws IOException {\n    assert baseFile.isDirectory();\n    FileFilter filter = file -> !file.getName().startsWith(\".\");\n    File[] partitionDirs = baseFile.listFiles(filter);\n    assertNotNull(partitionDirs);\n    assertThat(partitionDirs.length, is(partitions));\n    for (File partitionDir : partitionDirs) {\n      File[] dataFiles = partitionDir.listFiles(filter);\n      assertNotNull(dataFiles);\n      File latestDataFile = Arrays.stream(dataFiles)\n          .max(Comparator.comparing(f -> FSUtils.getCommitTime(f.getName())))\n          .orElse(dataFiles[0]);\n      ParquetReader<GenericRecord> reader = AvroParquetReader\n          .<GenericRecord>builder(new Path(latestDataFile.getAbsolutePath())).build();\n      List<String> readBuffer = new ArrayList<>();\n      GenericRecord nextRecord = reader.read();\n      while (nextRecord != null) {\n        readBuffer.add(filterOutVariables(nextRecord));\n        nextRecord = reader.read();\n      }\n      readBuffer.sort(Comparator.naturalOrder());\n      assertThat(readBuffer.toString(), is(expected.get(partitionDir.getName())));\n    }\n  }\n","date":"2021-03-15 16:02:05","endLine":328,"groupId":"1882","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"checkWrittenData","params":"(FilebaseFile@Map<String@String>expected@intpartitions)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ab/7f544bf447d9abf7aa14a013678459d9b12533.src","preCode":"  public static void checkWrittenData(\n      File baseFile,\n      Map<String, String> expected,\n      int partitions) throws IOException {\n    assert baseFile.isDirectory();\n    FileFilter filter = file -> !file.getName().startsWith(\".\");\n    File[] partitionDirs = baseFile.listFiles(filter);\n    assertNotNull(partitionDirs);\n    assertThat(partitionDirs.length, is(partitions));\n    for (File partitionDir : partitionDirs) {\n      File[] dataFiles = partitionDir.listFiles(filter);\n      assertNotNull(dataFiles);\n      File latestDataFile = Arrays.stream(dataFiles)\n          .max(Comparator.comparing(f -> FSUtils.getCommitTime(f.getName())))\n          .orElse(dataFiles[0]);\n      ParquetReader<GenericRecord> reader = AvroParquetReader\n          .<GenericRecord>builder(new Path(latestDataFile.getAbsolutePath())).build();\n      List<String> readBuffer = new ArrayList<>();\n      GenericRecord nextRecord = reader.read();\n      while (nextRecord != null) {\n        readBuffer.add(filterOutVariables(nextRecord));\n        nextRecord = reader.read();\n      }\n      readBuffer.sort(Comparator.naturalOrder());\n      assertThat(readBuffer.toString(), is(expected.get(partitionDir.getName())));\n    }\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/utils/TestData.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":302,"status":"NB"},{"authorDate":"2021-07-09 16:06:32","commitOrder":2,"curCode":"  public static void checkWrittenAllData(\n      File baseFile,\n      Map<String, List<String>> expected,\n      int partitions) throws IOException {\n    assert baseFile.isDirectory();\n    FileFilter filter = file -> !file.getName().startsWith(\".\");\n    File[] partitionDirs = baseFile.listFiles(filter);\n\n    assertNotNull(partitionDirs);\n    assertThat(partitionDirs.length, is(partitions));\n\n    for (File partitionDir : partitionDirs) {\n      File[] dataFiles = partitionDir.listFiles(filter);\n      assertNotNull(dataFiles);\n\n      List<String> readBuffer = new ArrayList<>();\n      for (File dataFile : dataFiles) {\n        ParquetReader<GenericRecord> reader = AvroParquetReader\n            .<GenericRecord>builder(new Path(dataFile.getAbsolutePath())).build();\n        GenericRecord nextRecord = reader.read();\n        while (nextRecord != null) {\n          readBuffer.add(filterOutVariables(nextRecord));\n          nextRecord = reader.read();\n        }\n        readBuffer.sort(Comparator.naturalOrder());\n      }\n\n      assertThat(readBuffer, is(expected.get(partitionDir.getName())));\n    }\n  }\n","date":"2021-07-09 16:06:32","endLine":419,"groupId":"5885","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"checkWrittenAllData","params":"(FilebaseFile@Map<String@List<String>>expected@intpartitions)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/5a/00da8cfe805d87450a1fb19d6644afe6efdd88.src","preCode":"  public static void checkWrittenAllData(\n      File baseFile,\n      Map<String, List<String>> expected,\n      int partitions) throws IOException {\n    assert baseFile.isDirectory();\n    FileFilter filter = file -> !file.getName().startsWith(\".\");\n    File[] partitionDirs = baseFile.listFiles(filter);\n\n    assertNotNull(partitionDirs);\n    assertThat(partitionDirs.length, is(partitions));\n\n    for (File partitionDir : partitionDirs) {\n      File[] dataFiles = partitionDir.listFiles(filter);\n      assertNotNull(dataFiles);\n\n      List<String> readBuffer = new ArrayList<>();\n      for (File dataFile : dataFiles) {\n        ParquetReader<GenericRecord> reader = AvroParquetReader\n            .<GenericRecord>builder(new Path(dataFile.getAbsolutePath())).build();\n        GenericRecord nextRecord = reader.read();\n        while (nextRecord != null) {\n          readBuffer.add(filterOutVariables(nextRecord));\n          nextRecord = reader.read();\n        }\n        readBuffer.sort(Comparator.naturalOrder());\n      }\n\n      assertThat(readBuffer, is(expected.get(partitionDir.getName())));\n    }\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/utils/TestData.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":390,"status":"B"}],"commitId":"371526789d663dee85041eb31c27c52c81ef87ef","commitMessage":"@@@[HUDI-2087] Support Append only in Flink stream (#3174)\n\nCo-authored-by: ??? <yuzhaojing@bilibili.com>","date":"2021-07-09 16:06:32","modifiedFileCount":"14","status":"M","submitter":"yuzhaojing"},{"authorTime":"2021-08-04 17:53:20","codes":[{"authorDate":"2021-03-15 16:02:05","commitOrder":3,"curCode":"  public static void checkWrittenData(\n      File baseFile,\n      Map<String, String> expected,\n      int partitions) throws IOException {\n    assert baseFile.isDirectory();\n    FileFilter filter = file -> !file.getName().startsWith(\".\");\n    File[] partitionDirs = baseFile.listFiles(filter);\n    assertNotNull(partitionDirs);\n    assertThat(partitionDirs.length, is(partitions));\n    for (File partitionDir : partitionDirs) {\n      File[] dataFiles = partitionDir.listFiles(filter);\n      assertNotNull(dataFiles);\n      File latestDataFile = Arrays.stream(dataFiles)\n          .max(Comparator.comparing(f -> FSUtils.getCommitTime(f.getName())))\n          .orElse(dataFiles[0]);\n      ParquetReader<GenericRecord> reader = AvroParquetReader\n          .<GenericRecord>builder(new Path(latestDataFile.getAbsolutePath())).build();\n      List<String> readBuffer = new ArrayList<>();\n      GenericRecord nextRecord = reader.read();\n      while (nextRecord != null) {\n        readBuffer.add(filterOutVariables(nextRecord));\n        nextRecord = reader.read();\n      }\n      readBuffer.sort(Comparator.naturalOrder());\n      assertThat(readBuffer.toString(), is(expected.get(partitionDir.getName())));\n    }\n  }\n","date":"2021-03-15 16:02:05","endLine":328,"groupId":"10438","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"checkWrittenData","params":"(FilebaseFile@Map<String@String>expected@intpartitions)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ab/7f544bf447d9abf7aa14a013678459d9b12533.src","preCode":"  public static void checkWrittenData(\n      File baseFile,\n      Map<String, String> expected,\n      int partitions) throws IOException {\n    assert baseFile.isDirectory();\n    FileFilter filter = file -> !file.getName().startsWith(\".\");\n    File[] partitionDirs = baseFile.listFiles(filter);\n    assertNotNull(partitionDirs);\n    assertThat(partitionDirs.length, is(partitions));\n    for (File partitionDir : partitionDirs) {\n      File[] dataFiles = partitionDir.listFiles(filter);\n      assertNotNull(dataFiles);\n      File latestDataFile = Arrays.stream(dataFiles)\n          .max(Comparator.comparing(f -> FSUtils.getCommitTime(f.getName())))\n          .orElse(dataFiles[0]);\n      ParquetReader<GenericRecord> reader = AvroParquetReader\n          .<GenericRecord>builder(new Path(latestDataFile.getAbsolutePath())).build();\n      List<String> readBuffer = new ArrayList<>();\n      GenericRecord nextRecord = reader.read();\n      while (nextRecord != null) {\n        readBuffer.add(filterOutVariables(nextRecord));\n        nextRecord = reader.read();\n      }\n      readBuffer.sort(Comparator.naturalOrder());\n      assertThat(readBuffer.toString(), is(expected.get(partitionDir.getName())));\n    }\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/utils/TestData.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":302,"status":"N"},{"authorDate":"2021-08-04 17:53:20","commitOrder":3,"curCode":"  public static void checkWrittenAllData(\n      File baseFile,\n      Map<String, String> expected,\n      int partitions) throws IOException {\n    assert baseFile.isDirectory();\n    FileFilter filter = file -> !file.getName().startsWith(\".\");\n    File[] partitionDirs = baseFile.listFiles(filter);\n\n    assertNotNull(partitionDirs);\n    assertThat(partitionDirs.length, is(partitions));\n\n    for (File partitionDir : partitionDirs) {\n      File[] dataFiles = partitionDir.listFiles(filter);\n      assertNotNull(dataFiles);\n\n      List<String> readBuffer = new ArrayList<>();\n      for (File dataFile : dataFiles) {\n        ParquetReader<GenericRecord> reader = AvroParquetReader\n            .<GenericRecord>builder(new Path(dataFile.getAbsolutePath())).build();\n        GenericRecord nextRecord = reader.read();\n        while (nextRecord != null) {\n          readBuffer.add(filterOutVariables(nextRecord));\n          nextRecord = reader.read();\n        }\n      }\n\n      readBuffer.sort(Comparator.naturalOrder());\n      assertThat(readBuffer.toString(), is(expected.get(partitionDir.getName())));\n    }\n  }\n","date":"2021-08-04 17:53:20","endLine":437,"groupId":"10438","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"checkWrittenAllData","params":"(FilebaseFile@Map<String@String>expected@intpartitions)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/f5/ac9c5290a4c724cc5aad89001f132d222b9ae1.src","preCode":"  public static void checkWrittenAllData(\n      File baseFile,\n      Map<String, List<String>> expected,\n      int partitions) throws IOException {\n    assert baseFile.isDirectory();\n    FileFilter filter = file -> !file.getName().startsWith(\".\");\n    File[] partitionDirs = baseFile.listFiles(filter);\n\n    assertNotNull(partitionDirs);\n    assertThat(partitionDirs.length, is(partitions));\n\n    for (File partitionDir : partitionDirs) {\n      File[] dataFiles = partitionDir.listFiles(filter);\n      assertNotNull(dataFiles);\n\n      List<String> readBuffer = new ArrayList<>();\n      for (File dataFile : dataFiles) {\n        ParquetReader<GenericRecord> reader = AvroParquetReader\n            .<GenericRecord>builder(new Path(dataFile.getAbsolutePath())).build();\n        GenericRecord nextRecord = reader.read();\n        while (nextRecord != null) {\n          readBuffer.add(filterOutVariables(nextRecord));\n          nextRecord = reader.read();\n        }\n        readBuffer.sort(Comparator.naturalOrder());\n      }\n\n      assertThat(readBuffer, is(expected.get(partitionDir.getName())));\n    }\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/utils/TestData.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":408,"status":"M"}],"commitId":"b8b9d6db83657cfdf35832eb26eb7e46ca318048","commitMessage":"@@@[HUDI-2087] Support Append only in Flink stream (#3390)\n\nCo-authored-by: ??? <yuzhaojing@bilibili.com>","date":"2021-08-04 17:53:20","modifiedFileCount":"13","status":"M","submitter":"yuzhaojing"}]
