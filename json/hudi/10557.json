[{"authorTime":"2020-10-02 05:25:29","codes":[{"authorDate":"2020-10-02 05:25:29","commitOrder":1,"curCode":"  protected void twoUpsertCommitDataWithTwoPartitions(List<FileSlice> firstPartitionCommit2FileSlices,\n                                                      List<FileSlice> secondPartitionCommit2FileSlices,\n                                                      HoodieWriteConfig cfg,\n                                                      boolean commitSecondUpsert) throws IOException {\n    \r\n    dataGen = new HoodieTestDataGenerator(new String[]{DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH});\n    \r\n    HoodieTestDataGenerator.writePartitionMetadata(fs, new String[]{DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH}, basePath);\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n    \r\n\n    String newCommitTime = \"001\";\n    client.startCommitWithTime(newCommitTime);\n    List<HoodieRecord> records = dataGen.generateInsertsContainsAllPartitions(newCommitTime, 2);\n    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n    JavaRDD<WriteStatus> statuses = client.upsert(writeRecords, newCommitTime);\n    Assertions.assertNoWriteErrors(statuses.collect());\n    client.commit(newCommitTime, statuses);\n\n    \r\n\n    newCommitTime = \"002\";\n    client.startCommitWithTime(newCommitTime);\n    records = dataGen.generateUpdates(newCommitTime, records);\n    statuses = client.upsert(jsc.parallelize(records, 1), newCommitTime);\n    Assertions.assertNoWriteErrors(statuses.collect());\n    if (commitSecondUpsert) {\n      client.commit(newCommitTime, statuses);\n    }\n\n\n    \r\n    HoodieTable table = this.getHoodieTable(metaClient, cfg);\n    SyncableFileSystemView fsView = getFileSystemViewWithUnCommittedSlices(table.getMetaClient());\n    List<HoodieFileGroup> firstPartitionCommit2FileGroups = fsView.getAllFileGroups(DEFAULT_FIRST_PARTITION_PATH).collect(Collectors.toList());\n    assertEquals(1, firstPartitionCommit2FileGroups.size());\n    firstPartitionCommit2FileSlices.addAll(firstPartitionCommit2FileGroups.get(0).getAllFileSlices().collect(Collectors.toList()));\n    \r\n    List<HoodieFileGroup> secondPartitionCommit2FileGroups = fsView.getAllFileGroups(DEFAULT_SECOND_PARTITION_PATH).collect(Collectors.toList());\n    assertEquals(1, secondPartitionCommit2FileGroups.size());\n    secondPartitionCommit2FileSlices.addAll(secondPartitionCommit2FileGroups.get(0).getAllFileSlices().collect(Collectors.toList()));\n\n    \r\n    HoodieTableType tableType = this.getTableType();\n    if (tableType.equals(HoodieTableType.COPY_ON_WRITE)) {\n      assertEquals(2, firstPartitionCommit2FileSlices.size());\n      assertEquals(2, secondPartitionCommit2FileSlices.size());\n    } else {\n      assertEquals(1, firstPartitionCommit2FileSlices.size());\n      assertEquals(1, secondPartitionCommit2FileSlices.size());\n    }\n  }\n","date":"2020-10-02 05:25:29","endLine":98,"groupId":"2791","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"twoUpsertCommitDataWithTwoPartitions","params":"(List<FileSlice>firstPartitionCommit2FileSlices@List<FileSlice>secondPartitionCommit2FileSlices@HoodieWriteConfigcfg@booleancommitSecondUpsert)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/eb/0e8711a484a51065983c41560e04247c332c29.src","preCode":"  protected void twoUpsertCommitDataWithTwoPartitions(List<FileSlice> firstPartitionCommit2FileSlices,\n                                                      List<FileSlice> secondPartitionCommit2FileSlices,\n                                                      HoodieWriteConfig cfg,\n                                                      boolean commitSecondUpsert) throws IOException {\n    \r\n    dataGen = new HoodieTestDataGenerator(new String[]{DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH});\n    \r\n    HoodieTestDataGenerator.writePartitionMetadata(fs, new String[]{DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH}, basePath);\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n    \r\n\n    String newCommitTime = \"001\";\n    client.startCommitWithTime(newCommitTime);\n    List<HoodieRecord> records = dataGen.generateInsertsContainsAllPartitions(newCommitTime, 2);\n    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n    JavaRDD<WriteStatus> statuses = client.upsert(writeRecords, newCommitTime);\n    Assertions.assertNoWriteErrors(statuses.collect());\n    client.commit(newCommitTime, statuses);\n\n    \r\n\n    newCommitTime = \"002\";\n    client.startCommitWithTime(newCommitTime);\n    records = dataGen.generateUpdates(newCommitTime, records);\n    statuses = client.upsert(jsc.parallelize(records, 1), newCommitTime);\n    Assertions.assertNoWriteErrors(statuses.collect());\n    if (commitSecondUpsert) {\n      client.commit(newCommitTime, statuses);\n    }\n\n\n    \r\n    HoodieTable table = this.getHoodieTable(metaClient, cfg);\n    SyncableFileSystemView fsView = getFileSystemViewWithUnCommittedSlices(table.getMetaClient());\n    List<HoodieFileGroup> firstPartitionCommit2FileGroups = fsView.getAllFileGroups(DEFAULT_FIRST_PARTITION_PATH).collect(Collectors.toList());\n    assertEquals(1, firstPartitionCommit2FileGroups.size());\n    firstPartitionCommit2FileSlices.addAll(firstPartitionCommit2FileGroups.get(0).getAllFileSlices().collect(Collectors.toList()));\n    \r\n    List<HoodieFileGroup> secondPartitionCommit2FileGroups = fsView.getAllFileGroups(DEFAULT_SECOND_PARTITION_PATH).collect(Collectors.toList());\n    assertEquals(1, secondPartitionCommit2FileGroups.size());\n    secondPartitionCommit2FileSlices.addAll(secondPartitionCommit2FileGroups.get(0).getAllFileSlices().collect(Collectors.toList()));\n\n    \r\n    HoodieTableType tableType = this.getTableType();\n    if (tableType.equals(HoodieTableType.COPY_ON_WRITE)) {\n      assertEquals(2, firstPartitionCommit2FileSlices.size());\n      assertEquals(2, secondPartitionCommit2FileSlices.size());\n    } else {\n      assertEquals(1, firstPartitionCommit2FileSlices.size());\n      assertEquals(1, secondPartitionCommit2FileSlices.size());\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/rollback/HoodieClientRollbackTestBase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":45,"status":"B"},{"authorDate":"2020-10-02 05:25:29","commitOrder":1,"curCode":"  private Pair<List<HoodieRecord>, List<HoodieRecord>> twoUpsertCommitDataWithTwoPartitions(List<FileSlice> firstPartitionCommit2FileSlices,\n      List<FileSlice> secondPartitionCommit2FileSlices,\n      HoodieWriteConfig cfg, SparkRDDWriteClient client,\n      boolean commitSecondUpsert) throws IOException {\n    \r\n    dataGen = new HoodieTestDataGenerator(new String[] {DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH});\n    \r\n    HoodieTestDataGenerator.writePartitionMetadata(dfs, new String[] {DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH}, dfsBasePath);\n    \r\n\n    String newCommitTime = \"001\";\n    client.startCommitWithTime(newCommitTime);\n    List<HoodieRecord> records = dataGen.generateInsertsContainsAllPartitions(newCommitTime, 2);\n    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n    JavaRDD<WriteStatus> statuses = client.upsert(writeRecords, newCommitTime);\n    Assertions.assertNoWriteErrors(statuses.collect());\n    client.commit(newCommitTime, statuses);\n    \r\n\n    newCommitTime = \"002\";\n    client.startCommitWithTime(newCommitTime);\n\n    List<HoodieRecord> records2 = dataGen.generateUpdates(newCommitTime, records);\n    statuses = client.upsert(jsc.parallelize(records2, 1), newCommitTime);\n    Assertions.assertNoWriteErrors(statuses.collect());\n    if (commitSecondUpsert) {\n      client.commit(newCommitTime, statuses);\n    }\n\n    \r\n    HoodieTable table = this.getHoodieTable(metaClient, cfg);\n    SyncableFileSystemView fsView = getFileSystemViewWithUnCommittedSlices(table.getMetaClient());\n    List<HoodieFileGroup> firstPartitionCommit2FileGroups = fsView.getAllFileGroups(DEFAULT_FIRST_PARTITION_PATH).collect(Collectors.toList());\n    assertEquals(1, firstPartitionCommit2FileGroups.size());\n    firstPartitionCommit2FileSlices.addAll(firstPartitionCommit2FileGroups.get(0).getAllFileSlices().collect(Collectors.toList()));\n    \r\n    List<HoodieFileGroup> secondPartitionCommit2FileGroups = fsView.getAllFileGroups(DEFAULT_SECOND_PARTITION_PATH).collect(Collectors.toList());\n    assertEquals(1, secondPartitionCommit2FileGroups.size());\n    secondPartitionCommit2FileSlices.addAll(secondPartitionCommit2FileGroups.get(0).getAllFileSlices().collect(Collectors.toList()));\n\n    \r\n    HoodieTableType tableType = metaClient.getTableType();\n    if (tableType.equals(HoodieTableType.COPY_ON_WRITE)) {\n      assertEquals(2, firstPartitionCommit2FileSlices.size());\n      assertEquals(2, secondPartitionCommit2FileSlices.size());\n    } else {\n      assertEquals(1, firstPartitionCommit2FileSlices.size());\n      assertEquals(1, secondPartitionCommit2FileSlices.size());\n    }\n    return Pair.of(records, records2);\n  }\n","date":"2020-10-02 05:25:29","endLine":389,"groupId":"1663","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"twoUpsertCommitDataWithTwoPartitions","params":"(List<FileSlice>firstPartitionCommit2FileSlices@List<FileSlice>secondPartitionCommit2FileSlices@HoodieWriteConfigcfg@SparkRDDWriteClientclient@booleancommitSecondUpsert)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/b8/e02b905b8cbe1b748ac1a8ee24b6570ae637f2.src","preCode":"  private Pair<List<HoodieRecord>, List<HoodieRecord>> twoUpsertCommitDataWithTwoPartitions(List<FileSlice> firstPartitionCommit2FileSlices,\n      List<FileSlice> secondPartitionCommit2FileSlices,\n      HoodieWriteConfig cfg, SparkRDDWriteClient client,\n      boolean commitSecondUpsert) throws IOException {\n    \r\n    dataGen = new HoodieTestDataGenerator(new String[] {DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH});\n    \r\n    HoodieTestDataGenerator.writePartitionMetadata(dfs, new String[] {DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH}, dfsBasePath);\n    \r\n\n    String newCommitTime = \"001\";\n    client.startCommitWithTime(newCommitTime);\n    List<HoodieRecord> records = dataGen.generateInsertsContainsAllPartitions(newCommitTime, 2);\n    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n    JavaRDD<WriteStatus> statuses = client.upsert(writeRecords, newCommitTime);\n    Assertions.assertNoWriteErrors(statuses.collect());\n    client.commit(newCommitTime, statuses);\n    \r\n\n    newCommitTime = \"002\";\n    client.startCommitWithTime(newCommitTime);\n\n    List<HoodieRecord> records2 = dataGen.generateUpdates(newCommitTime, records);\n    statuses = client.upsert(jsc.parallelize(records2, 1), newCommitTime);\n    Assertions.assertNoWriteErrors(statuses.collect());\n    if (commitSecondUpsert) {\n      client.commit(newCommitTime, statuses);\n    }\n\n    \r\n    HoodieTable table = this.getHoodieTable(metaClient, cfg);\n    SyncableFileSystemView fsView = getFileSystemViewWithUnCommittedSlices(table.getMetaClient());\n    List<HoodieFileGroup> firstPartitionCommit2FileGroups = fsView.getAllFileGroups(DEFAULT_FIRST_PARTITION_PATH).collect(Collectors.toList());\n    assertEquals(1, firstPartitionCommit2FileGroups.size());\n    firstPartitionCommit2FileSlices.addAll(firstPartitionCommit2FileGroups.get(0).getAllFileSlices().collect(Collectors.toList()));\n    \r\n    List<HoodieFileGroup> secondPartitionCommit2FileGroups = fsView.getAllFileGroups(DEFAULT_SECOND_PARTITION_PATH).collect(Collectors.toList());\n    assertEquals(1, secondPartitionCommit2FileGroups.size());\n    secondPartitionCommit2FileSlices.addAll(secondPartitionCommit2FileGroups.get(0).getAllFileSlices().collect(Collectors.toList()));\n\n    \r\n    HoodieTableType tableType = metaClient.getTableType();\n    if (tableType.equals(HoodieTableType.COPY_ON_WRITE)) {\n      assertEquals(2, firstPartitionCommit2FileSlices.size());\n      assertEquals(2, secondPartitionCommit2FileSlices.size());\n    } else {\n      assertEquals(1, firstPartitionCommit2FileSlices.size());\n      assertEquals(1, secondPartitionCommit2FileSlices.size());\n    }\n    return Pair.of(records, records2);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":337,"status":"B"}],"commitId":"1f7add92916c37b05be270d9c75a9042134ec506","commitMessage":"@@@[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)\n\n- This change breaks `hudi-client` into `hudi-client-common` and `hudi-spark-client` modules \n- Simple usages of Spark using jsc.parallelize() has been redone using EngineContext#map.  EngineContext#flatMap etc\n- Code changes in the PR.  break classes into `BaseXYZ` parent classes with no spark dependencies living in `hudi-client-common`\n- Classes on `hudi-spark-client` are named `SparkXYZ` extending the parent classes with all the Spark dependencies\n- To simplify/cleanup.  HoodieIndex#fetchRecordLocation has been removed and its usages in tests replaced with alternatives\n\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2020-10-02 05:25:29","modifiedFileCount":"31","status":"B","submitter":"Mathieu"},{"authorTime":"2021-08-15 08:20:23","codes":[{"authorDate":"2020-10-02 05:25:29","commitOrder":2,"curCode":"  protected void twoUpsertCommitDataWithTwoPartitions(List<FileSlice> firstPartitionCommit2FileSlices,\n                                                      List<FileSlice> secondPartitionCommit2FileSlices,\n                                                      HoodieWriteConfig cfg,\n                                                      boolean commitSecondUpsert) throws IOException {\n    \r\n    dataGen = new HoodieTestDataGenerator(new String[]{DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH});\n    \r\n    HoodieTestDataGenerator.writePartitionMetadata(fs, new String[]{DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH}, basePath);\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n    \r\n\n    String newCommitTime = \"001\";\n    client.startCommitWithTime(newCommitTime);\n    List<HoodieRecord> records = dataGen.generateInsertsContainsAllPartitions(newCommitTime, 2);\n    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n    JavaRDD<WriteStatus> statuses = client.upsert(writeRecords, newCommitTime);\n    Assertions.assertNoWriteErrors(statuses.collect());\n    client.commit(newCommitTime, statuses);\n\n    \r\n\n    newCommitTime = \"002\";\n    client.startCommitWithTime(newCommitTime);\n    records = dataGen.generateUpdates(newCommitTime, records);\n    statuses = client.upsert(jsc.parallelize(records, 1), newCommitTime);\n    Assertions.assertNoWriteErrors(statuses.collect());\n    if (commitSecondUpsert) {\n      client.commit(newCommitTime, statuses);\n    }\n\n\n    \r\n    HoodieTable table = this.getHoodieTable(metaClient, cfg);\n    SyncableFileSystemView fsView = getFileSystemViewWithUnCommittedSlices(table.getMetaClient());\n    List<HoodieFileGroup> firstPartitionCommit2FileGroups = fsView.getAllFileGroups(DEFAULT_FIRST_PARTITION_PATH).collect(Collectors.toList());\n    assertEquals(1, firstPartitionCommit2FileGroups.size());\n    firstPartitionCommit2FileSlices.addAll(firstPartitionCommit2FileGroups.get(0).getAllFileSlices().collect(Collectors.toList()));\n    \r\n    List<HoodieFileGroup> secondPartitionCommit2FileGroups = fsView.getAllFileGroups(DEFAULT_SECOND_PARTITION_PATH).collect(Collectors.toList());\n    assertEquals(1, secondPartitionCommit2FileGroups.size());\n    secondPartitionCommit2FileSlices.addAll(secondPartitionCommit2FileGroups.get(0).getAllFileSlices().collect(Collectors.toList()));\n\n    \r\n    HoodieTableType tableType = this.getTableType();\n    if (tableType.equals(HoodieTableType.COPY_ON_WRITE)) {\n      assertEquals(2, firstPartitionCommit2FileSlices.size());\n      assertEquals(2, secondPartitionCommit2FileSlices.size());\n    } else {\n      assertEquals(1, firstPartitionCommit2FileSlices.size());\n      assertEquals(1, secondPartitionCommit2FileSlices.size());\n    }\n  }\n","date":"2020-10-02 05:25:29","endLine":98,"groupId":"10557","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"twoUpsertCommitDataWithTwoPartitions","params":"(List<FileSlice>firstPartitionCommit2FileSlices@List<FileSlice>secondPartitionCommit2FileSlices@HoodieWriteConfigcfg@booleancommitSecondUpsert)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/eb/0e8711a484a51065983c41560e04247c332c29.src","preCode":"  protected void twoUpsertCommitDataWithTwoPartitions(List<FileSlice> firstPartitionCommit2FileSlices,\n                                                      List<FileSlice> secondPartitionCommit2FileSlices,\n                                                      HoodieWriteConfig cfg,\n                                                      boolean commitSecondUpsert) throws IOException {\n    \r\n    dataGen = new HoodieTestDataGenerator(new String[]{DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH});\n    \r\n    HoodieTestDataGenerator.writePartitionMetadata(fs, new String[]{DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH}, basePath);\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n    \r\n\n    String newCommitTime = \"001\";\n    client.startCommitWithTime(newCommitTime);\n    List<HoodieRecord> records = dataGen.generateInsertsContainsAllPartitions(newCommitTime, 2);\n    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n    JavaRDD<WriteStatus> statuses = client.upsert(writeRecords, newCommitTime);\n    Assertions.assertNoWriteErrors(statuses.collect());\n    client.commit(newCommitTime, statuses);\n\n    \r\n\n    newCommitTime = \"002\";\n    client.startCommitWithTime(newCommitTime);\n    records = dataGen.generateUpdates(newCommitTime, records);\n    statuses = client.upsert(jsc.parallelize(records, 1), newCommitTime);\n    Assertions.assertNoWriteErrors(statuses.collect());\n    if (commitSecondUpsert) {\n      client.commit(newCommitTime, statuses);\n    }\n\n\n    \r\n    HoodieTable table = this.getHoodieTable(metaClient, cfg);\n    SyncableFileSystemView fsView = getFileSystemViewWithUnCommittedSlices(table.getMetaClient());\n    List<HoodieFileGroup> firstPartitionCommit2FileGroups = fsView.getAllFileGroups(DEFAULT_FIRST_PARTITION_PATH).collect(Collectors.toList());\n    assertEquals(1, firstPartitionCommit2FileGroups.size());\n    firstPartitionCommit2FileSlices.addAll(firstPartitionCommit2FileGroups.get(0).getAllFileSlices().collect(Collectors.toList()));\n    \r\n    List<HoodieFileGroup> secondPartitionCommit2FileGroups = fsView.getAllFileGroups(DEFAULT_SECOND_PARTITION_PATH).collect(Collectors.toList());\n    assertEquals(1, secondPartitionCommit2FileGroups.size());\n    secondPartitionCommit2FileSlices.addAll(secondPartitionCommit2FileGroups.get(0).getAllFileSlices().collect(Collectors.toList()));\n\n    \r\n    HoodieTableType tableType = this.getTableType();\n    if (tableType.equals(HoodieTableType.COPY_ON_WRITE)) {\n      assertEquals(2, firstPartitionCommit2FileSlices.size());\n      assertEquals(2, secondPartitionCommit2FileSlices.size());\n    } else {\n      assertEquals(1, firstPartitionCommit2FileSlices.size());\n      assertEquals(1, secondPartitionCommit2FileSlices.size());\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/rollback/HoodieClientRollbackTestBase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":45,"status":"N"},{"authorDate":"2021-08-15 08:20:23","commitOrder":2,"curCode":"  private Pair<List<HoodieRecord>, List<HoodieRecord>> twoUpsertCommitDataWithTwoPartitions(List<FileSlice> firstPartitionCommit2FileSlices,\n      List<FileSlice> secondPartitionCommit2FileSlices,\n      HoodieWriteConfig cfg, SparkRDDWriteClient client,\n      boolean commitSecondUpsert) throws IOException {\n    \r\n    dataGen = new HoodieTestDataGenerator(new String[] {DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH});\n    \r\n    HoodieTestDataGenerator.writePartitionMetadata(metaClient.getFs(), new String[] {DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH}, basePath);\n    \r\n\n    String newCommitTime = \"001\";\n    client.startCommitWithTime(newCommitTime);\n    List<HoodieRecord> records = dataGen.generateInsertsContainsAllPartitions(newCommitTime, 2);\n    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n    JavaRDD<WriteStatus> statuses = client.upsert(writeRecords, newCommitTime);\n    Assertions.assertNoWriteErrors(statuses.collect());\n    client.commit(newCommitTime, statuses);\n    \r\n\n    newCommitTime = \"002\";\n    client.startCommitWithTime(newCommitTime);\n\n    List<HoodieRecord> records2 = dataGen.generateUpdates(newCommitTime, records);\n    statuses = client.upsert(jsc.parallelize(records2, 1), newCommitTime);\n    Assertions.assertNoWriteErrors(statuses.collect());\n    if (commitSecondUpsert) {\n      client.commit(newCommitTime, statuses);\n    }\n\n    \r\n    HoodieTable table = this.getHoodieTable(metaClient, cfg);\n    SyncableFileSystemView fsView = getFileSystemViewWithUnCommittedSlices(table.getMetaClient());\n    List<HoodieFileGroup> firstPartitionCommit2FileGroups = fsView.getAllFileGroups(DEFAULT_FIRST_PARTITION_PATH).collect(Collectors.toList());\n    assertEquals(1, firstPartitionCommit2FileGroups.size());\n    firstPartitionCommit2FileSlices.addAll(firstPartitionCommit2FileGroups.get(0).getAllFileSlices().collect(Collectors.toList()));\n    \r\n    List<HoodieFileGroup> secondPartitionCommit2FileGroups = fsView.getAllFileGroups(DEFAULT_SECOND_PARTITION_PATH).collect(Collectors.toList());\n    assertEquals(1, secondPartitionCommit2FileGroups.size());\n    secondPartitionCommit2FileSlices.addAll(secondPartitionCommit2FileGroups.get(0).getAllFileSlices().collect(Collectors.toList()));\n\n    \r\n    HoodieTableType tableType = metaClient.getTableType();\n    if (tableType.equals(HoodieTableType.COPY_ON_WRITE)) {\n      assertEquals(2, firstPartitionCommit2FileSlices.size());\n      assertEquals(2, secondPartitionCommit2FileSlices.size());\n    } else {\n      assertEquals(1, firstPartitionCommit2FileSlices.size());\n      assertEquals(1, secondPartitionCommit2FileSlices.size());\n    }\n    return Pair.of(records, records2);\n  }\n","date":"2021-08-15 08:20:23","endLine":540,"groupId":"10557","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"twoUpsertCommitDataWithTwoPartitions","params":"(List<FileSlice>firstPartitionCommit2FileSlices@List<FileSlice>secondPartitionCommit2FileSlices@HoodieWriteConfigcfg@SparkRDDWriteClientclient@booleancommitSecondUpsert)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/68/876d79bd887c5e8e266c9eed5f220122d47d65.src","preCode":"  private Pair<List<HoodieRecord>, List<HoodieRecord>> twoUpsertCommitDataWithTwoPartitions(List<FileSlice> firstPartitionCommit2FileSlices,\n      List<FileSlice> secondPartitionCommit2FileSlices,\n      HoodieWriteConfig cfg, SparkRDDWriteClient client,\n      boolean commitSecondUpsert) throws IOException {\n    \r\n    dataGen = new HoodieTestDataGenerator(new String[] {DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH});\n    \r\n    HoodieTestDataGenerator.writePartitionMetadata(dfs, new String[] {DEFAULT_FIRST_PARTITION_PATH, DEFAULT_SECOND_PARTITION_PATH}, dfsBasePath);\n    \r\n\n    String newCommitTime = \"001\";\n    client.startCommitWithTime(newCommitTime);\n    List<HoodieRecord> records = dataGen.generateInsertsContainsAllPartitions(newCommitTime, 2);\n    JavaRDD<HoodieRecord> writeRecords = jsc.parallelize(records, 1);\n    JavaRDD<WriteStatus> statuses = client.upsert(writeRecords, newCommitTime);\n    Assertions.assertNoWriteErrors(statuses.collect());\n    client.commit(newCommitTime, statuses);\n    \r\n\n    newCommitTime = \"002\";\n    client.startCommitWithTime(newCommitTime);\n\n    List<HoodieRecord> records2 = dataGen.generateUpdates(newCommitTime, records);\n    statuses = client.upsert(jsc.parallelize(records2, 1), newCommitTime);\n    Assertions.assertNoWriteErrors(statuses.collect());\n    if (commitSecondUpsert) {\n      client.commit(newCommitTime, statuses);\n    }\n\n    \r\n    HoodieTable table = this.getHoodieTable(metaClient, cfg);\n    SyncableFileSystemView fsView = getFileSystemViewWithUnCommittedSlices(table.getMetaClient());\n    List<HoodieFileGroup> firstPartitionCommit2FileGroups = fsView.getAllFileGroups(DEFAULT_FIRST_PARTITION_PATH).collect(Collectors.toList());\n    assertEquals(1, firstPartitionCommit2FileGroups.size());\n    firstPartitionCommit2FileSlices.addAll(firstPartitionCommit2FileGroups.get(0).getAllFileSlices().collect(Collectors.toList()));\n    \r\n    List<HoodieFileGroup> secondPartitionCommit2FileGroups = fsView.getAllFileGroups(DEFAULT_SECOND_PARTITION_PATH).collect(Collectors.toList());\n    assertEquals(1, secondPartitionCommit2FileGroups.size());\n    secondPartitionCommit2FileSlices.addAll(secondPartitionCommit2FileGroups.get(0).getAllFileSlices().collect(Collectors.toList()));\n\n    \r\n    HoodieTableType tableType = metaClient.getTableType();\n    if (tableType.equals(HoodieTableType.COPY_ON_WRITE)) {\n      assertEquals(2, firstPartitionCommit2FileSlices.size());\n      assertEquals(2, secondPartitionCommit2FileSlices.size());\n    } else {\n      assertEquals(1, firstPartitionCommit2FileSlices.size());\n      assertEquals(1, secondPartitionCommit2FileSlices.size());\n    }\n    return Pair.of(records, records2);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":488,"status":"M"}],"commitId":"23dca6c237517bc9d9407556af6b1042f09ae76b","commitMessage":"@@@[HUDI-2268] Add upgrade and downgrade to and from 0.9.0 (#3470)\n\n- Added upgrade and downgrade step to and from 0.9.0. Upgrade adds few table properties. Downgrade recreates timeline server based marker files if any. ","date":"2021-08-15 08:20:23","modifiedFileCount":"16","status":"M","submitter":"Y Ethan Guo"}]
