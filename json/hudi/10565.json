[{"authorTime":"2021-02-17 23:44:53","codes":[{"authorDate":"2021-02-17 23:44:53","commitOrder":2,"curCode":"  public void testCompactionIsNotScheduledEarly() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 60, CompactionTriggerStrategy.NUM_COMMITS);\n    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 100);\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n\n      \r\n      assertEquals(2, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n    }\n  }\n","date":"2021-02-17 23:44:53","endLine":65,"groupId":"4818","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompactionIsNotScheduledEarly","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/80/542edfa7f154605db16f4f62e5e6c1ffbaaa52.src","preCode":"  public void testCompactionIsNotScheduledEarly() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 60, CompactionTriggerStrategy.NUM_COMMITS);\n    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 100);\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n\n      \r\n      assertEquals(2, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":52,"status":"MB"},{"authorDate":"2021-02-17 23:44:53","commitOrder":2,"curCode":"  public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);\n    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n      \r\n      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n\n      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n      assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n      \r\n      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n      finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n\n      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n      assertEquals(6, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n    }\n  }\n","date":"2021-02-17 23:44:53","endLine":137,"groupId":"4818","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testSuccessfulCompactionBasedOnNumOrTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/80/542edfa7f154605db16f4f62e5e6c1ffbaaa52.src","preCode":"  public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);\n    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n      \r\n      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n\n      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n      assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n      \r\n      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n      finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n\n      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n      assertEquals(6, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":114,"status":"B"}],"commitId":"9431aabfab47a3b679bd6ccfa8b5fa584260fc9e","commitMessage":"@@@[HUDI-1381] Schedule compaction based on time elapsed (#2260)\n\n- introduce configs to control how compaction is triggered\n- Compaction can be triggered using time.  number of delta commits and/or combinations\n- Default behaviour remains the same.","date":"2021-02-17 23:44:53","modifiedFileCount":"5","status":"M","submitter":"Karl_Wang"},{"authorTime":"2021-02-20 09:54:26","codes":[{"authorDate":"2021-02-20 09:54:26","commitOrder":3,"curCode":"  public void testCompactionIsNotScheduledEarly() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 60, CompactionTriggerStrategy.NUM_COMMITS);\n    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 100);\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n      HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n\n      \r\n      assertEquals(2, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n    }\n  }\n","date":"2021-02-20 09:54:26","endLine":65,"groupId":"4818","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompactionIsNotScheduledEarly","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/97/d287592b4b27d0aae63e754410edf16a249865.src","preCode":"  public void testCompactionIsNotScheduledEarly() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 60, CompactionTriggerStrategy.NUM_COMMITS);\n    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 100);\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n\n      \r\n      assertEquals(2, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":52,"status":"M"},{"authorDate":"2021-02-20 09:54:26","commitOrder":3,"curCode":"  public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);\n    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n      \r\n      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n      HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n\n      metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n      assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n      \r\n      metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n      finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n\n      metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n      assertEquals(6, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n    }\n  }\n","date":"2021-02-20 09:54:26","endLine":137,"groupId":"4818","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testSuccessfulCompactionBasedOnNumOrTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/97/d287592b4b27d0aae63e754410edf16a249865.src","preCode":"  public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);\n    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n      \r\n      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n      HoodieTableMetaClient metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n\n      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n      assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n      \r\n      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n      finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n\n      metaClient = new HoodieTableMetaClient(hadoopConf, cfg.getBasePath());\n      assertEquals(6, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":114,"status":"M"}],"commitId":"c9fcf964b2bae56a54cb72951c8d8999eb323ed6","commitMessage":"@@@[HUDI-1315] Adding builder for HoodieTableMetaClient initialization (#2534)\n\n","date":"2021-02-20 09:54:26","modifiedFileCount":"57","status":"M","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-02-20 12:12:22","codes":[{"authorDate":"2021-02-20 12:12:22","commitOrder":4,"curCode":"  public void testCompactionIsNotScheduledEarly() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 60, CompactionTriggerStrategy.NUM_COMMITS);\n    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 100);\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n      HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n\n      \r\n      assertEquals(2, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n    }\n  }\n","date":"2021-02-20 12:12:22","endLine":65,"groupId":"10565","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testCompactionIsNotScheduledEarly","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/82/3d651aa1589d94531741efe58f823315e1ee50.src","preCode":"  public void testCompactionIsNotScheduledEarly() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 60, CompactionTriggerStrategy.NUM_COMMITS);\n    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 100);\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n      HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n\n      \r\n      assertEquals(2, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":52,"status":"M"},{"authorDate":"2021-02-20 12:12:22","commitOrder":4,"curCode":"  public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);\n    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n      \r\n      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n      HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n\n      metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n      assertEquals(4, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n      \r\n      metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n      finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n\n      metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n      assertEquals(6, metaClient.getActiveTimeline().getWriteTimeline().countInstants());\n    }\n  }\n","date":"2021-02-20 12:12:22","endLine":137,"groupId":"10565","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testSuccessfulCompactionBasedOnNumOrTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/82/3d651aa1589d94531741efe58f823315e1ee50.src","preCode":"  public void testSuccessfulCompactionBasedOnNumOrTime() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigForInlineCompaction(3, 20, CompactionTriggerStrategy.NUM_OR_TIME);\n    try (SparkRDDWriteClient<?> writeClient = getHoodieWriteClient(cfg)) {\n      List<HoodieRecord> records = dataGen.generateInserts(HoodieActiveTimeline.createNewInstantTime(), 10);\n      HoodieReadClient readClient = getHoodieReadClient(cfg.getBasePath());\n      List<String> instants = IntStream.range(0, 2).mapToObj(i -> HoodieActiveTimeline.createNewInstantTime()).collect(Collectors.toList());\n      runNextDeltaCommits(writeClient, readClient, instants, records, cfg, true, new ArrayList<>());\n      \r\n      String finalInstant = HoodieActiveTimeline.createNewInstantTime();\n      HoodieTableMetaClient metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n\n      metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n      assertEquals(4, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n      \r\n      metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n      finalInstant = HoodieActiveTimeline.createNewInstantTime(20000);\n      createNextDeltaCommit(finalInstant, dataGen.generateUpdates(finalInstant, 10), writeClient, metaClient, cfg, false);\n\n      metaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(cfg.getBasePath()).build();\n      assertEquals(6, metaClient.getActiveTimeline().getCommitsAndCompactionTimeline().countInstants());\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestInlineCompaction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":114,"status":"M"}],"commitId":"ffcfb58bacab377bc72d20041baa54a3fd8fc812","commitMessage":"@@@[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)\n\n1. Refactor rollback and move cleaning failed commits logic into cleaner\n2. Introduce hoodie heartbeat to ascertain failed commits\n3. Fix test cases","date":"2021-02-20 12:12:22","modifiedFileCount":"56","status":"M","submitter":"n3nash"}]
