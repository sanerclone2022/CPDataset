[{"authorTime":"2021-06-17 19:18:21","codes":[{"authorDate":"2021-07-24 00:03:15","commitOrder":4,"curCode":"  public static void createCOWTableWithSchema(String instantTime, String schemaFileName)\n      throws IOException, URISyntaxException {\n    Path path = new Path(hiveSyncConfig.basePath);\n    FileIOUtils.deleteDirectory(new File(hiveSyncConfig.basePath));\n    HoodieTableMetaClient.withPropertyBuilder()\n        .setTableType(HoodieTableType.COPY_ON_WRITE)\n        .setTableName(hiveSyncConfig.tableName)\n        .setPayloadClass(HoodieAvroPayload.class)\n        .initTable(configuration, hiveSyncConfig.basePath);\n\n    boolean result = fileSystem.mkdirs(path);\n    checkResult(result);\n    DateTime dateTime = DateTime.now().withTimeAtStartOfDay();\n\n    HoodieCommitMetadata commitMetadata = new HoodieCommitMetadata();\n    String partitionPath = dtfOut.print(dateTime);\n    Path partPath = new Path(hiveSyncConfig.basePath + \"/\" + partitionPath);\n    fileSystem.makeQualified(partPath);\n    fileSystem.mkdirs(partPath);\n    List<HoodieWriteStat> writeStats = new ArrayList<>();\n    String fileId = UUID.randomUUID().toString();\n    Path filePath = new Path(partPath.toString() + \"/\" + FSUtils.makeDataFileName(instantTime, \"1-0-1\", fileId));\n    Schema schema = SchemaTestUtil.getSchemaFromResource(HiveTestUtil.class, schemaFileName);\n    generateParquetDataWithSchema(filePath, schema);\n    HoodieWriteStat writeStat = new HoodieWriteStat();\n    writeStat.setFileId(fileId);\n    writeStat.setPath(filePath.toString());\n    writeStats.add(writeStat);\n    writeStats.forEach(s -> commitMetadata.addWriteStat(partitionPath, s));\n    commitMetadata.addMetadata(HoodieCommitMetadata.SCHEMA_KEY, schema.toString());\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName);\n    createCommitFile(commitMetadata, instantTime);\n  }\n","date":"2021-07-24 00:03:15","endLine":211,"groupId":"2893","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"createCOWTableWithSchema","params":"(StringinstantTime@StringschemaFileName)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/7b/9f1d127fd216350c1a10e71def80662b9eac6a.src","preCode":"  public static void createCOWTableWithSchema(String instantTime, String schemaFileName)\n      throws IOException, URISyntaxException {\n    Path path = new Path(hiveSyncConfig.basePath);\n    FileIOUtils.deleteDirectory(new File(hiveSyncConfig.basePath));\n    HoodieTableMetaClient.withPropertyBuilder()\n        .setTableType(HoodieTableType.COPY_ON_WRITE)\n        .setTableName(hiveSyncConfig.tableName)\n        .setPayloadClass(HoodieAvroPayload.class)\n        .initTable(configuration, hiveSyncConfig.basePath);\n\n    boolean result = fileSystem.mkdirs(path);\n    checkResult(result);\n    DateTime dateTime = DateTime.now().withTimeAtStartOfDay();\n\n    HoodieCommitMetadata commitMetadata = new HoodieCommitMetadata();\n    String partitionPath = dtfOut.print(dateTime);\n    Path partPath = new Path(hiveSyncConfig.basePath + \"/\" + partitionPath);\n    fileSystem.makeQualified(partPath);\n    fileSystem.mkdirs(partPath);\n    List<HoodieWriteStat> writeStats = new ArrayList<>();\n    String fileId = UUID.randomUUID().toString();\n    Path filePath = new Path(partPath.toString() + \"/\" + FSUtils.makeDataFileName(instantTime, \"1-0-1\", fileId));\n    Schema schema = SchemaTestUtil.getSchemaFromResource(HiveTestUtil.class, schemaFileName);\n    generateParquetDataWithSchema(filePath, schema);\n    HoodieWriteStat writeStat = new HoodieWriteStat();\n    writeStat.setFileId(fileId);\n    writeStat.setPath(filePath.toString());\n    writeStats.add(writeStat);\n    writeStats.forEach(s -> commitMetadata.addWriteStat(partitionPath, s));\n    commitMetadata.addMetadata(HoodieCommitMetadata.SCHEMA_KEY, schema.toString());\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName);\n    createCommitFile(commitMetadata, instantTime);\n  }\n","realPath":"hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":179,"status":"B"},{"authorDate":"2021-06-17 19:18:21","commitOrder":4,"curCode":"  public static void createMORTable(String commitTime, String deltaCommitTime, int numberOfPartitions,\n      boolean createDeltaCommit, boolean useSchemaFromCommitMetadata)\n      throws IOException, URISyntaxException, InterruptedException {\n    Path path = new Path(hiveSyncConfig.basePath);\n    FileIOUtils.deleteDirectory(new File(hiveSyncConfig.basePath));\n    HoodieTableMetaClient.withPropertyBuilder()\n      .setTableType(HoodieTableType.MERGE_ON_READ)\n      .setTableName(hiveSyncConfig.tableName)\n      .setPayloadClass(HoodieAvroPayload.class)\n      .initTable(configuration, hiveSyncConfig.basePath);\n\n    boolean result = fileSystem.mkdirs(path);\n    checkResult(result);\n    DateTime dateTime = DateTime.now();\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, true,\n        useSchemaFromCommitMetadata, dateTime, commitTime);\n    createdTablesSet\n      .add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_READ_OPTIMIZED_TABLE);\n    createdTablesSet\n        .add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n                             useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, commitTime);\n    if (createDeltaCommit) {\n      \r\n      HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), true,\n                                                          useSchemaFromCommitMetadata);\n      createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n    }\n  }\n","date":"2021-06-17 19:18:21","endLine":207,"groupId":"5366","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"createMORTable","params":"(StringcommitTime@StringdeltaCommitTime@intnumberOfPartitions@booleancreateDeltaCommit@booleanuseSchemaFromCommitMetadata)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/1d/6bfb4426462b93bd61d502aae941038d15cc6f.src","preCode":"  public static void createMORTable(String commitTime, String deltaCommitTime, int numberOfPartitions,\n      boolean createDeltaCommit, boolean useSchemaFromCommitMetadata)\n      throws IOException, URISyntaxException, InterruptedException {\n    Path path = new Path(hiveSyncConfig.basePath);\n    FileIOUtils.deleteDirectory(new File(hiveSyncConfig.basePath));\n    HoodieTableMetaClient.withPropertyBuilder()\n      .setTableType(HoodieTableType.MERGE_ON_READ)\n      .setTableName(hiveSyncConfig.tableName)\n      .setPayloadClass(HoodieAvroPayload.class)\n      .initTable(configuration, hiveSyncConfig.basePath);\n\n    boolean result = fileSystem.mkdirs(path);\n    checkResult(result);\n    DateTime dateTime = DateTime.now();\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, true,\n        useSchemaFromCommitMetadata, dateTime, commitTime);\n    createdTablesSet\n      .add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_READ_OPTIMIZED_TABLE);\n    createdTablesSet\n        .add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n                             useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, commitTime);\n    if (createDeltaCommit) {\n      \r\n      HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), true,\n                                                          useSchemaFromCommitMetadata);\n      createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n    }\n  }\n","realPath":"hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":175,"status":"NB"}],"commitId":"66207ed91a75ce8e91ccc0c417dc0d310dc36a5c","commitMessage":"@@@[HUDI-1848] Adding support for HMS for running DDL queries in hive-sy? (#2879)\n\n* [HUDI-1848] Adding support for HMS for running DDL queries in hive-sync-tool\n\n* [HUDI-1848] Fixing test cases\n\n* [HUDI-1848] CR changes\n\n* [HUDI-1848] Fix checkstyle violations\n\n* [HUDI-1848] Fixed a bug when metastore api fails for complex schemas with multiple levels.\n\n* [HUDI-1848] Adding the complex schema and resolving merge conflicts\n\n* [HUDI-1848] Adding some more javadocs\n\n* [HUDI-1848] Added javadocs for DDLExecutor impls\n\n* [HUDI-1848] Fixed style issue","date":"2021-07-24 00:03:15","modifiedFileCount":"9","status":"M","submitter":"jsbali"},{"authorTime":"2021-08-11 11:25:41","codes":[{"authorDate":"2021-08-11 11:25:41","commitOrder":5,"curCode":"  public static void createCOWTableWithSchema(String instantTime, String schemaFileName)\n      throws IOException, URISyntaxException {\n    Path path = new Path(hiveSyncConfig.basePath);\n    FileIOUtils.deleteDirectory(new File(hiveSyncConfig.basePath));\n    HoodieTableMetaClient.withPropertyBuilder()\n        .setTableType(HoodieTableType.COPY_ON_WRITE)\n        .setTableName(hiveSyncConfig.tableName)\n        .setPayloadClass(HoodieAvroPayload.class)\n        .initTable(configuration, hiveSyncConfig.basePath);\n\n    boolean result = fileSystem.mkdirs(path);\n    checkResult(result);\n    ZonedDateTime dateTime = ZonedDateTime.now().truncatedTo(ChronoUnit.DAYS);\n\n    HoodieCommitMetadata commitMetadata = new HoodieCommitMetadata();\n    String partitionPath = dateTime.format(dtfOut);\n    Path partPath = new Path(hiveSyncConfig.basePath + \"/\" + partitionPath);\n    fileSystem.makeQualified(partPath);\n    fileSystem.mkdirs(partPath);\n    List<HoodieWriteStat> writeStats = new ArrayList<>();\n    String fileId = UUID.randomUUID().toString();\n    Path filePath = new Path(partPath.toString() + \"/\" + FSUtils.makeDataFileName(instantTime, \"1-0-1\", fileId));\n    Schema schema = SchemaTestUtil.getSchemaFromResource(HiveTestUtil.class, schemaFileName);\n    generateParquetDataWithSchema(filePath, schema);\n    HoodieWriteStat writeStat = new HoodieWriteStat();\n    writeStat.setFileId(fileId);\n    writeStat.setPath(filePath.toString());\n    writeStats.add(writeStat);\n    writeStats.forEach(s -> commitMetadata.addWriteStat(partitionPath, s));\n    commitMetadata.addMetadata(HoodieCommitMetadata.SCHEMA_KEY, schema.toString());\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName);\n    createCommitFile(commitMetadata, instantTime);\n  }\n","date":"2021-08-11 11:25:41","endLine":211,"groupId":"10347","id":3,"instanceNumber":1,"isCurCommit":1,"methodName":"createCOWTableWithSchema","params":"(StringinstantTime@StringschemaFileName)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/a3/bc2268dcac24706513384a719971574dd0b48b.src","preCode":"  public static void createCOWTableWithSchema(String instantTime, String schemaFileName)\n      throws IOException, URISyntaxException {\n    Path path = new Path(hiveSyncConfig.basePath);\n    FileIOUtils.deleteDirectory(new File(hiveSyncConfig.basePath));\n    HoodieTableMetaClient.withPropertyBuilder()\n        .setTableType(HoodieTableType.COPY_ON_WRITE)\n        .setTableName(hiveSyncConfig.tableName)\n        .setPayloadClass(HoodieAvroPayload.class)\n        .initTable(configuration, hiveSyncConfig.basePath);\n\n    boolean result = fileSystem.mkdirs(path);\n    checkResult(result);\n    DateTime dateTime = DateTime.now().withTimeAtStartOfDay();\n\n    HoodieCommitMetadata commitMetadata = new HoodieCommitMetadata();\n    String partitionPath = dtfOut.print(dateTime);\n    Path partPath = new Path(hiveSyncConfig.basePath + \"/\" + partitionPath);\n    fileSystem.makeQualified(partPath);\n    fileSystem.mkdirs(partPath);\n    List<HoodieWriteStat> writeStats = new ArrayList<>();\n    String fileId = UUID.randomUUID().toString();\n    Path filePath = new Path(partPath.toString() + \"/\" + FSUtils.makeDataFileName(instantTime, \"1-0-1\", fileId));\n    Schema schema = SchemaTestUtil.getSchemaFromResource(HiveTestUtil.class, schemaFileName);\n    generateParquetDataWithSchema(filePath, schema);\n    HoodieWriteStat writeStat = new HoodieWriteStat();\n    writeStat.setFileId(fileId);\n    writeStat.setPath(filePath.toString());\n    writeStats.add(writeStat);\n    writeStats.forEach(s -> commitMetadata.addWriteStat(partitionPath, s));\n    commitMetadata.addMetadata(HoodieCommitMetadata.SCHEMA_KEY, schema.toString());\n    createdTablesSet.add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName);\n    createCommitFile(commitMetadata, instantTime);\n  }\n","realPath":"hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":179,"status":"M"},{"authorDate":"2021-08-11 11:25:41","commitOrder":5,"curCode":"  public static void createMORTable(String commitTime, String deltaCommitTime, int numberOfPartitions,\n                                    boolean createDeltaCommit, boolean useSchemaFromCommitMetadata)\n      throws IOException, URISyntaxException, InterruptedException {\n    Path path = new Path(hiveSyncConfig.basePath);\n    FileIOUtils.deleteDirectory(new File(hiveSyncConfig.basePath));\n    HoodieTableMetaClient.withPropertyBuilder()\n        .setTableType(HoodieTableType.MERGE_ON_READ)\n        .setTableName(hiveSyncConfig.tableName)\n        .setPayloadClass(HoodieAvroPayload.class)\n        .initTable(configuration, hiveSyncConfig.basePath);\n\n    boolean result = fileSystem.mkdirs(path);\n    checkResult(result);\n    ZonedDateTime dateTime = ZonedDateTime.now();\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, true,\n        useSchemaFromCommitMetadata, dateTime, commitTime);\n    createdTablesSet\n        .add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_READ_OPTIMIZED_TABLE);\n    createdTablesSet\n        .add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n        useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, commitTime);\n    if (createDeltaCommit) {\n      \r\n      HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), true,\n          useSchemaFromCommitMetadata);\n      createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n    }\n  }\n","date":"2021-08-11 11:25:41","endLine":245,"groupId":"10347","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"createMORTable","params":"(StringcommitTime@StringdeltaCommitTime@intnumberOfPartitions@booleancreateDeltaCommit@booleanuseSchemaFromCommitMetadata)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/a3/bc2268dcac24706513384a719971574dd0b48b.src","preCode":"  public static void createMORTable(String commitTime, String deltaCommitTime, int numberOfPartitions,\n                                    boolean createDeltaCommit, boolean useSchemaFromCommitMetadata)\n      throws IOException, URISyntaxException, InterruptedException {\n    Path path = new Path(hiveSyncConfig.basePath);\n    FileIOUtils.deleteDirectory(new File(hiveSyncConfig.basePath));\n    HoodieTableMetaClient.withPropertyBuilder()\n        .setTableType(HoodieTableType.MERGE_ON_READ)\n        .setTableName(hiveSyncConfig.tableName)\n        .setPayloadClass(HoodieAvroPayload.class)\n        .initTable(configuration, hiveSyncConfig.basePath);\n\n    boolean result = fileSystem.mkdirs(path);\n    checkResult(result);\n    DateTime dateTime = DateTime.now();\n    HoodieCommitMetadata commitMetadata = createPartitions(numberOfPartitions, true,\n        useSchemaFromCommitMetadata, dateTime, commitTime);\n    createdTablesSet\n        .add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_READ_OPTIMIZED_TABLE);\n    createdTablesSet\n        .add(hiveSyncConfig.databaseName + \".\" + hiveSyncConfig.tableName + HiveSyncTool.SUFFIX_SNAPSHOT_TABLE);\n    HoodieCommitMetadata compactionMetadata = new HoodieCommitMetadata();\n    commitMetadata.getPartitionToWriteStats()\n        .forEach((key, value) -> value.forEach(l -> compactionMetadata.addWriteStat(key, l)));\n    addSchemaToCommitMetadata(compactionMetadata, commitMetadata.getMetadata(HoodieCommitMetadata.SCHEMA_KEY),\n        useSchemaFromCommitMetadata);\n    createCompactionCommitFile(compactionMetadata, commitTime);\n    if (createDeltaCommit) {\n      \r\n      HoodieCommitMetadata deltaMetadata = createLogFiles(commitMetadata.getPartitionToWriteStats(), true,\n          useSchemaFromCommitMetadata);\n      createDeltaCommitFile(deltaMetadata, deltaCommitTime);\n    }\n  }\n","realPath":"hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":213,"status":"M"}],"commitId":"8255a86cb4d7f2173f0adcf0d752096b0b4df78c","commitMessage":"@@@[HUDI-1939] remove joda time in hivesync module (#3430)\n\n","date":"2021-08-11 11:25:41","modifiedFileCount":"5","status":"M","submitter":"Raymond Xu"}]
