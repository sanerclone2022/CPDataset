[{"authorTime":"2021-09-02 16:32:40","codes":[{"authorDate":"2021-08-19 23:21:20","commitOrder":5,"curCode":"  void testBulkInsert(boolean hiveStylePartitioning) {\n    TableEnvironment tableEnv = batchTableEnv;\n    \r\n    String csvSourceDDL = TestConfigurations.getCsvSourceDDL(\"csv_source\", \"test_source_5.data\");\n    tableEnv.executeSql(csvSourceDDL);\n\n    String hoodieTableDDL = sql(\"hoodie_sink\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.OPERATION, \"bulk_insert\")\n        .option(FlinkOptions.WRITE_BULK_INSERT_SHUFFLE_BY_PARTITION, \"true\")\n        .option(FlinkOptions.HIVE_STYLE_PARTITIONING, hiveStylePartitioning)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    String insertInto = \"insert into hoodie_sink select * from csv_source\";\n    execInsertSql(tableEnv, insertInto);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from hoodie_sink\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n    \r\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from hoodie_sink where uuid > 'id5'\").execute().collect());\n    assertRowsEquals(result2, \"[\"\n        + \"+I[id6, Emma, 20, 1970-01-01T00:00:06, par3], \"\n        + \"+I[id7, Bob, 44, 1970-01-01T00:00:07, par4], \"\n        + \"+I[id8, Han, 56, 1970-01-01T00:00:08, par4]]\");\n  }\n","date":"2021-08-19 23:21:20","endLine":670,"groupId":"791","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testBulkInsert","params":"(booleanhiveStylePartitioning)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9e/ffdcc8c67f59ad51cfd9a0e5ed56ecd41b530e.src","preCode":"  void testBulkInsert(boolean hiveStylePartitioning) {\n    TableEnvironment tableEnv = batchTableEnv;\n    \r\n    String csvSourceDDL = TestConfigurations.getCsvSourceDDL(\"csv_source\", \"test_source_5.data\");\n    tableEnv.executeSql(csvSourceDDL);\n\n    String hoodieTableDDL = sql(\"hoodie_sink\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.OPERATION, \"bulk_insert\")\n        .option(FlinkOptions.WRITE_BULK_INSERT_SHUFFLE_BY_PARTITION, \"true\")\n        .option(FlinkOptions.HIVE_STYLE_PARTITIONING, hiveStylePartitioning)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    String insertInto = \"insert into hoodie_sink select * from csv_source\";\n    execInsertSql(tableEnv, insertInto);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from hoodie_sink\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n    \r\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from hoodie_sink where uuid > 'id5'\").execute().collect());\n    assertRowsEquals(result2, \"[\"\n        + \"+I[id6, Emma, 20, 1970-01-01T00:00:06, par3], \"\n        + \"+I[id7, Bob, 44, 1970-01-01T00:00:07, par4], \"\n        + \"+I[id8, Han, 56, 1970-01-01T00:00:08, par4]]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":643,"status":"NB"},{"authorDate":"2021-09-02 16:32:40","commitOrder":5,"curCode":"  void testAppendWrite() {\n    TableEnvironment tableEnv = batchTableEnv;\n    \r\n    String csvSourceDDL = TestConfigurations.getCsvSourceDDL(\"csv_source\", \"test_source_5.data\");\n    tableEnv.executeSql(csvSourceDDL);\n\n    String hoodieTableDDL = sql(\"hoodie_sink\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.OPERATION, \"insert\")\n        .option(FlinkOptions.INSERT_DEDUP, false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    String insertInto = \"insert into hoodie_sink select * from csv_source\";\n    execInsertSql(tableEnv, insertInto);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from hoodie_sink\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n    \r\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from hoodie_sink where uuid > 'id5'\").execute().collect());\n    assertRowsEquals(result2, \"[\"\n        + \"+I[id6, Emma, 20, 1970-01-01T00:00:06, par3], \"\n        + \"+I[id7, Bob, 44, 1970-01-01T00:00:07, par4], \"\n        + \"+I[id8, Han, 56, 1970-01-01T00:00:08, par4]]\");\n  }\n","date":"2021-09-02 16:32:40","endLine":826,"groupId":"4115","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testAppendWrite","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/a0/4e7bb992d932f700c76f00ee1fadf919b6ac2c.src","preCode":"  void testAppendWrite() {\n    TableEnvironment tableEnv = batchTableEnv;\n    \r\n    String csvSourceDDL = TestConfigurations.getCsvSourceDDL(\"csv_source\", \"test_source_5.data\");\n    tableEnv.executeSql(csvSourceDDL);\n\n    String hoodieTableDDL = sql(\"hoodie_sink\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.OPERATION, \"insert\")\n        .option(FlinkOptions.INSERT_DEDUP, false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    String insertInto = \"insert into hoodie_sink select * from csv_source\";\n    execInsertSql(tableEnv, insertInto);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from hoodie_sink\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n    \r\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from hoodie_sink where uuid > 'id5'\").execute().collect());\n    assertRowsEquals(result2, \"[\"\n        + \"+I[id6, Emma, 20, 1970-01-01T00:00:06, par3], \"\n        + \"+I[id7, Bob, 44, 1970-01-01T00:00:07, par4], \"\n        + \"+I[id8, Han, 56, 1970-01-01T00:00:08, par4]]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":800,"status":"B"}],"commitId":"7a1bd225cab6b1737d689d3659351a8c950a4d78","commitMessage":"@@@[HUDI-2376] Add pipeline for Append mode (#3573)\n\nCo-authored-by: ??? <yuzhaojing@bilibili.com>","date":"2021-09-02 16:32:40","modifiedFileCount":"35","status":"M","submitter":"yuzhaojing"},{"authorTime":"2021-09-02 16:32:40","codes":[{"authorDate":"2021-09-11 13:17:16","commitOrder":6,"curCode":"  void testBulkInsert(boolean hiveStylePartitioning) {\n    TableEnvironment tableEnv = batchTableEnv;\n    \r\n    String csvSourceDDL = TestConfigurations.getCsvSourceDDL(\"csv_source\", \"test_source_5.data\");\n    tableEnv.executeSql(csvSourceDDL);\n\n    String hoodieTableDDL = sql(\"hoodie_sink\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.OPERATION, \"bulk_insert\")\n        .option(FlinkOptions.WRITE_BULK_INSERT_SHUFFLE_BY_PARTITION, true)\n        .option(FlinkOptions.HIVE_STYLE_PARTITIONING, hiveStylePartitioning)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    String insertInto = \"insert into hoodie_sink select * from csv_source\";\n    execInsertSql(tableEnv, insertInto);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from hoodie_sink\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n    \r\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from hoodie_sink where uuid > 'id5'\").execute().collect());\n    assertRowsEquals(result2, \"[\"\n        + \"+I[id6, Emma, 20, 1970-01-01T00:00:06, par3], \"\n        + \"+I[id7, Bob, 44, 1970-01-01T00:00:07, par4], \"\n        + \"+I[id8, Han, 56, 1970-01-01T00:00:08, par4]]\");\n  }\n","date":"2021-09-11 13:17:16","endLine":765,"groupId":"10393","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testBulkInsert","params":"(booleanhiveStylePartitioning)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/5b/e603f7838e5195aa26e8cf17398d597ca9ee2f.src","preCode":"  void testBulkInsert(boolean hiveStylePartitioning) {\n    TableEnvironment tableEnv = batchTableEnv;\n    \r\n    String csvSourceDDL = TestConfigurations.getCsvSourceDDL(\"csv_source\", \"test_source_5.data\");\n    tableEnv.executeSql(csvSourceDDL);\n\n    String hoodieTableDDL = sql(\"hoodie_sink\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.OPERATION, \"bulk_insert\")\n        .option(FlinkOptions.WRITE_BULK_INSERT_SHUFFLE_BY_PARTITION, \"true\")\n        .option(FlinkOptions.HIVE_STYLE_PARTITIONING, hiveStylePartitioning)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    String insertInto = \"insert into hoodie_sink select * from csv_source\";\n    execInsertSql(tableEnv, insertInto);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from hoodie_sink\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n    \r\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from hoodie_sink where uuid > 'id5'\").execute().collect());\n    assertRowsEquals(result2, \"[\"\n        + \"+I[id6, Emma, 20, 1970-01-01T00:00:06, par3], \"\n        + \"+I[id7, Bob, 44, 1970-01-01T00:00:07, par4], \"\n        + \"+I[id8, Han, 56, 1970-01-01T00:00:08, par4]]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":738,"status":"M"},{"authorDate":"2021-09-02 16:32:40","commitOrder":6,"curCode":"  void testAppendWrite() {\n    TableEnvironment tableEnv = batchTableEnv;\n    \r\n    String csvSourceDDL = TestConfigurations.getCsvSourceDDL(\"csv_source\", \"test_source_5.data\");\n    tableEnv.executeSql(csvSourceDDL);\n\n    String hoodieTableDDL = sql(\"hoodie_sink\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.OPERATION, \"insert\")\n        .option(FlinkOptions.INSERT_DEDUP, false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    String insertInto = \"insert into hoodie_sink select * from csv_source\";\n    execInsertSql(tableEnv, insertInto);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from hoodie_sink\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n    \r\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from hoodie_sink where uuid > 'id5'\").execute().collect());\n    assertRowsEquals(result2, \"[\"\n        + \"+I[id6, Emma, 20, 1970-01-01T00:00:06, par3], \"\n        + \"+I[id7, Bob, 44, 1970-01-01T00:00:07, par4], \"\n        + \"+I[id8, Han, 56, 1970-01-01T00:00:08, par4]]\");\n  }\n","date":"2021-09-02 16:32:40","endLine":826,"groupId":"10393","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testAppendWrite","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/a0/4e7bb992d932f700c76f00ee1fadf919b6ac2c.src","preCode":"  void testAppendWrite() {\n    TableEnvironment tableEnv = batchTableEnv;\n    \r\n    String csvSourceDDL = TestConfigurations.getCsvSourceDDL(\"csv_source\", \"test_source_5.data\");\n    tableEnv.executeSql(csvSourceDDL);\n\n    String hoodieTableDDL = sql(\"hoodie_sink\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.OPERATION, \"insert\")\n        .option(FlinkOptions.INSERT_DEDUP, false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    String insertInto = \"insert into hoodie_sink select * from csv_source\";\n    execInsertSql(tableEnv, insertInto);\n\n    List<Row> result1 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from hoodie_sink\").execute().collect());\n    assertRowsEquals(result1, TestData.DATA_SET_SOURCE_INSERT);\n    \r\n    List<Row> result2 = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from hoodie_sink where uuid > 'id5'\").execute().collect());\n    assertRowsEquals(result2, \"[\"\n        + \"+I[id6, Emma, 20, 1970-01-01T00:00:06, par3], \"\n        + \"+I[id7, Bob, 44, 1970-01-01T00:00:07, par4], \"\n        + \"+I[id8, Han, 56, 1970-01-01T00:00:08, par4]]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":800,"status":"N"}],"commitId":"b30c5bdaef77aee9f564ac24f80f5c364014bb17","commitMessage":"@@@[HUDI-2412] Add timestamp based partitioning for flink writer (#3638)\n\n","date":"2021-09-11 13:17:16","modifiedFileCount":"11","status":"M","submitter":"Danny Chan"}]
