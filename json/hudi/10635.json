[{"authorTime":"2020-10-02 05:25:29","codes":[{"authorDate":"2020-10-02 05:25:29","commitOrder":1,"curCode":"  public void testRollbackCommit() throws Exception {\n    \r\n    final String p1 = \"2016/05/01\";\n    final String p2 = \"2016/05/02\";\n    final String p3 = \"2016/05/06\";\n    final String commitTime1 = \"20160501010101\";\n    final String commitTime2 = \"20160502020601\";\n    final String commitTime3 = \"20160506030611\";\n    Map<String, String> partitionAndFileId1 = new HashMap<String, String>() {\n      {\n        put(p1, \"id11\");\n        put(p2, \"id12\");\n        put(p3, \"id13\");\n      }\n    };\n    Map<String, String> partitionAndFileId2 = new HashMap<String, String>() {\n      {\n        put(p1, \"id21\");\n        put(p2, \"id22\");\n        put(p3, \"id23\");\n      }\n    };\n    Map<String, String> partitionAndFileId3 = new HashMap<String, String>() {\n      {\n        put(p1, \"id31\");\n        put(p2, \"id32\");\n        put(p3, \"id33\");\n      }\n    };\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient)\n        .withPartitionMetaFiles(p1, p2, p3)\n        .addCommit(commitTime1)\n        .withBaseFilesInPartitions(partitionAndFileId1)\n        .addCommit(commitTime2)\n        .withBaseFilesInPartitions(partitionAndFileId2)\n        .addInflightCommit(commitTime3)\n        .withBaseFilesInPartitions(partitionAndFileId3);\n\n    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build()).build();\n\n    try (SparkRDDWriteClient client = getHoodieWriteClient(config, false)) {\n\n      \r\n      assertThrows(HoodieRollbackException.class, () -> {\n        client.rollback(commitTime1);\n      }, \"Should have thrown an exception \");\n\n      \r\n      client.rollback(commitTime3);\n      assertFalse(testTable.inflightCommitExists(commitTime3));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId3, commitTime3));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n\n      \r\n      testTable.addInflightCommit(commitTime3);\n      client.rollback(commitTime3);\n      assertFalse(testTable.inflightCommitExists(commitTime3));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n\n      \r\n      client.rollback(commitTime2);\n      assertFalse(testTable.commitExists(commitTime2));\n      assertFalse(testTable.inflightCommitExists(commitTime2));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n\n      \r\n      \r\n      testTable.addInflightCommit(commitTime2).withBaseFilesInPartitions(partitionAndFileId2);\n\n      client.rollback(commitTime2);\n      assertFalse(testTable.commitExists(commitTime2));\n      assertFalse(testTable.inflightCommitExists(commitTime2));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n\n      \r\n      client.rollback(commitTime1);\n      assertFalse(testTable.commitExists(commitTime1));\n      assertFalse(testTable.inflightCommitExists(commitTime1));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n    }\n  }\n","date":"2020-10-02 05:25:29","endLine":248,"groupId":"5897","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testRollbackCommit","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/d0/4a2df1b4eb4083dcc14262531808afe7cb3398.src","preCode":"  public void testRollbackCommit() throws Exception {\n    \r\n    final String p1 = \"2016/05/01\";\n    final String p2 = \"2016/05/02\";\n    final String p3 = \"2016/05/06\";\n    final String commitTime1 = \"20160501010101\";\n    final String commitTime2 = \"20160502020601\";\n    final String commitTime3 = \"20160506030611\";\n    Map<String, String> partitionAndFileId1 = new HashMap<String, String>() {\n      {\n        put(p1, \"id11\");\n        put(p2, \"id12\");\n        put(p3, \"id13\");\n      }\n    };\n    Map<String, String> partitionAndFileId2 = new HashMap<String, String>() {\n      {\n        put(p1, \"id21\");\n        put(p2, \"id22\");\n        put(p3, \"id23\");\n      }\n    };\n    Map<String, String> partitionAndFileId3 = new HashMap<String, String>() {\n      {\n        put(p1, \"id31\");\n        put(p2, \"id32\");\n        put(p3, \"id33\");\n      }\n    };\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient)\n        .withPartitionMetaFiles(p1, p2, p3)\n        .addCommit(commitTime1)\n        .withBaseFilesInPartitions(partitionAndFileId1)\n        .addCommit(commitTime2)\n        .withBaseFilesInPartitions(partitionAndFileId2)\n        .addInflightCommit(commitTime3)\n        .withBaseFilesInPartitions(partitionAndFileId3);\n\n    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build()).build();\n\n    try (SparkRDDWriteClient client = getHoodieWriteClient(config, false)) {\n\n      \r\n      assertThrows(HoodieRollbackException.class, () -> {\n        client.rollback(commitTime1);\n      }, \"Should have thrown an exception \");\n\n      \r\n      client.rollback(commitTime3);\n      assertFalse(testTable.inflightCommitExists(commitTime3));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId3, commitTime3));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n\n      \r\n      testTable.addInflightCommit(commitTime3);\n      client.rollback(commitTime3);\n      assertFalse(testTable.inflightCommitExists(commitTime3));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n\n      \r\n      client.rollback(commitTime2);\n      assertFalse(testTable.commitExists(commitTime2));\n      assertFalse(testTable.inflightCommitExists(commitTime2));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n\n      \r\n      \r\n      testTable.addInflightCommit(commitTime2).withBaseFilesInPartitions(partitionAndFileId2);\n\n      client.rollback(commitTime2);\n      assertFalse(testTable.commitExists(commitTime2));\n      assertFalse(testTable.inflightCommitExists(commitTime2));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n\n      \r\n      client.rollback(commitTime1);\n      assertFalse(testTable.commitExists(commitTime1));\n      assertFalse(testTable.inflightCommitExists(commitTime1));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestClientRollback.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":163,"status":"B"},{"authorDate":"2020-10-02 05:25:29","commitOrder":1,"curCode":"  public void testAutoRollbackInflightCommit() throws Exception {\n    \r\n    final String p1 = \"2016/05/01\";\n    final String p2 = \"2016/05/02\";\n    final String p3 = \"2016/05/06\";\n    final String commitTime1 = \"20160501010101\";\n    final String commitTime2 = \"20160502020601\";\n    final String commitTime3 = \"20160506030611\";\n    Map<String, String> partitionAndFileId1 = new HashMap<String, String>() {\n      {\n        put(p1, \"id11\");\n        put(p2, \"id12\");\n        put(p3, \"id13\");\n      }\n    };\n    Map<String, String> partitionAndFileId2 = new HashMap<String, String>() {\n      {\n        put(p1, \"id21\");\n        put(p2, \"id22\");\n        put(p3, \"id23\");\n      }\n    };\n    Map<String, String> partitionAndFileId3 = new HashMap<String, String>() {\n      {\n        put(p1, \"id31\");\n        put(p2, \"id32\");\n        put(p3, \"id33\");\n      }\n    };\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient)\n        .withPartitionMetaFiles(p1, p2, p3)\n        .addCommit(commitTime1)\n        .withBaseFilesInPartitions(partitionAndFileId1)\n        .addInflightCommit(commitTime2)\n        .withBaseFilesInPartitions(partitionAndFileId2)\n        .addInflightCommit(commitTime3)\n        .withBaseFilesInPartitions(partitionAndFileId3);\n\n    \r\n    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build()).build();\n\n    final String commitTime4 = \"20160506030621\";\n    try (SparkRDDWriteClient client = getHoodieWriteClient(config, false)) {\n      client.startCommitWithTime(commitTime4);\n      \r\n      assertTrue(testTable.commitExists(commitTime1));\n      assertTrue(testTable.inflightCommitExists(commitTime2));\n      assertTrue(testTable.inflightCommitExists(commitTime3));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId3, commitTime3));\n    }\n\n    \r\n    final String commitTime5 = \"20160506030631\";\n    try (SparkRDDWriteClient client = getHoodieWriteClient(config, true)) {\n      client.startCommitWithTime(commitTime5);\n      assertTrue(testTable.commitExists(commitTime1));\n      assertFalse(testTable.inflightCommitExists(commitTime2));\n      assertFalse(testTable.inflightCommitExists(commitTime3));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId3, commitTime3));\n    }\n  }\n","date":"2020-10-02 05:25:29","endLine":319,"groupId":"1573","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testAutoRollbackInflightCommit","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/d0/4a2df1b4eb4083dcc14262531808afe7cb3398.src","preCode":"  public void testAutoRollbackInflightCommit() throws Exception {\n    \r\n    final String p1 = \"2016/05/01\";\n    final String p2 = \"2016/05/02\";\n    final String p3 = \"2016/05/06\";\n    final String commitTime1 = \"20160501010101\";\n    final String commitTime2 = \"20160502020601\";\n    final String commitTime3 = \"20160506030611\";\n    Map<String, String> partitionAndFileId1 = new HashMap<String, String>() {\n      {\n        put(p1, \"id11\");\n        put(p2, \"id12\");\n        put(p3, \"id13\");\n      }\n    };\n    Map<String, String> partitionAndFileId2 = new HashMap<String, String>() {\n      {\n        put(p1, \"id21\");\n        put(p2, \"id22\");\n        put(p3, \"id23\");\n      }\n    };\n    Map<String, String> partitionAndFileId3 = new HashMap<String, String>() {\n      {\n        put(p1, \"id31\");\n        put(p2, \"id32\");\n        put(p3, \"id33\");\n      }\n    };\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient)\n        .withPartitionMetaFiles(p1, p2, p3)\n        .addCommit(commitTime1)\n        .withBaseFilesInPartitions(partitionAndFileId1)\n        .addInflightCommit(commitTime2)\n        .withBaseFilesInPartitions(partitionAndFileId2)\n        .addInflightCommit(commitTime3)\n        .withBaseFilesInPartitions(partitionAndFileId3);\n\n    \r\n    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build()).build();\n\n    final String commitTime4 = \"20160506030621\";\n    try (SparkRDDWriteClient client = getHoodieWriteClient(config, false)) {\n      client.startCommitWithTime(commitTime4);\n      \r\n      assertTrue(testTable.commitExists(commitTime1));\n      assertTrue(testTable.inflightCommitExists(commitTime2));\n      assertTrue(testTable.inflightCommitExists(commitTime3));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId3, commitTime3));\n    }\n\n    \r\n    final String commitTime5 = \"20160506030631\";\n    try (SparkRDDWriteClient client = getHoodieWriteClient(config, true)) {\n      client.startCommitWithTime(commitTime5);\n      assertTrue(testTable.commitExists(commitTime1));\n      assertFalse(testTable.inflightCommitExists(commitTime2));\n      assertFalse(testTable.inflightCommitExists(commitTime3));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId3, commitTime3));\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestClientRollback.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":254,"status":"B"}],"commitId":"1f7add92916c37b05be270d9c75a9042134ec506","commitMessage":"@@@[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)\n\n- This change breaks `hudi-client` into `hudi-client-common` and `hudi-spark-client` modules \n- Simple usages of Spark using jsc.parallelize() has been redone using EngineContext#map.  EngineContext#flatMap etc\n- Code changes in the PR.  break classes into `BaseXYZ` parent classes with no spark dependencies living in `hudi-client-common`\n- Classes on `hudi-spark-client` are named `SparkXYZ` extending the parent classes with all the Spark dependencies\n- To simplify/cleanup.  HoodieIndex#fetchRecordLocation has been removed and its usages in tests replaced with alternatives\n\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2020-10-02 05:25:29","modifiedFileCount":"31","status":"B","submitter":"Mathieu"},{"authorTime":"2021-02-20 12:12:22","codes":[{"authorDate":"2021-02-20 12:12:22","commitOrder":2,"curCode":"  public void testRollbackCommit() throws Exception {\n    \r\n    final String p1 = \"2016/05/01\";\n    final String p2 = \"2016/05/02\";\n    final String p3 = \"2016/05/06\";\n    final String commitTime1 = \"20160501010101\";\n    final String commitTime2 = \"20160502020601\";\n    final String commitTime3 = \"20160506030611\";\n    Map<String, String> partitionAndFileId1 = new HashMap<String, String>() {\n      {\n        put(p1, \"id11\");\n        put(p2, \"id12\");\n        put(p3, \"id13\");\n      }\n    };\n    Map<String, String> partitionAndFileId2 = new HashMap<String, String>() {\n      {\n        put(p1, \"id21\");\n        put(p2, \"id22\");\n        put(p3, \"id23\");\n      }\n    };\n    Map<String, String> partitionAndFileId3 = new HashMap<String, String>() {\n      {\n        put(p1, \"id31\");\n        put(p2, \"id32\");\n        put(p3, \"id33\");\n      }\n    };\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient)\n        .withPartitionMetaFiles(p1, p2, p3)\n        .addCommit(commitTime1)\n        .withBaseFilesInPartitions(partitionAndFileId1)\n        .addCommit(commitTime2)\n        .withBaseFilesInPartitions(partitionAndFileId2)\n        .addInflightCommit(commitTime3)\n        .withBaseFilesInPartitions(partitionAndFileId3);\n\n    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath)\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n            .withFailedWritesCleaningPolicy(HoodieFailedWritesCleaningPolicy.LAZY).build())\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build()).build();\n\n    try (SparkRDDWriteClient client = getHoodieWriteClient(config)) {\n\n      \r\n      client.rollback(commitTime3);\n      assertFalse(testTable.inflightCommitExists(commitTime3));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId3, commitTime3));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n\n      \r\n      testTable.addInflightCommit(commitTime3);\n      client.rollback(commitTime3);\n      assertFalse(testTable.inflightCommitExists(commitTime3));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n\n      \r\n      client.rollback(commitTime2);\n      assertFalse(testTable.commitExists(commitTime2));\n      assertFalse(testTable.inflightCommitExists(commitTime2));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n\n      \r\n      \r\n      testTable.addInflightCommit(commitTime2).withBaseFilesInPartitions(partitionAndFileId2);\n\n      client.rollback(commitTime2);\n      assertFalse(testTable.commitExists(commitTime2));\n      assertFalse(testTable.inflightCommitExists(commitTime2));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n\n      \r\n      client.rollback(commitTime1);\n      assertFalse(testTable.commitExists(commitTime1));\n      assertFalse(testTable.inflightCommitExists(commitTime1));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n    }\n  }\n","date":"2021-02-20 12:12:22","endLine":247,"groupId":"10635","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testRollbackCommit","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/b4/b5c05dfef51059fa27e36a518558dce0484d8e.src","preCode":"  public void testRollbackCommit() throws Exception {\n    \r\n    final String p1 = \"2016/05/01\";\n    final String p2 = \"2016/05/02\";\n    final String p3 = \"2016/05/06\";\n    final String commitTime1 = \"20160501010101\";\n    final String commitTime2 = \"20160502020601\";\n    final String commitTime3 = \"20160506030611\";\n    Map<String, String> partitionAndFileId1 = new HashMap<String, String>() {\n      {\n        put(p1, \"id11\");\n        put(p2, \"id12\");\n        put(p3, \"id13\");\n      }\n    };\n    Map<String, String> partitionAndFileId2 = new HashMap<String, String>() {\n      {\n        put(p1, \"id21\");\n        put(p2, \"id22\");\n        put(p3, \"id23\");\n      }\n    };\n    Map<String, String> partitionAndFileId3 = new HashMap<String, String>() {\n      {\n        put(p1, \"id31\");\n        put(p2, \"id32\");\n        put(p3, \"id33\");\n      }\n    };\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient)\n        .withPartitionMetaFiles(p1, p2, p3)\n        .addCommit(commitTime1)\n        .withBaseFilesInPartitions(partitionAndFileId1)\n        .addCommit(commitTime2)\n        .withBaseFilesInPartitions(partitionAndFileId2)\n        .addInflightCommit(commitTime3)\n        .withBaseFilesInPartitions(partitionAndFileId3);\n\n    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build()).build();\n\n    try (SparkRDDWriteClient client = getHoodieWriteClient(config, false)) {\n\n      \r\n      assertThrows(HoodieRollbackException.class, () -> {\n        client.rollback(commitTime1);\n      }, \"Should have thrown an exception \");\n\n      \r\n      client.rollback(commitTime3);\n      assertFalse(testTable.inflightCommitExists(commitTime3));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId3, commitTime3));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n\n      \r\n      testTable.addInflightCommit(commitTime3);\n      client.rollback(commitTime3);\n      assertFalse(testTable.inflightCommitExists(commitTime3));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n\n      \r\n      client.rollback(commitTime2);\n      assertFalse(testTable.commitExists(commitTime2));\n      assertFalse(testTable.inflightCommitExists(commitTime2));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n\n      \r\n      \r\n      testTable.addInflightCommit(commitTime2).withBaseFilesInPartitions(partitionAndFileId2);\n\n      client.rollback(commitTime2);\n      assertFalse(testTable.commitExists(commitTime2));\n      assertFalse(testTable.inflightCommitExists(commitTime2));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n\n      \r\n      client.rollback(commitTime1);\n      assertFalse(testTable.commitExists(commitTime1));\n      assertFalse(testTable.inflightCommitExists(commitTime1));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestClientRollback.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":165,"status":"M"},{"authorDate":"2021-02-20 12:12:22","commitOrder":2,"curCode":"  public void testAutoRollbackInflightCommit() throws Exception {\n    \r\n    final String p1 = \"2016/05/01\";\n    final String p2 = \"2016/05/02\";\n    final String p3 = \"2016/05/06\";\n    final String commitTime1 = \"20160501010101\";\n    final String commitTime2 = \"20160502020601\";\n    final String commitTime3 = \"20160506030611\";\n    Map<String, String> partitionAndFileId1 = new HashMap<String, String>() {\n      {\n        put(p1, \"id11\");\n        put(p2, \"id12\");\n        put(p3, \"id13\");\n      }\n    };\n    Map<String, String> partitionAndFileId2 = new HashMap<String, String>() {\n      {\n        put(p1, \"id21\");\n        put(p2, \"id22\");\n        put(p3, \"id23\");\n      }\n    };\n    Map<String, String> partitionAndFileId3 = new HashMap<String, String>() {\n      {\n        put(p1, \"id31\");\n        put(p2, \"id32\");\n        put(p3, \"id33\");\n      }\n    };\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient)\n        .withPartitionMetaFiles(p1, p2, p3)\n        .addCommit(commitTime1)\n        .withBaseFilesInPartitions(partitionAndFileId1)\n        .addInflightCommit(commitTime2)\n        .withBaseFilesInPartitions(partitionAndFileId2)\n        .addInflightCommit(commitTime3)\n        .withBaseFilesInPartitions(partitionAndFileId3);\n\n    \r\n    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build())\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n            .withFailedWritesCleaningPolicy(HoodieFailedWritesCleaningPolicy.LAZY).build()).build();\n\n    final String commitTime4 = \"20160506030621\";\n    try (SparkRDDWriteClient client = getHoodieWriteClient(config)) {\n      client.startCommitWithTime(commitTime4);\n      \r\n      assertTrue(testTable.commitExists(commitTime1));\n      assertTrue(testTable.inflightCommitExists(commitTime2));\n      assertTrue(testTable.inflightCommitExists(commitTime3));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId3, commitTime3));\n    }\n\n    \r\n    config = HoodieWriteConfig.newBuilder().withPath(basePath)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build()).build();\n    final String commitTime5 = \"20160506030631\";\n    try (SparkRDDWriteClient client = getHoodieWriteClient(config)) {\n      client.startCommitWithTime(commitTime5);\n      assertTrue(testTable.commitExists(commitTime1));\n      assertFalse(testTable.inflightCommitExists(commitTime2));\n      assertFalse(testTable.inflightCommitExists(commitTime3));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId3, commitTime3));\n    }\n  }\n","date":"2021-02-20 12:12:22","endLine":322,"groupId":"10635","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testAutoRollbackInflightCommit","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/b4/b5c05dfef51059fa27e36a518558dce0484d8e.src","preCode":"  public void testAutoRollbackInflightCommit() throws Exception {\n    \r\n    final String p1 = \"2016/05/01\";\n    final String p2 = \"2016/05/02\";\n    final String p3 = \"2016/05/06\";\n    final String commitTime1 = \"20160501010101\";\n    final String commitTime2 = \"20160502020601\";\n    final String commitTime3 = \"20160506030611\";\n    Map<String, String> partitionAndFileId1 = new HashMap<String, String>() {\n      {\n        put(p1, \"id11\");\n        put(p2, \"id12\");\n        put(p3, \"id13\");\n      }\n    };\n    Map<String, String> partitionAndFileId2 = new HashMap<String, String>() {\n      {\n        put(p1, \"id21\");\n        put(p2, \"id22\");\n        put(p3, \"id23\");\n      }\n    };\n    Map<String, String> partitionAndFileId3 = new HashMap<String, String>() {\n      {\n        put(p1, \"id31\");\n        put(p2, \"id32\");\n        put(p3, \"id33\");\n      }\n    };\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient)\n        .withPartitionMetaFiles(p1, p2, p3)\n        .addCommit(commitTime1)\n        .withBaseFilesInPartitions(partitionAndFileId1)\n        .addInflightCommit(commitTime2)\n        .withBaseFilesInPartitions(partitionAndFileId2)\n        .addInflightCommit(commitTime3)\n        .withBaseFilesInPartitions(partitionAndFileId3);\n\n    \r\n    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build()).build();\n\n    final String commitTime4 = \"20160506030621\";\n    try (SparkRDDWriteClient client = getHoodieWriteClient(config, false)) {\n      client.startCommitWithTime(commitTime4);\n      \r\n      assertTrue(testTable.commitExists(commitTime1));\n      assertTrue(testTable.inflightCommitExists(commitTime2));\n      assertTrue(testTable.inflightCommitExists(commitTime3));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId3, commitTime3));\n    }\n\n    \r\n    final String commitTime5 = \"20160506030631\";\n    try (SparkRDDWriteClient client = getHoodieWriteClient(config, true)) {\n      client.startCommitWithTime(commitTime5);\n      assertTrue(testTable.commitExists(commitTime1));\n      assertFalse(testTable.inflightCommitExists(commitTime2));\n      assertFalse(testTable.inflightCommitExists(commitTime3));\n      assertTrue(testTable.baseFilesExist(partitionAndFileId1, commitTime1));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId2, commitTime2));\n      assertFalse(testTable.baseFilesExist(partitionAndFileId3, commitTime3));\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestClientRollback.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":253,"status":"M"}],"commitId":"ffcfb58bacab377bc72d20041baa54a3fd8fc812","commitMessage":"@@@[HUDI-1486] Remove inline inflight rollback in hoodie writer (#2359)\n\n1. Refactor rollback and move cleaning failed commits logic into cleaner\n2. Introduce hoodie heartbeat to ascertain failed commits\n3. Fix test cases","date":"2021-02-20 12:12:22","modifiedFileCount":"56","status":"M","submitter":"n3nash"}]
