[{"authorTime":"2020-06-09 03:09:21","codes":[{"authorDate":"2019-07-18 02:51:49","commitOrder":2,"curCode":"  public void testInputFormatLoad() throws IOException {\n    \r\n    File partitionDir = InputFormatTestUtil.prepareTable(basePath, 10, \"100\");\n    InputFormatTestUtil.commit(basePath, \"100\");\n\n    \r\n    FileInputFormat.setInputPaths(jobConf, partitionDir.getPath());\n\n    InputSplit[] inputSplits = inputFormat.getSplits(jobConf, 10);\n    assertEquals(10, inputSplits.length);\n\n    FileStatus[] files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n  }\n","date":"2020-01-09 06:53:05","endLine":76,"groupId":"5157","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testInputFormatLoad","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ed/501e70030d2fb8c72a854817bc7664a153a03a.src","preCode":"  public void testInputFormatLoad() throws IOException {\n    \r\n    File partitionDir = InputFormatTestUtil.prepareTable(basePath, 10, \"100\");\n    InputFormatTestUtil.commit(basePath, \"100\");\n\n    \r\n    FileInputFormat.setInputPaths(jobConf, partitionDir.getPath());\n\n    InputSplit[] inputSplits = inputFormat.getSplits(jobConf, 10);\n    assertEquals(10, inputSplits.length);\n\n    FileStatus[] files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n  }\n","realPath":"hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/TestHoodieParquetInputFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":63,"status":"NB"},{"authorDate":"2020-06-09 03:09:21","commitOrder":2,"curCode":"  public void testInputFormatWithCompaction() throws IOException {\n    \r\n    File partitionDir = InputFormatTestUtil.prepareTable(basePath, 10, \"100\");\n    InputFormatTestUtil.commit(basePath, \"100\");\n\n    \r\n    FileInputFormat.setInputPaths(jobConf, partitionDir.getPath());\n\n    InputSplit[] inputSplits = inputFormat.getSplits(jobConf, 10);\n    assertEquals(10, inputSplits.length);\n\n    FileStatus[] files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n\n    \r\n    createCompactionFile(basePath, \"125\");\n\n    \r\n    InputFormatTestUtil.simulateInserts(partitionDir, \"fileId2\", 5, \"200\");\n    InputFormatTestUtil.commit(basePath, \"200\");\n\n    \r\n    files = inputFormat.listStatus(jobConf);\n    assertEquals(15, files.length);\n\n    \r\n    InputFormatTestUtil.setupIncremental(jobConf, \"100\", 10);\n    files = inputFormat.listStatus(jobConf);\n    assertEquals(0, files.length,\n        \"We should exclude commit 200 when there is a pending compaction at 150\");\n  }\n","date":"2020-06-14 01:23:05","endLine":219,"groupId":"5157","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testInputFormatWithCompaction","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/6e/413cc39d734e9eec715f173988a1cdd63d0f12.src","preCode":"  public void testInputFormatWithCompaction() throws IOException {\n    \r\n    File partitionDir = InputFormatTestUtil.prepareTable(basePath, 10, \"100\");\n    InputFormatTestUtil.commit(basePath, \"100\");\n\n    \r\n    FileInputFormat.setInputPaths(jobConf, partitionDir.getPath());\n\n    InputSplit[] inputSplits = inputFormat.getSplits(jobConf, 10);\n    assertEquals(10, inputSplits.length);\n\n    FileStatus[] files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n\n    \r\n    createCompactionFile(basePath, \"125\");\n\n    \r\n    InputFormatTestUtil.simulateInserts(partitionDir, \"fileId2\", 5, \"200\");\n    InputFormatTestUtil.commit(basePath, \"200\");\n\n    \r\n    files = inputFormat.listStatus(jobConf);\n    assertEquals(15, files.length);\n\n    \r\n    InputFormatTestUtil.setupIncremental(jobConf, \"100\", 10);\n    files = inputFormat.listStatus(jobConf);\n    assertEquals(0, files.length,\n        \"We should exclude commit 200 when there is a pending compaction at 150\");\n  }\n","realPath":"hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/TestHoodieParquetInputFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":189,"status":"B"}],"commitId":"a7fd33162447b4c2816d1910bb166721cff1ed31","commitMessage":"@@@Add unit test for snapshot reads in hadoop-mr\n","date":"2020-06-14 01:23:05","modifiedFileCount":"2","status":"M","submitter":"Satish Kotha"},{"authorTime":"2020-06-26 14:46:55","codes":[{"authorDate":"2020-06-26 14:46:55","commitOrder":3,"curCode":"  public void testInputFormatLoad() throws IOException {\n    \r\n    File partitionDir = InputFormatTestUtil.prepareTable(basePath, baseFileFormat, 10, \"100\");\n    InputFormatTestUtil.commit(basePath, \"100\");\n\n    \r\n    FileInputFormat.setInputPaths(jobConf, partitionDir.getPath());\n\n    InputSplit[] inputSplits = inputFormat.getSplits(jobConf, 10);\n    assertEquals(10, inputSplits.length);\n\n    FileStatus[] files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n  }\n","date":"2020-06-26 14:46:55","endLine":162,"groupId":"1038","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testInputFormatLoad","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/91/8aade813327c4dc971ab4cc996cd8bf760f5e8.src","preCode":"  public void testInputFormatLoad() throws IOException {\n    \r\n    File partitionDir = InputFormatTestUtil.prepareTable(basePath, 10, \"100\");\n    InputFormatTestUtil.commit(basePath, \"100\");\n\n    \r\n    FileInputFormat.setInputPaths(jobConf, partitionDir.getPath());\n\n    InputSplit[] inputSplits = inputFormat.getSplits(jobConf, 10);\n    assertEquals(10, inputSplits.length);\n\n    FileStatus[] files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n  }\n","realPath":"hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/TestHoodieParquetInputFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":149,"status":"M"},{"authorDate":"2020-06-26 14:46:55","commitOrder":3,"curCode":"  public void testInputFormatWithCompaction() throws IOException {\n    \r\n    File partitionDir = InputFormatTestUtil.prepareTable(basePath, baseFileFormat, 10, \"100\");\n    InputFormatTestUtil.commit(basePath, \"100\");\n\n    \r\n    FileInputFormat.setInputPaths(jobConf, partitionDir.getPath());\n\n    InputSplit[] inputSplits = inputFormat.getSplits(jobConf, 10);\n    assertEquals(10, inputSplits.length);\n\n    FileStatus[] files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n\n    \r\n    createCompactionFile(basePath, \"125\");\n\n    \r\n    InputFormatTestUtil.simulateInserts(partitionDir, baseFileExtension, \"fileId2\", 5, \"200\");\n    InputFormatTestUtil.commit(basePath, \"200\");\n\n    \r\n    files = inputFormat.listStatus(jobConf);\n    assertEquals(15, files.length);\n\n    \r\n    InputFormatTestUtil.setupIncremental(jobConf, \"100\", 10);\n    files = inputFormat.listStatus(jobConf);\n    assertEquals(0, files.length,\n        \"We should exclude commit 200 when there is a pending compaction at 150\");\n  }\n","date":"2020-06-26 14:46:55","endLine":222,"groupId":"1038","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testInputFormatWithCompaction","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/91/8aade813327c4dc971ab4cc996cd8bf760f5e8.src","preCode":"  public void testInputFormatWithCompaction() throws IOException {\n    \r\n    File partitionDir = InputFormatTestUtil.prepareTable(basePath, 10, \"100\");\n    InputFormatTestUtil.commit(basePath, \"100\");\n\n    \r\n    FileInputFormat.setInputPaths(jobConf, partitionDir.getPath());\n\n    InputSplit[] inputSplits = inputFormat.getSplits(jobConf, 10);\n    assertEquals(10, inputSplits.length);\n\n    FileStatus[] files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n\n    \r\n    createCompactionFile(basePath, \"125\");\n\n    \r\n    InputFormatTestUtil.simulateInserts(partitionDir, \"fileId2\", 5, \"200\");\n    InputFormatTestUtil.commit(basePath, \"200\");\n\n    \r\n    files = inputFormat.listStatus(jobConf);\n    assertEquals(15, files.length);\n\n    \r\n    InputFormatTestUtil.setupIncremental(jobConf, \"100\", 10);\n    files = inputFormat.listStatus(jobConf);\n    assertEquals(0, files.length,\n        \"We should exclude commit 200 when there is a pending compaction at 150\");\n  }\n","realPath":"hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/TestHoodieParquetInputFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":192,"status":"M"}],"commitId":"2603cfb33e272632d7f36a53e1b13fe86dbb8627","commitMessage":"@@@[HUDI-684] Introduced abstraction for writing and reading different types of base file formats. (#1687)\n\nNotable changes:\n    1. HoodieFileWriter and HoodieFileReader abstractions for writer/reader side of a base file format\n    2. HoodieDataBlock abstraction for creation specific data blocks for base file formats. (e.g. Parquet has HoodieAvroDataBlock)\n    3. All hardocded references to Parquet / Parquet based classes have been abstracted to call methods which accept a base file format\n    4. HiveSyncTool accepts the base file format as a CLI parameter\n    5. HoodieDeltaStreamer accepts the base file format as a CLI parameter\n    6. HoodieSparkSqlWriter accepts the base file format as a parameter","date":"2020-06-26 14:46:55","modifiedFileCount":"42","status":"M","submitter":"Prashant Wason"}]
