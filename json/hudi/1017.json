[{"authorTime":"2020-10-02 05:25:29","codes":[{"authorDate":"2021-05-12 20:52:37","commitOrder":2,"curCode":"  public static void setProjectFieldsForInputFormat(JobConf jobConf,\n      Schema schema, String hiveColumnTypes) {\n    List<Schema.Field> fields = schema.getFields();\n    String names = fields.stream().map(f -> f.name().toString()).collect(Collectors.joining(\",\"));\n    String postions = fields.stream().map(f -> String.valueOf(f.pos())).collect(Collectors.joining(\",\"));\n    Configuration conf = HoodieTestUtils.getDefaultHadoopConf();\n\n    String hiveColumnNames = fields.stream().filter(field -> !field.name().equalsIgnoreCase(\"datestr\"))\n        .map(Schema.Field::name).collect(Collectors.joining(\",\"));\n    hiveColumnNames = hiveColumnNames + \",datestr\";\n    String modifiedHiveColumnTypes = HoodieAvroUtils.addMetadataColumnTypes(hiveColumnTypes);\n    modifiedHiveColumnTypes = modifiedHiveColumnTypes + \",string\";\n    jobConf.set(hive_metastoreConstants.META_TABLE_COLUMNS, hiveColumnNames);\n    jobConf.set(hive_metastoreConstants.META_TABLE_COLUMN_TYPES, modifiedHiveColumnTypes);\n    \r\n    jobConf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, names.split(\",\")[5]);\n    jobConf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, postions.split(\",\")[5]);\n    jobConf.set(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS, \"datestr\");\n    conf.set(hive_metastoreConstants.META_TABLE_COLUMNS, hiveColumnNames);\n    \r\n    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, names.split(\",\")[5]);\n    conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, postions.split(\",\")[5]);\n    conf.set(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS, \"datestr\");\n    conf.set(hive_metastoreConstants.META_TABLE_COLUMN_TYPES, modifiedHiveColumnTypes);\n    jobConf.addResource(conf);\n  }\n","date":"2021-05-12 20:52:37","endLine":354,"groupId":"4602","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"setProjectFieldsForInputFormat","params":"(JobConfjobConf@Schemaschema@StringhiveColumnTypes)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/f4/9ce062493c4cfb9e8d914b89abdb03c33305d4.src","preCode":"  public static void setProjectFieldsForInputFormat(JobConf jobConf,\n      Schema schema, String hiveColumnTypes) {\n    List<Schema.Field> fields = schema.getFields();\n    String names = fields.stream().map(f -> f.name().toString()).collect(Collectors.joining(\",\"));\n    String postions = fields.stream().map(f -> String.valueOf(f.pos())).collect(Collectors.joining(\",\"));\n    Configuration conf = HoodieTestUtils.getDefaultHadoopConf();\n\n    String hiveColumnNames = fields.stream().filter(field -> !field.name().equalsIgnoreCase(\"datestr\"))\n        .map(Schema.Field::name).collect(Collectors.joining(\",\"));\n    hiveColumnNames = hiveColumnNames + \",datestr\";\n    String modifiedHiveColumnTypes = HoodieAvroUtils.addMetadataColumnTypes(hiveColumnTypes);\n    modifiedHiveColumnTypes = modifiedHiveColumnTypes + \",string\";\n    jobConf.set(hive_metastoreConstants.META_TABLE_COLUMNS, hiveColumnNames);\n    jobConf.set(hive_metastoreConstants.META_TABLE_COLUMN_TYPES, modifiedHiveColumnTypes);\n    \r\n    jobConf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, names.split(\",\")[5]);\n    jobConf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, postions.split(\",\")[5]);\n    jobConf.set(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS, \"datestr\");\n    conf.set(hive_metastoreConstants.META_TABLE_COLUMNS, hiveColumnNames);\n    \r\n    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, names.split(\",\")[5]);\n    conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, postions.split(\",\")[5]);\n    conf.set(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS, \"datestr\");\n    conf.set(hive_metastoreConstants.META_TABLE_COLUMN_TYPES, modifiedHiveColumnTypes);\n    jobConf.addResource(conf);\n  }\n","realPath":"hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/testutils/InputFormatTestUtil.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":329,"status":"B"},{"authorDate":"2020-10-02 05:25:29","commitOrder":2,"curCode":"  private static void setPropsForInputFormat(FileInputFormat inputFormat, JobConf jobConf, Schema schema, String hiveColumnTypes, boolean projectCols, List<String> projectedCols) {\n    List<Schema.Field> fields = schema.getFields();\n    final List<String> projectedColNames;\n    if (!projectCols) {\n      projectedColNames = fields.stream().map(Field::name).collect(Collectors.toList());\n    } else {\n      projectedColNames = projectedCols;\n    }\n\n    String names = fields.stream()\n        .filter(f -> projectedColNames.contains(f.name()))\n        .map(f -> f.name()).collect(Collectors.joining(\",\"));\n    String positions = fields.stream()\n        .filter(f -> projectedColNames.contains(f.name()))\n        .map(f -> String.valueOf(f.pos())).collect(Collectors.joining(\",\"));\n    String hiveColumnNames = fields.stream()\n        .filter(field -> !field.name().equalsIgnoreCase(\"datestr\"))\n        .map(Schema.Field::name).collect(Collectors.joining(\",\"));\n    hiveColumnNames = hiveColumnNames + \",datestr\";\n\n    Configuration conf = HoodieTestUtils.getDefaultHadoopConf();\n    String hiveColumnTypesWithDatestr = hiveColumnTypes + \",string\";\n    jobConf.set(hive_metastoreConstants.META_TABLE_COLUMNS, hiveColumnNames);\n    jobConf.set(hive_metastoreConstants.META_TABLE_COLUMN_TYPES, hiveColumnTypesWithDatestr);\n    jobConf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, names);\n    jobConf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, positions);\n    jobConf.set(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS, \"datestr\");\n    conf.set(hive_metastoreConstants.META_TABLE_COLUMNS, hiveColumnNames);\n    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, names);\n    conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, positions);\n    conf.set(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS, \"datestr\");\n    conf.set(hive_metastoreConstants.META_TABLE_COLUMN_TYPES, hiveColumnTypesWithDatestr);\n    conf.set(IOConstants.COLUMNS, hiveColumnNames);\n    conf.get(IOConstants.COLUMNS_TYPES, hiveColumnTypesWithDatestr);\n\n    \r\n    Configurable configurable = (Configurable)inputFormat;\n    configurable.setConf(conf);\n    jobConf.addResource(conf);\n  }\n","date":"2020-10-02 05:25:29","endLine":158,"groupId":"4560","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"setPropsForInputFormat","params":"(FileInputFormatinputFormat@JobConfjobConf@Schemaschema@StringhiveColumnTypes@booleanprojectCols@List<String>projectedCols)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/81/04ef7744fce7889661a9988e07e296777e5a40.src","preCode":"  private static void setPropsForInputFormat(FileInputFormat inputFormat, JobConf jobConf, Schema schema, String hiveColumnTypes, boolean projectCols, List<String> projectedCols) {\n    List<Schema.Field> fields = schema.getFields();\n    final List<String> projectedColNames;\n    if (!projectCols) {\n      projectedColNames = fields.stream().map(Field::name).collect(Collectors.toList());\n    } else {\n      projectedColNames = projectedCols;\n    }\n\n    String names = fields.stream()\n        .filter(f -> projectedColNames.contains(f.name()))\n        .map(f -> f.name()).collect(Collectors.joining(\",\"));\n    String positions = fields.stream()\n        .filter(f -> projectedColNames.contains(f.name()))\n        .map(f -> String.valueOf(f.pos())).collect(Collectors.joining(\",\"));\n    String hiveColumnNames = fields.stream()\n        .filter(field -> !field.name().equalsIgnoreCase(\"datestr\"))\n        .map(Schema.Field::name).collect(Collectors.joining(\",\"));\n    hiveColumnNames = hiveColumnNames + \",datestr\";\n\n    Configuration conf = HoodieTestUtils.getDefaultHadoopConf();\n    String hiveColumnTypesWithDatestr = hiveColumnTypes + \",string\";\n    jobConf.set(hive_metastoreConstants.META_TABLE_COLUMNS, hiveColumnNames);\n    jobConf.set(hive_metastoreConstants.META_TABLE_COLUMN_TYPES, hiveColumnTypesWithDatestr);\n    jobConf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, names);\n    jobConf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, positions);\n    jobConf.set(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS, \"datestr\");\n    conf.set(hive_metastoreConstants.META_TABLE_COLUMNS, hiveColumnNames);\n    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, names);\n    conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, positions);\n    conf.set(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS, \"datestr\");\n    conf.set(hive_metastoreConstants.META_TABLE_COLUMN_TYPES, hiveColumnTypesWithDatestr);\n    conf.set(IOConstants.COLUMNS, hiveColumnNames);\n    conf.get(IOConstants.COLUMNS_TYPES, hiveColumnTypesWithDatestr);\n\n    \r\n    Configurable configurable = (Configurable)inputFormat;\n    configurable.setConf(conf);\n    jobConf.addResource(conf);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieMergeOnReadTestUtils.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":119,"status":"NB"}],"commitId":"6f7ff7e8ca3a83d858561dd08f8f093787caa9b2","commitMessage":"@@@[HUDI-1722]Fix hive beeline/spark-sql query specified field on mor table occur NPE (#2722)\n\n","date":"2021-05-12 20:52:37","modifiedFileCount":"4","status":"M","submitter":"xiarixiaoyao"},{"authorTime":"2021-08-02 21:45:09","codes":[{"authorDate":"2021-05-12 20:52:37","commitOrder":3,"curCode":"  public static void setProjectFieldsForInputFormat(JobConf jobConf,\n      Schema schema, String hiveColumnTypes) {\n    List<Schema.Field> fields = schema.getFields();\n    String names = fields.stream().map(f -> f.name().toString()).collect(Collectors.joining(\",\"));\n    String postions = fields.stream().map(f -> String.valueOf(f.pos())).collect(Collectors.joining(\",\"));\n    Configuration conf = HoodieTestUtils.getDefaultHadoopConf();\n\n    String hiveColumnNames = fields.stream().filter(field -> !field.name().equalsIgnoreCase(\"datestr\"))\n        .map(Schema.Field::name).collect(Collectors.joining(\",\"));\n    hiveColumnNames = hiveColumnNames + \",datestr\";\n    String modifiedHiveColumnTypes = HoodieAvroUtils.addMetadataColumnTypes(hiveColumnTypes);\n    modifiedHiveColumnTypes = modifiedHiveColumnTypes + \",string\";\n    jobConf.set(hive_metastoreConstants.META_TABLE_COLUMNS, hiveColumnNames);\n    jobConf.set(hive_metastoreConstants.META_TABLE_COLUMN_TYPES, modifiedHiveColumnTypes);\n    \r\n    jobConf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, names.split(\",\")[5]);\n    jobConf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, postions.split(\",\")[5]);\n    jobConf.set(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS, \"datestr\");\n    conf.set(hive_metastoreConstants.META_TABLE_COLUMNS, hiveColumnNames);\n    \r\n    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, names.split(\",\")[5]);\n    conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, postions.split(\",\")[5]);\n    conf.set(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS, \"datestr\");\n    conf.set(hive_metastoreConstants.META_TABLE_COLUMN_TYPES, modifiedHiveColumnTypes);\n    jobConf.addResource(conf);\n  }\n","date":"2021-05-12 20:52:37","endLine":354,"groupId":"1017","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"setProjectFieldsForInputFormat","params":"(JobConfjobConf@Schemaschema@StringhiveColumnTypes)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/f4/9ce062493c4cfb9e8d914b89abdb03c33305d4.src","preCode":"  public static void setProjectFieldsForInputFormat(JobConf jobConf,\n      Schema schema, String hiveColumnTypes) {\n    List<Schema.Field> fields = schema.getFields();\n    String names = fields.stream().map(f -> f.name().toString()).collect(Collectors.joining(\",\"));\n    String postions = fields.stream().map(f -> String.valueOf(f.pos())).collect(Collectors.joining(\",\"));\n    Configuration conf = HoodieTestUtils.getDefaultHadoopConf();\n\n    String hiveColumnNames = fields.stream().filter(field -> !field.name().equalsIgnoreCase(\"datestr\"))\n        .map(Schema.Field::name).collect(Collectors.joining(\",\"));\n    hiveColumnNames = hiveColumnNames + \",datestr\";\n    String modifiedHiveColumnTypes = HoodieAvroUtils.addMetadataColumnTypes(hiveColumnTypes);\n    modifiedHiveColumnTypes = modifiedHiveColumnTypes + \",string\";\n    jobConf.set(hive_metastoreConstants.META_TABLE_COLUMNS, hiveColumnNames);\n    jobConf.set(hive_metastoreConstants.META_TABLE_COLUMN_TYPES, modifiedHiveColumnTypes);\n    \r\n    jobConf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, names.split(\",\")[5]);\n    jobConf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, postions.split(\",\")[5]);\n    jobConf.set(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS, \"datestr\");\n    conf.set(hive_metastoreConstants.META_TABLE_COLUMNS, hiveColumnNames);\n    \r\n    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, names.split(\",\")[5]);\n    conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, postions.split(\",\")[5]);\n    conf.set(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS, \"datestr\");\n    conf.set(hive_metastoreConstants.META_TABLE_COLUMN_TYPES, modifiedHiveColumnTypes);\n    jobConf.addResource(conf);\n  }\n","realPath":"hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/testutils/InputFormatTestUtil.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":329,"status":"N"},{"authorDate":"2021-08-02 21:45:09","commitOrder":3,"curCode":"  private static void setPropsForInputFormat(FileInputFormat inputFormat, JobConf jobConf, Schema schema, String hiveColumnTypes, boolean projectCols, List<String> projectedCols,\n                                             boolean populateMetaFieldsConfigValue) {\n    List<Schema.Field> fields = schema.getFields();\n    final List<String> projectedColNames;\n    if (!projectCols) {\n      projectedColNames = fields.stream().map(Field::name).collect(Collectors.toList());\n    } else {\n      projectedColNames = projectedCols;\n    }\n\n    String names = fields.stream()\n        .filter(f -> projectedColNames.contains(f.name()))\n        .map(f -> f.name()).collect(Collectors.joining(\",\"));\n    String positions = fields.stream()\n        .filter(f -> projectedColNames.contains(f.name()))\n        .map(f -> String.valueOf(f.pos())).collect(Collectors.joining(\",\"));\n    String hiveColumnNames = fields.stream()\n        .filter(field -> !field.name().equalsIgnoreCase(\"datestr\"))\n        .map(Schema.Field::name).collect(Collectors.joining(\",\"));\n    hiveColumnNames = hiveColumnNames + \",datestr\";\n\n    Configuration conf = HoodieTestUtils.getDefaultHadoopConf();\n    String hiveColumnTypesWithDatestr = hiveColumnTypes + \",string\";\n    jobConf.set(hive_metastoreConstants.META_TABLE_COLUMNS, hiveColumnNames);\n    jobConf.set(hive_metastoreConstants.META_TABLE_COLUMN_TYPES, hiveColumnTypesWithDatestr);\n    jobConf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, names);\n    jobConf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, positions);\n    jobConf.set(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS, \"datestr\");\n    conf.set(hive_metastoreConstants.META_TABLE_COLUMNS, hiveColumnNames);\n    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, names);\n    conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, positions);\n    conf.set(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS, \"datestr\");\n    conf.set(hive_metastoreConstants.META_TABLE_COLUMN_TYPES, hiveColumnTypesWithDatestr);\n    conf.set(IOConstants.COLUMNS, hiveColumnNames);\n    conf.get(IOConstants.COLUMNS_TYPES, hiveColumnTypesWithDatestr);\n\n    \r\n    Configurable configurable = (Configurable)inputFormat;\n    configurable.setConf(conf);\n    jobConf.addResource(conf);\n  }\n","date":"2021-08-02 21:45:09","endLine":168,"groupId":"1017","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"setPropsForInputFormat","params":"(FileInputFormatinputFormat@JobConfjobConf@Schemaschema@StringhiveColumnTypes@booleanprojectCols@List<String>projectedCols@booleanpopulateMetaFieldsConfigValue)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/8c/d68ff8335d84d9aa6827c37763297c3435defc.src","preCode":"  private static void setPropsForInputFormat(FileInputFormat inputFormat, JobConf jobConf, Schema schema, String hiveColumnTypes, boolean projectCols, List<String> projectedCols) {\n    List<Schema.Field> fields = schema.getFields();\n    final List<String> projectedColNames;\n    if (!projectCols) {\n      projectedColNames = fields.stream().map(Field::name).collect(Collectors.toList());\n    } else {\n      projectedColNames = projectedCols;\n    }\n\n    String names = fields.stream()\n        .filter(f -> projectedColNames.contains(f.name()))\n        .map(f -> f.name()).collect(Collectors.joining(\",\"));\n    String positions = fields.stream()\n        .filter(f -> projectedColNames.contains(f.name()))\n        .map(f -> String.valueOf(f.pos())).collect(Collectors.joining(\",\"));\n    String hiveColumnNames = fields.stream()\n        .filter(field -> !field.name().equalsIgnoreCase(\"datestr\"))\n        .map(Schema.Field::name).collect(Collectors.joining(\",\"));\n    hiveColumnNames = hiveColumnNames + \",datestr\";\n\n    Configuration conf = HoodieTestUtils.getDefaultHadoopConf();\n    String hiveColumnTypesWithDatestr = hiveColumnTypes + \",string\";\n    jobConf.set(hive_metastoreConstants.META_TABLE_COLUMNS, hiveColumnNames);\n    jobConf.set(hive_metastoreConstants.META_TABLE_COLUMN_TYPES, hiveColumnTypesWithDatestr);\n    jobConf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, names);\n    jobConf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, positions);\n    jobConf.set(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS, \"datestr\");\n    conf.set(hive_metastoreConstants.META_TABLE_COLUMNS, hiveColumnNames);\n    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, names);\n    conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, positions);\n    conf.set(hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS, \"datestr\");\n    conf.set(hive_metastoreConstants.META_TABLE_COLUMN_TYPES, hiveColumnTypesWithDatestr);\n    conf.set(IOConstants.COLUMNS, hiveColumnNames);\n    conf.get(IOConstants.COLUMNS_TYPES, hiveColumnTypesWithDatestr);\n\n    \r\n    Configurable configurable = (Configurable)inputFormat;\n    configurable.setConf(conf);\n    jobConf.addResource(conf);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieMergeOnReadTestUtils.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":128,"status":"M"}],"commitId":"fe508376faaa3c36e9f821f3d6fee731983798c4","commitMessage":"@@@[HUDI-2177][HUDI-2200] Adding virtual keys support for MOR table (#3315)\n\n","date":"2021-08-02 21:45:09","modifiedFileCount":"30","status":"M","submitter":"Sivabalan Narayanan"}]
