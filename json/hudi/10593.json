[{"authorTime":"2021-08-15 08:20:23","codes":[{"authorDate":"2021-08-15 08:20:23","commitOrder":1,"curCode":"  public void testUpgradeZeroToOneInternal(boolean induceResiduesFromPrevUpgrade, boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n    \r\n    Map<String, String> params = new HashMap<>();\n    if (tableType == HoodieTableType.MERGE_ON_READ) {\n      params.put(HOODIE_TABLE_TYPE_PROP.key(), HoodieTableType.MERGE_ON_READ.name());\n      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    }\n    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n\n    \r\n    List<FileSlice> firstPartitionCommit2FileSlices = new ArrayList<>();\n    List<FileSlice> secondPartitionCommit2FileSlices = new ArrayList<>();\n    Pair<List<HoodieRecord>, List<HoodieRecord>> inputRecords = twoUpsertCommitDataWithTwoPartitions(firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices, cfg, client, false);\n\n    HoodieTable table = this.getHoodieTable(metaClient, cfg);\n    HoodieInstant commitInstant = table.getPendingCommitTimeline().lastInstant().get();\n\n    \r\n    WriteMarkers writeMarkers =\n        WriteMarkersFactory.get(getConfig().getMarkersType(), table, commitInstant.getTimestamp());\n    List<String> markerPaths = new ArrayList<>(writeMarkers.allMarkerFilePaths());\n    if (deletePartialMarkerFiles) {\n      String toDeleteMarkerFile = markerPaths.get(0);\n      table.getMetaClient().getFs().delete(new Path(table.getMetaClient().getTempFolderPath() + \"/\" + commitInstant.getTimestamp() + \"/\" + toDeleteMarkerFile));\n      markerPaths.remove(toDeleteMarkerFile);\n    }\n\n    \r\n    metaClient.getTableConfig().setTableVersion(HoodieTableVersion.ZERO);\n\n    if (induceResiduesFromPrevUpgrade) {\n      createResidualFile();\n    }\n\n    \r\n    new SparkUpgradeDowngrade(metaClient, cfg, context).run(metaClient, HoodieTableVersion.ONE, cfg, context, null);\n\n    \r\n    assertMarkerFilesForUpgrade(table, commitInstant, firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices);\n\n    \r\n    metaClient = HoodieTableMetaClient.builder().setConf(context.getHadoopConf().get()).setBasePath(cfg.getBasePath())\n        .setLayoutVersion(Option.of(new TimelineLayoutVersion(cfg.getTimelineLayoutVersion()))).build();\n    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.ONE.versionCode());\n    assertTableVersionFromPropertyFile(HoodieTableVersion.ONE);\n\n    \r\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","date":"2021-08-15 08:20:23","endLine":200,"groupId":"2841","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpgradeZeroToOneInternal","params":"(booleaninduceResiduesFromPrevUpgrade@booleandeletePartialMarkerFiles@HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/68/876d79bd887c5e8e266c9eed5f220122d47d65.src","preCode":"  public void testUpgradeZeroToOneInternal(boolean induceResiduesFromPrevUpgrade, boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n    \r\n    Map<String, String> params = new HashMap<>();\n    if (tableType == HoodieTableType.MERGE_ON_READ) {\n      params.put(HOODIE_TABLE_TYPE_PROP.key(), HoodieTableType.MERGE_ON_READ.name());\n      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    }\n    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n\n    \r\n    List<FileSlice> firstPartitionCommit2FileSlices = new ArrayList<>();\n    List<FileSlice> secondPartitionCommit2FileSlices = new ArrayList<>();\n    Pair<List<HoodieRecord>, List<HoodieRecord>> inputRecords = twoUpsertCommitDataWithTwoPartitions(firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices, cfg, client, false);\n\n    HoodieTable table = this.getHoodieTable(metaClient, cfg);\n    HoodieInstant commitInstant = table.getPendingCommitTimeline().lastInstant().get();\n\n    \r\n    WriteMarkers writeMarkers =\n        WriteMarkersFactory.get(getConfig().getMarkersType(), table, commitInstant.getTimestamp());\n    List<String> markerPaths = new ArrayList<>(writeMarkers.allMarkerFilePaths());\n    if (deletePartialMarkerFiles) {\n      String toDeleteMarkerFile = markerPaths.get(0);\n      table.getMetaClient().getFs().delete(new Path(table.getMetaClient().getTempFolderPath() + \"/\" + commitInstant.getTimestamp() + \"/\" + toDeleteMarkerFile));\n      markerPaths.remove(toDeleteMarkerFile);\n    }\n\n    \r\n    metaClient.getTableConfig().setTableVersion(HoodieTableVersion.ZERO);\n\n    if (induceResiduesFromPrevUpgrade) {\n      createResidualFile();\n    }\n\n    \r\n    new SparkUpgradeDowngrade(metaClient, cfg, context).run(metaClient, HoodieTableVersion.ONE, cfg, context, null);\n\n    \r\n    assertMarkerFilesForUpgrade(table, commitInstant, firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices);\n\n    \r\n    metaClient = HoodieTableMetaClient.builder().setConf(context.getHadoopConf().get()).setBasePath(cfg.getBasePath())\n        .setLayoutVersion(Option.of(new TimelineLayoutVersion(cfg.getTimelineLayoutVersion()))).build();\n    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.ONE.versionCode());\n    assertTableVersionFromPropertyFile(HoodieTableVersion.ONE);\n\n    \r\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":144,"status":"B"},{"authorDate":"2021-08-15 08:20:23","commitOrder":1,"curCode":"  public void testUpgradeOneToTwo(HoodieTableType tableType) throws IOException {\n    \r\n    Map<String, String> params = new HashMap<>();\n    addNewTableParamsToProps(params);\n    if (tableType == HoodieTableType.MERGE_ON_READ) {\n      params.put(HOODIE_TABLE_TYPE_PROP.key(), HoodieTableType.MERGE_ON_READ.name());\n      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    }\n    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n    \r\n    doInsert(client);\n\n    \r\n    downgradeTableConfigsFromTwoToOne(cfg);\n\n    \r\n    new SparkUpgradeDowngrade(metaClient, cfg, context).run(metaClient, HoodieTableVersion.TWO, cfg, context, null);\n\n    \r\n    metaClient = HoodieTableMetaClient.builder().setConf(context.getHadoopConf().get()).setBasePath(cfg.getBasePath())\n        .setLayoutVersion(Option.of(new TimelineLayoutVersion(cfg.getTimelineLayoutVersion()))).build();\n    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.TWO.versionCode());\n    assertTableVersionFromPropertyFile(HoodieTableVersion.TWO);\n\n    \r\n    assertTableProps(cfg);\n  }\n","date":"2021-08-15 08:20:23","endLine":231,"groupId":"6004","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testUpgradeOneToTwo","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/68/876d79bd887c5e8e266c9eed5f220122d47d65.src","preCode":"  public void testUpgradeOneToTwo(HoodieTableType tableType) throws IOException {\n    \r\n    Map<String, String> params = new HashMap<>();\n    addNewTableParamsToProps(params);\n    if (tableType == HoodieTableType.MERGE_ON_READ) {\n      params.put(HOODIE_TABLE_TYPE_PROP.key(), HoodieTableType.MERGE_ON_READ.name());\n      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    }\n    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n    \r\n    doInsert(client);\n\n    \r\n    downgradeTableConfigsFromTwoToOne(cfg);\n\n    \r\n    new SparkUpgradeDowngrade(metaClient, cfg, context).run(metaClient, HoodieTableVersion.TWO, cfg, context, null);\n\n    \r\n    metaClient = HoodieTableMetaClient.builder().setConf(context.getHadoopConf().get()).setBasePath(cfg.getBasePath())\n        .setLayoutVersion(Option.of(new TimelineLayoutVersion(cfg.getTimelineLayoutVersion()))).build();\n    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.TWO.versionCode());\n    assertTableVersionFromPropertyFile(HoodieTableVersion.TWO);\n\n    \r\n    assertTableProps(cfg);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":204,"status":"B"}],"commitId":"23dca6c237517bc9d9407556af6b1042f09ae76b","commitMessage":"@@@[HUDI-2268] Add upgrade and downgrade to and from 0.9.0 (#3470)\n\n- Added upgrade and downgrade step to and from 0.9.0. Upgrade adds few table properties. Downgrade recreates timeline server based marker files if any. ","date":"2021-08-15 08:20:23","modifiedFileCount":"16","status":"B","submitter":"Y Ethan Guo"},{"authorTime":"2021-08-20 04:36:40","codes":[{"authorDate":"2021-08-20 04:36:40","commitOrder":2,"curCode":"  public void testUpgradeZeroToOneInternal(boolean induceResiduesFromPrevUpgrade, boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n    \r\n    Map<String, String> params = new HashMap<>();\n    if (tableType == HoodieTableType.MERGE_ON_READ) {\n      params.put(TYPE.key(), HoodieTableType.MERGE_ON_READ.name());\n      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    }\n    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n\n    \r\n    List<FileSlice> firstPartitionCommit2FileSlices = new ArrayList<>();\n    List<FileSlice> secondPartitionCommit2FileSlices = new ArrayList<>();\n    Pair<List<HoodieRecord>, List<HoodieRecord>> inputRecords = twoUpsertCommitDataWithTwoPartitions(firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices, cfg, client, false);\n\n    HoodieTable table = this.getHoodieTable(metaClient, cfg);\n    HoodieInstant commitInstant = table.getPendingCommitTimeline().lastInstant().get();\n\n    \r\n    WriteMarkers writeMarkers =\n        WriteMarkersFactory.get(getConfig().getMarkersType(), table, commitInstant.getTimestamp());\n    List<String> markerPaths = new ArrayList<>(writeMarkers.allMarkerFilePaths());\n    if (deletePartialMarkerFiles) {\n      String toDeleteMarkerFile = markerPaths.get(0);\n      table.getMetaClient().getFs().delete(new Path(table.getMetaClient().getTempFolderPath() + \"/\" + commitInstant.getTimestamp() + \"/\" + toDeleteMarkerFile));\n      markerPaths.remove(toDeleteMarkerFile);\n    }\n\n    \r\n    metaClient.getTableConfig().setTableVersion(HoodieTableVersion.ZERO);\n\n    if (induceResiduesFromPrevUpgrade) {\n      createResidualFile();\n    }\n\n    \r\n    new SparkUpgradeDowngrade(metaClient, cfg, context).run(metaClient, HoodieTableVersion.ONE, cfg, context, null);\n\n    \r\n    assertMarkerFilesForUpgrade(table, commitInstant, firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices);\n\n    \r\n    metaClient = HoodieTableMetaClient.builder().setConf(context.getHadoopConf().get()).setBasePath(cfg.getBasePath())\n        .setLayoutVersion(Option.of(new TimelineLayoutVersion(cfg.getTimelineLayoutVersion()))).build();\n    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.ONE.versionCode());\n    assertTableVersionFromPropertyFile(HoodieTableVersion.ONE);\n\n    \r\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","date":"2021-08-20 04:36:40","endLine":200,"groupId":"10593","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpgradeZeroToOneInternal","params":"(booleaninduceResiduesFromPrevUpgrade@booleandeletePartialMarkerFiles@HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/12/3a33c3d462bbb7017e875bbc7d5571941119de.src","preCode":"  public void testUpgradeZeroToOneInternal(boolean induceResiduesFromPrevUpgrade, boolean deletePartialMarkerFiles, HoodieTableType tableType) throws IOException {\n    \r\n    Map<String, String> params = new HashMap<>();\n    if (tableType == HoodieTableType.MERGE_ON_READ) {\n      params.put(HOODIE_TABLE_TYPE_PROP.key(), HoodieTableType.MERGE_ON_READ.name());\n      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    }\n    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n\n    \r\n    List<FileSlice> firstPartitionCommit2FileSlices = new ArrayList<>();\n    List<FileSlice> secondPartitionCommit2FileSlices = new ArrayList<>();\n    Pair<List<HoodieRecord>, List<HoodieRecord>> inputRecords = twoUpsertCommitDataWithTwoPartitions(firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices, cfg, client, false);\n\n    HoodieTable table = this.getHoodieTable(metaClient, cfg);\n    HoodieInstant commitInstant = table.getPendingCommitTimeline().lastInstant().get();\n\n    \r\n    WriteMarkers writeMarkers =\n        WriteMarkersFactory.get(getConfig().getMarkersType(), table, commitInstant.getTimestamp());\n    List<String> markerPaths = new ArrayList<>(writeMarkers.allMarkerFilePaths());\n    if (deletePartialMarkerFiles) {\n      String toDeleteMarkerFile = markerPaths.get(0);\n      table.getMetaClient().getFs().delete(new Path(table.getMetaClient().getTempFolderPath() + \"/\" + commitInstant.getTimestamp() + \"/\" + toDeleteMarkerFile));\n      markerPaths.remove(toDeleteMarkerFile);\n    }\n\n    \r\n    metaClient.getTableConfig().setTableVersion(HoodieTableVersion.ZERO);\n\n    if (induceResiduesFromPrevUpgrade) {\n      createResidualFile();\n    }\n\n    \r\n    new SparkUpgradeDowngrade(metaClient, cfg, context).run(metaClient, HoodieTableVersion.ONE, cfg, context, null);\n\n    \r\n    assertMarkerFilesForUpgrade(table, commitInstant, firstPartitionCommit2FileSlices, secondPartitionCommit2FileSlices);\n\n    \r\n    metaClient = HoodieTableMetaClient.builder().setConf(context.getHadoopConf().get()).setBasePath(cfg.getBasePath())\n        .setLayoutVersion(Option.of(new TimelineLayoutVersion(cfg.getTimelineLayoutVersion()))).build();\n    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.ONE.versionCode());\n    assertTableVersionFromPropertyFile(HoodieTableVersion.ONE);\n\n    \r\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":144,"status":"M"},{"authorDate":"2021-08-20 04:36:40","commitOrder":2,"curCode":"  public void testUpgradeOneToTwo(HoodieTableType tableType) throws IOException {\n    \r\n    Map<String, String> params = new HashMap<>();\n    addNewTableParamsToProps(params);\n    if (tableType == HoodieTableType.MERGE_ON_READ) {\n      params.put(TYPE.key(), HoodieTableType.MERGE_ON_READ.name());\n      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    }\n    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n    \r\n    doInsert(client);\n\n    \r\n    downgradeTableConfigsFromTwoToOne(cfg);\n\n    \r\n    new SparkUpgradeDowngrade(metaClient, cfg, context).run(metaClient, HoodieTableVersion.TWO, cfg, context, null);\n\n    \r\n    metaClient = HoodieTableMetaClient.builder().setConf(context.getHadoopConf().get()).setBasePath(cfg.getBasePath())\n        .setLayoutVersion(Option.of(new TimelineLayoutVersion(cfg.getTimelineLayoutVersion()))).build();\n    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.TWO.versionCode());\n    assertTableVersionFromPropertyFile(HoodieTableVersion.TWO);\n\n    \r\n    assertTableProps(cfg);\n  }\n","date":"2021-08-20 04:36:40","endLine":231,"groupId":"10593","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testUpgradeOneToTwo","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/12/3a33c3d462bbb7017e875bbc7d5571941119de.src","preCode":"  public void testUpgradeOneToTwo(HoodieTableType tableType) throws IOException {\n    \r\n    Map<String, String> params = new HashMap<>();\n    addNewTableParamsToProps(params);\n    if (tableType == HoodieTableType.MERGE_ON_READ) {\n      params.put(HOODIE_TABLE_TYPE_PROP.key(), HoodieTableType.MERGE_ON_READ.name());\n      metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    }\n    HoodieWriteConfig cfg = getConfigBuilder().withAutoCommit(false).withRollbackUsingMarkers(false).withProps(params).build();\n    SparkRDDWriteClient client = getHoodieWriteClient(cfg);\n    \r\n    doInsert(client);\n\n    \r\n    downgradeTableConfigsFromTwoToOne(cfg);\n\n    \r\n    new SparkUpgradeDowngrade(metaClient, cfg, context).run(metaClient, HoodieTableVersion.TWO, cfg, context, null);\n\n    \r\n    metaClient = HoodieTableMetaClient.builder().setConf(context.getHadoopConf().get()).setBasePath(cfg.getBasePath())\n        .setLayoutVersion(Option.of(new TimelineLayoutVersion(cfg.getTimelineLayoutVersion()))).build();\n    assertEquals(metaClient.getTableConfig().getTableVersion().versionCode(), HoodieTableVersion.TWO.versionCode());\n    assertTableVersionFromPropertyFile(HoodieTableVersion.TWO);\n\n    \r\n    assertTableProps(cfg);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/upgrade/TestUpgradeDowngrade.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":204,"status":"M"}],"commitId":"c350d05dd3301f14fa9d688746c9de2416db3f11","commitMessage":"@@@Restore 0.8.0 config keys with deprecated annotation (#3506)\n\nCo-authored-by: Sagar Sumit <sagarsumit09@gmail.com>\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2021-08-20 04:36:40","modifiedFileCount":"109","status":"M","submitter":"Udit Mehrotra"}]
