[{"authorTime":"2020-05-14 06:37:03","codes":[{"authorDate":"2020-05-14 06:37:03","commitOrder":1,"curCode":"  public void testImportWithRetries() throws Exception {\n    try (JavaSparkContext jsc = getJavaSparkContext()) {\n      \r\n      String schemaFile = new Path(basePath, \"file.schema\").toString();\n\n      HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n          \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n      AtomicInteger retry = new AtomicInteger(3);\n      AtomicInteger fileCreated = new AtomicInteger(0);\n      HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg) {\n        @Override\n        protected int dataImport(JavaSparkContext jsc) throws IOException {\n          int ret = super.dataImport(jsc);\n          if (retry.decrementAndGet() == 0) {\n            fileCreated.incrementAndGet();\n            createSchemaFile(schemaFile);\n          }\n\n          return ret;\n        }\n      };\n      \r\n      assertEquals(0, dataImporter.dataImport(jsc, retry.get()));\n      assertEquals(-1, retry.get());\n      assertEquals(1, fileCreated.get());\n\n      \r\n      \r\n      \r\n      \r\n      boolean isCommitFilePresent = false;\n      Map<String, Long> recordCounts = new HashMap<String, Long>();\n      RemoteIterator<LocatedFileStatus> hoodieFiles = dfs.listFiles(hoodieFolder, true);\n      while (hoodieFiles.hasNext()) {\n        LocatedFileStatus f = hoodieFiles.next();\n        isCommitFilePresent = isCommitFilePresent || f.getPath().toString().endsWith(HoodieTimeline.COMMIT_EXTENSION);\n\n        if (f.getPath().toString().endsWith(\"parquet\")) {\n          SQLContext sc = new SQLContext(jsc);\n          String partitionPath = f.getPath().getParent().toString();\n          long count = sc.read().parquet(f.getPath().toString()).count();\n          if (!recordCounts.containsKey(partitionPath)) {\n            recordCounts.put(partitionPath, 0L);\n          }\n          recordCounts.put(partitionPath, recordCounts.get(partitionPath) + count);\n        }\n      }\n      assertTrue(isCommitFilePresent, \"commit file is missing\");\n      assertEquals(4, recordCounts.size(), \"partition is missing\");\n      for (Entry<String, Long> e : recordCounts.entrySet()) {\n        assertEquals(24, e.getValue().longValue(), \"missing records\");\n      }\n    }\n  }\n","date":"2020-05-14 06:37:03","endLine":171,"groupId":"1214","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testImportWithRetries","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/f7/38cdfdece690b977ec31850ebdf81d1d20a181.src","preCode":"  public void testImportWithRetries() throws Exception {\n    try (JavaSparkContext jsc = getJavaSparkContext()) {\n      \r\n      String schemaFile = new Path(basePath, \"file.schema\").toString();\n\n      HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n          \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n      AtomicInteger retry = new AtomicInteger(3);\n      AtomicInteger fileCreated = new AtomicInteger(0);\n      HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg) {\n        @Override\n        protected int dataImport(JavaSparkContext jsc) throws IOException {\n          int ret = super.dataImport(jsc);\n          if (retry.decrementAndGet() == 0) {\n            fileCreated.incrementAndGet();\n            createSchemaFile(schemaFile);\n          }\n\n          return ret;\n        }\n      };\n      \r\n      assertEquals(0, dataImporter.dataImport(jsc, retry.get()));\n      assertEquals(-1, retry.get());\n      assertEquals(1, fileCreated.get());\n\n      \r\n      \r\n      \r\n      \r\n      boolean isCommitFilePresent = false;\n      Map<String, Long> recordCounts = new HashMap<String, Long>();\n      RemoteIterator<LocatedFileStatus> hoodieFiles = dfs.listFiles(hoodieFolder, true);\n      while (hoodieFiles.hasNext()) {\n        LocatedFileStatus f = hoodieFiles.next();\n        isCommitFilePresent = isCommitFilePresent || f.getPath().toString().endsWith(HoodieTimeline.COMMIT_EXTENSION);\n\n        if (f.getPath().toString().endsWith(\"parquet\")) {\n          SQLContext sc = new SQLContext(jsc);\n          String partitionPath = f.getPath().getParent().toString();\n          long count = sc.read().parquet(f.getPath().toString()).count();\n          if (!recordCounts.containsKey(partitionPath)) {\n            recordCounts.put(partitionPath, 0L);\n          }\n          recordCounts.put(partitionPath, recordCounts.get(partitionPath) + count);\n        }\n      }\n      assertTrue(isCommitFilePresent, \"commit file is missing\");\n      assertEquals(4, recordCounts.size(), \"partition is missing\");\n      for (Entry<String, Long> e : recordCounts.entrySet()) {\n        assertEquals(24, e.getValue().longValue(), \"missing records\");\n      }\n    }\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":118,"status":"B"},{"authorDate":"2020-05-14 06:37:03","commitOrder":1,"curCode":"  private void insert(JavaSparkContext jsc) throws IOException {\n    \r\n    String schemaFile = new Path(basePath, \"file.schema\").toString();\n    createSchemaFile(schemaFile);\n\n    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n\n    dataImporter.dataImport(jsc, 0);\n  }\n","date":"2020-05-14 06:37:03","endLine":183,"groupId":"1936","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"insert","params":"(JavaSparkContextjsc)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/f7/38cdfdece690b977ec31850ebdf81d1d20a181.src","preCode":"  private void insert(JavaSparkContext jsc) throws IOException {\n    \r\n    String schemaFile = new Path(basePath, \"file.schema\").toString();\n    createSchemaFile(schemaFile);\n\n    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n\n    dataImporter.dataImport(jsc, 0);\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":173,"status":"B"}],"commitId":"0d4848b68b625a17d05b38864a84a6cc71189bfa","commitMessage":"@@@[HUDI-811] Restructure test packages (#1607)\n\n* restructure hudi-spark tests\n* restructure hudi-timeline-service tests\n* restructure hudi-hadoop-mr hudi-utilities tests\n* restructure hudi-hive-sync tests","date":"2020-05-14 06:37:03","modifiedFileCount":"11","status":"B","submitter":"Raymond Xu"},{"authorTime":"2020-05-14 06:37:03","codes":[{"authorDate":"2020-07-06 07:44:31","commitOrder":2,"curCode":"  public void testImportWithRetries() throws Exception {\n    \r\n    String schemaFile = new Path(basePath, \"file.schema\").toString();\n\n    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n    AtomicInteger retry = new AtomicInteger(3);\n    AtomicInteger fileCreated = new AtomicInteger(0);\n    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg) {\n      @Override\n      protected int dataImport(JavaSparkContext jsc) throws IOException {\n        int ret = super.dataImport(jsc);\n        if (retry.decrementAndGet() == 0) {\n          fileCreated.incrementAndGet();\n          createSchemaFile(schemaFile);\n        }\n\n        return ret;\n      }\n    };\n    \r\n    assertEquals(0, dataImporter.dataImport(jsc(), retry.get()));\n    assertEquals(-1, retry.get());\n    assertEquals(1, fileCreated.get());\n\n    \r\n    \r\n    \r\n    \r\n    boolean isCommitFilePresent = false;\n    Map<String, Long> recordCounts = new HashMap<String, Long>();\n    RemoteIterator<LocatedFileStatus> hoodieFiles = dfs().listFiles(hoodieFolder, true);\n    while (hoodieFiles.hasNext()) {\n      LocatedFileStatus f = hoodieFiles.next();\n      isCommitFilePresent = isCommitFilePresent || f.getPath().toString().endsWith(HoodieTimeline.COMMIT_EXTENSION);\n\n      if (f.getPath().toString().endsWith(\"parquet\")) {\n        String partitionPath = f.getPath().getParent().toString();\n        long count = sqlContext().read().parquet(f.getPath().toString()).count();\n        if (!recordCounts.containsKey(partitionPath)) {\n          recordCounts.put(partitionPath, 0L);\n        }\n        recordCounts.put(partitionPath, recordCounts.get(partitionPath) + count);\n      }\n    }\n    assertTrue(isCommitFilePresent, \"commit file is missing\");\n    assertEquals(4, recordCounts.size(), \"partition is missing\");\n    for (Entry<String, Long> e : recordCounts.entrySet()) {\n      assertEquals(24, e.getValue().longValue(), \"missing records\");\n    }\n  }\n","date":"2020-07-06 07:44:31","endLine":139,"groupId":"10274","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testImportWithRetries","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/90/fa31d0a221913499ac024a693662e815d2179d.src","preCode":"  public void testImportWithRetries() throws Exception {\n    try (JavaSparkContext jsc = getJavaSparkContext()) {\n      \r\n      String schemaFile = new Path(basePath, \"file.schema\").toString();\n\n      HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n          \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n      AtomicInteger retry = new AtomicInteger(3);\n      AtomicInteger fileCreated = new AtomicInteger(0);\n      HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg) {\n        @Override\n        protected int dataImport(JavaSparkContext jsc) throws IOException {\n          int ret = super.dataImport(jsc);\n          if (retry.decrementAndGet() == 0) {\n            fileCreated.incrementAndGet();\n            createSchemaFile(schemaFile);\n          }\n\n          return ret;\n        }\n      };\n      \r\n      assertEquals(0, dataImporter.dataImport(jsc, retry.get()));\n      assertEquals(-1, retry.get());\n      assertEquals(1, fileCreated.get());\n\n      \r\n      \r\n      \r\n      \r\n      boolean isCommitFilePresent = false;\n      Map<String, Long> recordCounts = new HashMap<String, Long>();\n      RemoteIterator<LocatedFileStatus> hoodieFiles = dfs.listFiles(hoodieFolder, true);\n      while (hoodieFiles.hasNext()) {\n        LocatedFileStatus f = hoodieFiles.next();\n        isCommitFilePresent = isCommitFilePresent || f.getPath().toString().endsWith(HoodieTimeline.COMMIT_EXTENSION);\n\n        if (f.getPath().toString().endsWith(\"parquet\")) {\n          SQLContext sc = new SQLContext(jsc);\n          String partitionPath = f.getPath().getParent().toString();\n          long count = sc.read().parquet(f.getPath().toString()).count();\n          if (!recordCounts.containsKey(partitionPath)) {\n            recordCounts.put(partitionPath, 0L);\n          }\n          recordCounts.put(partitionPath, recordCounts.get(partitionPath) + count);\n        }\n      }\n      assertTrue(isCommitFilePresent, \"commit file is missing\");\n      assertEquals(4, recordCounts.size(), \"partition is missing\");\n      for (Entry<String, Long> e : recordCounts.entrySet()) {\n        assertEquals(24, e.getValue().longValue(), \"missing records\");\n      }\n    }\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":89,"status":"M"},{"authorDate":"2020-05-14 06:37:03","commitOrder":2,"curCode":"  private void insert(JavaSparkContext jsc) throws IOException {\n    \r\n    String schemaFile = new Path(basePath, \"file.schema\").toString();\n    createSchemaFile(schemaFile);\n\n    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n\n    dataImporter.dataImport(jsc, 0);\n  }\n","date":"2020-05-14 06:37:03","endLine":183,"groupId":"10274","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"insert","params":"(JavaSparkContextjsc)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/f7/38cdfdece690b977ec31850ebdf81d1d20a181.src","preCode":"  private void insert(JavaSparkContext jsc) throws IOException {\n    \r\n    String schemaFile = new Path(basePath, \"file.schema\").toString();\n    createSchemaFile(schemaFile);\n\n    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n\n    dataImporter.dataImport(jsc, 0);\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":173,"status":"N"}],"commitId":"3b9a30528bd6a6369181702303f3384162b04a7f","commitMessage":"@@@[HUDI-996] Add functional test suite for hudi-utilities (#1746)\n\n- Share resources for functional tests\n- Add suite for functional test classes from hudi-utilities\n","date":"2020-07-06 07:44:31","modifiedFileCount":"8","status":"M","submitter":"Raymond Xu"}]
