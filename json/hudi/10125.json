[{"authorTime":"2021-07-07 23:15:25","codes":[{"authorDate":"2021-07-07 23:15:25","commitOrder":4,"curCode":"  public void testAbort() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String instantTime0 = \"00\" + 0;\n    \r\n    HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n        new HoodieDataSourceInternalBatchWrite(instantTime0, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n\n    DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(0, RANDOM.nextLong());\n\n    List<String> partitionPaths = Arrays.asList(HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS);\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(100);\n    int batches = 1;\n    Dataset<Row> totalInputRows = null;\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    String instantTime1 = \"00\" + 1;\n    dataSourceInternalBatchWrite =\n        new HoodieDataSourceInternalBatchWrite(instantTime1, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n    writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(1, RANDOM.nextLong());\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n    }\n\n    commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalBatchWrite.abort(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n  }\n","date":"2021-07-07 23:15:25","endLine":256,"groupId":"2141","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testAbort","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/bd/02663b5ee45d804fdb546ba7dac074a4795575.src","preCode":"  public void testAbort() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String instantTime0 = \"00\" + 0;\n    \r\n    HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n        new HoodieDataSourceInternalBatchWrite(instantTime0, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n\n    DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(0, RANDOM.nextLong());\n\n    List<String> partitionPaths = Arrays.asList(HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS);\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(100);\n    int batches = 1;\n    Dataset<Row> totalInputRows = null;\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    String instantTime1 = \"00\" + 1;\n    dataSourceInternalBatchWrite =\n        new HoodieDataSourceInternalBatchWrite(instantTime1, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n    writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(1, RANDOM.nextLong());\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n    }\n\n    commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalBatchWrite.abort(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":191,"status":"MB"},{"authorDate":"2021-07-07 23:15:25","commitOrder":4,"curCode":"  public void testAbort() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n\n    String instantTime0 = \"00\" + 0;\n    \r\n    HoodieDataSourceInternalWriter dataSourceInternalWriter =\n        new HoodieDataSourceInternalWriter(instantTime0, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n    DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(0, RANDOM.nextLong(), RANDOM.nextLong());\n\n    List<String> partitionPaths = Arrays.asList(HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS);\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(100);\n    int batches = 1;\n    Dataset<Row> totalInputRows = null;\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    String instantTime1 = \"00\" + 1;\n    dataSourceInternalWriter =\n        new HoodieDataSourceInternalWriter(instantTime1, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n    writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(1, RANDOM.nextLong(), RANDOM.nextLong());\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n    }\n\n    commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalWriter.abort(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n  }\n","date":"2021-07-07 23:15:25","endLine":249,"groupId":"1983","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testAbort","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ca/8058ff0af1cb1f2d224d9bde0c0f87cbdd670d.src","preCode":"  public void testAbort() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n\n    String instantTime0 = \"00\" + 0;\n    \r\n    HoodieDataSourceInternalWriter dataSourceInternalWriter =\n        new HoodieDataSourceInternalWriter(instantTime0, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n    DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(0, RANDOM.nextLong(), RANDOM.nextLong());\n\n    List<String> partitionPaths = Arrays.asList(HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS);\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(100);\n    int batches = 1;\n    Dataset<Row> totalInputRows = null;\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    String instantTime1 = \"00\" + 1;\n    dataSourceInternalWriter =\n        new HoodieDataSourceInternalWriter(instantTime1, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n    writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(1, RANDOM.nextLong(), RANDOM.nextLong());\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n    }\n\n    commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalWriter.abort(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":185,"status":"MB"}],"commitId":"ea9e5d0e8b7557ef82631ac173d67f15bad13690","commitMessage":"@@@[HUDI-1104] Adding support for UserDefinedPartitioners and SortModes to BulkInsert with Rows (#3149)\n\n","date":"2021-07-07 23:15:25","modifiedFileCount":"21","status":"M","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-07-08 15:07:27","codes":[{"authorDate":"2021-07-08 15:07:27","commitOrder":5,"curCode":"  public void testAbort() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String instantTime0 = \"00\" + 0;\n    \r\n    HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n        new HoodieDataSourceInternalBatchWrite(instantTime0, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, Collections.EMPTY_MAP, false);\n    DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(0, RANDOM.nextLong());\n\n    List<String> partitionPaths = Arrays.asList(HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS);\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(100);\n    int batches = 1;\n    Dataset<Row> totalInputRows = null;\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    String instantTime1 = \"00\" + 1;\n    dataSourceInternalBatchWrite =\n        new HoodieDataSourceInternalBatchWrite(instantTime1, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, Collections.EMPTY_MAP, false);\n    writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(1, RANDOM.nextLong());\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n    }\n\n    commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalBatchWrite.abort(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n  }\n","date":"2021-07-08 15:07:27","endLine":302,"groupId":"2141","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testAbort","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/a9/caebfa2ccb3a0d9cfe7a37d9107e2963f7a923.src","preCode":"  public void testAbort() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String instantTime0 = \"00\" + 0;\n    \r\n    HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n        new HoodieDataSourceInternalBatchWrite(instantTime0, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n\n    DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(0, RANDOM.nextLong());\n\n    List<String> partitionPaths = Arrays.asList(HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS);\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(100);\n    int batches = 1;\n    Dataset<Row> totalInputRows = null;\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    String instantTime1 = \"00\" + 1;\n    dataSourceInternalBatchWrite =\n        new HoodieDataSourceInternalBatchWrite(instantTime1, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n    writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(1, RANDOM.nextLong());\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n    }\n\n    commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalBatchWrite.abort(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":238,"status":"M"},{"authorDate":"2021-07-08 15:07:27","commitOrder":5,"curCode":"  public void testAbort() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n\n    String instantTime0 = \"00\" + 0;\n    \r\n    HoodieDataSourceInternalWriter dataSourceInternalWriter =\n        new HoodieDataSourceInternalWriter(instantTime0, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(Collections.EMPTY_MAP), false);\n    DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(0, RANDOM.nextLong(), RANDOM.nextLong());\n\n    List<String> partitionPaths = Arrays.asList(HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS);\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(100);\n    int batches = 1;\n    Dataset<Row> totalInputRows = null;\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    String instantTime1 = \"00\" + 1;\n    dataSourceInternalWriter =\n        new HoodieDataSourceInternalWriter(instantTime1, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(Collections.EMPTY_MAP), false);\n    writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(1, RANDOM.nextLong(), RANDOM.nextLong());\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n    }\n\n    commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalWriter.abort(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n  }\n","date":"2021-07-08 15:07:27","endLine":297,"groupId":"1983","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testAbort","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e4/98febc66a159991feb78f631d0f9b629ef208c.src","preCode":"  public void testAbort() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n\n    String instantTime0 = \"00\" + 0;\n    \r\n    HoodieDataSourceInternalWriter dataSourceInternalWriter =\n        new HoodieDataSourceInternalWriter(instantTime0, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n    DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(0, RANDOM.nextLong(), RANDOM.nextLong());\n\n    List<String> partitionPaths = Arrays.asList(HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS);\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(100);\n    int batches = 1;\n    Dataset<Row> totalInputRows = null;\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    String instantTime1 = \"00\" + 1;\n    dataSourceInternalWriter =\n        new HoodieDataSourceInternalWriter(instantTime1, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n    writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(1, RANDOM.nextLong(), RANDOM.nextLong());\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n    }\n\n    commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalWriter.abort(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":233,"status":"M"}],"commitId":"8c0dbaa9b3b6ced3826d0bc04e0a91272bbcab73","commitMessage":"@@@[HUDI-2009] Fixing extra commit metadata in row writer path (#3075)\n\n","date":"2021-07-08 15:07:27","modifiedFileCount":"12","status":"M","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-07-20 08:43:48","codes":[{"authorDate":"2021-07-20 08:43:48","commitOrder":6,"curCode":"  public void testAbort(boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String instantTime0 = \"00\" + 0;\n    \r\n    HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n        new HoodieDataSourceInternalBatchWrite(instantTime0, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, Collections.EMPTY_MAP, populateMetaFields, false);\n    DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(0, RANDOM.nextLong());\n\n    List<String> partitionPaths = Arrays.asList(HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS);\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(100);\n    int batches = 1;\n    Dataset<Row> totalInputRows = null;\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty(), populateMetaFields);\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    String instantTime1 = \"00\" + 1;\n    dataSourceInternalBatchWrite =\n        new HoodieDataSourceInternalBatchWrite(instantTime1, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, Collections.EMPTY_MAP, populateMetaFields, false);\n    writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(1, RANDOM.nextLong());\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n    }\n\n    commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalBatchWrite.abort(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty(), populateMetaFields);\n  }\n","date":"2021-07-20 08:43:48","endLine":318,"groupId":"10125","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testAbort","params":"(booleanpopulateMetaFields)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/3c/3866ee58f8a0c18b9d1b8dd9b3cb5bd528b940.src","preCode":"  public void testAbort() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String instantTime0 = \"00\" + 0;\n    \r\n    HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n        new HoodieDataSourceInternalBatchWrite(instantTime0, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, Collections.EMPTY_MAP, false);\n    DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(0, RANDOM.nextLong());\n\n    List<String> partitionPaths = Arrays.asList(HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS);\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(100);\n    int batches = 1;\n    Dataset<Row> totalInputRows = null;\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    String instantTime1 = \"00\" + 1;\n    dataSourceInternalBatchWrite =\n        new HoodieDataSourceInternalBatchWrite(instantTime1, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, Collections.EMPTY_MAP, false);\n    writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(1, RANDOM.nextLong());\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n    }\n\n    commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalBatchWrite.abort(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":254,"status":"M"},{"authorDate":"2021-07-20 08:43:48","commitOrder":6,"curCode":"  public void testAbort(boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n\n    String instantTime0 = \"00\" + 0;\n    \r\n    HoodieDataSourceInternalWriter dataSourceInternalWriter =\n        new HoodieDataSourceInternalWriter(instantTime0, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(Collections.EMPTY_MAP), populateMetaFields, false);\n    DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(0, RANDOM.nextLong(), RANDOM.nextLong());\n\n    List<String> partitionPaths = Arrays.asList(HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS);\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(100);\n    int batches = 1;\n    Dataset<Row> totalInputRows = null;\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty(), populateMetaFields);\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    String instantTime1 = \"00\" + 1;\n    dataSourceInternalWriter =\n        new HoodieDataSourceInternalWriter(instantTime1, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(Collections.EMPTY_MAP), populateMetaFields, false);\n    writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(1, RANDOM.nextLong(), RANDOM.nextLong());\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n    }\n\n    commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalWriter.abort(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty(), populateMetaFields);\n  }\n","date":"2021-07-20 08:43:48","endLine":315,"groupId":"10125","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testAbort","params":"(booleanpopulateMetaFields)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/34/2e2ae90ca5ca4ef04bfcd4bcb4a1fdf843e554.src","preCode":"  public void testAbort() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n\n    String instantTime0 = \"00\" + 0;\n    \r\n    HoodieDataSourceInternalWriter dataSourceInternalWriter =\n        new HoodieDataSourceInternalWriter(instantTime0, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(Collections.EMPTY_MAP), false);\n    DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(0, RANDOM.nextLong(), RANDOM.nextLong());\n\n    List<String> partitionPaths = Arrays.asList(HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS);\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(100);\n    int batches = 1;\n    Dataset<Row> totalInputRows = null;\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    String instantTime1 = \"00\" + 1;\n    dataSourceInternalWriter =\n        new HoodieDataSourceInternalWriter(instantTime1, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(Collections.EMPTY_MAP), false);\n    writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(1, RANDOM.nextLong(), RANDOM.nextLong());\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n    }\n\n    commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    \r\n    dataSourceInternalWriter.abort(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n    metaClient.reloadActiveTimeline();\n    result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    \r\n    assertOutput(totalInputRows, result, instantTime0, Option.empty());\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":251,"status":"M"}],"commitId":"d5026e9a24850bdcce9f6df3686bf2235d7d01c4","commitMessage":"@@@[HUDI-2161] Adding support to disable meta columns with bulk insert operation (#3247)\n\n","date":"2021-07-20 08:43:48","modifiedFileCount":"39","status":"M","submitter":"Sivabalan Narayanan"}]
