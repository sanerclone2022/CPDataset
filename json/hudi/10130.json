[{"authorTime":"2020-12-10 07:52:23","codes":[{"authorDate":"2020-12-10 07:52:23","commitOrder":1,"curCode":"  public void testMultipleDataSourceWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime);\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size);\n    }\n  }\n","date":"2020-12-10 07:52:23","endLine":161,"groupId":"3341","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleDataSourceWrites","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/45/4c74d967cd57237ca4addab4dd6508dfafc6c5.src","preCode":"  public void testMultipleDataSourceWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime);\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size);\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":120,"status":"B"},{"authorDate":"2020-12-10 07:52:23","commitOrder":1,"curCode":"  public void testLargeWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime);\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size);\n    }\n  }\n","date":"2020-12-10 07:52:23","endLine":205,"groupId":"3341","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testLargeWrites","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/45/4c74d967cd57237ca4addab4dd6508dfafc6c5.src","preCode":"  public void testLargeWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime);\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size);\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":164,"status":"B"}],"commitId":"fce1453fa608fcff5df4d5aca8c88107d4151b09","commitMessage":"@@@[HUDI-1040] Make Hudi support Spark 3 (#2208)\n\n* Fix flaky MOR unit test\n\n* Update Spark APIs to make it be compatible with both spark2 & spark3\n\n* Refactor bulk insert v2 part to make Hudi be able to compile with Spark3\n\n* Add spark3 profile to handle fasterxml & spark version\n\n* Create hudi-spark-common module & refactor hudi-spark related modules\n\nCo-authored-by: Wenning Ding <wenningd@amazon.com>","date":"2020-12-10 07:52:23","modifiedFileCount":"10","status":"B","submitter":"wenningd"},{"authorTime":"2020-12-25 22:43:34","codes":[{"authorDate":"2020-12-25 22:43:34","commitOrder":2,"curCode":"  public void testMultipleDataSourceWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2020-12-25 22:43:34","endLine":132,"groupId":"3341","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleDataSourceWrites","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/18/4ff771cef4a328d6e49e1560cfdcdf3a4959cc.src","preCode":"  public void testMultipleDataSourceWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime);\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size);\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":91,"status":"M"},{"authorDate":"2020-12-25 22:43:34","commitOrder":2,"curCode":"  public void testLargeWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2020-12-25 22:43:34","endLine":176,"groupId":"3341","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testLargeWrites","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/18/4ff771cef4a328d6e49e1560cfdcdf3a4959cc.src","preCode":"  public void testLargeWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime);\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size);\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":135,"status":"M"}],"commitId":"286055ce34bdbbac68c995a2710fc7be07734b12","commitMessage":"@@@[HUDI-1451] Support bulk insert v2 with Spark 3.0.0 (#2328)\n\nCo-authored-by: Wenning Ding <wenningd@amazon.com>\n\n- Added support for bulk insert v2 with datasource v2 api in Spark 3.0.0.","date":"2020-12-25 22:43:34","modifiedFileCount":"7","status":"M","submitter":"wenningd"},{"authorTime":"2021-07-07 23:15:25","codes":[{"authorDate":"2021-07-07 23:15:25","commitOrder":3,"curCode":"  public void testMultipleDataSourceWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2021-07-07 23:15:25","endLine":132,"groupId":"3341","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleDataSourceWrites","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ca/8058ff0af1cb1f2d224d9bde0c0f87cbdd670d.src","preCode":"  public void testMultipleDataSourceWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":91,"status":"M"},{"authorDate":"2021-07-07 23:15:25","commitOrder":3,"curCode":"  public void testLargeWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2021-07-07 23:15:25","endLine":176,"groupId":"3341","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testLargeWrites","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ca/8058ff0af1cb1f2d224d9bde0c0f87cbdd670d.src","preCode":"  public void testLargeWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":135,"status":"M"}],"commitId":"ea9e5d0e8b7557ef82631ac173d67f15bad13690","commitMessage":"@@@[HUDI-1104] Adding support for UserDefinedPartitioners and SortModes to BulkInsert with Rows (#3149)\n\n","date":"2021-07-07 23:15:25","modifiedFileCount":"21","status":"M","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-07-08 15:07:27","codes":[{"authorDate":"2021-07-08 15:07:27","commitOrder":4,"curCode":"  public void testMultipleDataSourceWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(Collections.EMPTY_MAP), false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2021-07-08 15:07:27","endLine":181,"groupId":"3341","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleDataSourceWrites","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e4/98febc66a159991feb78f631d0f9b629ef208c.src","preCode":"  public void testMultipleDataSourceWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":141,"status":"M"},{"authorDate":"2021-07-08 15:07:27","commitOrder":4,"curCode":"  public void testLargeWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(Collections.EMPTY_MAP), false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2021-07-08 15:07:27","endLine":224,"groupId":"3341","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testLargeWrites","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e4/98febc66a159991feb78f631d0f9b629ef208c.src","preCode":"  public void testLargeWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":184,"status":"M"}],"commitId":"8c0dbaa9b3b6ced3826d0bc04e0a91272bbcab73","commitMessage":"@@@[HUDI-2009] Fixing extra commit metadata in row writer path (#3075)\n\n","date":"2021-07-08 15:07:27","modifiedFileCount":"12","status":"M","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-07-20 08:43:48","codes":[{"authorDate":"2021-07-20 08:43:48","commitOrder":5,"curCode":"  public void testMultipleDataSourceWrites(boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(Collections.EMPTY_MAP), populateMetaFields, false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime,\n          populateMetaFields);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty(), populateMetaFields);\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2021-07-20 08:43:48","endLine":196,"groupId":"4299","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleDataSourceWrites","params":"(booleanpopulateMetaFields)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/34/2e2ae90ca5ca4ef04bfcd4bcb4a1fdf843e554.src","preCode":"  public void testMultipleDataSourceWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(Collections.EMPTY_MAP), false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":155,"status":"M"},{"authorDate":"2021-07-20 08:43:48","commitOrder":5,"curCode":"  public void testLargeWrites(boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(Collections.EMPTY_MAP), populateMetaFields, false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime,\n          populateMetaFields);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty(), populateMetaFields);\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2021-07-20 08:43:48","endLine":241,"groupId":"4299","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testLargeWrites","params":"(booleanpopulateMetaFields)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/34/2e2ae90ca5ca4ef04bfcd4bcb4a1fdf843e554.src","preCode":"  public void testLargeWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(Collections.EMPTY_MAP), false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":200,"status":"M"}],"commitId":"d5026e9a24850bdcce9f6df3686bf2235d7d01c4","commitMessage":"@@@[HUDI-2161] Adding support to disable meta columns with bulk insert operation (#3247)\n\n","date":"2021-07-20 08:43:48","modifiedFileCount":"39","status":"M","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-07-20 08:43:48","codes":[{"authorDate":"2021-07-30 13:22:26","commitOrder":6,"curCode":"  public void testMultipleDataSourceWrites(boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 2; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(Collections.EMPTY_MAP), populateMetaFields, false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 2; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime,\n          populateMetaFields);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty(), populateMetaFields);\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2021-07-30 13:22:26","endLine":197,"groupId":"10130","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleDataSourceWrites","params":"(booleanpopulateMetaFields)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ee/a49e667207ae2400d48223b14a008416ac26b3.src","preCode":"  public void testMultipleDataSourceWrites(boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(Collections.EMPTY_MAP), populateMetaFields, false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime,\n          populateMetaFields);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty(), populateMetaFields);\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":156,"status":"M"},{"authorDate":"2021-07-20 08:43:48","commitOrder":6,"curCode":"  public void testLargeWrites(boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(Collections.EMPTY_MAP), populateMetaFields, false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime,\n          populateMetaFields);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty(), populateMetaFields);\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2021-07-20 08:43:48","endLine":241,"groupId":"10130","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testLargeWrites","params":"(booleanpopulateMetaFields)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/34/2e2ae90ca5ca4ef04bfcd4bcb4a1fdf843e554.src","preCode":"  public void testLargeWrites(boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalWriter dataSourceInternalWriter =\n          new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(Collections.EMPTY_MAP), populateMetaFields, false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(partitionCounter++, RANDOM.nextLong(), RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime,\n          populateMetaFields);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty(), populateMetaFields);\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":200,"status":"N"}],"commitId":"7bdae69053afc5ef604a15806d78317cb976f2ce","commitMessage":"@@@[HUDI-2253] Refactoring few tests to reduce runningtime. DeltaStreamer and MultiDeltaStreamer tests. Bulk insert row writer tests (#3371)\n\nCo-authored-by: Sivabalan Narayanan <nsb@Sivabalans-MBP.attlocal.net>","date":"2021-07-30 13:22:26","modifiedFileCount":"6","status":"M","submitter":"Sivabalan Narayanan"}]
