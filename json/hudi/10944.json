[{"authorTime":"2020-05-27 16:28:17","codes":[{"authorDate":"2020-05-27 16:28:17","commitOrder":1,"curCode":"  public void testBasicWriteAndScan() throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords = records.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    Reader reader = HoodieLogFormat.newReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema());\n    assertTrue(reader.hasNext(), \"We wrote a block, we should be able to read it\");\n    HoodieLogBlock nextBlock = reader.next();\n    assertEquals(HoodieLogBlockType.AVRO_DATA_BLOCK, nextBlock.getBlockType(), \"The next block should be a data block\");\n    HoodieAvroDataBlock dataBlockRead = (HoodieAvroDataBlock) nextBlock;\n    assertEquals(copyOfRecords.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n    reader.close();\n  }\n","date":"2020-05-27 16:28:17","endLine":355,"groupId":"3950","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testBasicWriteAndScan","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e9/f0ef7b167a811f253b4412a326c85966d5382c.src","preCode":"  public void testBasicWriteAndScan() throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords = records.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    Reader reader = HoodieLogFormat.newReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema());\n    assertTrue(reader.hasNext(), \"We wrote a block, we should be able to read it\");\n    HoodieLogBlock nextBlock = reader.next();\n    assertEquals(HoodieLogBlockType.AVRO_DATA_BLOCK, nextBlock.getBlockType(), \"The next block should be a data block\");\n    HoodieAvroDataBlock dataBlockRead = (HoodieAvroDataBlock) nextBlock;\n    assertEquals(copyOfRecords.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n    reader.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":330,"status":"B"},{"authorDate":"2020-05-27 16:28:17","commitOrder":1,"curCode":"  public void testBasicAppendAndRead() throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords2 = records2.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    dataBlock = new HoodieAvroDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords3 = records3.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    dataBlock = new HoodieAvroDataBlock(records3, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    Reader reader = HoodieLogFormat.newReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema());\n    assertTrue(reader.hasNext(), \"First block should be available\");\n    HoodieLogBlock nextBlock = reader.next();\n    HoodieAvroDataBlock dataBlockRead = (HoodieAvroDataBlock) nextBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n    assertEquals(dataBlockRead.getSchema(), getSimpleSchema());\n\n    reader.hasNext();\n    nextBlock = reader.next();\n    dataBlockRead = (HoodieAvroDataBlock) nextBlock;\n    assertEquals(copyOfRecords2.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords2, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    reader.hasNext();\n    nextBlock = reader.next();\n    dataBlockRead = (HoodieAvroDataBlock) nextBlock;\n    assertEquals(copyOfRecords3.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords3, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n    reader.close();\n  }\n","date":"2020-05-27 16:28:17","endLine":422,"groupId":"2586","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testBasicAppendAndRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e9/f0ef7b167a811f253b4412a326c85966d5382c.src","preCode":"  public void testBasicAppendAndRead() throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords2 = records2.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    dataBlock = new HoodieAvroDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords3 = records3.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    dataBlock = new HoodieAvroDataBlock(records3, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    Reader reader = HoodieLogFormat.newReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema());\n    assertTrue(reader.hasNext(), \"First block should be available\");\n    HoodieLogBlock nextBlock = reader.next();\n    HoodieAvroDataBlock dataBlockRead = (HoodieAvroDataBlock) nextBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n    assertEquals(dataBlockRead.getSchema(), getSimpleSchema());\n\n    reader.hasNext();\n    nextBlock = reader.next();\n    dataBlockRead = (HoodieAvroDataBlock) nextBlock;\n    assertEquals(copyOfRecords2.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords2, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    reader.hasNext();\n    nextBlock = reader.next();\n    dataBlockRead = (HoodieAvroDataBlock) nextBlock;\n    assertEquals(copyOfRecords3.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords3, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n    reader.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":358,"status":"B"}],"commitId":"03f136361a5fed594855992ab10bee8bb5060c5b","commitMessage":"@@@[HUDI-811] Restructure test packages in hudi-common (#1644)\n\n* [HUDI-811] Restructure test packages in hudi-common","date":"2020-05-27 16:28:17","modifiedFileCount":"53","status":"B","submitter":"Raymond Xu"},{"authorTime":"2020-06-26 14:46:55","codes":[{"authorDate":"2020-06-26 14:46:55","commitOrder":2,"curCode":"  public void testBasicWriteAndScan() throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords = records.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    HoodieDataBlock dataBlock = getDataBlock(records, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    Reader reader = HoodieLogFormat.newReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema());\n    assertTrue(reader.hasNext(), \"We wrote a block, we should be able to read it\");\n    HoodieLogBlock nextBlock = reader.next();\n    assertEquals(dataBlockType, nextBlock.getBlockType(), \"The next block should be a data block\");\n    HoodieDataBlock dataBlockRead = (HoodieDataBlock) nextBlock;\n    assertEquals(copyOfRecords.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n    reader.close();\n  }\n","date":"2020-06-26 14:46:55","endLine":364,"groupId":"3950","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testBasicWriteAndScan","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/62/14af1af36313e2c739a9c15553b5c32d81dd94.src","preCode":"  public void testBasicWriteAndScan() throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords = records.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    Reader reader = HoodieLogFormat.newReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema());\n    assertTrue(reader.hasNext(), \"We wrote a block, we should be able to read it\");\n    HoodieLogBlock nextBlock = reader.next();\n    assertEquals(HoodieLogBlockType.AVRO_DATA_BLOCK, nextBlock.getBlockType(), \"The next block should be a data block\");\n    HoodieAvroDataBlock dataBlockRead = (HoodieAvroDataBlock) nextBlock;\n    assertEquals(copyOfRecords.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n    reader.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":339,"status":"M"},{"authorDate":"2020-06-26 14:46:55","commitOrder":2,"curCode":"  public void testBasicAppendAndRead() throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords2 = records2.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    dataBlock = getDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords3 = records3.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    dataBlock = getDataBlock(records3, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    Reader reader = HoodieLogFormat.newReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema());\n    assertTrue(reader.hasNext(), \"First block should be available\");\n    HoodieLogBlock nextBlock = reader.next();\n    HoodieDataBlock dataBlockRead = (HoodieDataBlock) nextBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n    assertEquals(dataBlockRead.getSchema(), getSimpleSchema());\n\n    reader.hasNext();\n    nextBlock = reader.next();\n    dataBlockRead = (HoodieDataBlock) nextBlock;\n    assertEquals(copyOfRecords2.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords2, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    reader.hasNext();\n    nextBlock = reader.next();\n    dataBlockRead = (HoodieDataBlock) nextBlock;\n    assertEquals(copyOfRecords3.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords3, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n    reader.close();\n  }\n","date":"2020-06-26 14:46:55","endLine":431,"groupId":"2586","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testBasicAppendAndRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/62/14af1af36313e2c739a9c15553b5c32d81dd94.src","preCode":"  public void testBasicAppendAndRead() throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords2 = records2.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    dataBlock = new HoodieAvroDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords3 = records3.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    dataBlock = new HoodieAvroDataBlock(records3, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    Reader reader = HoodieLogFormat.newReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema());\n    assertTrue(reader.hasNext(), \"First block should be available\");\n    HoodieLogBlock nextBlock = reader.next();\n    HoodieAvroDataBlock dataBlockRead = (HoodieAvroDataBlock) nextBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n    assertEquals(dataBlockRead.getSchema(), getSimpleSchema());\n\n    reader.hasNext();\n    nextBlock = reader.next();\n    dataBlockRead = (HoodieAvroDataBlock) nextBlock;\n    assertEquals(copyOfRecords2.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords2, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    reader.hasNext();\n    nextBlock = reader.next();\n    dataBlockRead = (HoodieAvroDataBlock) nextBlock;\n    assertEquals(copyOfRecords3.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords3, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n    reader.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":367,"status":"M"}],"commitId":"2603cfb33e272632d7f36a53e1b13fe86dbb8627","commitMessage":"@@@[HUDI-684] Introduced abstraction for writing and reading different types of base file formats. (#1687)\n\nNotable changes:\n    1. HoodieFileWriter and HoodieFileReader abstractions for writer/reader side of a base file format\n    2. HoodieDataBlock abstraction for creation specific data blocks for base file formats. (e.g. Parquet has HoodieAvroDataBlock)\n    3. All hardocded references to Parquet / Parquet based classes have been abstracted to call methods which accept a base file format\n    4. HiveSyncTool accepts the base file format as a CLI parameter\n    5. HoodieDeltaStreamer accepts the base file format as a CLI parameter\n    6. HoodieSparkSqlWriter accepts the base file format as a parameter","date":"2020-06-26 14:46:55","modifiedFileCount":"42","status":"M","submitter":"Prashant Wason"},{"authorTime":"2020-12-31 06:22:15","codes":[{"authorDate":"2020-12-31 06:22:15","commitOrder":3,"curCode":"  public void testBasicWriteAndScan() throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords = records.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    HoodieDataBlock dataBlock = getDataBlock(records, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    Reader reader = HoodieLogFormat.newReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema());\n    assertTrue(reader.hasNext(), \"We wrote a block, we should be able to read it\");\n    HoodieLogBlock nextBlock = reader.next();\n    assertEquals(dataBlockType, nextBlock.getBlockType(), \"The next block should be a data block\");\n    HoodieDataBlock dataBlockRead = (HoodieDataBlock) nextBlock;\n    assertEquals(copyOfRecords.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n    reader.close();\n  }\n","date":"2020-12-31 06:22:15","endLine":380,"groupId":"10944","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testBasicWriteAndScan","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/57/e814c33909030385442385ccab9edfb802bcf5.src","preCode":"  public void testBasicWriteAndScan() throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> records = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords = records.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    HoodieDataBlock dataBlock = getDataBlock(records, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    Reader reader = HoodieLogFormat.newReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema());\n    assertTrue(reader.hasNext(), \"We wrote a block, we should be able to read it\");\n    HoodieLogBlock nextBlock = reader.next();\n    assertEquals(dataBlockType, nextBlock.getBlockType(), \"The next block should be a data block\");\n    HoodieDataBlock dataBlockRead = (HoodieDataBlock) nextBlock;\n    assertEquals(copyOfRecords.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n    reader.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":355,"status":"M"},{"authorDate":"2020-12-31 06:22:15","commitOrder":3,"curCode":"  public void testBasicAppendAndRead() throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords2 = records2.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    dataBlock = getDataBlock(records2, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords3 = records3.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    dataBlock = getDataBlock(records3, header);\n    writer.appendBlock(dataBlock);\n    writer.close();\n\n    Reader reader = HoodieLogFormat.newReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema());\n    assertTrue(reader.hasNext(), \"First block should be available\");\n    HoodieLogBlock nextBlock = reader.next();\n    HoodieDataBlock dataBlockRead = (HoodieDataBlock) nextBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n    assertEquals(dataBlockRead.getSchema(), getSimpleSchema());\n\n    reader.hasNext();\n    nextBlock = reader.next();\n    dataBlockRead = (HoodieDataBlock) nextBlock;\n    assertEquals(copyOfRecords2.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords2, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    reader.hasNext();\n    nextBlock = reader.next();\n    dataBlockRead = (HoodieDataBlock) nextBlock;\n    assertEquals(copyOfRecords3.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords3, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n    reader.close();\n  }\n","date":"2020-12-31 06:22:15","endLine":447,"groupId":"10944","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testBasicAppendAndRead","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/57/e814c33909030385442385ccab9edfb802bcf5.src","preCode":"  public void testBasicAppendAndRead() throws IOException, URISyntaxException, InterruptedException {\n    Writer writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);\n    Schema schema = getSimpleSchema();\n    List<IndexedRecord> copyOfRecords1 = records1.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, \"100\");\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    HoodieDataBlock dataBlock = getDataBlock(records1, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records2 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords2 = records2.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    dataBlock = getDataBlock(records2, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    \r\n    writer =\n        HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)\n            .withFileId(\"test-fileid1\").overBaseCommit(\"100\").withFs(fs).build();\n    List<IndexedRecord> records3 = SchemaTestUtil.generateTestRecords(0, 100);\n    List<IndexedRecord> copyOfRecords3 = records3.stream()\n        .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());\n    dataBlock = getDataBlock(records3, header);\n    writer = writer.appendBlock(dataBlock);\n    writer.close();\n\n    Reader reader = HoodieLogFormat.newReader(fs, writer.getLogFile(), SchemaTestUtil.getSimpleSchema());\n    assertTrue(reader.hasNext(), \"First block should be available\");\n    HoodieLogBlock nextBlock = reader.next();\n    HoodieDataBlock dataBlockRead = (HoodieDataBlock) nextBlock;\n    assertEquals(copyOfRecords1.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords1, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n    assertEquals(dataBlockRead.getSchema(), getSimpleSchema());\n\n    reader.hasNext();\n    nextBlock = reader.next();\n    dataBlockRead = (HoodieDataBlock) nextBlock;\n    assertEquals(copyOfRecords2.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords2, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n\n    reader.hasNext();\n    nextBlock = reader.next();\n    dataBlockRead = (HoodieDataBlock) nextBlock;\n    assertEquals(copyOfRecords3.size(), dataBlockRead.getRecords().size(),\n        \"Read records size should be equal to the written records size\");\n    assertEquals(copyOfRecords3, dataBlockRead.getRecords(),\n        \"Both records lists should be the same. (ordering guaranteed)\");\n    reader.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieLogFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":383,"status":"M"}],"commitId":"605b617cfa9a25bff48e618beccfaa7b4eaeee56","commitMessage":"@@@[HUDI-1434] fix incorrect log file path in HoodieWriteStat (#2300)\n\n* [HUDI-1434] fix incorrect log file path in HoodieWriteStat\n\n* HoodieWriteHandle#close() returns a list of WriteStatus objs\n\n* Handle rolled-over log files and return a WriteStatus per log file written\n\n - Combined data and delete block logging into a single call\n - Lazily initialize and manage write status based on returned AppendResult\n - Use FSUtils.getFileSize() to set final file size.  consistent with other handles\n - Added tests around returned values in AppendResult\n - Added validation of the file sizes returned in write stat\n\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2020-12-31 06:22:15","modifiedFileCount":"27","status":"M","submitter":"Gary Li"}]
