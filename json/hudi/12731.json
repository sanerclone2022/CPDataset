[{"authorTime":"2021-03-01 12:29:41","codes":[{"authorDate":"2020-12-10 20:02:02","commitOrder":3,"curCode":"  private List<WriteStatus> compact(HoodieSparkCopyOnWriteTable hoodieCopyOnWriteTable, HoodieTableMetaClient metaClient,\n      HoodieWriteConfig config, CompactionOperation operation, String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new SparkTaskContextSupplier(), config.getProps());\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n              operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n    }).collect(toList());\n  }\n","date":"2020-12-10 20:02:02","endLine":177,"groupId":"2787","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"compact","params":"(HoodieSparkCopyOnWriteTablehoodieCopyOnWriteTable@HoodieTableMetaClientmetaClient@HoodieWriteConfigconfig@CompactionOperationoperation@StringinstantTime)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/65/cefc9b9923c11efa5c1cc9912fe67b851f5d1c.src","preCode":"  private List<WriteStatus> compact(HoodieSparkCopyOnWriteTable hoodieCopyOnWriteTable, HoodieTableMetaClient metaClient,\n      HoodieWriteConfig config, CompactionOperation operation, String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new SparkTaskContextSupplier(), config.getProps());\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n              operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n    }).collect(toList());\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/HoodieSparkMergeOnReadTableCompactor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":109,"status":"NB"},{"authorDate":"2021-03-01 12:29:41","commitOrder":3,"curCode":"  public List<WriteStatus> compact(HoodieFlinkCopyOnWriteTable hoodieCopyOnWriteTable,\n                                   HoodieTableMetaClient metaClient,\n                                   HoodieWriteConfig config,\n                                   CompactionOperation operation,\n                                   String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    \r\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new FlinkTaskContextSupplier(null), config.getProps());\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n          operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n    }).collect(toList());\n  }\n","date":"2021-03-01 12:29:41","endLine":166,"groupId":"2787","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"compact","params":"(HoodieFlinkCopyOnWriteTablehoodieCopyOnWriteTable@HoodieTableMetaClientmetaClient@HoodieWriteConfigconfig@CompactionOperationoperation@StringinstantTime)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/6a/2d1bf1f456df68e0e9ab4b69ee612ae5ea30a5.src","preCode":"  public List<WriteStatus> compact(HoodieFlinkCopyOnWriteTable hoodieCopyOnWriteTable,\n                                   HoodieTableMetaClient metaClient,\n                                   HoodieWriteConfig config,\n                                   CompactionOperation operation,\n                                   String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    \r\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new FlinkTaskContextSupplier(null), config.getProps());\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n          operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n    }).collect(toList());\n  }\n","realPath":"hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/compact/HoodieFlinkMergeOnReadTableCompactor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":94,"status":"B"}],"commitId":"7a11de12764d8f68f296c6e68a22822318bfbefa","commitMessage":"@@@[HUDI-1632] Supports merge on read write mode for Flink writer (#2593)\n\nAlso supports async compaction with pluggable strategies.","date":"2021-03-01 12:29:41","modifiedFileCount":"20","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-07-01 05:26:30","codes":[{"authorDate":"2021-07-01 05:26:30","commitOrder":4,"curCode":"  private List<WriteStatus> compact(HoodieSparkCopyOnWriteTable hoodieCopyOnWriteTable, HoodieTableMetaClient metaClient,\n      HoodieWriteConfig config, CompactionOperation operation, String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new SparkTaskContextSupplier(), config);\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n              operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n    }).collect(toList());\n  }\n","date":"2021-07-01 05:26:30","endLine":176,"groupId":"2787","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"compact","params":"(HoodieSparkCopyOnWriteTablehoodieCopyOnWriteTable@HoodieTableMetaClientmetaClient@HoodieWriteConfigconfig@CompactionOperationoperation@StringinstantTime)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/11/7be744dcea85139f8d60f0882637e2c9274493.src","preCode":"  private List<WriteStatus> compact(HoodieSparkCopyOnWriteTable hoodieCopyOnWriteTable, HoodieTableMetaClient metaClient,\n      HoodieWriteConfig config, CompactionOperation operation, String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new SparkTaskContextSupplier(), config.getProps());\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n              operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n    }).collect(toList());\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/HoodieSparkMergeOnReadTableCompactor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":108,"status":"M"},{"authorDate":"2021-07-01 05:26:30","commitOrder":4,"curCode":"  public List<WriteStatus> compact(HoodieFlinkCopyOnWriteTable hoodieCopyOnWriteTable,\n                                   HoodieTableMetaClient metaClient,\n                                   HoodieWriteConfig config,\n                                   CompactionOperation operation,\n                                   String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    \r\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new FlinkTaskContextSupplier(null), config);\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n          operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n    }).collect(toList());\n  }\n","date":"2021-07-01 05:26:30","endLine":166,"groupId":"2787","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"compact","params":"(HoodieFlinkCopyOnWriteTablehoodieCopyOnWriteTable@HoodieTableMetaClientmetaClient@HoodieWriteConfigconfig@CompactionOperationoperation@StringinstantTime)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/a5/109dccee6a1cb18b3af722e34ebbd08864d040.src","preCode":"  public List<WriteStatus> compact(HoodieFlinkCopyOnWriteTable hoodieCopyOnWriteTable,\n                                   HoodieTableMetaClient metaClient,\n                                   HoodieWriteConfig config,\n                                   CompactionOperation operation,\n                                   String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    \r\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new FlinkTaskContextSupplier(null), config.getProps());\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n          operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n    }).collect(toList());\n  }\n","realPath":"hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/compact/HoodieFlinkMergeOnReadTableCompactor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":94,"status":"M"}],"commitId":"d412fb2fe642417460532044cac162bb68f4bec4","commitMessage":"@@@[HUDI-89] Add configOption & refactor all configs based on that (#2833)\n\nCo-authored-by: Wenning Ding <wenningd@amazon.com>","date":"2021-07-01 05:26:30","modifiedFileCount":"138","status":"M","submitter":"wenningd"},{"authorTime":"2021-07-26 17:39:13","codes":[{"authorDate":"2021-07-26 17:39:13","commitOrder":5,"curCode":"  private List<WriteStatus> compact(HoodieSparkCopyOnWriteTable hoodieCopyOnWriteTable, HoodieTableMetaClient metaClient,\n      HoodieWriteConfig config, CompactionOperation operation, String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new SparkTaskContextSupplier(), config);\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n              operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n      scanner.close();\n    }).collect(toList());\n  }\n","date":"2021-07-26 17:39:13","endLine":189,"groupId":"2787","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"compact","params":"(HoodieSparkCopyOnWriteTablehoodieCopyOnWriteTable@HoodieTableMetaClientmetaClient@HoodieWriteConfigconfig@CompactionOperationoperation@StringinstantTime)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/7b/6f3fcced94f8d3bacf654d8ed90fcb43e171b0.src","preCode":"  private List<WriteStatus> compact(HoodieSparkCopyOnWriteTable hoodieCopyOnWriteTable, HoodieTableMetaClient metaClient,\n      HoodieWriteConfig config, CompactionOperation operation, String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new SparkTaskContextSupplier(), config);\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n              operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n    }).collect(toList());\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/HoodieSparkMergeOnReadTableCompactor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":121,"status":"M"},{"authorDate":"2021-07-26 17:39:13","commitOrder":5,"curCode":"  public List<WriteStatus> compact(HoodieFlinkCopyOnWriteTable hoodieCopyOnWriteTable,\n                                   HoodieTableMetaClient metaClient,\n                                   HoodieWriteConfig config,\n                                   CompactionOperation operation,\n                                   String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    \r\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new FlinkTaskContextSupplier(null), config);\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n          operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n      scanner.close();\n    }).collect(toList());\n  }\n","date":"2021-07-26 17:39:13","endLine":167,"groupId":"2787","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"compact","params":"(HoodieFlinkCopyOnWriteTablehoodieCopyOnWriteTable@HoodieTableMetaClientmetaClient@HoodieWriteConfigconfig@CompactionOperationoperation@StringinstantTime)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e8/9ba564544c404455b61414ce314d4e6c629320.src","preCode":"  public List<WriteStatus> compact(HoodieFlinkCopyOnWriteTable hoodieCopyOnWriteTable,\n                                   HoodieTableMetaClient metaClient,\n                                   HoodieWriteConfig config,\n                                   CompactionOperation operation,\n                                   String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    \r\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new FlinkTaskContextSupplier(null), config);\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n          operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n    }).collect(toList());\n  }\n","realPath":"hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/compact/HoodieFlinkMergeOnReadTableCompactor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":94,"status":"M"}],"commitId":"a5638b995b5373826d435734fc6cdfbb5fbf7acb","commitMessage":"@@@[MINOR] Close log scanner after compaction completed (#3294)\n\n","date":"2021-07-26 17:39:13","modifiedFileCount":"2","status":"M","submitter":"Gary Li"},{"authorTime":"2021-07-28 13:31:03","codes":[{"authorDate":"2021-07-28 13:31:03","commitOrder":6,"curCode":"  private List<WriteStatus> compact(HoodieSparkCopyOnWriteTable hoodieCopyOnWriteTable, HoodieTableMetaClient metaClient,\n      HoodieWriteConfig config, CompactionOperation operation, String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new SparkTaskContextSupplier(), config);\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .withDiskMapType(config.getCommonConfig().getSpillableDiskMapType())\n        .withBitCaskDiskMapCompressionEnabled(config.getCommonConfig().isBitCaskDiskMapCompressionEnabled())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n              operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n      scanner.close();\n    }).collect(toList());\n  }\n","date":"2021-07-28 13:31:03","endLine":191,"groupId":"2216","id":7,"instanceNumber":1,"isCurCommit":1,"methodName":"compact","params":"(HoodieSparkCopyOnWriteTablehoodieCopyOnWriteTable@HoodieTableMetaClientmetaClient@HoodieWriteConfigconfig@CompactionOperationoperation@StringinstantTime)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/27/85403bab90b0b7ae0e22f6b89709d37d8bd686.src","preCode":"  private List<WriteStatus> compact(HoodieSparkCopyOnWriteTable hoodieCopyOnWriteTable, HoodieTableMetaClient metaClient,\n      HoodieWriteConfig config, CompactionOperation operation, String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new SparkTaskContextSupplier(), config);\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n              operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n      scanner.close();\n    }).collect(toList());\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/HoodieSparkMergeOnReadTableCompactor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":121,"status":"M"},{"authorDate":"2021-07-28 13:31:03","commitOrder":6,"curCode":"  public List<WriteStatus> compact(HoodieFlinkCopyOnWriteTable hoodieCopyOnWriteTable,\n                                   HoodieTableMetaClient metaClient,\n                                   HoodieWriteConfig config,\n                                   CompactionOperation operation,\n                                   String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    \r\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new FlinkTaskContextSupplier(null), config);\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .withDiskMapType(config.getCommonConfig().getSpillableDiskMapType())\n        .withBitCaskDiskMapCompressionEnabled(config.getCommonConfig().isBitCaskDiskMapCompressionEnabled())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n          operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n      scanner.close();\n    }).collect(toList());\n  }\n","date":"2021-07-28 13:31:03","endLine":169,"groupId":"2216","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"compact","params":"(HoodieFlinkCopyOnWriteTablehoodieCopyOnWriteTable@HoodieTableMetaClientmetaClient@HoodieWriteConfigconfig@CompactionOperationoperation@StringinstantTime)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/f0/8c8b5d140546e2ecc839ae8f6f4074f3bcb4e3.src","preCode":"  public List<WriteStatus> compact(HoodieFlinkCopyOnWriteTable hoodieCopyOnWriteTable,\n                                   HoodieTableMetaClient metaClient,\n                                   HoodieWriteConfig config,\n                                   CompactionOperation operation,\n                                   String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    \r\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new FlinkTaskContextSupplier(null), config);\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n          operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n      scanner.close();\n    }).collect(toList());\n  }\n","realPath":"hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/compact/HoodieFlinkMergeOnReadTableCompactor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":94,"status":"M"}],"commitId":"8fef50e237b2342ea3366be32950a2b87a9608c4","commitMessage":"@@@[HUDI-2044] Integrate consumers with rocksDB and compression within External Spillable Map (#3318)\n\n","date":"2021-07-28 13:31:03","modifiedFileCount":"26","status":"M","submitter":"rmahindra123"},{"authorTime":"2021-08-10 20:23:23","codes":[{"authorDate":"2021-07-28 13:31:03","commitOrder":7,"curCode":"  private List<WriteStatus> compact(HoodieSparkCopyOnWriteTable hoodieCopyOnWriteTable, HoodieTableMetaClient metaClient,\n      HoodieWriteConfig config, CompactionOperation operation, String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new SparkTaskContextSupplier(), config);\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .withDiskMapType(config.getCommonConfig().getSpillableDiskMapType())\n        .withBitCaskDiskMapCompressionEnabled(config.getCommonConfig().isBitCaskDiskMapCompressionEnabled())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n              operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n      scanner.close();\n    }).collect(toList());\n  }\n","date":"2021-07-28 13:31:03","endLine":191,"groupId":"12731","id":9,"instanceNumber":1,"isCurCommit":1,"methodName":"compact","params":"(HoodieSparkCopyOnWriteTablehoodieCopyOnWriteTable@HoodieTableMetaClientmetaClient@HoodieWriteConfigconfig@CompactionOperationoperation@StringinstantTime)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/27/85403bab90b0b7ae0e22f6b89709d37d8bd686.src","preCode":"  private List<WriteStatus> compact(HoodieSparkCopyOnWriteTable hoodieCopyOnWriteTable, HoodieTableMetaClient metaClient,\n      HoodieWriteConfig config, CompactionOperation operation, String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new SparkTaskContextSupplier(), config);\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .withDiskMapType(config.getCommonConfig().getSpillableDiskMapType())\n        .withBitCaskDiskMapCompressionEnabled(config.getCommonConfig().isBitCaskDiskMapCompressionEnabled())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n              operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n      scanner.close();\n    }).collect(toList());\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/compact/HoodieSparkMergeOnReadTableCompactor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":121,"status":"N"},{"authorDate":"2021-08-10 20:23:23","commitOrder":7,"curCode":"  public List<WriteStatus> compact(HoodieFlinkCopyOnWriteTable hoodieCopyOnWriteTable,\n                                   HoodieTableMetaClient metaClient,\n                                   HoodieWriteConfig config,\n                                   CompactionOperation operation,\n                                   String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()), config.allowOperationMetadataField());\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    \r\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new FlinkTaskContextSupplier(null), config);\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .withDiskMapType(config.getCommonConfig().getSpillableDiskMapType())\n        .withBitCaskDiskMapCompressionEnabled(config.getCommonConfig().isBitCaskDiskMapCompressionEnabled())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n          operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n      scanner.close();\n    }).collect(toList());\n  }\n","date":"2021-08-10 20:23:23","endLine":169,"groupId":"12731","id":10,"instanceNumber":2,"isCurCommit":1,"methodName":"compact","params":"(HoodieFlinkCopyOnWriteTablehoodieCopyOnWriteTable@HoodieTableMetaClientmetaClient@HoodieWriteConfigconfig@CompactionOperationoperation@StringinstantTime)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/1f/4a5248411f0b2ff4ce44e331cf85e8dab971c7.src","preCode":"  public List<WriteStatus> compact(HoodieFlinkCopyOnWriteTable hoodieCopyOnWriteTable,\n                                   HoodieTableMetaClient metaClient,\n                                   HoodieWriteConfig config,\n                                   CompactionOperation operation,\n                                   String instantTime) throws IOException {\n    FileSystem fs = metaClient.getFs();\n\n    Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n    LOG.info(\"Compacting base \" + operation.getDataFileName() + \" with delta files \" + operation.getDeltaFileNames()\n        + \" for commit \" + instantTime);\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    String maxInstantTime = metaClient\n        .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n        .filterCompletedInstants().lastInstant().get().getTimestamp();\n    \r\n    long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new FlinkTaskContextSupplier(null), config);\n    LOG.info(\"MaxMemoryPerCompaction => \" + maxMemoryPerCompaction);\n\n    List<String> logFiles = operation.getDeltaFileNames().stream().map(\n        p -> new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), p).toString())\n        .collect(toList());\n    HoodieMergedLogRecordScanner scanner = HoodieMergedLogRecordScanner.newBuilder()\n        .withFileSystem(fs)\n        .withBasePath(metaClient.getBasePath())\n        .withLogFilePaths(logFiles)\n        .withReaderSchema(readerSchema)\n        .withLatestInstantTime(maxInstantTime)\n        .withMaxMemorySizeInBytes(maxMemoryPerCompaction)\n        .withReadBlocksLazily(config.getCompactionLazyBlockReadEnabled())\n        .withReverseReader(config.getCompactionReverseLogReadEnabled())\n        .withBufferSize(config.getMaxDFSStreamBufferSize())\n        .withSpillableMapBasePath(config.getSpillableMapBasePath())\n        .withDiskMapType(config.getCommonConfig().getSpillableDiskMapType())\n        .withBitCaskDiskMapCompressionEnabled(config.getCommonConfig().isBitCaskDiskMapCompressionEnabled())\n        .build();\n    if (!scanner.iterator().hasNext()) {\n      return new ArrayList<>();\n    }\n\n    Option<HoodieBaseFile> oldDataFileOpt =\n        operation.getBaseFile(metaClient.getBasePath(), operation.getPartitionPath());\n\n    \r\n    Iterator<List<WriteStatus>> result;\n    \r\n    if (oldDataFileOpt.isPresent()) {\n      result = hoodieCopyOnWriteTable.handleUpdate(instantTime, operation.getPartitionPath(),\n          operation.getFileId(), scanner.getRecords(),\n          oldDataFileOpt.get());\n    } else {\n      result = hoodieCopyOnWriteTable.handleInsert(instantTime, operation.getPartitionPath(), operation.getFileId(),\n          scanner.getRecords());\n    }\n    Iterable<List<WriteStatus>> resultIterable = () -> result;\n    return StreamSupport.stream(resultIterable.spliterator(), false).flatMap(Collection::stream).peek(s -> {\n      s.getStat().setTotalUpdatedRecordsCompacted(scanner.getNumMergedRecordsInLog());\n      s.getStat().setTotalLogFilesCompacted(scanner.getTotalLogFiles());\n      s.getStat().setTotalLogRecords(scanner.getTotalLogRecords());\n      s.getStat().setPartitionPath(operation.getPartitionPath());\n      s.getStat()\n          .setTotalLogSizeCompacted(operation.getMetrics().get(CompactionStrategy.TOTAL_LOG_FILE_SIZE).longValue());\n      s.getStat().setTotalLogBlocks(scanner.getTotalLogBlocks());\n      s.getStat().setTotalCorruptLogBlock(scanner.getTotalCorruptBlocks());\n      s.getStat().setTotalRollbackBlocks(scanner.getTotalRollbacks());\n      RuntimeStats runtimeStats = new RuntimeStats();\n      runtimeStats.setTotalScanTime(scanner.getTotalTimeTakenToReadAndMergeBlocks());\n      s.getStat().setRuntimeStats(runtimeStats);\n      scanner.close();\n    }).collect(toList());\n  }\n","realPath":"hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/compact/HoodieFlinkMergeOnReadTableCompactor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":94,"status":"M"}],"commitId":"21db6d7a84d4a83ec98c110e92ff9c92d05dd530","commitMessage":"@@@[HUDI-1771] Propagate CDC format for hoodie (#3285)\n\n","date":"2021-08-10 20:23:23","modifiedFileCount":"47","status":"M","submitter":"swuferhong"}]
