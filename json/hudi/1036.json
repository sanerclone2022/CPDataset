[{"authorTime":"2019-07-18 02:51:49","codes":[{"authorDate":"2019-07-18 02:51:49","commitOrder":1,"curCode":"  public void testInputFormatLoad() throws IOException {\n    \r\n    File partitionDir = InputFormatTestUtil.prepareTable(basePath, 10, \"100\");\n    InputFormatTestUtil.commit(basePath, \"100\");\n\n    \r\n    FileInputFormat.setInputPaths(jobConf, partitionDir.getPath());\n\n    InputSplit[] inputSplits = inputFormat.getSplits(jobConf, 10);\n    assertEquals(10, inputSplits.length);\n\n    FileStatus[] files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n  }\n","date":"2020-01-09 06:53:05","endLine":76,"groupId":"5157","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testInputFormatLoad","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ed/501e70030d2fb8c72a854817bc7664a153a03a.src","preCode":"  public void testInputFormatLoad() throws IOException {\n    \r\n    File partitionDir = InputFormatTestUtil.prepareTable(basePath, 10, \"100\");\n    InputFormatTestUtil.commit(basePath, \"100\");\n\n    \r\n    FileInputFormat.setInputPaths(jobConf, partitionDir.getPath());\n\n    InputSplit[] inputSplits = inputFormat.getSplits(jobConf, 10);\n    assertEquals(10, inputSplits.length);\n\n    FileStatus[] files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n  }\n","realPath":"hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/TestHoodieParquetInputFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":63,"status":"B"},{"authorDate":"2019-07-18 02:51:49","commitOrder":1,"curCode":"  public void testInputFormatUpdates() throws IOException {\n    \r\n    File partitionDir = InputFormatTestUtil.prepareTable(basePath, 10, \"100\");\n    InputFormatTestUtil.commit(basePath, \"100\");\n\n    \r\n    FileInputFormat.setInputPaths(jobConf, partitionDir.getPath());\n\n    FileStatus[] files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n\n    \r\n    InputFormatTestUtil.simulateUpdates(partitionDir, \"100\", 5, \"200\", true);\n    \r\n    files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n    ensureFilesInCommit(\"Commit 200 has not been committed. We should not see files from this commit\", files, \"200\", 0);\n    InputFormatTestUtil.commit(basePath, \"200\");\n    files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n    ensureFilesInCommit(\"5 files have been updated to commit 200. We should see 5 files from commit 200 and 5 \"\n        + \"files from 100 commit\", files, \"200\", 5);\n    ensureFilesInCommit(\"5 files have been updated to commit 200. We should see 5 files from commit 100 and 5 \"\n        + \"files from 200 commit\", files, \"100\", 5);\n  }\n","date":"2020-01-09 06:53:05","endLine":103,"groupId":"970","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testInputFormatUpdates","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ed/501e70030d2fb8c72a854817bc7664a153a03a.src","preCode":"  public void testInputFormatUpdates() throws IOException {\n    \r\n    File partitionDir = InputFormatTestUtil.prepareTable(basePath, 10, \"100\");\n    InputFormatTestUtil.commit(basePath, \"100\");\n\n    \r\n    FileInputFormat.setInputPaths(jobConf, partitionDir.getPath());\n\n    FileStatus[] files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n\n    \r\n    InputFormatTestUtil.simulateUpdates(partitionDir, \"100\", 5, \"200\", true);\n    \r\n    files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n    ensureFilesInCommit(\"Commit 200 has not been committed. We should not see files from this commit\", files, \"200\", 0);\n    InputFormatTestUtil.commit(basePath, \"200\");\n    files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n    ensureFilesInCommit(\"5 files have been updated to commit 200. We should see 5 files from commit 200 and 5 \"\n        + \"files from 100 commit\", files, \"200\", 5);\n    ensureFilesInCommit(\"5 files have been updated to commit 200. We should see 5 files from commit 100 and 5 \"\n        + \"files from 200 commit\", files, \"100\", 5);\n  }\n","realPath":"hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/TestHoodieParquetInputFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":79,"status":"B"}],"commitId":"d09eacdc13b9f19f69a317c8d08bda69a43678bc","commitMessage":"@@@[HUDI-25] Optimize HoodieInputformat.listStatus() for faster Hive incremental queries on Hoodie\n\n    Summary:\n    - InputPathHandler class classifies  inputPaths into incremental.  non incremental and non hoodie paths.\n    - Incremental queries leverage HoodieCommitMetadata to get partitions that are affected and only lists those partitions as opposed to listing all partitions\n    - listStatus() processes each category separately\n","date":"2020-01-09 06:53:05","modifiedFileCount":"2","status":"B","submitter":"Bhavani Sudha Saktheeswaran"},{"authorTime":"2020-06-26 14:46:55","codes":[{"authorDate":"2020-06-26 14:46:55","commitOrder":2,"curCode":"  public void testInputFormatLoad() throws IOException {\n    \r\n    File partitionDir = InputFormatTestUtil.prepareTable(basePath, baseFileFormat, 10, \"100\");\n    InputFormatTestUtil.commit(basePath, \"100\");\n\n    \r\n    FileInputFormat.setInputPaths(jobConf, partitionDir.getPath());\n\n    InputSplit[] inputSplits = inputFormat.getSplits(jobConf, 10);\n    assertEquals(10, inputSplits.length);\n\n    FileStatus[] files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n  }\n","date":"2020-06-26 14:46:55","endLine":162,"groupId":"1036","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testInputFormatLoad","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/91/8aade813327c4dc971ab4cc996cd8bf760f5e8.src","preCode":"  public void testInputFormatLoad() throws IOException {\n    \r\n    File partitionDir = InputFormatTestUtil.prepareTable(basePath, 10, \"100\");\n    InputFormatTestUtil.commit(basePath, \"100\");\n\n    \r\n    FileInputFormat.setInputPaths(jobConf, partitionDir.getPath());\n\n    InputSplit[] inputSplits = inputFormat.getSplits(jobConf, 10);\n    assertEquals(10, inputSplits.length);\n\n    FileStatus[] files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n  }\n","realPath":"hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/TestHoodieParquetInputFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":149,"status":"M"},{"authorDate":"2020-06-26 14:46:55","commitOrder":2,"curCode":"  public void testInputFormatUpdates() throws IOException {\n    \r\n    File partitionDir = InputFormatTestUtil.prepareTable(basePath, baseFileFormat, 10, \"100\");\n    InputFormatTestUtil.commit(basePath, \"100\");\n\n    \r\n    FileInputFormat.setInputPaths(jobConf, partitionDir.getPath());\n\n    FileStatus[] files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n\n    \r\n    InputFormatTestUtil.simulateUpdates(partitionDir, baseFileExtension, \"100\", 5, \"200\", true);\n    \r\n    files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n    ensureFilesInCommit(\"Commit 200 has not been committed. We should not see files from this commit\", files, \"200\", 0);\n    InputFormatTestUtil.commit(basePath, \"200\");\n    files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n    ensureFilesInCommit(\"5 files have been updated to commit 200. We should see 5 files from commit 200 and 5 \"\n        + \"files from 100 commit\", files, \"200\", 5);\n    ensureFilesInCommit(\"5 files have been updated to commit 200. We should see 5 files from commit 100 and 5 \"\n        + \"files from 200 commit\", files, \"100\", 5);\n  }\n","date":"2020-06-26 14:46:55","endLine":189,"groupId":"1036","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testInputFormatUpdates","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/91/8aade813327c4dc971ab4cc996cd8bf760f5e8.src","preCode":"  public void testInputFormatUpdates() throws IOException {\n    \r\n    File partitionDir = InputFormatTestUtil.prepareTable(basePath, 10, \"100\");\n    InputFormatTestUtil.commit(basePath, \"100\");\n\n    \r\n    FileInputFormat.setInputPaths(jobConf, partitionDir.getPath());\n\n    FileStatus[] files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n\n    \r\n    InputFormatTestUtil.simulateUpdates(partitionDir, \"100\", 5, \"200\", true);\n    \r\n    files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n    ensureFilesInCommit(\"Commit 200 has not been committed. We should not see files from this commit\", files, \"200\", 0);\n    InputFormatTestUtil.commit(basePath, \"200\");\n    files = inputFormat.listStatus(jobConf);\n    assertEquals(10, files.length);\n    ensureFilesInCommit(\"5 files have been updated to commit 200. We should see 5 files from commit 200 and 5 \"\n        + \"files from 100 commit\", files, \"200\", 5);\n    ensureFilesInCommit(\"5 files have been updated to commit 200. We should see 5 files from commit 100 and 5 \"\n        + \"files from 200 commit\", files, \"100\", 5);\n  }\n","realPath":"hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/TestHoodieParquetInputFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":165,"status":"M"}],"commitId":"2603cfb33e272632d7f36a53e1b13fe86dbb8627","commitMessage":"@@@[HUDI-684] Introduced abstraction for writing and reading different types of base file formats. (#1687)\n\nNotable changes:\n    1. HoodieFileWriter and HoodieFileReader abstractions for writer/reader side of a base file format\n    2. HoodieDataBlock abstraction for creation specific data blocks for base file formats. (e.g. Parquet has HoodieAvroDataBlock)\n    3. All hardocded references to Parquet / Parquet based classes have been abstracted to call methods which accept a base file format\n    4. HiveSyncTool accepts the base file format as a CLI parameter\n    5. HoodieDeltaStreamer accepts the base file format as a CLI parameter\n    6. HoodieSparkSqlWriter accepts the base file format as a parameter","date":"2020-06-26 14:46:55","modifiedFileCount":"42","status":"M","submitter":"Prashant Wason"}]
