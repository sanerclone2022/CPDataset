[{"authorTime":"2021-01-06 23:07:24","codes":[{"authorDate":"2021-01-06 23:07:24","commitOrder":1,"curCode":"  public void testMultipleDataSourceWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2021-01-06 23:07:24","endLine":137,"groupId":"2637","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleDataSourceWrites","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/69/829ec281a493578060f25f6c8e743e3d8b2084.src","preCode":"  public void testMultipleDataSourceWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":95,"status":"B"},{"authorDate":"2021-01-06 23:07:24","commitOrder":1,"curCode":"  public void testLargeWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2021-01-06 23:07:24","endLine":182,"groupId":"2637","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testLargeWrites","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/69/829ec281a493578060f25f6c8e743e3d8b2084.src","preCode":"  public void testLargeWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":140,"status":"B"}],"commitId":"b593f1062931a4d017ae8bd7dd42e47a8873a39f","commitMessage":"@@@[MINOR] Rename unit test package of hudi-spark3 from scala to java (#2411)\n\n","date":"2021-01-06 23:07:24","modifiedFileCount":"0","status":"B","submitter":"wangxianghu"},{"authorTime":"2021-07-07 23:15:25","codes":[{"authorDate":"2021-07-07 23:15:25","commitOrder":2,"curCode":"  public void testMultipleDataSourceWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2021-07-07 23:15:25","endLine":137,"groupId":"2637","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleDataSourceWrites","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/bd/02663b5ee45d804fdb546ba7dac074a4795575.src","preCode":"  public void testMultipleDataSourceWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":95,"status":"M"},{"authorDate":"2021-07-07 23:15:25","commitOrder":2,"curCode":"  public void testLargeWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2021-07-07 23:15:25","endLine":182,"groupId":"2637","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testLargeWrites","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/bd/02663b5ee45d804fdb546ba7dac074a4795575.src","preCode":"  public void testLargeWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":140,"status":"M"}],"commitId":"ea9e5d0e8b7557ef82631ac173d67f15bad13690","commitMessage":"@@@[HUDI-1104] Adding support for UserDefinedPartitioners and SortModes to BulkInsert with Rows (#3149)\n\n","date":"2021-07-07 23:15:25","modifiedFileCount":"21","status":"M","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-07-08 15:07:27","codes":[{"authorDate":"2021-07-08 15:07:27","commitOrder":3,"curCode":"  public void testMultipleDataSourceWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, Collections.EMPTY_MAP, false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2021-07-08 15:07:27","endLine":185,"groupId":"2637","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleDataSourceWrites","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/a9/caebfa2ccb3a0d9cfe7a37d9107e2963f7a923.src","preCode":"  public void testMultipleDataSourceWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":144,"status":"M"},{"authorDate":"2021-07-08 15:07:27","commitOrder":3,"curCode":"  public void testLargeWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, Collections.EMPTY_MAP, false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2021-07-08 15:07:27","endLine":229,"groupId":"2637","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testLargeWrites","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/a9/caebfa2ccb3a0d9cfe7a37d9107e2963f7a923.src","preCode":"  public void testLargeWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, false);\n\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":188,"status":"M"}],"commitId":"8c0dbaa9b3b6ced3826d0bc04e0a91272bbcab73","commitMessage":"@@@[HUDI-2009] Fixing extra commit metadata in row writer path (#3075)\n\n","date":"2021-07-08 15:07:27","modifiedFileCount":"12","status":"M","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-07-20 08:43:48","codes":[{"authorDate":"2021-07-20 08:43:48","commitOrder":4,"curCode":"  public void testMultipleDataSourceWrites(boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, Collections.EMPTY_MAP, populateMetaFields, false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime, populateMetaFields);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty(), populateMetaFields);\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2021-07-20 08:43:48","endLine":198,"groupId":"2637","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleDataSourceWrites","params":"(booleanpopulateMetaFields)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/3c/3866ee58f8a0c18b9d1b8dd9b3cb5bd528b940.src","preCode":"  public void testMultipleDataSourceWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, Collections.EMPTY_MAP, false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":157,"status":"M"},{"authorDate":"2021-07-20 08:43:48","commitOrder":4,"curCode":"  public void testLargeWrites(boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, Collections.EMPTY_MAP, populateMetaFields, false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime,\n          populateMetaFields);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty(), populateMetaFields);\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2021-07-20 08:43:48","endLine":244,"groupId":"2637","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testLargeWrites","params":"(booleanpopulateMetaFields)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/3c/3866ee58f8a0c18b9d1b8dd9b3cb5bd528b940.src","preCode":"  public void testLargeWrites() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, Collections.EMPTY_MAP, false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty());\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":202,"status":"M"}],"commitId":"d5026e9a24850bdcce9f6df3686bf2235d7d01c4","commitMessage":"@@@[HUDI-2161] Adding support to disable meta columns with bulk insert operation (#3247)\n\n","date":"2021-07-20 08:43:48","modifiedFileCount":"39","status":"M","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-07-20 08:43:48","codes":[{"authorDate":"2021-07-30 13:22:26","commitOrder":5,"curCode":"  public void testMultipleDataSourceWrites(boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 2; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, Collections.EMPTY_MAP, populateMetaFields, false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime, populateMetaFields);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty(), populateMetaFields);\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2021-07-30 13:22:26","endLine":199,"groupId":"10124","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testMultipleDataSourceWrites","params":"(booleanpopulateMetaFields)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ae/49804614ddd23c3a279b86041bdbe1a72f2d4c.src","preCode":"  public void testMultipleDataSourceWrites(boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, Collections.EMPTY_MAP, populateMetaFields, false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10 + RANDOM.nextInt(1000);\n      int batches = 5; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime, populateMetaFields);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty(), populateMetaFields);\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":158,"status":"M"},{"authorDate":"2021-07-20 08:43:48","commitOrder":5,"curCode":"  public void testLargeWrites(boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, Collections.EMPTY_MAP, populateMetaFields, false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime,\n          populateMetaFields);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty(), populateMetaFields);\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","date":"2021-07-20 08:43:48","endLine":244,"groupId":"10124","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testLargeWrites","params":"(booleanpopulateMetaFields)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/3c/3866ee58f8a0c18b9d1b8dd9b3cb5bd528b940.src","preCode":"  public void testLargeWrites(boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    int partitionCounter = 0;\n\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n          new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, Collections.EMPTY_MAP, populateMetaFields, false);\n      List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n      Dataset<Row> totalInputRows = null;\n      DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(partitionCounter++, RANDOM.nextLong());\n\n      int size = 10000 + RANDOM.nextInt(10000);\n      int batches = 3; \r\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      commitMessages.add(commitMetadata);\n      dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n      metaClient.reloadActiveTimeline();\n\n      Dataset<Row> result = HoodieClientTestUtils.readCommit(basePath, sqlContext, metaClient.getCommitTimeline(), instantTime,\n          populateMetaFields);\n\n      \r\n      assertOutput(totalInputRows, result, instantTime, Option.empty(), populateMetaFields);\n      assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":202,"status":"N"}],"commitId":"7bdae69053afc5ef604a15806d78317cb976f2ce","commitMessage":"@@@[HUDI-2253] Refactoring few tests to reduce runningtime. DeltaStreamer and MultiDeltaStreamer tests. Bulk insert row writer tests (#3371)\n\nCo-authored-by: Sivabalan Narayanan <nsb@Sivabalans-MBP.attlocal.net>","date":"2021-07-30 13:22:26","modifiedFileCount":"6","status":"M","submitter":"Sivabalan Narayanan"}]
