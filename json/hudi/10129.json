[{"authorTime":"2020-12-25 22:43:34","codes":[{"authorDate":"2020-12-25 22:43:34","commitOrder":2,"curCode":"  public DataWriterFactory createBatchWriterFactory(PhysicalWriteInfo info) {\n    dataSourceInternalWriterHelper.createInflightCommit();\n    if (WriteOperationType.BULK_INSERT == dataSourceInternalWriterHelper.getWriteOperationType()) {\n      return new HoodieBulkInsertDataInternalWriterFactory(dataSourceInternalWriterHelper.getHoodieTable(),\n          writeConfig, instantTime, structType);\n    } else {\n      throw new IllegalArgumentException(\"Write Operation Type + \" + dataSourceInternalWriterHelper.getWriteOperationType() + \" not supported \");\n    }\n  }\n","date":"2020-12-25 22:43:34","endLine":68,"groupId":"5122","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"createBatchWriterFactory","params":"(PhysicalWriteInfoinfo)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/b0/945156d703de955d86206863257ee2455dc2b7.src","preCode":"  public DataWriterFactory createBatchWriterFactory(PhysicalWriteInfo info) {\n    dataSourceInternalWriterHelper.createInflightCommit();\n    if (WriteOperationType.BULK_INSERT == dataSourceInternalWriterHelper.getWriteOperationType()) {\n      return new HoodieBulkInsertDataInternalWriterFactory(dataSourceInternalWriterHelper.getHoodieTable(),\n          writeConfig, instantTime, structType);\n    } else {\n      throw new IllegalArgumentException(\"Write Operation Type + \" + dataSourceInternalWriterHelper.getWriteOperationType() + \" not supported \");\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWrite.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":60,"status":"B"},{"authorDate":"2020-12-25 22:43:34","commitOrder":2,"curCode":"  public DataWriterFactory<InternalRow> createWriterFactory() {\n    dataSourceInternalWriterHelper.createInflightCommit();\n    if (WriteOperationType.BULK_INSERT == dataSourceInternalWriterHelper.getWriteOperationType()) {\n      return new HoodieBulkInsertDataInternalWriterFactory(dataSourceInternalWriterHelper.getHoodieTable(),\n          writeConfig, instantTime, structType);\n    } else {\n      throw new IllegalArgumentException(\"Write Operation Type + \" + dataSourceInternalWriterHelper.getWriteOperationType() + \" not supported \");\n    }\n  }\n","date":"2020-12-25 22:43:34","endLine":67,"groupId":"5122","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"createWriterFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/4b/3dafc6264f76e90a67652c20397ef005d731dc.src","preCode":"  public DataWriterFactory<InternalRow> createWriterFactory() {\n    dataSourceInternalWriterHelper.createInflightCommit();\n    if (WriteOperationType.BULK_INSERT == dataSourceInternalWriterHelper.getWriteOperationType()) {\n      return new HoodieBulkInsertDataInternalWriterFactory(dataSourceInternalWriterHelper.getHoodieTable(),\n          writeConfig, instantTime, structType);\n    } else {\n      throw new IllegalArgumentException(\"Write Operation Type + \" + dataSourceInternalWriterHelper.getWriteOperationType() + \" not supported \");\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":59,"status":"MB"}],"commitId":"286055ce34bdbbac68c995a2710fc7be07734b12","commitMessage":"@@@[HUDI-1451] Support bulk insert v2 with Spark 3.0.0 (#2328)\n\nCo-authored-by: Wenning Ding <wenningd@amazon.com>\n\n- Added support for bulk insert v2 with datasource v2 api in Spark 3.0.0.","date":"2020-12-25 22:43:34","modifiedFileCount":"7","status":"M","submitter":"wenningd"},{"authorTime":"2021-07-07 23:15:25","codes":[{"authorDate":"2021-07-07 23:15:25","commitOrder":3,"curCode":"  public DataWriterFactory createBatchWriterFactory(PhysicalWriteInfo info) {\n    dataSourceInternalWriterHelper.createInflightCommit();\n    if (WriteOperationType.BULK_INSERT == dataSourceInternalWriterHelper.getWriteOperationType()) {\n      return new HoodieBulkInsertDataInternalWriterFactory(dataSourceInternalWriterHelper.getHoodieTable(),\n          writeConfig, instantTime, structType, arePartitionRecordsSorted);\n    } else {\n      throw new IllegalArgumentException(\"Write Operation Type + \" + dataSourceInternalWriterHelper.getWriteOperationType() + \" not supported \");\n    }\n  }\n","date":"2021-07-07 23:15:25","endLine":70,"groupId":"1780","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"createBatchWriterFactory","params":"(PhysicalWriteInfoinfo)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/c8/9758a377a92b24e1e38ba5036d286cde228352.src","preCode":"  public DataWriterFactory createBatchWriterFactory(PhysicalWriteInfo info) {\n    dataSourceInternalWriterHelper.createInflightCommit();\n    if (WriteOperationType.BULK_INSERT == dataSourceInternalWriterHelper.getWriteOperationType()) {\n      return new HoodieBulkInsertDataInternalWriterFactory(dataSourceInternalWriterHelper.getHoodieTable(),\n          writeConfig, instantTime, structType);\n    } else {\n      throw new IllegalArgumentException(\"Write Operation Type + \" + dataSourceInternalWriterHelper.getWriteOperationType() + \" not supported \");\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWrite.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":62,"status":"M"},{"authorDate":"2021-07-07 23:15:25","commitOrder":3,"curCode":"  public DataWriterFactory<InternalRow> createWriterFactory() {\n    dataSourceInternalWriterHelper.createInflightCommit();\n    if (WriteOperationType.BULK_INSERT == dataSourceInternalWriterHelper.getWriteOperationType()) {\n      return new HoodieBulkInsertDataInternalWriterFactory(dataSourceInternalWriterHelper.getHoodieTable(),\n          writeConfig, instantTime, structType, arePartitionRecordsSorted);\n    } else {\n      throw new IllegalArgumentException(\"Write Operation Type + \" + dataSourceInternalWriterHelper.getWriteOperationType() + \" not supported \");\n    }\n  }\n","date":"2021-07-07 23:15:25","endLine":69,"groupId":"1780","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"createWriterFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/6a/a5329f86e8d82aef7f5a678195747b32693b7d.src","preCode":"  public DataWriterFactory<InternalRow> createWriterFactory() {\n    dataSourceInternalWriterHelper.createInflightCommit();\n    if (WriteOperationType.BULK_INSERT == dataSourceInternalWriterHelper.getWriteOperationType()) {\n      return new HoodieBulkInsertDataInternalWriterFactory(dataSourceInternalWriterHelper.getHoodieTable(),\n          writeConfig, instantTime, structType);\n    } else {\n      throw new IllegalArgumentException(\"Write Operation Type + \" + dataSourceInternalWriterHelper.getWriteOperationType() + \" not supported \");\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":61,"status":"M"}],"commitId":"ea9e5d0e8b7557ef82631ac173d67f15bad13690","commitMessage":"@@@[HUDI-1104] Adding support for UserDefinedPartitioners and SortModes to BulkInsert with Rows (#3149)\n\n","date":"2021-07-07 23:15:25","modifiedFileCount":"21","status":"M","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-07-20 08:43:48","codes":[{"authorDate":"2021-07-20 08:43:48","commitOrder":4,"curCode":"  public DataWriterFactory createBatchWriterFactory(PhysicalWriteInfo info) {\n    dataSourceInternalWriterHelper.createInflightCommit();\n    if (WriteOperationType.BULK_INSERT == dataSourceInternalWriterHelper.getWriteOperationType()) {\n      return new HoodieBulkInsertDataInternalWriterFactory(dataSourceInternalWriterHelper.getHoodieTable(),\n          writeConfig, instantTime, structType, populateMetaFields, arePartitionRecordsSorted);\n    } else {\n      throw new IllegalArgumentException(\"Write Operation Type + \" + dataSourceInternalWriterHelper.getWriteOperationType() + \" not supported \");\n    }\n  }\n","date":"2021-07-20 08:43:48","endLine":77,"groupId":"10129","id":5,"instanceNumber":1,"isCurCommit":1,"methodName":"createBatchWriterFactory","params":"(PhysicalWriteInfoinfo)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/fb/5f609d79ef2e67bb7a4f20d7627800fe11c872.src","preCode":"  public DataWriterFactory createBatchWriterFactory(PhysicalWriteInfo info) {\n    dataSourceInternalWriterHelper.createInflightCommit();\n    if (WriteOperationType.BULK_INSERT == dataSourceInternalWriterHelper.getWriteOperationType()) {\n      return new HoodieBulkInsertDataInternalWriterFactory(dataSourceInternalWriterHelper.getHoodieTable(),\n          writeConfig, instantTime, structType, arePartitionRecordsSorted);\n    } else {\n      throw new IllegalArgumentException(\"Write Operation Type + \" + dataSourceInternalWriterHelper.getWriteOperationType() + \" not supported \");\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWrite.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":69,"status":"M"},{"authorDate":"2021-07-20 08:43:48","commitOrder":4,"curCode":"  public DataWriterFactory<InternalRow> createWriterFactory() {\n    dataSourceInternalWriterHelper.createInflightCommit();\n    if (WriteOperationType.BULK_INSERT == dataSourceInternalWriterHelper.getWriteOperationType()) {\n      return new HoodieBulkInsertDataInternalWriterFactory(dataSourceInternalWriterHelper.getHoodieTable(),\n          writeConfig, instantTime, structType, populateMetaFields, arePartitionRecordsSorted);\n    } else {\n      throw new IllegalArgumentException(\"Write Operation Type + \" + dataSourceInternalWriterHelper.getWriteOperationType() + \" not supported \");\n    }\n  }\n","date":"2021-07-20 08:43:48","endLine":78,"groupId":"10129","id":6,"instanceNumber":2,"isCurCommit":1,"methodName":"createWriterFactory","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/c4/b21483e8fee5d619af82dc937306d5262b7a13.src","preCode":"  public DataWriterFactory<InternalRow> createWriterFactory() {\n    dataSourceInternalWriterHelper.createInflightCommit();\n    if (WriteOperationType.BULK_INSERT == dataSourceInternalWriterHelper.getWriteOperationType()) {\n      return new HoodieBulkInsertDataInternalWriterFactory(dataSourceInternalWriterHelper.getHoodieTable(),\n          writeConfig, instantTime, structType, arePartitionRecordsSorted);\n    } else {\n      throw new IllegalArgumentException(\"Write Operation Type + \" + dataSourceInternalWriterHelper.getWriteOperationType() + \" not supported \");\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/main/java/org/apache/hudi/internal/HoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":70,"status":"M"}],"commitId":"d5026e9a24850bdcce9f6df3686bf2235d7d01c4","commitMessage":"@@@[HUDI-2161] Adding support to disable meta columns with bulk insert operation (#3247)\n\n","date":"2021-07-20 08:43:48","modifiedFileCount":"39","status":"M","submitter":"Sivabalan Narayanan"}]
