[{"authorTime":"2020-10-12 14:39:10","codes":[{"authorDate":"2020-12-13 22:28:53","commitOrder":3,"curCode":"  public void testKeepLatestCommitsMOR() throws Exception {\n\n    HoodieWriteConfig config =\n            HoodieWriteConfig.newBuilder().withPath(basePath).withAssumeDatePartitioning(true)\n                    .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                            .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS).retainCommits(1).build())\n                    .build();\n\n    HoodieTableMetaClient metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient);\n    String p0 = \"2020/01/01\";\n\n    \r\n    String file1P0 = testTable.addDeltaCommit(\"000\").getFileIdsWithBaseFilesInPartitions(p0).get(p0);\n    testTable.forDeltaCommit(\"000\")\n            .withLogFile(p0, file1P0, 1)\n            .withLogFile(p0, file1P0, 2);\n\n    \r\n    testTable.addDeltaCommit(\"001\")\n            .withBaseFilesInPartition(p0, file1P0)\n            .withLogFile(p0, file1P0, 3);\n\n    \r\n    testTable.addDeltaCommit(\"002\")\n            .withBaseFilesInPartition(p0, file1P0)\n            .withLogFile(p0, file1P0, 4);\n\n    List<HoodieCleanStat> hoodieCleanStats = runCleaner(config);\n    assertEquals(3,\n            getCleanStat(hoodieCleanStats, p0).getSuccessDeleteFiles()\n                    .size(), \"Must clean three files, one parquet and 2 log files\");\n    assertFalse(testTable.baseFileExists(p0, \"000\", file1P0));\n    assertFalse(testTable.logFilesExist(p0, \"000\", file1P0, 1, 2));\n    assertTrue(testTable.baseFileExists(p0, \"001\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"001\", file1P0, 3));\n    assertTrue(testTable.baseFileExists(p0, \"002\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"002\", file1P0, 4));\n  }\n","date":"2020-12-13 22:28:53","endLine":689,"groupId":"4571","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testKeepLatestCommitsMOR","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/69/c6f98c672379e003ed7d94aa5ee322151b8851.src","preCode":"  public void testKeepLatestCommitsMOR() throws Exception {\n\n    HoodieWriteConfig config =\n            HoodieWriteConfig.newBuilder().withPath(basePath).withAssumeDatePartitioning(true)\n                    .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                            .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS).retainCommits(1).build())\n                    .build();\n\n    HoodieTableMetaClient metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient);\n    String p0 = \"2020/01/01\";\n\n    \r\n    String file1P0 = testTable.addDeltaCommit(\"000\").getFileIdsWithBaseFilesInPartitions(p0).get(p0);\n    testTable.forDeltaCommit(\"000\")\n            .withLogFile(p0, file1P0, 1)\n            .withLogFile(p0, file1P0, 2);\n\n    \r\n    testTable.addDeltaCommit(\"001\")\n            .withBaseFilesInPartition(p0, file1P0)\n            .withLogFile(p0, file1P0, 3);\n\n    \r\n    testTable.addDeltaCommit(\"002\")\n            .withBaseFilesInPartition(p0, file1P0)\n            .withLogFile(p0, file1P0, 4);\n\n    List<HoodieCleanStat> hoodieCleanStats = runCleaner(config);\n    assertEquals(3,\n            getCleanStat(hoodieCleanStats, p0).getSuccessDeleteFiles()\n                    .size(), \"Must clean three files, one parquet and 2 log files\");\n    assertFalse(testTable.baseFileExists(p0, \"000\", file1P0));\n    assertFalse(testTable.logFilesExist(p0, \"000\", file1P0, 1, 2));\n    assertTrue(testTable.baseFileExists(p0, \"001\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"001\", file1P0, 3));\n    assertTrue(testTable.baseFileExists(p0, \"002\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"002\", file1P0, 4));\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":651,"status":"B"},{"authorDate":"2020-10-12 14:39:10","commitOrder":3,"curCode":"  public void testKeepLatestVersionsWithPendingCompactions(boolean retryFailure) throws Exception {\n    HoodieWriteConfig config =\n        HoodieWriteConfig.newBuilder().withPath(basePath).withAssumeDatePartitioning(true)\n            .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS).retainFileVersions(2).build())\n            .build();\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    testPendingCompactions(config, 36, 9, retryFailure);\n  }\n","date":"2020-10-12 14:39:10","endLine":1061,"groupId":"4833","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testKeepLatestVersionsWithPendingCompactions","params":"(booleanretryFailure)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/00/f1ea00ea94bf95a807008e252d2a524e4b64ab.src","preCode":"  public void testKeepLatestVersionsWithPendingCompactions(boolean retryFailure) throws Exception {\n    HoodieWriteConfig config =\n        HoodieWriteConfig.newBuilder().withPath(basePath).withAssumeDatePartitioning(true)\n            .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS).retainFileVersions(2).build())\n            .build();\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    testPendingCompactions(config, 36, 9, retryFailure);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":1045,"status":"NB"}],"commitId":"11bc1fe6f498850d2c496151741813001d3850a3","commitMessage":"@@@[HUDI-1428] Clean old fileslice is invalid (#2292)\n\n\nCo-authored-by: zhang wen <wen.zhang@dmall.com>\nCo-authored-by: zhang wen <steven@stevendeMac-mini.local>","date":"2020-12-13 22:28:53","modifiedFileCount":"2","status":"M","submitter":"steven zhang"},{"authorTime":"2021-01-20 13:20:28","codes":[{"authorDate":"2021-01-20 13:20:28","commitOrder":4,"curCode":"  public void testKeepLatestCommitsMOR() throws Exception {\n\n    HoodieWriteConfig config =\n            HoodieWriteConfig.newBuilder().withPath(basePath)\n                .withMetadataConfig(HoodieMetadataConfig.newBuilder().withAssumeDatePartitioning(true).build())\n                    .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                            .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS).retainCommits(1).build())\n                    .build();\n\n    HoodieTableMetaClient metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient);\n    String p0 = \"2020/01/01\";\n\n    \r\n    String file1P0 = testTable.addDeltaCommit(\"000\").getFileIdsWithBaseFilesInPartitions(p0).get(p0);\n    testTable.forDeltaCommit(\"000\")\n            .withLogFile(p0, file1P0, 1)\n            .withLogFile(p0, file1P0, 2);\n\n    \r\n    testTable.addDeltaCommit(\"001\")\n            .withBaseFilesInPartition(p0, file1P0)\n            .withLogFile(p0, file1P0, 3);\n\n    \r\n    testTable.addDeltaCommit(\"002\")\n            .withBaseFilesInPartition(p0, file1P0)\n            .withLogFile(p0, file1P0, 4);\n\n    List<HoodieCleanStat> hoodieCleanStats = runCleaner(config);\n    assertEquals(3,\n            getCleanStat(hoodieCleanStats, p0).getSuccessDeleteFiles()\n                    .size(), \"Must clean three files, one parquet and 2 log files\");\n    assertFalse(testTable.baseFileExists(p0, \"000\", file1P0));\n    assertFalse(testTable.logFilesExist(p0, \"000\", file1P0, 1, 2));\n    assertTrue(testTable.baseFileExists(p0, \"001\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"001\", file1P0, 3));\n    assertTrue(testTable.baseFileExists(p0, \"002\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"002\", file1P0, 4));\n  }\n","date":"2021-01-20 13:20:28","endLine":694,"groupId":"4571","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testKeepLatestCommitsMOR","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/4f/ff08abfbcef6651c952fa0d168eff4cfa8682c.src","preCode":"  public void testKeepLatestCommitsMOR() throws Exception {\n\n    HoodieWriteConfig config =\n            HoodieWriteConfig.newBuilder().withPath(basePath).withAssumeDatePartitioning(true)\n                    .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                            .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS).retainCommits(1).build())\n                    .build();\n\n    HoodieTableMetaClient metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient);\n    String p0 = \"2020/01/01\";\n\n    \r\n    String file1P0 = testTable.addDeltaCommit(\"000\").getFileIdsWithBaseFilesInPartitions(p0).get(p0);\n    testTable.forDeltaCommit(\"000\")\n            .withLogFile(p0, file1P0, 1)\n            .withLogFile(p0, file1P0, 2);\n\n    \r\n    testTable.addDeltaCommit(\"001\")\n            .withBaseFilesInPartition(p0, file1P0)\n            .withLogFile(p0, file1P0, 3);\n\n    \r\n    testTable.addDeltaCommit(\"002\")\n            .withBaseFilesInPartition(p0, file1P0)\n            .withLogFile(p0, file1P0, 4);\n\n    List<HoodieCleanStat> hoodieCleanStats = runCleaner(config);\n    assertEquals(3,\n            getCleanStat(hoodieCleanStats, p0).getSuccessDeleteFiles()\n                    .size(), \"Must clean three files, one parquet and 2 log files\");\n    assertFalse(testTable.baseFileExists(p0, \"000\", file1P0));\n    assertFalse(testTable.logFilesExist(p0, \"000\", file1P0, 1, 2));\n    assertTrue(testTable.baseFileExists(p0, \"001\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"001\", file1P0, 3));\n    assertTrue(testTable.baseFileExists(p0, \"002\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"002\", file1P0, 4));\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":655,"status":"M"},{"authorDate":"2021-01-20 13:20:28","commitOrder":4,"curCode":"  public void testKeepLatestVersionsWithPendingCompactions(boolean retryFailure) throws Exception {\n    HoodieWriteConfig config =\n        HoodieWriteConfig.newBuilder().withPath(basePath)\n            .withMetadataConfig(HoodieMetadataConfig.newBuilder().withAssumeDatePartitioning(true).build())\n            .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS).retainFileVersions(2).build())\n            .build();\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    testPendingCompactions(config, 36, 9, retryFailure);\n  }\n","date":"2021-01-20 13:20:28","endLine":1216,"groupId":"4833","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testKeepLatestVersionsWithPendingCompactions","params":"(booleanretryFailure)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/4f/ff08abfbcef6651c952fa0d168eff4cfa8682c.src","preCode":"  public void testKeepLatestVersionsWithPendingCompactions(boolean retryFailure) throws Exception {\n    HoodieWriteConfig config =\n        HoodieWriteConfig.newBuilder().withPath(basePath).withAssumeDatePartitioning(true)\n            .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS).retainFileVersions(2).build())\n            .build();\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    testPendingCompactions(config, 36, 9, retryFailure);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":1199,"status":"M"}],"commitId":"5ca0625b277efa3a73d2ae0fbdfa4c6163f312d2","commitMessage":"@@@[HUDI 1308] Harden RFC-15 Implementation based on production testing (#2441)\n\nAddresses leaks.  perf degradation observed during testing. These were regressions from the original rfc-15 PoC implementation.\n\n* Pass a single instance of HoodieTableMetadata everywhere\n* Fix tests and add config for enabling metrics\n - Removed special casing of assumeDatePartitioning inside FSUtils#getAllPartitionPaths()\n - Consequently.  IOException is never thrown and many files had to be adjusted\n- More diligent handling of open file handles in metadata table\n - Added config for controlling reuse of connections\n - Added config for turning off fallback to listing.  so we can see tests fail\n - Changed all ipf listing code to cache/amortize the open/close for better performance\n - Timelineserver also reuses connections.  for better performance\n - Without timelineserver.  when metadata table is opened from executors.  reuse is not allowed\n - HoodieMetadataConfig passed into HoodieTableMetadata#create as argument.\n -  Fix TestHoodieBackedTableMetadata#testSync","date":"2021-01-20 13:20:28","modifiedFileCount":"53","status":"M","submitter":"vinoth chandar"},{"authorTime":"2021-01-20 13:20:28","codes":[{"authorDate":"2021-05-12 01:01:45","commitOrder":5,"curCode":"  public void testKeepLatestCommitsMOR() throws Exception {\n\n    HoodieWriteConfig config =\n            HoodieWriteConfig.newBuilder().withPath(basePath)\n                .withMetadataConfig(HoodieMetadataConfig.newBuilder().withAssumeDatePartitioning(true).build())\n                    .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                            .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS).retainCommits(1).build())\n                    .build();\n\n    HoodieTableMetaClient metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient);\n    String p0 = \"2020/01/01\";\n\n    \r\n    String file1P0 = testTable.addDeltaCommit(\"000\").getFileIdsWithBaseFilesInPartitions(p0).get(p0);\n    testTable.forDeltaCommit(\"000\")\n            .withLogFile(p0, file1P0, 1)\n            .withLogFile(p0, file1P0, 2);\n\n    \r\n    testTable.addDeltaCommit(\"001\")\n            .withBaseFilesInPartition(p0, file1P0)\n            .withLogFile(p0, file1P0, 3);\n\n    \r\n    testTable.addDeltaCommit(\"002\")\n            .withBaseFilesInPartition(p0, file1P0)\n            .withLogFile(p0, file1P0, 4);\n\n    List<HoodieCleanStat> hoodieCleanStats = runCleaner(config);\n    assertEquals(3,\n            getCleanStat(hoodieCleanStats, p0).getSuccessDeleteFiles()\n                    .size(), \"Must clean three files, one base and 2 log files\");\n    assertFalse(testTable.baseFileExists(p0, \"000\", file1P0));\n    assertFalse(testTable.logFilesExist(p0, \"000\", file1P0, 1, 2));\n    assertTrue(testTable.baseFileExists(p0, \"001\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"001\", file1P0, 3));\n    assertTrue(testTable.baseFileExists(p0, \"002\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"002\", file1P0, 4));\n  }\n","date":"2021-05-12 01:01:45","endLine":807,"groupId":"10541","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testKeepLatestCommitsMOR","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/cb/37ed4bcf2732df46a8ab47fda68fa27b9d1582.src","preCode":"  public void testKeepLatestCommitsMOR() throws Exception {\n\n    HoodieWriteConfig config =\n            HoodieWriteConfig.newBuilder().withPath(basePath)\n                .withMetadataConfig(HoodieMetadataConfig.newBuilder().withAssumeDatePartitioning(true).build())\n                    .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                            .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS).retainCommits(1).build())\n                    .build();\n\n    HoodieTableMetaClient metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient);\n    String p0 = \"2020/01/01\";\n\n    \r\n    String file1P0 = testTable.addDeltaCommit(\"000\").getFileIdsWithBaseFilesInPartitions(p0).get(p0);\n    testTable.forDeltaCommit(\"000\")\n            .withLogFile(p0, file1P0, 1)\n            .withLogFile(p0, file1P0, 2);\n\n    \r\n    testTable.addDeltaCommit(\"001\")\n            .withBaseFilesInPartition(p0, file1P0)\n            .withLogFile(p0, file1P0, 3);\n\n    \r\n    testTable.addDeltaCommit(\"002\")\n            .withBaseFilesInPartition(p0, file1P0)\n            .withLogFile(p0, file1P0, 4);\n\n    List<HoodieCleanStat> hoodieCleanStats = runCleaner(config);\n    assertEquals(3,\n            getCleanStat(hoodieCleanStats, p0).getSuccessDeleteFiles()\n                    .size(), \"Must clean three files, one parquet and 2 log files\");\n    assertFalse(testTable.baseFileExists(p0, \"000\", file1P0));\n    assertFalse(testTable.logFilesExist(p0, \"000\", file1P0, 1, 2));\n    assertTrue(testTable.baseFileExists(p0, \"001\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"001\", file1P0, 3));\n    assertTrue(testTable.baseFileExists(p0, \"002\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"002\", file1P0, 4));\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":768,"status":"M"},{"authorDate":"2021-01-20 13:20:28","commitOrder":5,"curCode":"  public void testKeepLatestVersionsWithPendingCompactions(boolean retryFailure) throws Exception {\n    HoodieWriteConfig config =\n        HoodieWriteConfig.newBuilder().withPath(basePath)\n            .withMetadataConfig(HoodieMetadataConfig.newBuilder().withAssumeDatePartitioning(true).build())\n            .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS).retainFileVersions(2).build())\n            .build();\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    testPendingCompactions(config, 36, 9, retryFailure);\n  }\n","date":"2021-01-20 13:20:28","endLine":1216,"groupId":"10541","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testKeepLatestVersionsWithPendingCompactions","params":"(booleanretryFailure)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/4f/ff08abfbcef6651c952fa0d168eff4cfa8682c.src","preCode":"  public void testKeepLatestVersionsWithPendingCompactions(boolean retryFailure) throws Exception {\n    HoodieWriteConfig config =\n        HoodieWriteConfig.newBuilder().withPath(basePath)\n            .withMetadataConfig(HoodieMetadataConfig.newBuilder().withAssumeDatePartitioning(true).build())\n            .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS).retainFileVersions(2).build())\n            .build();\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    testPendingCompactions(config, 36, 9, retryFailure);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":1199,"status":"N"}],"commitId":"be9db2c4f5a570fcaa555618b34ad11109ed6b00","commitMessage":"@@@[HUDI-1055] Remove hardcoded parquet in tests (#2740)\n\n* Remove hardcoded parquet in tests\n* Use DataFileUtils.getInstance\n* Renaming DataFileUtils to BaseFileUtils\n\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2021-05-12 01:01:45","modifiedFileCount":"40","status":"M","submitter":"TeRS-K"}]
