[{"authorTime":"2020-12-20 11:25:27","codes":[{"authorDate":"2020-10-02 05:25:29","commitOrder":4,"curCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n    SparkConf sparkConf = HoodieExampleSparkUtils.defaultSparkConf(\"hoodie-client-example\");\n\n    try (JavaSparkContext jsc = new JavaSparkContext(sparkConf)) {\n\n      \r\n      HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n      \r\n      Path path = new Path(tablePath);\n      FileSystem fs = FSUtils.getFs(tablePath, jsc.hadoopConfiguration());\n      if (!fs.exists(path)) {\n        HoodieTableMetaClient.initTableType(jsc.hadoopConfiguration(), tablePath, HoodieTableType.valueOf(tableType),\n                tableName, HoodieAvroPayload.class.getName());\n      }\n\n      \r\n      HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n              .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n              .withDeleteParallelism(2).forTable(tableName)\n              .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())\n              .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n      SparkRDDWriteClient<HoodieAvroPayload> client = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jsc), cfg);\n\n      \r\n      String newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n\n      List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n      List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n      JavaRDD<HoodieRecord<HoodieAvroPayload>> writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n      records.addAll(toBeUpdated);\n      recordsSoFar.addAll(toBeUpdated);\n      writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      \r\n      int numToDelete = recordsSoFar.size() / 2;\n      List<HoodieKey> toBeDeleted = recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n      JavaRDD<HoodieKey> deleteRecords = jsc.parallelize(toBeDeleted, 1);\n      client.delete(deleteRecords, newCommitTime);\n\n      \r\n      if (HoodieTableType.valueOf(tableType) == HoodieTableType.MERGE_ON_READ) {\n        Option<String> instant = client.scheduleCompaction(Option.empty());\n        JavaRDD<WriteStatus> writeStatues = client.compact(instant.get());\n        client.commitCompaction(instant.get(), writeStatues, Option.empty());\n      }\n\n    }\n  }\n","date":"2020-10-02 05:25:29","endLine":135,"groupId":"4294","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/b6/06c527b0306ee54e00972e514941f10abd1b59.src","preCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n    SparkConf sparkConf = HoodieExampleSparkUtils.defaultSparkConf(\"hoodie-client-example\");\n\n    try (JavaSparkContext jsc = new JavaSparkContext(sparkConf)) {\n\n      \r\n      HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n      \r\n      Path path = new Path(tablePath);\n      FileSystem fs = FSUtils.getFs(tablePath, jsc.hadoopConfiguration());\n      if (!fs.exists(path)) {\n        HoodieTableMetaClient.initTableType(jsc.hadoopConfiguration(), tablePath, HoodieTableType.valueOf(tableType),\n                tableName, HoodieAvroPayload.class.getName());\n      }\n\n      \r\n      HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n              .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n              .withDeleteParallelism(2).forTable(tableName)\n              .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())\n              .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n      SparkRDDWriteClient<HoodieAvroPayload> client = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jsc), cfg);\n\n      \r\n      String newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n\n      List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n      List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n      JavaRDD<HoodieRecord<HoodieAvroPayload>> writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n      records.addAll(toBeUpdated);\n      recordsSoFar.addAll(toBeUpdated);\n      writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      \r\n      int numToDelete = recordsSoFar.size() / 2;\n      List<HoodieKey> toBeDeleted = recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n      JavaRDD<HoodieKey> deleteRecords = jsc.parallelize(toBeDeleted, 1);\n      client.delete(deleteRecords, newCommitTime);\n\n      \r\n      if (HoodieTableType.valueOf(tableType) == HoodieTableType.MERGE_ON_READ) {\n        Option<String> instant = client.scheduleCompaction(Option.empty());\n        JavaRDD<WriteStatus> writeStatues = client.compact(instant.get());\n        client.commitCompaction(instant.get(), writeStatues, Option.empty());\n      }\n\n    }\n  }\n","realPath":"hudi-examples/src/main/java/org/apache/hudi/examples/spark/HoodieWriteClientExample.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":70,"status":"NB"},{"authorDate":"2020-12-20 11:25:27","commitOrder":4,"curCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n\n    \r\n    HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n    Configuration hadoopConf = new Configuration();\n    \r\n    Path path = new Path(tablePath);\n    FileSystem fs = FSUtils.getFs(tablePath, hadoopConf);\n    if (!fs.exists(path)) {\n      HoodieTableMetaClient.initTableType(hadoopConf, tablePath, HoodieTableType.valueOf(tableType),\n          tableName, HoodieAvroPayload.class.getName());\n    }\n\n    \r\n    HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n        .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n        .withDeleteParallelism(2).forTable(tableName)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build())\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n    HoodieJavaWriteClient<HoodieAvroPayload> client =\n        new HoodieJavaWriteClient<>(new HoodieJavaEngineContext(hadoopConf), cfg);\n\n    \r\n    String newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n\n    List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n    List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n    List<HoodieRecord<HoodieAvroPayload>> writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n    records.addAll(toBeUpdated);\n    recordsSoFar.addAll(toBeUpdated);\n    writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    client.close();\n  }\n","date":"2020-12-20 11:25:27","endLine":108,"groupId":"193","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/31/fccfa7725e8945a8395307c14a36abfa5ad9c1.src","preCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n\n    \r\n    HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n    Configuration hadoopConf = new Configuration();\n    \r\n    Path path = new Path(tablePath);\n    FileSystem fs = FSUtils.getFs(tablePath, hadoopConf);\n    if (!fs.exists(path)) {\n      HoodieTableMetaClient.initTableType(hadoopConf, tablePath, HoodieTableType.valueOf(tableType),\n          tableName, HoodieAvroPayload.class.getName());\n    }\n\n    \r\n    HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n        .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n        .withDeleteParallelism(2).forTable(tableName)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build())\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n    HoodieJavaWriteClient<HoodieAvroPayload> client =\n        new HoodieJavaWriteClient<>(new HoodieJavaEngineContext(hadoopConf), cfg);\n\n    \r\n    String newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n\n    List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n    List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n    List<HoodieRecord<HoodieAvroPayload>> writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n    records.addAll(toBeUpdated);\n    recordsSoFar.addAll(toBeUpdated);\n    writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    client.close();\n  }\n","realPath":"hudi-examples/src/main/java/org/apache/hudi/examples/java/HoodieJavaWriteClientExample.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":58,"status":"B"}],"commitId":"e4e2fbc3bb2c4796c6813114dd1c37ffa5a1e03a","commitMessage":"@@@[HUDI-1419] Add base implementation for hudi java client (#2286)\n\n","date":"2020-12-20 11:25:27","modifiedFileCount":"2","status":"M","submitter":"Shen Hong"},{"authorTime":"2021-01-03 20:38:45","codes":[{"authorDate":"2020-10-02 05:25:29","commitOrder":5,"curCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n    SparkConf sparkConf = HoodieExampleSparkUtils.defaultSparkConf(\"hoodie-client-example\");\n\n    try (JavaSparkContext jsc = new JavaSparkContext(sparkConf)) {\n\n      \r\n      HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n      \r\n      Path path = new Path(tablePath);\n      FileSystem fs = FSUtils.getFs(tablePath, jsc.hadoopConfiguration());\n      if (!fs.exists(path)) {\n        HoodieTableMetaClient.initTableType(jsc.hadoopConfiguration(), tablePath, HoodieTableType.valueOf(tableType),\n                tableName, HoodieAvroPayload.class.getName());\n      }\n\n      \r\n      HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n              .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n              .withDeleteParallelism(2).forTable(tableName)\n              .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())\n              .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n      SparkRDDWriteClient<HoodieAvroPayload> client = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jsc), cfg);\n\n      \r\n      String newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n\n      List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n      List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n      JavaRDD<HoodieRecord<HoodieAvroPayload>> writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n      records.addAll(toBeUpdated);\n      recordsSoFar.addAll(toBeUpdated);\n      writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      \r\n      int numToDelete = recordsSoFar.size() / 2;\n      List<HoodieKey> toBeDeleted = recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n      JavaRDD<HoodieKey> deleteRecords = jsc.parallelize(toBeDeleted, 1);\n      client.delete(deleteRecords, newCommitTime);\n\n      \r\n      if (HoodieTableType.valueOf(tableType) == HoodieTableType.MERGE_ON_READ) {\n        Option<String> instant = client.scheduleCompaction(Option.empty());\n        JavaRDD<WriteStatus> writeStatues = client.compact(instant.get());\n        client.commitCompaction(instant.get(), writeStatues, Option.empty());\n      }\n\n    }\n  }\n","date":"2020-10-02 05:25:29","endLine":135,"groupId":"4294","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/b6/06c527b0306ee54e00972e514941f10abd1b59.src","preCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n    SparkConf sparkConf = HoodieExampleSparkUtils.defaultSparkConf(\"hoodie-client-example\");\n\n    try (JavaSparkContext jsc = new JavaSparkContext(sparkConf)) {\n\n      \r\n      HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n      \r\n      Path path = new Path(tablePath);\n      FileSystem fs = FSUtils.getFs(tablePath, jsc.hadoopConfiguration());\n      if (!fs.exists(path)) {\n        HoodieTableMetaClient.initTableType(jsc.hadoopConfiguration(), tablePath, HoodieTableType.valueOf(tableType),\n                tableName, HoodieAvroPayload.class.getName());\n      }\n\n      \r\n      HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n              .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n              .withDeleteParallelism(2).forTable(tableName)\n              .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())\n              .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n      SparkRDDWriteClient<HoodieAvroPayload> client = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jsc), cfg);\n\n      \r\n      String newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n\n      List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n      List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n      JavaRDD<HoodieRecord<HoodieAvroPayload>> writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n      records.addAll(toBeUpdated);\n      recordsSoFar.addAll(toBeUpdated);\n      writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      \r\n      int numToDelete = recordsSoFar.size() / 2;\n      List<HoodieKey> toBeDeleted = recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n      JavaRDD<HoodieKey> deleteRecords = jsc.parallelize(toBeDeleted, 1);\n      client.delete(deleteRecords, newCommitTime);\n\n      \r\n      if (HoodieTableType.valueOf(tableType) == HoodieTableType.MERGE_ON_READ) {\n        Option<String> instant = client.scheduleCompaction(Option.empty());\n        JavaRDD<WriteStatus> writeStatues = client.compact(instant.get());\n        client.commitCompaction(instant.get(), writeStatues, Option.empty());\n      }\n\n    }\n  }\n","realPath":"hudi-examples/src/main/java/org/apache/hudi/examples/spark/HoodieWriteClientExample.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":70,"status":"N"},{"authorDate":"2021-01-03 20:38:45","commitOrder":5,"curCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n\n    \r\n    HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n    Configuration hadoopConf = new Configuration();\n    \r\n    Path path = new Path(tablePath);\n    FileSystem fs = FSUtils.getFs(tablePath, hadoopConf);\n    if (!fs.exists(path)) {\n      HoodieTableMetaClient.initTableType(hadoopConf, tablePath, HoodieTableType.valueOf(tableType),\n          tableName, HoodieAvroPayload.class.getName());\n    }\n\n    \r\n    HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n        .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n        .withDeleteParallelism(2).forTable(tableName)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build())\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n    HoodieJavaWriteClient<HoodieAvroPayload> client =\n        new HoodieJavaWriteClient<>(new HoodieJavaEngineContext(hadoopConf), cfg);\n\n    \r\n    String newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n\n    List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n    List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n    List<HoodieRecord<HoodieAvroPayload>> writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n    records.addAll(toBeUpdated);\n    recordsSoFar.addAll(toBeUpdated);\n    writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    \r\n    int numToDelete = recordsSoFar.size() / 2;\n    List<HoodieKey> toBeDeleted =\n        recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n    client.delete(toBeDeleted, newCommitTime);\n\n    client.close();\n  }\n","date":"2021-01-03 20:38:45","endLine":118,"groupId":"193","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/6c/b1ea9d2fda574be6285b8b9be528a23fdad745.src","preCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n\n    \r\n    HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n    Configuration hadoopConf = new Configuration();\n    \r\n    Path path = new Path(tablePath);\n    FileSystem fs = FSUtils.getFs(tablePath, hadoopConf);\n    if (!fs.exists(path)) {\n      HoodieTableMetaClient.initTableType(hadoopConf, tablePath, HoodieTableType.valueOf(tableType),\n          tableName, HoodieAvroPayload.class.getName());\n    }\n\n    \r\n    HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n        .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n        .withDeleteParallelism(2).forTable(tableName)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build())\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n    HoodieJavaWriteClient<HoodieAvroPayload> client =\n        new HoodieJavaWriteClient<>(new HoodieJavaEngineContext(hadoopConf), cfg);\n\n    \r\n    String newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n\n    List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n    List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n    List<HoodieRecord<HoodieAvroPayload>> writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n    records.addAll(toBeUpdated);\n    recordsSoFar.addAll(toBeUpdated);\n    writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    client.close();\n  }\n","realPath":"hudi-examples/src/main/java/org/apache/hudi/examples/java/HoodieJavaWriteClientExample.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":59,"status":"M"}],"commitId":"ff8313caf1dfe0b13fa9f160489a74600b0e8756","commitMessage":"@@@[HUDI-1423] Support delete in hudi-java-client (#2353)\n\n","date":"2021-01-03 20:38:45","modifiedFileCount":"3","status":"M","submitter":"Shen Hong"},{"authorTime":"2021-02-10 02:33:34","codes":[{"authorDate":"2020-10-02 05:25:29","commitOrder":6,"curCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n    SparkConf sparkConf = HoodieExampleSparkUtils.defaultSparkConf(\"hoodie-client-example\");\n\n    try (JavaSparkContext jsc = new JavaSparkContext(sparkConf)) {\n\n      \r\n      HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n      \r\n      Path path = new Path(tablePath);\n      FileSystem fs = FSUtils.getFs(tablePath, jsc.hadoopConfiguration());\n      if (!fs.exists(path)) {\n        HoodieTableMetaClient.initTableType(jsc.hadoopConfiguration(), tablePath, HoodieTableType.valueOf(tableType),\n                tableName, HoodieAvroPayload.class.getName());\n      }\n\n      \r\n      HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n              .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n              .withDeleteParallelism(2).forTable(tableName)\n              .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())\n              .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n      SparkRDDWriteClient<HoodieAvroPayload> client = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jsc), cfg);\n\n      \r\n      String newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n\n      List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n      List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n      JavaRDD<HoodieRecord<HoodieAvroPayload>> writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n      records.addAll(toBeUpdated);\n      recordsSoFar.addAll(toBeUpdated);\n      writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      \r\n      int numToDelete = recordsSoFar.size() / 2;\n      List<HoodieKey> toBeDeleted = recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n      JavaRDD<HoodieKey> deleteRecords = jsc.parallelize(toBeDeleted, 1);\n      client.delete(deleteRecords, newCommitTime);\n\n      \r\n      if (HoodieTableType.valueOf(tableType) == HoodieTableType.MERGE_ON_READ) {\n        Option<String> instant = client.scheduleCompaction(Option.empty());\n        JavaRDD<WriteStatus> writeStatues = client.compact(instant.get());\n        client.commitCompaction(instant.get(), writeStatues, Option.empty());\n      }\n\n    }\n  }\n","date":"2020-10-02 05:25:29","endLine":135,"groupId":"4294","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/b6/06c527b0306ee54e00972e514941f10abd1b59.src","preCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n    SparkConf sparkConf = HoodieExampleSparkUtils.defaultSparkConf(\"hoodie-client-example\");\n\n    try (JavaSparkContext jsc = new JavaSparkContext(sparkConf)) {\n\n      \r\n      HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n      \r\n      Path path = new Path(tablePath);\n      FileSystem fs = FSUtils.getFs(tablePath, jsc.hadoopConfiguration());\n      if (!fs.exists(path)) {\n        HoodieTableMetaClient.initTableType(jsc.hadoopConfiguration(), tablePath, HoodieTableType.valueOf(tableType),\n                tableName, HoodieAvroPayload.class.getName());\n      }\n\n      \r\n      HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n              .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n              .withDeleteParallelism(2).forTable(tableName)\n              .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())\n              .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n      SparkRDDWriteClient<HoodieAvroPayload> client = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jsc), cfg);\n\n      \r\n      String newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n\n      List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n      List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n      JavaRDD<HoodieRecord<HoodieAvroPayload>> writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n      records.addAll(toBeUpdated);\n      recordsSoFar.addAll(toBeUpdated);\n      writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      \r\n      int numToDelete = recordsSoFar.size() / 2;\n      List<HoodieKey> toBeDeleted = recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n      JavaRDD<HoodieKey> deleteRecords = jsc.parallelize(toBeDeleted, 1);\n      client.delete(deleteRecords, newCommitTime);\n\n      \r\n      if (HoodieTableType.valueOf(tableType) == HoodieTableType.MERGE_ON_READ) {\n        Option<String> instant = client.scheduleCompaction(Option.empty());\n        JavaRDD<WriteStatus> writeStatues = client.compact(instant.get());\n        client.commitCompaction(instant.get(), writeStatues, Option.empty());\n      }\n\n    }\n  }\n","realPath":"hudi-examples/src/main/java/org/apache/hudi/examples/spark/HoodieWriteClientExample.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":70,"status":"N"},{"authorDate":"2021-02-10 02:33:34","commitOrder":6,"curCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieJavaWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n\n    \r\n    HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n    Configuration hadoopConf = new Configuration();\n    \r\n    Path path = new Path(tablePath);\n    FileSystem fs = FSUtils.getFs(tablePath, hadoopConf);\n    if (!fs.exists(path)) {\n      HoodieTableMetaClient.initTableType(hadoopConf, tablePath, HoodieTableType.valueOf(tableType),\n          tableName, HoodieAvroPayload.class.getName());\n    }\n\n    \r\n    HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n        .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n        .withDeleteParallelism(2).forTable(tableName)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build())\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n    HoodieJavaWriteClient<HoodieAvroPayload> client =\n        new HoodieJavaWriteClient<>(new HoodieJavaEngineContext(hadoopConf), cfg);\n\n    \r\n    String newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n\n    List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n    List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n    List<HoodieRecord<HoodieAvroPayload>> writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n    records.addAll(toBeUpdated);\n    recordsSoFar.addAll(toBeUpdated);\n    writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    \r\n    int numToDelete = recordsSoFar.size() / 2;\n    List<HoodieKey> toBeDeleted =\n        recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n    client.delete(toBeDeleted, newCommitTime);\n\n    client.close();\n  }\n","date":"2021-02-10 02:33:34","endLine":118,"groupId":"193","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/1e/e5d1a09327bc17e55f4db58c189a1a537123c6.src","preCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n\n    \r\n    HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n    Configuration hadoopConf = new Configuration();\n    \r\n    Path path = new Path(tablePath);\n    FileSystem fs = FSUtils.getFs(tablePath, hadoopConf);\n    if (!fs.exists(path)) {\n      HoodieTableMetaClient.initTableType(hadoopConf, tablePath, HoodieTableType.valueOf(tableType),\n          tableName, HoodieAvroPayload.class.getName());\n    }\n\n    \r\n    HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n        .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n        .withDeleteParallelism(2).forTable(tableName)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build())\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n    HoodieJavaWriteClient<HoodieAvroPayload> client =\n        new HoodieJavaWriteClient<>(new HoodieJavaEngineContext(hadoopConf), cfg);\n\n    \r\n    String newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n\n    List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n    List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n    List<HoodieRecord<HoodieAvroPayload>> writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n    records.addAll(toBeUpdated);\n    recordsSoFar.addAll(toBeUpdated);\n    writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    \r\n    int numToDelete = recordsSoFar.size() / 2;\n    List<HoodieKey> toBeDeleted =\n        recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n    client.delete(toBeDeleted, newCommitTime);\n\n    client.close();\n  }\n","realPath":"hudi-examples/src/main/java/org/apache/hudi/examples/java/HoodieJavaWriteClientExample.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":59,"status":"M"}],"commitId":"a2f85d90de73a58e924b4de757d09d6133b046a4","commitMessage":"@@@[MINOR] Fix the wrong comment for HoodieJavaWriteClientExample (#2559)\n\n","date":"2021-02-10 02:33:34","modifiedFileCount":"1","status":"M","submitter":"vinoyang"},{"authorTime":"2021-03-05 14:10:27","codes":[{"authorDate":"2021-03-05 14:10:27","commitOrder":7,"curCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n    SparkConf sparkConf = HoodieExampleSparkUtils.defaultSparkConf(\"hoodie-client-example\");\n\n    try (JavaSparkContext jsc = new JavaSparkContext(sparkConf)) {\n\n      \r\n      HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n      \r\n      Path path = new Path(tablePath);\n      FileSystem fs = FSUtils.getFs(tablePath, jsc.hadoopConfiguration());\n      if (!fs.exists(path)) {\n        HoodieTableMetaClient.withPropertyBuilder()\n          .setTableType(tableType)\n          .setTableName(tableName)\n          .setPayloadClass(HoodieAvroPayload.class)\n          .initTable(jsc.hadoopConfiguration(), tablePath);\n      }\n\n      \r\n      HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n              .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n              .withDeleteParallelism(2).forTable(tableName)\n              .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())\n              .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n      SparkRDDWriteClient<HoodieAvroPayload> client = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jsc), cfg);\n\n      \r\n      String newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n\n      List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n      List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n      JavaRDD<HoodieRecord<HoodieAvroPayload>> writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n      records.addAll(toBeUpdated);\n      recordsSoFar.addAll(toBeUpdated);\n      writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      \r\n      int numToDelete = recordsSoFar.size() / 2;\n      List<HoodieKey> toBeDeleted = recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n      JavaRDD<HoodieKey> deleteRecords = jsc.parallelize(toBeDeleted, 1);\n      client.delete(deleteRecords, newCommitTime);\n\n      \r\n      if (HoodieTableType.valueOf(tableType) == HoodieTableType.MERGE_ON_READ) {\n        Option<String> instant = client.scheduleCompaction(Option.empty());\n        JavaRDD<WriteStatus> writeStatues = client.compact(instant.get());\n        client.commitCompaction(instant.get(), writeStatues, Option.empty());\n      }\n\n    }\n  }\n","date":"2021-03-05 14:10:27","endLine":138,"groupId":"4294","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/25/7519b468fc50318b2e11139bc5044835fc37b0.src","preCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n    SparkConf sparkConf = HoodieExampleSparkUtils.defaultSparkConf(\"hoodie-client-example\");\n\n    try (JavaSparkContext jsc = new JavaSparkContext(sparkConf)) {\n\n      \r\n      HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n      \r\n      Path path = new Path(tablePath);\n      FileSystem fs = FSUtils.getFs(tablePath, jsc.hadoopConfiguration());\n      if (!fs.exists(path)) {\n        HoodieTableMetaClient.initTableType(jsc.hadoopConfiguration(), tablePath, HoodieTableType.valueOf(tableType),\n                tableName, HoodieAvroPayload.class.getName());\n      }\n\n      \r\n      HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n              .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n              .withDeleteParallelism(2).forTable(tableName)\n              .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())\n              .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n      SparkRDDWriteClient<HoodieAvroPayload> client = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jsc), cfg);\n\n      \r\n      String newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n\n      List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n      List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n      JavaRDD<HoodieRecord<HoodieAvroPayload>> writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n      records.addAll(toBeUpdated);\n      recordsSoFar.addAll(toBeUpdated);\n      writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      \r\n      int numToDelete = recordsSoFar.size() / 2;\n      List<HoodieKey> toBeDeleted = recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n      JavaRDD<HoodieKey> deleteRecords = jsc.parallelize(toBeDeleted, 1);\n      client.delete(deleteRecords, newCommitTime);\n\n      \r\n      if (HoodieTableType.valueOf(tableType) == HoodieTableType.MERGE_ON_READ) {\n        Option<String> instant = client.scheduleCompaction(Option.empty());\n        JavaRDD<WriteStatus> writeStatues = client.compact(instant.get());\n        client.commitCompaction(instant.get(), writeStatues, Option.empty());\n      }\n\n    }\n  }\n","realPath":"hudi-examples/src/main/java/org/apache/hudi/examples/spark/HoodieWriteClientExample.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":70,"status":"M"},{"authorDate":"2021-03-05 14:10:27","commitOrder":7,"curCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieJavaWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n\n    \r\n    HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n    Configuration hadoopConf = new Configuration();\n    \r\n    Path path = new Path(tablePath);\n    FileSystem fs = FSUtils.getFs(tablePath, hadoopConf);\n    if (!fs.exists(path)) {\n      HoodieTableMetaClient.withPropertyBuilder()\n        .setTableType(tableType)\n        .setTableName(tableName)\n        .setPayloadClassName(HoodieAvroPayload.class.getName())\n        .initTable(hadoopConf, tablePath);\n    }\n\n    \r\n    HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n        .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n        .withDeleteParallelism(2).forTable(tableName)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build())\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n    HoodieJavaWriteClient<HoodieAvroPayload> client =\n        new HoodieJavaWriteClient<>(new HoodieJavaEngineContext(hadoopConf), cfg);\n\n    \r\n    String newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n\n    List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n    List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n    List<HoodieRecord<HoodieAvroPayload>> writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n    records.addAll(toBeUpdated);\n    recordsSoFar.addAll(toBeUpdated);\n    writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    \r\n    int numToDelete = recordsSoFar.size() / 2;\n    List<HoodieKey> toBeDeleted =\n        recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n    client.delete(toBeDeleted, newCommitTime);\n\n    client.close();\n  }\n","date":"2021-03-05 14:10:27","endLine":121,"groupId":"193","id":8,"instanceNumber":2,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/4d/06e4d15fa094067d1f118d5c78eca659c908a4.src","preCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieJavaWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n\n    \r\n    HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n    Configuration hadoopConf = new Configuration();\n    \r\n    Path path = new Path(tablePath);\n    FileSystem fs = FSUtils.getFs(tablePath, hadoopConf);\n    if (!fs.exists(path)) {\n      HoodieTableMetaClient.initTableType(hadoopConf, tablePath, HoodieTableType.valueOf(tableType),\n          tableName, HoodieAvroPayload.class.getName());\n    }\n\n    \r\n    HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n        .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n        .withDeleteParallelism(2).forTable(tableName)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build())\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n    HoodieJavaWriteClient<HoodieAvroPayload> client =\n        new HoodieJavaWriteClient<>(new HoodieJavaEngineContext(hadoopConf), cfg);\n\n    \r\n    String newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n\n    List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n    List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n    List<HoodieRecord<HoodieAvroPayload>> writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n    records.addAll(toBeUpdated);\n    recordsSoFar.addAll(toBeUpdated);\n    writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    \r\n    int numToDelete = recordsSoFar.size() / 2;\n    List<HoodieKey> toBeDeleted =\n        recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n    client.delete(toBeDeleted, newCommitTime);\n\n    client.close();\n  }\n","realPath":"hudi-examples/src/main/java/org/apache/hudi/examples/java/HoodieJavaWriteClientExample.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":59,"status":"M"}],"commitId":"bc883db5de5832fa429bbb04a35d3606fdacdb2a","commitMessage":"@@@[HUDI-1636] Support Builder Pattern To Build Table Properties For HoodieTableConfig (#2596)\n\n","date":"2021-03-05 14:10:27","modifiedFileCount":"19","status":"M","submitter":"pengzhiwei"},{"authorTime":"2021-03-05 14:10:27","codes":[{"authorDate":"2021-08-13 19:59:51","commitOrder":8,"curCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n    SparkConf sparkConf = HoodieExampleSparkUtils.defaultSparkConf(\"hoodie-client-example\");\n\n    try (JavaSparkContext jsc = new JavaSparkContext(sparkConf)) {\n\n      \r\n      HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n      \r\n      Path path = new Path(tablePath);\n      FileSystem fs = FSUtils.getFs(tablePath, jsc.hadoopConfiguration());\n      if (!fs.exists(path)) {\n        HoodieTableMetaClient.withPropertyBuilder()\n          .setTableType(tableType)\n          .setTableName(tableName)\n          .setPayloadClass(HoodieAvroPayload.class)\n          .initTable(jsc.hadoopConfiguration(), tablePath);\n      }\n\n      \r\n      HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n              .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n              .withDeleteParallelism(2).forTable(tableName)\n              .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())\n              .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n      SparkRDDWriteClient<HoodieAvroPayload> client = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jsc), cfg);\n\n      \r\n      String newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n\n      List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n      List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n      JavaRDD<HoodieRecord<HoodieAvroPayload>> writeRecords = jsc.parallelize(records, 1);\n      client.insert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n      records.addAll(toBeUpdated);\n      recordsSoFar.addAll(toBeUpdated);\n      writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      \r\n      int numToDelete = recordsSoFar.size() / 2;\n      List<HoodieKey> toBeDeleted = recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n      JavaRDD<HoodieKey> deleteRecords = jsc.parallelize(toBeDeleted, 1);\n      client.delete(deleteRecords, newCommitTime);\n\n      \r\n      if (HoodieTableType.valueOf(tableType) == HoodieTableType.MERGE_ON_READ) {\n        Option<String> instant = client.scheduleCompaction(Option.empty());\n        JavaRDD<WriteStatus> writeStatues = client.compact(instant.get());\n        client.commitCompaction(instant.get(), writeStatues, Option.empty());\n      }\n\n    }\n  }\n","date":"2021-08-13 19:59:51","endLine":138,"groupId":"4294","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/fe/ddd1b9ff788646721e460f471ea4b84bc589f0.src","preCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n    SparkConf sparkConf = HoodieExampleSparkUtils.defaultSparkConf(\"hoodie-client-example\");\n\n    try (JavaSparkContext jsc = new JavaSparkContext(sparkConf)) {\n\n      \r\n      HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n      \r\n      Path path = new Path(tablePath);\n      FileSystem fs = FSUtils.getFs(tablePath, jsc.hadoopConfiguration());\n      if (!fs.exists(path)) {\n        HoodieTableMetaClient.withPropertyBuilder()\n          .setTableType(tableType)\n          .setTableName(tableName)\n          .setPayloadClass(HoodieAvroPayload.class)\n          .initTable(jsc.hadoopConfiguration(), tablePath);\n      }\n\n      \r\n      HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n              .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n              .withDeleteParallelism(2).forTable(tableName)\n              .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())\n              .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n      SparkRDDWriteClient<HoodieAvroPayload> client = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jsc), cfg);\n\n      \r\n      String newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n\n      List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n      List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n      JavaRDD<HoodieRecord<HoodieAvroPayload>> writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n      records.addAll(toBeUpdated);\n      recordsSoFar.addAll(toBeUpdated);\n      writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      \r\n      int numToDelete = recordsSoFar.size() / 2;\n      List<HoodieKey> toBeDeleted = recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n      JavaRDD<HoodieKey> deleteRecords = jsc.parallelize(toBeDeleted, 1);\n      client.delete(deleteRecords, newCommitTime);\n\n      \r\n      if (HoodieTableType.valueOf(tableType) == HoodieTableType.MERGE_ON_READ) {\n        Option<String> instant = client.scheduleCompaction(Option.empty());\n        JavaRDD<WriteStatus> writeStatues = client.compact(instant.get());\n        client.commitCompaction(instant.get(), writeStatues, Option.empty());\n      }\n\n    }\n  }\n","realPath":"hudi-examples/src/main/java/org/apache/hudi/examples/spark/HoodieWriteClientExample.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":70,"status":"M"},{"authorDate":"2021-03-05 14:10:27","commitOrder":8,"curCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieJavaWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n\n    \r\n    HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n    Configuration hadoopConf = new Configuration();\n    \r\n    Path path = new Path(tablePath);\n    FileSystem fs = FSUtils.getFs(tablePath, hadoopConf);\n    if (!fs.exists(path)) {\n      HoodieTableMetaClient.withPropertyBuilder()\n        .setTableType(tableType)\n        .setTableName(tableName)\n        .setPayloadClassName(HoodieAvroPayload.class.getName())\n        .initTable(hadoopConf, tablePath);\n    }\n\n    \r\n    HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n        .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n        .withDeleteParallelism(2).forTable(tableName)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build())\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n    HoodieJavaWriteClient<HoodieAvroPayload> client =\n        new HoodieJavaWriteClient<>(new HoodieJavaEngineContext(hadoopConf), cfg);\n\n    \r\n    String newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n\n    List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n    List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n    List<HoodieRecord<HoodieAvroPayload>> writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n    records.addAll(toBeUpdated);\n    recordsSoFar.addAll(toBeUpdated);\n    writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    \r\n    int numToDelete = recordsSoFar.size() / 2;\n    List<HoodieKey> toBeDeleted =\n        recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n    client.delete(toBeDeleted, newCommitTime);\n\n    client.close();\n  }\n","date":"2021-03-05 14:10:27","endLine":121,"groupId":"193","id":10,"instanceNumber":2,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/4d/06e4d15fa094067d1f118d5c78eca659c908a4.src","preCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieJavaWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n\n    \r\n    HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n    Configuration hadoopConf = new Configuration();\n    \r\n    Path path = new Path(tablePath);\n    FileSystem fs = FSUtils.getFs(tablePath, hadoopConf);\n    if (!fs.exists(path)) {\n      HoodieTableMetaClient.withPropertyBuilder()\n        .setTableType(tableType)\n        .setTableName(tableName)\n        .setPayloadClassName(HoodieAvroPayload.class.getName())\n        .initTable(hadoopConf, tablePath);\n    }\n\n    \r\n    HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n        .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n        .withDeleteParallelism(2).forTable(tableName)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build())\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n    HoodieJavaWriteClient<HoodieAvroPayload> client =\n        new HoodieJavaWriteClient<>(new HoodieJavaEngineContext(hadoopConf), cfg);\n\n    \r\n    String newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n\n    List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n    List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n    List<HoodieRecord<HoodieAvroPayload>> writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n    records.addAll(toBeUpdated);\n    recordsSoFar.addAll(toBeUpdated);\n    writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    \r\n    int numToDelete = recordsSoFar.size() / 2;\n    List<HoodieKey> toBeDeleted =\n        recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n    client.delete(toBeDeleted, newCommitTime);\n\n    client.close();\n  }\n","realPath":"hudi-examples/src/main/java/org/apache/hudi/examples/java/HoodieJavaWriteClientExample.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":59,"status":"N"}],"commitId":"d4c2974eae98bde4ae21cd38c88e77227eb4b2c6","commitMessage":"@@@MINOR fix method use error (#3467)\n\n","date":"2021-08-13 19:59:51","modifiedFileCount":"1","status":"M","submitter":"liujinhui"},{"authorTime":"2021-03-05 14:10:27","codes":[{"authorDate":"2021-08-18 13:45:48","commitOrder":9,"curCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n    SparkConf sparkConf = HoodieExampleSparkUtils.defaultSparkConf(\"hoodie-client-example\");\n\n    try (JavaSparkContext jsc = new JavaSparkContext(sparkConf)) {\n\n      \r\n      HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n      \r\n      Path path = new Path(tablePath);\n      FileSystem fs = FSUtils.getFs(tablePath, jsc.hadoopConfiguration());\n      if (!fs.exists(path)) {\n        HoodieTableMetaClient.withPropertyBuilder()\n          .setTableType(tableType)\n          .setTableName(tableName)\n          .setPayloadClass(HoodieAvroPayload.class)\n          .initTable(jsc.hadoopConfiguration(), tablePath);\n      }\n\n      \r\n      HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n              .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n              .withDeleteParallelism(2).forTable(tableName)\n              .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())\n              .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n      SparkRDDWriteClient<HoodieAvroPayload> client = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jsc), cfg);\n\n      \r\n      String newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n\n      List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n      List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n      JavaRDD<HoodieRecord<HoodieAvroPayload>> writeRecords = jsc.parallelize(records, 1);\n      client.insert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n      records.addAll(toBeUpdated);\n      recordsSoFar.addAll(toBeUpdated);\n      writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      \r\n      int numToDelete = recordsSoFar.size() / 2;\n      List<HoodieKey> toBeDeleted = recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n      JavaRDD<HoodieKey> deleteRecords = jsc.parallelize(toBeDeleted, 1);\n      client.delete(deleteRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      client.startCommitWithTime(newCommitTime, HoodieTimeline.REPLACE_COMMIT_ACTION);\n      LOG.info(\"Starting commit \" + newCommitTime);\n      \r\n      List<String> partitionList = toBeDeleted.stream().map(s -> s.getPartitionPath()).distinct().collect(Collectors.toList());\n      List<String> deleteList = recordsSoFar.stream().filter(f -> !partitionList.contains(f.getPartitionPath()))\n          .map(m -> m.getKey().getPartitionPath()).distinct().collect(Collectors.toList());\n      client.deletePartitions(deleteList, newCommitTime);\n\n      \r\n      if (HoodieTableType.valueOf(tableType) == HoodieTableType.MERGE_ON_READ) {\n        Option<String> instant = client.scheduleCompaction(Option.empty());\n        JavaRDD<WriteStatus> writeStatues = client.compact(instant.get());\n        client.commitCompaction(instant.get(), writeStatues, Option.empty());\n      }\n\n    }\n  }\n","date":"2021-08-18 13:45:48","endLine":148,"groupId":"102","id":11,"instanceNumber":1,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/35/e46605f17b2a57f19b59201e7675cf094cebee.src","preCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n    SparkConf sparkConf = HoodieExampleSparkUtils.defaultSparkConf(\"hoodie-client-example\");\n\n    try (JavaSparkContext jsc = new JavaSparkContext(sparkConf)) {\n\n      \r\n      HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n      \r\n      Path path = new Path(tablePath);\n      FileSystem fs = FSUtils.getFs(tablePath, jsc.hadoopConfiguration());\n      if (!fs.exists(path)) {\n        HoodieTableMetaClient.withPropertyBuilder()\n          .setTableType(tableType)\n          .setTableName(tableName)\n          .setPayloadClass(HoodieAvroPayload.class)\n          .initTable(jsc.hadoopConfiguration(), tablePath);\n      }\n\n      \r\n      HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n              .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n              .withDeleteParallelism(2).forTable(tableName)\n              .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.BLOOM).build())\n              .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n      SparkRDDWriteClient<HoodieAvroPayload> client = new SparkRDDWriteClient<>(new HoodieSparkEngineContext(jsc), cfg);\n\n      \r\n      String newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n\n      List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n      List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n      JavaRDD<HoodieRecord<HoodieAvroPayload>> writeRecords = jsc.parallelize(records, 1);\n      client.insert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n      records.addAll(toBeUpdated);\n      recordsSoFar.addAll(toBeUpdated);\n      writeRecords = jsc.parallelize(records, 1);\n      client.upsert(writeRecords, newCommitTime);\n\n      \r\n      newCommitTime = client.startCommit();\n      LOG.info(\"Starting commit \" + newCommitTime);\n      \r\n      int numToDelete = recordsSoFar.size() / 2;\n      List<HoodieKey> toBeDeleted = recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n      JavaRDD<HoodieKey> deleteRecords = jsc.parallelize(toBeDeleted, 1);\n      client.delete(deleteRecords, newCommitTime);\n\n      \r\n      if (HoodieTableType.valueOf(tableType) == HoodieTableType.MERGE_ON_READ) {\n        Option<String> instant = client.scheduleCompaction(Option.empty());\n        JavaRDD<WriteStatus> writeStatues = client.compact(instant.get());\n        client.commitCompaction(instant.get(), writeStatues, Option.empty());\n      }\n\n    }\n  }\n","realPath":"hudi-examples/src/main/java/org/apache/hudi/examples/spark/HoodieWriteClientExample.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":70,"status":"M"},{"authorDate":"2021-03-05 14:10:27","commitOrder":9,"curCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieJavaWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n\n    \r\n    HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n    Configuration hadoopConf = new Configuration();\n    \r\n    Path path = new Path(tablePath);\n    FileSystem fs = FSUtils.getFs(tablePath, hadoopConf);\n    if (!fs.exists(path)) {\n      HoodieTableMetaClient.withPropertyBuilder()\n        .setTableType(tableType)\n        .setTableName(tableName)\n        .setPayloadClassName(HoodieAvroPayload.class.getName())\n        .initTable(hadoopConf, tablePath);\n    }\n\n    \r\n    HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n        .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n        .withDeleteParallelism(2).forTable(tableName)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build())\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n    HoodieJavaWriteClient<HoodieAvroPayload> client =\n        new HoodieJavaWriteClient<>(new HoodieJavaEngineContext(hadoopConf), cfg);\n\n    \r\n    String newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n\n    List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n    List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n    List<HoodieRecord<HoodieAvroPayload>> writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n    records.addAll(toBeUpdated);\n    recordsSoFar.addAll(toBeUpdated);\n    writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    \r\n    int numToDelete = recordsSoFar.size() / 2;\n    List<HoodieKey> toBeDeleted =\n        recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n    client.delete(toBeDeleted, newCommitTime);\n\n    client.close();\n  }\n","date":"2021-03-05 14:10:27","endLine":121,"groupId":"102","id":12,"instanceNumber":2,"isCurCommit":1,"methodName":"main","params":"(String[]args)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/4d/06e4d15fa094067d1f118d5c78eca659c908a4.src","preCode":"  public static void main(String[] args) throws Exception {\n    if (args.length < 2) {\n      System.err.println(\"Usage: HoodieJavaWriteClientExample <tablePath> <tableName>\");\n      System.exit(1);\n    }\n    String tablePath = args[0];\n    String tableName = args[1];\n\n    \r\n    HoodieExampleDataGenerator<HoodieAvroPayload> dataGen = new HoodieExampleDataGenerator<>();\n\n    Configuration hadoopConf = new Configuration();\n    \r\n    Path path = new Path(tablePath);\n    FileSystem fs = FSUtils.getFs(tablePath, hadoopConf);\n    if (!fs.exists(path)) {\n      HoodieTableMetaClient.withPropertyBuilder()\n        .setTableType(tableType)\n        .setTableName(tableName)\n        .setPayloadClassName(HoodieAvroPayload.class.getName())\n        .initTable(hadoopConf, tablePath);\n    }\n\n    \r\n    HoodieWriteConfig cfg = HoodieWriteConfig.newBuilder().withPath(tablePath)\n        .withSchema(HoodieExampleDataGenerator.TRIP_EXAMPLE_SCHEMA).withParallelism(2, 2)\n        .withDeleteParallelism(2).forTable(tableName)\n        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(HoodieIndex.IndexType.INMEMORY).build())\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder().archiveCommitsWith(20, 30).build()).build();\n    HoodieJavaWriteClient<HoodieAvroPayload> client =\n        new HoodieJavaWriteClient<>(new HoodieJavaEngineContext(hadoopConf), cfg);\n\n    \r\n    String newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n\n    List<HoodieRecord<HoodieAvroPayload>> records = dataGen.generateInserts(newCommitTime, 10);\n    List<HoodieRecord<HoodieAvroPayload>> recordsSoFar = new ArrayList<>(records);\n    List<HoodieRecord<HoodieAvroPayload>> writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    List<HoodieRecord<HoodieAvroPayload>> toBeUpdated = dataGen.generateUpdates(newCommitTime, 2);\n    records.addAll(toBeUpdated);\n    recordsSoFar.addAll(toBeUpdated);\n    writeRecords =\n        recordsSoFar.stream().map(r -> new HoodieRecord<HoodieAvroPayload>(r)).collect(Collectors.toList());\n    client.upsert(writeRecords, newCommitTime);\n\n    \r\n    newCommitTime = client.startCommit();\n    LOG.info(\"Starting commit \" + newCommitTime);\n    \r\n    int numToDelete = recordsSoFar.size() / 2;\n    List<HoodieKey> toBeDeleted =\n        recordsSoFar.stream().map(HoodieRecord::getKey).limit(numToDelete).collect(Collectors.toList());\n    client.delete(toBeDeleted, newCommitTime);\n\n    client.close();\n  }\n","realPath":"hudi-examples/src/main/java/org/apache/hudi/examples/java/HoodieJavaWriteClientExample.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":59,"status":"N"}],"commitId":"5ee35a0a92ab0de6c36885a515fad04c08109c78","commitMessage":"@@@HUDI-1674 (#3488)\n\n","date":"2021-08-18 13:45:48","modifiedFileCount":"2","status":"M","submitter":"liujinhui"}]
