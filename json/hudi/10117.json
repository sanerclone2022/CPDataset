[{"authorTime":"2021-07-08 15:07:27","codes":[{"authorDate":"2021-07-08 15:07:27","commitOrder":1,"curCode":"  private void testDataSourceWriterInternal(Map<String, String> extraMetadata, Map<String, String> expectedExtraMetadata) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String instantTime = \"001\";\n    \r\n    HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n        new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, extraMetadata, false);\n    DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(0, RANDOM.nextLong());\n\n    String[] partitionPaths = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS;\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(1000);\n    int batches = 5;\n    Dataset<Row> totalInputRows = null;\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime, Option.empty());\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    Option<HoodieCommitMetadata> commitMetadataOption = HoodieClientTestUtils.getCommitMetadataForLatestInstant(metaClient);\n    assertTrue(commitMetadataOption.isPresent());\n    Map<String, String> actualExtraMetadata = new HashMap<>();\n    commitMetadataOption.get().getExtraMetadata().entrySet().stream().filter(entry ->\n        !entry.getKey().equals(HoodieCommitMetadata.SCHEMA_KEY)).forEach(entry -> actualExtraMetadata.put(entry.getKey(), entry.getValue()));\n    assertEquals(actualExtraMetadata, expectedExtraMetadata);\n  }\n","date":"2021-07-08 15:07:27","endLine":112,"groupId":"2636","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testDataSourceWriterInternal","params":"(Map<String@String>extraMetadata@Map<String@String>expectedExtraMetadata)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/a9/caebfa2ccb3a0d9cfe7a37d9107e2963f7a923.src","preCode":"  private void testDataSourceWriterInternal(Map<String, String> extraMetadata, Map<String, String> expectedExtraMetadata) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String instantTime = \"001\";\n    \r\n    HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n        new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, extraMetadata, false);\n    DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(0, RANDOM.nextLong());\n\n    String[] partitionPaths = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS;\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(1000);\n    int batches = 5;\n    Dataset<Row> totalInputRows = null;\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime, Option.empty());\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    Option<HoodieCommitMetadata> commitMetadataOption = HoodieClientTestUtils.getCommitMetadataForLatestInstant(metaClient);\n    assertTrue(commitMetadataOption.isPresent());\n    Map<String, String> actualExtraMetadata = new HashMap<>();\n    commitMetadataOption.get().getExtraMetadata().entrySet().stream().filter(entry ->\n        !entry.getKey().equals(HoodieCommitMetadata.SCHEMA_KEY)).forEach(entry -> actualExtraMetadata.put(entry.getKey(), entry.getValue()));\n    assertEquals(actualExtraMetadata, expectedExtraMetadata);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":63,"status":"B"},{"authorDate":"2021-07-08 15:07:27","commitOrder":1,"curCode":"  private void testDataSourceWriterInternal(Map<String, String> extraMetadata, Map<String, String> expectedExtraMetadata) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    String instantTime = \"001\";\n    \r\n    HoodieDataSourceInternalWriter dataSourceInternalWriter =\n        new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(extraMetadata), false);\n    DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(0, RANDOM.nextLong(), RANDOM.nextLong());\n\n    String[] partitionPaths = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS;\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(1000);\n    int batches = 5;\n    Dataset<Row> totalInputRows = null;\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime, Option.empty());\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    Option<HoodieCommitMetadata> commitMetadataOption = HoodieClientTestUtils.getCommitMetadataForLatestInstant(metaClient);\n    assertTrue(commitMetadataOption.isPresent());\n    Map<String, String> actualExtraMetadata = new HashMap<>();\n    commitMetadataOption.get().getExtraMetadata().entrySet().stream().filter(entry ->\n        !entry.getKey().equals(HoodieCommitMetadata.SCHEMA_KEY)).forEach(entry -> actualExtraMetadata.put(entry.getKey(), entry.getValue()));\n    assertEquals(actualExtraMetadata, expectedExtraMetadata);\n  }\n","date":"2021-07-08 15:07:27","endLine":108,"groupId":"3340","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testDataSourceWriterInternal","params":"(Map<String@String>extraMetadata@Map<String@String>expectedExtraMetadata)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e4/98febc66a159991feb78f631d0f9b629ef208c.src","preCode":"  private void testDataSourceWriterInternal(Map<String, String> extraMetadata, Map<String, String> expectedExtraMetadata) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    String instantTime = \"001\";\n    \r\n    HoodieDataSourceInternalWriter dataSourceInternalWriter =\n        new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(extraMetadata), false);\n    DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(0, RANDOM.nextLong(), RANDOM.nextLong());\n\n    String[] partitionPaths = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS;\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(1000);\n    int batches = 5;\n    Dataset<Row> totalInputRows = null;\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime, Option.empty());\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    Option<HoodieCommitMetadata> commitMetadataOption = HoodieClientTestUtils.getCommitMetadataForLatestInstant(metaClient);\n    assertTrue(commitMetadataOption.isPresent());\n    Map<String, String> actualExtraMetadata = new HashMap<>();\n    commitMetadataOption.get().getExtraMetadata().entrySet().stream().filter(entry ->\n        !entry.getKey().equals(HoodieCommitMetadata.SCHEMA_KEY)).forEach(entry -> actualExtraMetadata.put(entry.getKey(), entry.getValue()));\n    assertEquals(actualExtraMetadata, expectedExtraMetadata);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":61,"status":"B"}],"commitId":"8c0dbaa9b3b6ced3826d0bc04e0a91272bbcab73","commitMessage":"@@@[HUDI-2009] Fixing extra commit metadata in row writer path (#3075)\n\n","date":"2021-07-08 15:07:27","modifiedFileCount":"12","status":"B","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-07-20 08:43:48","codes":[{"authorDate":"2021-07-20 08:43:48","commitOrder":2,"curCode":"  private void testDataSourceWriterInternal(Map<String, String> extraMetadata, Map<String, String> expectedExtraMetadata, boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String instantTime = \"001\";\n    \r\n    HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n        new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, extraMetadata, populateMetaFields, false);\n    DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(0, RANDOM.nextLong());\n\n    String[] partitionPaths = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS;\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(1000);\n    int batches = 5;\n    Dataset<Row> totalInputRows = null;\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime, Option.empty(), populateMetaFields);\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    Option<HoodieCommitMetadata> commitMetadataOption = HoodieClientTestUtils.getCommitMetadataForLatestInstant(metaClient);\n    assertTrue(commitMetadataOption.isPresent());\n    Map<String, String> actualExtraMetadata = new HashMap<>();\n    commitMetadataOption.get().getExtraMetadata().entrySet().stream().filter(entry ->\n        !entry.getKey().equals(HoodieCommitMetadata.SCHEMA_KEY)).forEach(entry -> actualExtraMetadata.put(entry.getKey(), entry.getValue()));\n    assertEquals(actualExtraMetadata, expectedExtraMetadata);\n  }\n","date":"2021-07-20 08:43:48","endLine":124,"groupId":"2139","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testDataSourceWriterInternal","params":"(Map<String@String>extraMetadata@Map<String@String>expectedExtraMetadata@booleanpopulateMetaFields)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/3c/3866ee58f8a0c18b9d1b8dd9b3cb5bd528b940.src","preCode":"  private void testDataSourceWriterInternal(Map<String, String> extraMetadata, Map<String, String> expectedExtraMetadata) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String instantTime = \"001\";\n    \r\n    HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n        new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, extraMetadata, false);\n    DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(0, RANDOM.nextLong());\n\n    String[] partitionPaths = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS;\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(1000);\n    int batches = 5;\n    Dataset<Row> totalInputRows = null;\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime, Option.empty());\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    Option<HoodieCommitMetadata> commitMetadataOption = HoodieClientTestUtils.getCommitMetadataForLatestInstant(metaClient);\n    assertTrue(commitMetadataOption.isPresent());\n    Map<String, String> actualExtraMetadata = new HashMap<>();\n    commitMetadataOption.get().getExtraMetadata().entrySet().stream().filter(entry ->\n        !entry.getKey().equals(HoodieCommitMetadata.SCHEMA_KEY)).forEach(entry -> actualExtraMetadata.put(entry.getKey(), entry.getValue()));\n    assertEquals(actualExtraMetadata, expectedExtraMetadata);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":75,"status":"M"},{"authorDate":"2021-07-20 08:43:48","commitOrder":2,"curCode":"  private void testDataSourceWriterInternal(Map<String, String> extraMetadata, Map<String, String> expectedExtraMetadata, boolean populateMetaFields)\n      throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    String instantTime = \"001\";\n    \r\n    HoodieDataSourceInternalWriter dataSourceInternalWriter =\n        new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(extraMetadata), populateMetaFields, false);\n    DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(0, RANDOM.nextLong(), RANDOM.nextLong());\n\n    String[] partitionPaths = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS;\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(1000);\n    int batches = 5;\n    Dataset<Row> totalInputRows = null;\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime, Option.empty(), populateMetaFields);\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    Option<HoodieCommitMetadata> commitMetadataOption = HoodieClientTestUtils.getCommitMetadataForLatestInstant(metaClient);\n    assertTrue(commitMetadataOption.isPresent());\n    Map<String, String> actualExtraMetadata = new HashMap<>();\n    commitMetadataOption.get().getExtraMetadata().entrySet().stream().filter(entry ->\n        !entry.getKey().equals(HoodieCommitMetadata.SCHEMA_KEY)).forEach(entry -> actualExtraMetadata.put(entry.getKey(), entry.getValue()));\n    assertEquals(actualExtraMetadata, expectedExtraMetadata);\n  }\n","date":"2021-07-20 08:43:48","endLine":121,"groupId":"4395","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testDataSourceWriterInternal","params":"(Map<String@String>extraMetadata@Map<String@String>expectedExtraMetadata@booleanpopulateMetaFields)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/34/2e2ae90ca5ca4ef04bfcd4bcb4a1fdf843e554.src","preCode":"  private void testDataSourceWriterInternal(Map<String, String> extraMetadata, Map<String, String> expectedExtraMetadata) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    String instantTime = \"001\";\n    \r\n    HoodieDataSourceInternalWriter dataSourceInternalWriter =\n        new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(extraMetadata), false);\n    DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(0, RANDOM.nextLong(), RANDOM.nextLong());\n\n    String[] partitionPaths = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS;\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(1000);\n    int batches = 5;\n    Dataset<Row> totalInputRows = null;\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime, Option.empty());\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    Option<HoodieCommitMetadata> commitMetadataOption = HoodieClientTestUtils.getCommitMetadataForLatestInstant(metaClient);\n    assertTrue(commitMetadataOption.isPresent());\n    Map<String, String> actualExtraMetadata = new HashMap<>();\n    commitMetadataOption.get().getExtraMetadata().entrySet().stream().filter(entry ->\n        !entry.getKey().equals(HoodieCommitMetadata.SCHEMA_KEY)).forEach(entry -> actualExtraMetadata.put(entry.getKey(), entry.getValue()));\n    assertEquals(actualExtraMetadata, expectedExtraMetadata);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":73,"status":"M"}],"commitId":"d5026e9a24850bdcce9f6df3686bf2235d7d01c4","commitMessage":"@@@[HUDI-2161] Adding support to disable meta columns with bulk insert operation (#3247)\n\n","date":"2021-07-20 08:43:48","modifiedFileCount":"39","status":"M","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-07-30 13:22:26","codes":[{"authorDate":"2021-07-20 08:43:48","commitOrder":3,"curCode":"  private void testDataSourceWriterInternal(Map<String, String> extraMetadata, Map<String, String> expectedExtraMetadata, boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String instantTime = \"001\";\n    \r\n    HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n        new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, extraMetadata, populateMetaFields, false);\n    DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(0, RANDOM.nextLong());\n\n    String[] partitionPaths = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS;\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(1000);\n    int batches = 5;\n    Dataset<Row> totalInputRows = null;\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime, Option.empty(), populateMetaFields);\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    Option<HoodieCommitMetadata> commitMetadataOption = HoodieClientTestUtils.getCommitMetadataForLatestInstant(metaClient);\n    assertTrue(commitMetadataOption.isPresent());\n    Map<String, String> actualExtraMetadata = new HashMap<>();\n    commitMetadataOption.get().getExtraMetadata().entrySet().stream().filter(entry ->\n        !entry.getKey().equals(HoodieCommitMetadata.SCHEMA_KEY)).forEach(entry -> actualExtraMetadata.put(entry.getKey(), entry.getValue()));\n    assertEquals(actualExtraMetadata, expectedExtraMetadata);\n  }\n","date":"2021-07-20 08:43:48","endLine":124,"groupId":"10117","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testDataSourceWriterInternal","params":"(Map<String@String>extraMetadata@Map<String@String>expectedExtraMetadata@booleanpopulateMetaFields)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/3c/3866ee58f8a0c18b9d1b8dd9b3cb5bd528b940.src","preCode":"  private void testDataSourceWriterInternal(Map<String, String> extraMetadata, Map<String, String> expectedExtraMetadata, boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    String instantTime = \"001\";\n    \r\n    HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n        new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, extraMetadata, populateMetaFields, false);\n    DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(0, RANDOM.nextLong());\n\n    String[] partitionPaths = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS;\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(1000);\n    int batches = 5;\n    Dataset<Row> totalInputRows = null;\n\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    dataSourceInternalBatchWrite.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime, Option.empty(), populateMetaFields);\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    Option<HoodieCommitMetadata> commitMetadataOption = HoodieClientTestUtils.getCommitMetadataForLatestInstant(metaClient);\n    assertTrue(commitMetadataOption.isPresent());\n    Map<String, String> actualExtraMetadata = new HashMap<>();\n    commitMetadataOption.get().getExtraMetadata().entrySet().stream().filter(entry ->\n        !entry.getKey().equals(HoodieCommitMetadata.SCHEMA_KEY)).forEach(entry -> actualExtraMetadata.put(entry.getKey(), entry.getValue()));\n    assertEquals(actualExtraMetadata, expectedExtraMetadata);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":75,"status":"N"},{"authorDate":"2021-07-30 13:22:26","commitOrder":3,"curCode":"  private void testDataSourceWriterInternal(Map<String, String> extraMetadata, Map<String, String> expectedExtraMetadata, boolean populateMetaFields)\n      throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    String instantTime = \"001\";\n    \r\n    HoodieDataSourceInternalWriter dataSourceInternalWriter =\n        new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(extraMetadata), populateMetaFields, false);\n    DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(0, RANDOM.nextLong(), RANDOM.nextLong());\n\n    String[] partitionPaths = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS;\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(1000);\n    int batches = 2;\n    Dataset<Row> totalInputRows = null;\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime, Option.empty(), populateMetaFields);\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    Option<HoodieCommitMetadata> commitMetadataOption = HoodieClientTestUtils.getCommitMetadataForLatestInstant(metaClient);\n    assertTrue(commitMetadataOption.isPresent());\n    Map<String, String> actualExtraMetadata = new HashMap<>();\n    commitMetadataOption.get().getExtraMetadata().entrySet().stream().filter(entry ->\n        !entry.getKey().equals(HoodieCommitMetadata.SCHEMA_KEY)).forEach(entry -> actualExtraMetadata.put(entry.getKey(), entry.getValue()));\n    assertEquals(actualExtraMetadata, expectedExtraMetadata);\n  }\n","date":"2021-07-30 13:22:26","endLine":122,"groupId":"10117","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testDataSourceWriterInternal","params":"(Map<String@String>extraMetadata@Map<String@String>expectedExtraMetadata@booleanpopulateMetaFields)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ee/a49e667207ae2400d48223b14a008416ac26b3.src","preCode":"  private void testDataSourceWriterInternal(Map<String, String> extraMetadata, Map<String, String> expectedExtraMetadata, boolean populateMetaFields)\n      throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    String instantTime = \"001\";\n    \r\n    HoodieDataSourceInternalWriter dataSourceInternalWriter =\n        new HoodieDataSourceInternalWriter(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf, new DataSourceOptions(extraMetadata), populateMetaFields, false);\n    DataWriter<InternalRow> writer = dataSourceInternalWriter.createWriterFactory().createDataWriter(0, RANDOM.nextLong(), RANDOM.nextLong());\n\n    String[] partitionPaths = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS;\n    List<String> partitionPathsAbs = new ArrayList<>();\n    for (String partitionPath : partitionPaths) {\n      partitionPathsAbs.add(basePath + \"/\" + partitionPath + \"/*\");\n    }\n\n    int size = 10 + RANDOM.nextInt(1000);\n    int batches = 5;\n    Dataset<Row> totalInputRows = null;\n    for (int j = 0; j < batches; j++) {\n      String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n      Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n      writeRows(inputRows, writer);\n      if (totalInputRows == null) {\n        totalInputRows = inputRows;\n      } else {\n        totalInputRows = totalInputRows.union(inputRows);\n      }\n    }\n\n    HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n    List<HoodieWriterCommitMessage> commitMessages = new ArrayList<>();\n    commitMessages.add(commitMetadata);\n    dataSourceInternalWriter.commit(commitMessages.toArray(new HoodieWriterCommitMessage[0]));\n\n    metaClient.reloadActiveTimeline();\n    Dataset<Row> result = HoodieClientTestUtils.read(jsc, basePath, sqlContext, metaClient.getFs(), partitionPathsAbs.toArray(new String[0]));\n    \r\n    assertOutput(totalInputRows, result, instantTime, Option.empty(), populateMetaFields);\n    assertWriteStatuses(commitMessages.get(0).getWriteStatuses(), batches, size, Option.empty(), Option.empty());\n\n    \r\n    Option<HoodieCommitMetadata> commitMetadataOption = HoodieClientTestUtils.getCommitMetadataForLatestInstant(metaClient);\n    assertTrue(commitMetadataOption.isPresent());\n    Map<String, String> actualExtraMetadata = new HashMap<>();\n    commitMetadataOption.get().getExtraMetadata().entrySet().stream().filter(entry ->\n        !entry.getKey().equals(HoodieCommitMetadata.SCHEMA_KEY)).forEach(entry -> actualExtraMetadata.put(entry.getKey(), entry.getValue()));\n    assertEquals(actualExtraMetadata, expectedExtraMetadata);\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieDataSourceInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":74,"status":"M"}],"commitId":"7bdae69053afc5ef604a15806d78317cb976f2ce","commitMessage":"@@@[HUDI-2253] Refactoring few tests to reduce runningtime. DeltaStreamer and MultiDeltaStreamer tests. Bulk insert row writer tests (#3371)\n\nCo-authored-by: Sivabalan Narayanan <nsb@Sivabalans-MBP.attlocal.net>","date":"2021-07-30 13:22:26","modifiedFileCount":"6","status":"M","submitter":"Sivabalan Narayanan"}]
