[{"authorTime":"2020-10-02 05:25:29","codes":[{"authorDate":"2020-10-02 05:25:29","commitOrder":1,"curCode":"  public void testCheckExists(boolean rangePruning, boolean treeFiltering, boolean bucketizedChecking) throws Exception {\n    \r\n\n    String recordStr1 = \"{\\\"_row_key\\\":\\\"1eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n    String recordStr2 = \"{\\\"_row_key\\\":\\\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n    String recordStr3 = \"{\\\"_row_key\\\":\\\"3eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n    \r\n    String recordStr4 = \"{\\\"_row_key\\\":\\\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2015-01-31T03:16:41.415Z\\\",\\\"number\\\":32}\";\n    RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n    HoodieKey key1 = new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath());\n    HoodieRecord record1 = new HoodieRecord(key1, rowChange1);\n    RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n    HoodieKey key2 = new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath());\n    HoodieRecord record2 = new HoodieRecord(key2, rowChange2);\n    RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n    HoodieKey key3 = new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath());\n    RawTripTestPayload rowChange4 = new RawTripTestPayload(recordStr4);\n    HoodieKey key4 = new HoodieKey(rowChange4.getRowKey(), rowChange4.getPartitionPath());\n    HoodieRecord record4 = new HoodieRecord(key4, rowChange4);\n    JavaRDD<HoodieKey> keysRDD = jsc.parallelize(Arrays.asList(key1, key2, key3, key4));\n\n    \r\n    HoodieWriteConfig config = makeConfig(rangePruning, treeFiltering, bucketizedChecking);\n    HoodieTable hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n    HoodieWriteableTestTable testTable = HoodieWriteableTestTable.of(hoodieTable, SCHEMA);\n\n    \r\n    SparkHoodieBloomIndex bloomIndex = new SparkHoodieBloomIndex(config);\n    JavaRDD<HoodieRecord> taggedRecords = bloomIndex.tagLocation(keysRDD.map(k -> new HoodieRecord(k, null)), context, hoodieTable);\n    JavaPairRDD<HoodieKey, Option<Pair<String, String>>> recordLocationsRDD = taggedRecords\n        .mapToPair(hr -> new Tuple2<>(hr.getKey(), hr.isCurrentLocationKnown()\n            ? Option.of(Pair.of(hr.getPartitionPath(), hr.getCurrentLocation().getFileId()))\n            : Option.empty())\n        );\n\n    \r\n    for (Tuple2<HoodieKey, Option<Pair<String, String>>> record : recordLocationsRDD.collect()) {\n      assertTrue(!record._2.isPresent());\n    }\n\n    \r\n    String fileId1 = testTable.addCommit(\"001\").withInserts(\"2016/01/31\", record1);\n    String fileId2 = testTable.addCommit(\"002\").withInserts(\"2016/01/31\", record2);\n    String fileId3 = testTable.addCommit(\"003\").withInserts(\"2015/01/31\", record4);\n\n    \r\n    metaClient = HoodieTableMetaClient.reload(metaClient);\n    hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n    taggedRecords = bloomIndex.tagLocation(keysRDD.map(k -> new HoodieRecord(k, null)), context, hoodieTable);\n    recordLocationsRDD = taggedRecords\n        .mapToPair(hr -> new Tuple2<>(hr.getKey(), hr.isCurrentLocationKnown()\n            ? Option.of(Pair.of(hr.getPartitionPath(), hr.getCurrentLocation().getFileId()))\n            : Option.empty())\n        );\n\n    \r\n    for (Tuple2<HoodieKey, Option<Pair<String, String>>> record : recordLocationsRDD.collect()) {\n      if (record._1.getRecordKey().equals(\"1eb5b87a-1feh-4edd-87b4-6ec96dc405a0\")) {\n        assertTrue(record._2.isPresent());\n        assertEquals(fileId1, record._2.get().getRight());\n      } else if (record._1.getRecordKey().equals(\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\")) {\n        assertTrue(record._2.isPresent());\n        if (record._1.getPartitionPath().equals(\"2015/01/31\")) {\n          assertEquals(fileId3, record._2.get().getRight());\n        } else {\n          assertEquals(fileId2, record._2.get().getRight());\n        }\n      } else if (record._1.getRecordKey().equals(\"3eb5b87c-1fej-4edd-87b4-6ec96dc405a0\")) {\n        assertFalse(record._2.isPresent());\n      }\n    }\n  }\n","date":"2020-10-02 05:25:29","endLine":413,"groupId":"3544","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testCheckExists","params":"(booleanrangePruning@booleantreeFiltering@booleanbucketizedChecking)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/2d/091a0eaa77890d4a71d48834e84630112b248a.src","preCode":"  public void testCheckExists(boolean rangePruning, boolean treeFiltering, boolean bucketizedChecking) throws Exception {\n    \r\n\n    String recordStr1 = \"{\\\"_row_key\\\":\\\"1eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n    String recordStr2 = \"{\\\"_row_key\\\":\\\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n    String recordStr3 = \"{\\\"_row_key\\\":\\\"3eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n    \r\n    String recordStr4 = \"{\\\"_row_key\\\":\\\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2015-01-31T03:16:41.415Z\\\",\\\"number\\\":32}\";\n    RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n    HoodieKey key1 = new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath());\n    HoodieRecord record1 = new HoodieRecord(key1, rowChange1);\n    RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n    HoodieKey key2 = new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath());\n    HoodieRecord record2 = new HoodieRecord(key2, rowChange2);\n    RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n    HoodieKey key3 = new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath());\n    RawTripTestPayload rowChange4 = new RawTripTestPayload(recordStr4);\n    HoodieKey key4 = new HoodieKey(rowChange4.getRowKey(), rowChange4.getPartitionPath());\n    HoodieRecord record4 = new HoodieRecord(key4, rowChange4);\n    JavaRDD<HoodieKey> keysRDD = jsc.parallelize(Arrays.asList(key1, key2, key3, key4));\n\n    \r\n    HoodieWriteConfig config = makeConfig(rangePruning, treeFiltering, bucketizedChecking);\n    HoodieTable hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n    HoodieWriteableTestTable testTable = HoodieWriteableTestTable.of(hoodieTable, SCHEMA);\n\n    \r\n    SparkHoodieBloomIndex bloomIndex = new SparkHoodieBloomIndex(config);\n    JavaRDD<HoodieRecord> taggedRecords = bloomIndex.tagLocation(keysRDD.map(k -> new HoodieRecord(k, null)), context, hoodieTable);\n    JavaPairRDD<HoodieKey, Option<Pair<String, String>>> recordLocationsRDD = taggedRecords\n        .mapToPair(hr -> new Tuple2<>(hr.getKey(), hr.isCurrentLocationKnown()\n            ? Option.of(Pair.of(hr.getPartitionPath(), hr.getCurrentLocation().getFileId()))\n            : Option.empty())\n        );\n\n    \r\n    for (Tuple2<HoodieKey, Option<Pair<String, String>>> record : recordLocationsRDD.collect()) {\n      assertTrue(!record._2.isPresent());\n    }\n\n    \r\n    String fileId1 = testTable.addCommit(\"001\").withInserts(\"2016/01/31\", record1);\n    String fileId2 = testTable.addCommit(\"002\").withInserts(\"2016/01/31\", record2);\n    String fileId3 = testTable.addCommit(\"003\").withInserts(\"2015/01/31\", record4);\n\n    \r\n    metaClient = HoodieTableMetaClient.reload(metaClient);\n    hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n    taggedRecords = bloomIndex.tagLocation(keysRDD.map(k -> new HoodieRecord(k, null)), context, hoodieTable);\n    recordLocationsRDD = taggedRecords\n        .mapToPair(hr -> new Tuple2<>(hr.getKey(), hr.isCurrentLocationKnown()\n            ? Option.of(Pair.of(hr.getPartitionPath(), hr.getCurrentLocation().getFileId()))\n            : Option.empty())\n        );\n\n    \r\n    for (Tuple2<HoodieKey, Option<Pair<String, String>>> record : recordLocationsRDD.collect()) {\n      if (record._1.getRecordKey().equals(\"1eb5b87a-1feh-4edd-87b4-6ec96dc405a0\")) {\n        assertTrue(record._2.isPresent());\n        assertEquals(fileId1, record._2.get().getRight());\n      } else if (record._1.getRecordKey().equals(\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\")) {\n        assertTrue(record._2.isPresent());\n        if (record._1.getPartitionPath().equals(\"2015/01/31\")) {\n          assertEquals(fileId3, record._2.get().getRight());\n        } else {\n          assertEquals(fileId2, record._2.get().getRight());\n        }\n      } else if (record._1.getRecordKey().equals(\"3eb5b87c-1fej-4edd-87b4-6ec96dc405a0\")) {\n        assertFalse(record._2.isPresent());\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieBloomIndex.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":338,"status":"B"},{"authorDate":"2020-10-02 05:25:29","commitOrder":1,"curCode":"  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> checkExists(JavaRDD<HoodieKey> hoodieKeys) {\n    return index.tagLocation(hoodieKeys.map(k -> new HoodieRecord<>(k, null)), context, hoodieTable)\n        .mapToPair(hr -> new Tuple2<>(hr.getKey(), hr.isCurrentLocationKnown()\n            ? Option.of(Pair.of(hr.getPartitionPath(), hr.getCurrentLocation().getFileId()))\n            : Option.empty())\n        );\n  }\n","date":"2020-10-02 05:25:29","endLine":171,"groupId":"2999","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"checkExists","params":"(JavaRDD<HoodieKey>hoodieKeys)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/4f/b9f221cbdd7486ed90aa8b8c0882a8a4556a19.src","preCode":"  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> checkExists(JavaRDD<HoodieKey> hoodieKeys) {\n    return index.tagLocation(hoodieKeys.map(k -> new HoodieRecord<>(k, null)), context, hoodieTable)\n        .mapToPair(hr -> new Tuple2<>(hr.getKey(), hr.isCurrentLocationKnown()\n            ? Option.of(Pair.of(hr.getPartitionPath(), hr.getCurrentLocation().getFileId()))\n            : Option.empty())\n        );\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieReadClient.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":165,"status":"B"}],"commitId":"1f7add92916c37b05be270d9c75a9042134ec506","commitMessage":"@@@[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)\n\n- This change breaks `hudi-client` into `hudi-client-common` and `hudi-spark-client` modules \n- Simple usages of Spark using jsc.parallelize() has been redone using EngineContext#map.  EngineContext#flatMap etc\n- Code changes in the PR.  break classes into `BaseXYZ` parent classes with no spark dependencies living in `hudi-client-common`\n- Classes on `hudi-spark-client` are named `SparkXYZ` extending the parent classes with all the Spark dependencies\n- To simplify/cleanup.  HoodieIndex#fetchRecordLocation has been removed and its usages in tests replaced with alternatives\n\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2020-10-02 05:25:29","modifiedFileCount":"31","status":"B","submitter":"Mathieu"},{"authorTime":"2020-10-02 05:25:29","codes":[{"authorDate":"2020-10-09 10:21:27","commitOrder":2,"curCode":"  public void testCheckExists(boolean rangePruning, boolean treeFiltering, boolean bucketizedChecking) throws Exception {\n    \r\n\n    String recordStr1 = \"{\\\"_row_key\\\":\\\"1eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n    String recordStr2 = \"{\\\"_row_key\\\":\\\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n    String recordStr3 = \"{\\\"_row_key\\\":\\\"3eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n    \r\n    String recordStr4 = \"{\\\"_row_key\\\":\\\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2015-01-31T03:16:41.415Z\\\",\\\"number\\\":32}\";\n    RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n    HoodieKey key1 = new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath());\n    HoodieRecord record1 = new HoodieRecord(key1, rowChange1);\n    RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n    HoodieKey key2 = new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath());\n    HoodieRecord record2 = new HoodieRecord(key2, rowChange2);\n    RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n    HoodieKey key3 = new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath());\n    RawTripTestPayload rowChange4 = new RawTripTestPayload(recordStr4);\n    HoodieKey key4 = new HoodieKey(rowChange4.getRowKey(), rowChange4.getPartitionPath());\n    HoodieRecord record4 = new HoodieRecord(key4, rowChange4);\n    JavaRDD<HoodieKey> keysRDD = jsc.parallelize(Arrays.asList(key1, key2, key3, key4));\n\n    \r\n    HoodieWriteConfig config = makeConfig(rangePruning, treeFiltering, bucketizedChecking);\n    HoodieTable hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n    HoodieWriteableTestTable testTable = HoodieWriteableTestTable.of(hoodieTable, SCHEMA);\n\n    \r\n    SparkHoodieBloomIndex bloomIndex = new SparkHoodieBloomIndex(config);\n    JavaRDD<HoodieRecord> taggedRecords = bloomIndex.tagLocation(keysRDD.map(k -> new HoodieRecord(k, null)), context, hoodieTable);\n    JavaPairRDD<HoodieKey, Option<Pair<String, String>>> recordLocationsRDD = taggedRecords\n        .mapToPair(hr -> new Tuple2<>(hr.getKey(), hr.isCurrentLocationKnown()\n            ? Option.of(Pair.of(hr.getPartitionPath(), hr.getCurrentLocation().getFileId()))\n            : Option.empty())\n        );\n\n    \r\n    for (Tuple2<HoodieKey, Option<Pair<String, String>>> record : recordLocationsRDD.collect()) {\n      assertTrue(!record._2.isPresent());\n    }\n\n    \r\n    String fileId1 = testTable.addCommit(\"001\").getFileIdWithInserts(\"2016/01/31\", record1);\n    String fileId2 = testTable.addCommit(\"002\").getFileIdWithInserts(\"2016/01/31\", record2);\n    String fileId3 = testTable.addCommit(\"003\").getFileIdWithInserts(\"2015/01/31\", record4);\n\n    \r\n    metaClient = HoodieTableMetaClient.reload(metaClient);\n    hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n    taggedRecords = bloomIndex.tagLocation(keysRDD.map(k -> new HoodieRecord(k, null)), context, hoodieTable);\n    recordLocationsRDD = taggedRecords\n        .mapToPair(hr -> new Tuple2<>(hr.getKey(), hr.isCurrentLocationKnown()\n            ? Option.of(Pair.of(hr.getPartitionPath(), hr.getCurrentLocation().getFileId()))\n            : Option.empty())\n        );\n\n    \r\n    for (Tuple2<HoodieKey, Option<Pair<String, String>>> record : recordLocationsRDD.collect()) {\n      if (record._1.getRecordKey().equals(\"1eb5b87a-1feh-4edd-87b4-6ec96dc405a0\")) {\n        assertTrue(record._2.isPresent());\n        assertEquals(fileId1, record._2.get().getRight());\n      } else if (record._1.getRecordKey().equals(\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\")) {\n        assertTrue(record._2.isPresent());\n        if (record._1.getPartitionPath().equals(\"2015/01/31\")) {\n          assertEquals(fileId3, record._2.get().getRight());\n        } else {\n          assertEquals(fileId2, record._2.get().getRight());\n        }\n      } else if (record._1.getRecordKey().equals(\"3eb5b87c-1fej-4edd-87b4-6ec96dc405a0\")) {\n        assertFalse(record._2.isPresent());\n      }\n    }\n  }\n","date":"2020-10-09 10:21:27","endLine":413,"groupId":"3544","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testCheckExists","params":"(booleanrangePruning@booleantreeFiltering@booleanbucketizedChecking)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/39/d9b6408ecbcdb4aa73961a95fe0d54a4a1df76.src","preCode":"  public void testCheckExists(boolean rangePruning, boolean treeFiltering, boolean bucketizedChecking) throws Exception {\n    \r\n\n    String recordStr1 = \"{\\\"_row_key\\\":\\\"1eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n    String recordStr2 = \"{\\\"_row_key\\\":\\\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n    String recordStr3 = \"{\\\"_row_key\\\":\\\"3eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n    \r\n    String recordStr4 = \"{\\\"_row_key\\\":\\\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2015-01-31T03:16:41.415Z\\\",\\\"number\\\":32}\";\n    RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n    HoodieKey key1 = new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath());\n    HoodieRecord record1 = new HoodieRecord(key1, rowChange1);\n    RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n    HoodieKey key2 = new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath());\n    HoodieRecord record2 = new HoodieRecord(key2, rowChange2);\n    RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n    HoodieKey key3 = new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath());\n    RawTripTestPayload rowChange4 = new RawTripTestPayload(recordStr4);\n    HoodieKey key4 = new HoodieKey(rowChange4.getRowKey(), rowChange4.getPartitionPath());\n    HoodieRecord record4 = new HoodieRecord(key4, rowChange4);\n    JavaRDD<HoodieKey> keysRDD = jsc.parallelize(Arrays.asList(key1, key2, key3, key4));\n\n    \r\n    HoodieWriteConfig config = makeConfig(rangePruning, treeFiltering, bucketizedChecking);\n    HoodieTable hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n    HoodieWriteableTestTable testTable = HoodieWriteableTestTable.of(hoodieTable, SCHEMA);\n\n    \r\n    SparkHoodieBloomIndex bloomIndex = new SparkHoodieBloomIndex(config);\n    JavaRDD<HoodieRecord> taggedRecords = bloomIndex.tagLocation(keysRDD.map(k -> new HoodieRecord(k, null)), context, hoodieTable);\n    JavaPairRDD<HoodieKey, Option<Pair<String, String>>> recordLocationsRDD = taggedRecords\n        .mapToPair(hr -> new Tuple2<>(hr.getKey(), hr.isCurrentLocationKnown()\n            ? Option.of(Pair.of(hr.getPartitionPath(), hr.getCurrentLocation().getFileId()))\n            : Option.empty())\n        );\n\n    \r\n    for (Tuple2<HoodieKey, Option<Pair<String, String>>> record : recordLocationsRDD.collect()) {\n      assertTrue(!record._2.isPresent());\n    }\n\n    \r\n    String fileId1 = testTable.addCommit(\"001\").withInserts(\"2016/01/31\", record1);\n    String fileId2 = testTable.addCommit(\"002\").withInserts(\"2016/01/31\", record2);\n    String fileId3 = testTable.addCommit(\"003\").withInserts(\"2015/01/31\", record4);\n\n    \r\n    metaClient = HoodieTableMetaClient.reload(metaClient);\n    hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n    taggedRecords = bloomIndex.tagLocation(keysRDD.map(k -> new HoodieRecord(k, null)), context, hoodieTable);\n    recordLocationsRDD = taggedRecords\n        .mapToPair(hr -> new Tuple2<>(hr.getKey(), hr.isCurrentLocationKnown()\n            ? Option.of(Pair.of(hr.getPartitionPath(), hr.getCurrentLocation().getFileId()))\n            : Option.empty())\n        );\n\n    \r\n    for (Tuple2<HoodieKey, Option<Pair<String, String>>> record : recordLocationsRDD.collect()) {\n      if (record._1.getRecordKey().equals(\"1eb5b87a-1feh-4edd-87b4-6ec96dc405a0\")) {\n        assertTrue(record._2.isPresent());\n        assertEquals(fileId1, record._2.get().getRight());\n      } else if (record._1.getRecordKey().equals(\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\")) {\n        assertTrue(record._2.isPresent());\n        if (record._1.getPartitionPath().equals(\"2015/01/31\")) {\n          assertEquals(fileId3, record._2.get().getRight());\n        } else {\n          assertEquals(fileId2, record._2.get().getRight());\n        }\n      } else if (record._1.getRecordKey().equals(\"3eb5b87c-1fej-4edd-87b4-6ec96dc405a0\")) {\n        assertFalse(record._2.isPresent());\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieBloomIndex.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":338,"status":"M"},{"authorDate":"2020-10-02 05:25:29","commitOrder":2,"curCode":"  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> checkExists(JavaRDD<HoodieKey> hoodieKeys) {\n    return index.tagLocation(hoodieKeys.map(k -> new HoodieRecord<>(k, null)), context, hoodieTable)\n        .mapToPair(hr -> new Tuple2<>(hr.getKey(), hr.isCurrentLocationKnown()\n            ? Option.of(Pair.of(hr.getPartitionPath(), hr.getCurrentLocation().getFileId()))\n            : Option.empty())\n        );\n  }\n","date":"2020-10-02 05:25:29","endLine":171,"groupId":"2999","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"checkExists","params":"(JavaRDD<HoodieKey>hoodieKeys)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/4f/b9f221cbdd7486ed90aa8b8c0882a8a4556a19.src","preCode":"  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> checkExists(JavaRDD<HoodieKey> hoodieKeys) {\n    return index.tagLocation(hoodieKeys.map(k -> new HoodieRecord<>(k, null)), context, hoodieTable)\n        .mapToPair(hr -> new Tuple2<>(hr.getKey(), hr.isCurrentLocationKnown()\n            ? Option.of(Pair.of(hr.getPartitionPath(), hr.getCurrentLocation().getFileId()))\n            : Option.empty())\n        );\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieReadClient.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":165,"status":"N"}],"commitId":"1d1d91d444b6af2b24b17d94068512a930877a98","commitMessage":"@@@[HUDI-995] Migrate HoodieTestUtils APIs to HoodieTestTable (#2143)\n\n* [HUDI-995] Migrate HoodieTestUtils APIs to HoodieTestTable\n\nRemove APIs in `HoodieTestUtils`\n- listAllDataFilesAndLogFilesInPath\n- listAllLogFilesInPath\n- listAllDataFilesInPath\n- writeRecordsToLogFiles\n- createCleanFiles\n- createPendingCleanFiles\n\nMigrate the callers to use `HoodieTestTable` and `HoodieWriteableTestTable` with new APIs added\n- listAllBaseAndLogFiles\n- listAllLogFiles\n- listAllBaseFiles\n- withLogAppends\n- addClean\n- addInflightClean\n\nAlso added related APIs in `FileCreateUtils`\n- createCleanFile\n- createRequestedCleanFile\n- createInflightCleanFile\n","date":"2020-10-09 10:21:27","modifiedFileCount":"15","status":"M","submitter":"Raymond Xu"},{"authorTime":"2020-10-02 05:25:29","codes":[{"authorDate":"2020-12-31 08:57:22","commitOrder":3,"curCode":"  public void testCheckExists(boolean rangePruning, boolean treeFiltering, boolean bucketizedChecking) throws Exception {\n    \r\n\n    String recordStr1 = \"{\\\"_row_key\\\":\\\"1eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n    String recordStr2 = \"{\\\"_row_key\\\":\\\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n    String recordStr3 = \"{\\\"_row_key\\\":\\\"3eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n    \r\n    String recordStr4 = \"{\\\"_row_key\\\":\\\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2015-01-31T03:16:41.415Z\\\",\\\"number\\\":32}\";\n    RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n    HoodieKey key1 = new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath());\n    HoodieRecord record1 = new HoodieRecord(key1, rowChange1);\n    RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n    HoodieKey key2 = new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath());\n    HoodieRecord record2 = new HoodieRecord(key2, rowChange2);\n    RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n    HoodieKey key3 = new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath());\n    RawTripTestPayload rowChange4 = new RawTripTestPayload(recordStr4);\n    HoodieKey key4 = new HoodieKey(rowChange4.getRowKey(), rowChange4.getPartitionPath());\n    HoodieRecord record4 = new HoodieRecord(key4, rowChange4);\n    JavaRDD<HoodieKey> keysRDD = jsc.parallelize(Arrays.asList(key1, key2, key3, key4));\n\n    \r\n    HoodieWriteConfig config = makeConfig(rangePruning, treeFiltering, bucketizedChecking);\n    HoodieTable hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n    HoodieSparkWriteableTestTable testTable = HoodieSparkWriteableTestTable.of(hoodieTable, SCHEMA);\n\n    \r\n    SparkHoodieBloomIndex bloomIndex = new SparkHoodieBloomIndex(config);\n    JavaRDD<HoodieRecord> taggedRecords = bloomIndex.tagLocation(keysRDD.map(k -> new HoodieRecord(k, null)), context, hoodieTable);\n    JavaPairRDD<HoodieKey, Option<Pair<String, String>>> recordLocationsRDD = taggedRecords\n        .mapToPair(hr -> new Tuple2<>(hr.getKey(), hr.isCurrentLocationKnown()\n            ? Option.of(Pair.of(hr.getPartitionPath(), hr.getCurrentLocation().getFileId()))\n            : Option.empty())\n        );\n\n    \r\n    for (Tuple2<HoodieKey, Option<Pair<String, String>>> record : recordLocationsRDD.collect()) {\n      assertTrue(!record._2.isPresent());\n    }\n\n    \r\n    String fileId1 = testTable.addCommit(\"001\").getFileIdWithInserts(\"2016/01/31\", record1);\n    String fileId2 = testTable.addCommit(\"002\").getFileIdWithInserts(\"2016/01/31\", record2);\n    String fileId3 = testTable.addCommit(\"003\").getFileIdWithInserts(\"2015/01/31\", record4);\n\n    \r\n    metaClient = HoodieTableMetaClient.reload(metaClient);\n    hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n    taggedRecords = bloomIndex.tagLocation(keysRDD.map(k -> new HoodieRecord(k, null)), context, hoodieTable);\n    recordLocationsRDD = taggedRecords\n        .mapToPair(hr -> new Tuple2<>(hr.getKey(), hr.isCurrentLocationKnown()\n            ? Option.of(Pair.of(hr.getPartitionPath(), hr.getCurrentLocation().getFileId()))\n            : Option.empty())\n        );\n\n    \r\n    for (Tuple2<HoodieKey, Option<Pair<String, String>>> record : recordLocationsRDD.collect()) {\n      if (record._1.getRecordKey().equals(\"1eb5b87a-1feh-4edd-87b4-6ec96dc405a0\")) {\n        assertTrue(record._2.isPresent());\n        assertEquals(fileId1, record._2.get().getRight());\n      } else if (record._1.getRecordKey().equals(\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\")) {\n        assertTrue(record._2.isPresent());\n        if (record._1.getPartitionPath().equals(\"2015/01/31\")) {\n          assertEquals(fileId3, record._2.get().getRight());\n        } else {\n          assertEquals(fileId2, record._2.get().getRight());\n        }\n      } else if (record._1.getRecordKey().equals(\"3eb5b87c-1fej-4edd-87b4-6ec96dc405a0\")) {\n        assertFalse(record._2.isPresent());\n      }\n    }\n  }\n","date":"2020-12-31 08:57:22","endLine":413,"groupId":"10606","id":5,"instanceNumber":1,"isCurCommit":1,"methodName":"testCheckExists","params":"(booleanrangePruning@booleantreeFiltering@booleanbucketizedChecking)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/b3/25eb6b1c4046352a4b1a0f12e701954f1dfaef.src","preCode":"  public void testCheckExists(boolean rangePruning, boolean treeFiltering, boolean bucketizedChecking) throws Exception {\n    \r\n\n    String recordStr1 = \"{\\\"_row_key\\\":\\\"1eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n    String recordStr2 = \"{\\\"_row_key\\\":\\\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n    String recordStr3 = \"{\\\"_row_key\\\":\\\"3eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n    \r\n    String recordStr4 = \"{\\\"_row_key\\\":\\\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n        + \"\\\"time\\\":\\\"2015-01-31T03:16:41.415Z\\\",\\\"number\\\":32}\";\n    RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n    HoodieKey key1 = new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath());\n    HoodieRecord record1 = new HoodieRecord(key1, rowChange1);\n    RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n    HoodieKey key2 = new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath());\n    HoodieRecord record2 = new HoodieRecord(key2, rowChange2);\n    RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n    HoodieKey key3 = new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath());\n    RawTripTestPayload rowChange4 = new RawTripTestPayload(recordStr4);\n    HoodieKey key4 = new HoodieKey(rowChange4.getRowKey(), rowChange4.getPartitionPath());\n    HoodieRecord record4 = new HoodieRecord(key4, rowChange4);\n    JavaRDD<HoodieKey> keysRDD = jsc.parallelize(Arrays.asList(key1, key2, key3, key4));\n\n    \r\n    HoodieWriteConfig config = makeConfig(rangePruning, treeFiltering, bucketizedChecking);\n    HoodieTable hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n    HoodieWriteableTestTable testTable = HoodieWriteableTestTable.of(hoodieTable, SCHEMA);\n\n    \r\n    SparkHoodieBloomIndex bloomIndex = new SparkHoodieBloomIndex(config);\n    JavaRDD<HoodieRecord> taggedRecords = bloomIndex.tagLocation(keysRDD.map(k -> new HoodieRecord(k, null)), context, hoodieTable);\n    JavaPairRDD<HoodieKey, Option<Pair<String, String>>> recordLocationsRDD = taggedRecords\n        .mapToPair(hr -> new Tuple2<>(hr.getKey(), hr.isCurrentLocationKnown()\n            ? Option.of(Pair.of(hr.getPartitionPath(), hr.getCurrentLocation().getFileId()))\n            : Option.empty())\n        );\n\n    \r\n    for (Tuple2<HoodieKey, Option<Pair<String, String>>> record : recordLocationsRDD.collect()) {\n      assertTrue(!record._2.isPresent());\n    }\n\n    \r\n    String fileId1 = testTable.addCommit(\"001\").getFileIdWithInserts(\"2016/01/31\", record1);\n    String fileId2 = testTable.addCommit(\"002\").getFileIdWithInserts(\"2016/01/31\", record2);\n    String fileId3 = testTable.addCommit(\"003\").getFileIdWithInserts(\"2015/01/31\", record4);\n\n    \r\n    metaClient = HoodieTableMetaClient.reload(metaClient);\n    hoodieTable = HoodieSparkTable.create(config, context, metaClient);\n    taggedRecords = bloomIndex.tagLocation(keysRDD.map(k -> new HoodieRecord(k, null)), context, hoodieTable);\n    recordLocationsRDD = taggedRecords\n        .mapToPair(hr -> new Tuple2<>(hr.getKey(), hr.isCurrentLocationKnown()\n            ? Option.of(Pair.of(hr.getPartitionPath(), hr.getCurrentLocation().getFileId()))\n            : Option.empty())\n        );\n\n    \r\n    for (Tuple2<HoodieKey, Option<Pair<String, String>>> record : recordLocationsRDD.collect()) {\n      if (record._1.getRecordKey().equals(\"1eb5b87a-1feh-4edd-87b4-6ec96dc405a0\")) {\n        assertTrue(record._2.isPresent());\n        assertEquals(fileId1, record._2.get().getRight());\n      } else if (record._1.getRecordKey().equals(\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\")) {\n        assertTrue(record._2.isPresent());\n        if (record._1.getPartitionPath().equals(\"2015/01/31\")) {\n          assertEquals(fileId3, record._2.get().getRight());\n        } else {\n          assertEquals(fileId2, record._2.get().getRight());\n        }\n      } else if (record._1.getRecordKey().equals(\"3eb5b87c-1fej-4edd-87b4-6ec96dc405a0\")) {\n        assertFalse(record._2.isPresent());\n      }\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieBloomIndex.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":338,"status":"M"},{"authorDate":"2020-10-02 05:25:29","commitOrder":3,"curCode":"  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> checkExists(JavaRDD<HoodieKey> hoodieKeys) {\n    return index.tagLocation(hoodieKeys.map(k -> new HoodieRecord<>(k, null)), context, hoodieTable)\n        .mapToPair(hr -> new Tuple2<>(hr.getKey(), hr.isCurrentLocationKnown()\n            ? Option.of(Pair.of(hr.getPartitionPath(), hr.getCurrentLocation().getFileId()))\n            : Option.empty())\n        );\n  }\n","date":"2020-10-02 05:25:29","endLine":171,"groupId":"10606","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"checkExists","params":"(JavaRDD<HoodieKey>hoodieKeys)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/4f/b9f221cbdd7486ed90aa8b8c0882a8a4556a19.src","preCode":"  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> checkExists(JavaRDD<HoodieKey> hoodieKeys) {\n    return index.tagLocation(hoodieKeys.map(k -> new HoodieRecord<>(k, null)), context, hoodieTable)\n        .mapToPair(hr -> new Tuple2<>(hr.getKey(), hr.isCurrentLocationKnown()\n            ? Option.of(Pair.of(hr.getPartitionPath(), hr.getCurrentLocation().getFileId()))\n            : Option.empty())\n        );\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/HoodieReadClient.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":165,"status":"N"}],"commitId":"c5e8a024f6a2169f8d6256039de00a44fdb2f5a0","commitMessage":"@@@[HUDI-1418] Set up flink client unit test infra (#2281)\n\n","date":"2020-12-31 08:57:22","modifiedFileCount":"14","status":"M","submitter":"Gary Li"}]
