[{"authorTime":"2020-08-31 23:05:59","codes":[{"authorDate":"2020-08-31 23:05:59","commitOrder":8,"curCode":"  public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSplit split, final JobConf jobConf,\n      final Reporter reporter) throws IOException {\n    \r\n    \r\n    \r\n    \r\n    \r\n    if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {\n      synchronized (jobConf) {\n        LOG.info(\n            \"Before adding Hoodie columns, Projections :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n                + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n        if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          HoodieRealtimeInputFormatUtils.cleanProjectionColumnIds(jobConf);\n          HoodieRealtimeInputFormatUtils.addRequiredProjectionFields(jobConf);\n\n          this.conf = jobConf;\n          this.conf.set(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP, \"true\");\n        }\n      }\n    }\n\n    LOG.info(\"Creating record reader with readCols :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n        + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n    \r\n    ValidationUtils.checkArgument(split instanceof HoodieRealtimeFileSplit,\n        \"HoodieRealtimeRecordReader can only work on HoodieRealtimeFileSplit and not with \" + split);\n\n    return new HoodieRealtimeRecordReader((HoodieRealtimeFileSplit) split, jobConf,\n        super.getRecordReader(split, jobConf, reporter));\n  }\n","date":"2020-08-31 23:05:59","endLine":109,"groupId":"2013","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"getRecordReader","params":"(finalInputSplitsplit@finalJobConfjobConf@finalReporterreporter)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e7/5cff6416954f57a8035a59b9bf83e3fd418199.src","preCode":"  public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSplit split, final JobConf jobConf,\n      final Reporter reporter) throws IOException {\n    \r\n    \r\n    \r\n    \r\n    \r\n    if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {\n      synchronized (jobConf) {\n        LOG.info(\n            \"Before adding Hoodie columns, Projections :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n                + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n        if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          HoodieRealtimeInputFormatUtils.cleanProjectionColumnIds(jobConf);\n          HoodieRealtimeInputFormatUtils.addRequiredProjectionFields(jobConf);\n\n          this.conf = jobConf;\n          this.conf.set(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP, \"true\");\n        }\n      }\n    }\n\n    LOG.info(\"Creating record reader with readCols :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n        + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n    \r\n    ValidationUtils.checkArgument(split instanceof HoodieRealtimeFileSplit,\n        \"HoodieRealtimeRecordReader can only work on HoodieRealtimeFileSplit and not with \" + split);\n\n    return new HoodieRealtimeRecordReader((HoodieRealtimeFileSplit) split, jobConf,\n        super.getRecordReader(split, jobConf, reporter));\n  }\n","realPath":"hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieHFileRealtimeInputFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":72,"status":"B"},{"authorDate":"2020-08-31 23:05:59","commitOrder":8,"curCode":"  public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSplit split, final JobConf jobConf,\n                                                                   final Reporter reporter) throws IOException {\n    \r\n    \r\n    \r\n    \r\n    \r\n    if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {\n      synchronized (jobConf) {\n        LOG.info(\n            \"Before adding Hoodie columns, Projections :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n                + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n        if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          HoodieRealtimeInputFormatUtils.cleanProjectionColumnIds(jobConf);\n          HoodieRealtimeInputFormatUtils.addRequiredProjectionFields(jobConf);\n\n          this.conf = jobConf;\n          this.conf.set(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP, \"true\");\n        }\n      }\n    }\n\n    LOG.info(\"Creating record reader with readCols :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n        + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n\n    \r\n    ValidationUtils.checkArgument(split instanceof RealtimeSplit,\n        \"HoodieRealtimeRecordReader can only work on RealtimeSplit and not with \" + split);\n\n    return new HoodieRealtimeRecordReader((RealtimeSplit) split, jobConf,\n        super.getRecordReader(split, jobConf, reporter));\n  }\n","date":"2020-08-31 23:05:59","endLine":121,"groupId":"2013","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"getRecordReader","params":"(finalInputSplitsplit@finalJobConfjobConf@finalReporterreporter)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/5b/cfbe94b6af14ce77fe24a5011324670a0006d9.src","preCode":"  public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSplit split, final JobConf jobConf,\n                                                                   final Reporter reporter) throws IOException {\n    \r\n    \r\n    \r\n    \r\n    \r\n    if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {\n      synchronized (jobConf) {\n        LOG.info(\n            \"Before adding Hoodie columns, Projections :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n                + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n        if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          HoodieRealtimeInputFormatUtils.cleanProjectionColumnIds(jobConf);\n          HoodieRealtimeInputFormatUtils.addRequiredProjectionFields(jobConf);\n\n          this.conf = jobConf;\n          this.conf.set(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP, \"true\");\n        }\n      }\n    }\n\n    LOG.info(\"Creating record reader with readCols :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n        + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n\n    \r\n    ValidationUtils.checkArgument(split instanceof RealtimeSplit,\n        \"HoodieRealtimeRecordReader can only work on RealtimeSplit and not with \" + split);\n\n    return new HoodieRealtimeRecordReader((RealtimeSplit) split, jobConf,\n        super.getRecordReader(split, jobConf, reporter));\n  }\n","realPath":"hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":83,"status":"MB"}],"commitId":"6461927eac3f8a225b17af5ecb6ace8c9cf1757b","commitMessage":"@@@[HUDI-960] Implementation of the HFile base and log file format. (#1804)\n\n* [HUDI-960] Implementation of the HFile base and log file format.\n\n1. Includes HFileWriter and HFileReader\n2. Includes HFileInputFormat for both snapshot and realtime input format for Hive\n3. Unit test for new code\n4. IT for using HFile format and querying using Hive (Presto and SparkSQL are not supported)\n\nAdvantage:\nHFile file format saves data as binary key-value pairs. This implementation chooses the following values:\n1. Key = Hoodie Record Key (as bytes)\n2. Value = Avro encoded GenericRecord (as bytes)\n\nHFile allows efficient lookup of a record by key or range of keys. Hence.  this base file format is well suited to applications like RFC-15.  RFC-08 which will benefit from the ability to lookup records by key or search in a range of keys without having to read the entire data/log format.\n\nLimitations:\nHFile storage format has certain limitations when used as a general purpose data storage format.\n1. Does not have a implemented reader for Presto and SparkSQL\n2. Is not a columnar file format and hence may lead to lower compression levels and greater IO on query side due to lack of column pruning\n\n\nOther changes: \n - Remove databricks/avro from pom\n - Fix HoodieClientTestUtils from not using scala imports/reflection based conversion etc\n - Breaking up limitFileSize().  per parquet and hfile base files\n - Added three new configs for HoodieHFileConfig - prefetchBlocksOnOpen.  cacheDataInL1.  dropBehindCacheCompaction\n - Throw UnsupportedException in HFileReader.getRecordKeys()\n - Updated HoodieCopyOnWriteTable to create the correct merge handle (HoodieSortedMergeHandle for HFile and HoodieMergeHandle otherwise)\n\n* Fixing checkstyle\n\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2020-08-31 23:05:59","modifiedFileCount":"41","status":"M","submitter":"Prashant Wason"},{"authorTime":"2020-11-03 12:00:12","codes":[{"authorDate":"2020-08-31 23:05:59","commitOrder":9,"curCode":"  public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSplit split, final JobConf jobConf,\n      final Reporter reporter) throws IOException {\n    \r\n    \r\n    \r\n    \r\n    \r\n    if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {\n      synchronized (jobConf) {\n        LOG.info(\n            \"Before adding Hoodie columns, Projections :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n                + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n        if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          HoodieRealtimeInputFormatUtils.cleanProjectionColumnIds(jobConf);\n          HoodieRealtimeInputFormatUtils.addRequiredProjectionFields(jobConf);\n\n          this.conf = jobConf;\n          this.conf.set(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP, \"true\");\n        }\n      }\n    }\n\n    LOG.info(\"Creating record reader with readCols :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n        + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n    \r\n    ValidationUtils.checkArgument(split instanceof HoodieRealtimeFileSplit,\n        \"HoodieRealtimeRecordReader can only work on HoodieRealtimeFileSplit and not with \" + split);\n\n    return new HoodieRealtimeRecordReader((HoodieRealtimeFileSplit) split, jobConf,\n        super.getRecordReader(split, jobConf, reporter));\n  }\n","date":"2020-08-31 23:05:59","endLine":109,"groupId":"2013","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"getRecordReader","params":"(finalInputSplitsplit@finalJobConfjobConf@finalReporterreporter)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e7/5cff6416954f57a8035a59b9bf83e3fd418199.src","preCode":"  public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSplit split, final JobConf jobConf,\n      final Reporter reporter) throws IOException {\n    \r\n    \r\n    \r\n    \r\n    \r\n    if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {\n      synchronized (jobConf) {\n        LOG.info(\n            \"Before adding Hoodie columns, Projections :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n                + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n        if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          HoodieRealtimeInputFormatUtils.cleanProjectionColumnIds(jobConf);\n          HoodieRealtimeInputFormatUtils.addRequiredProjectionFields(jobConf);\n\n          this.conf = jobConf;\n          this.conf.set(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP, \"true\");\n        }\n      }\n    }\n\n    LOG.info(\"Creating record reader with readCols :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n        + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n    \r\n    ValidationUtils.checkArgument(split instanceof HoodieRealtimeFileSplit,\n        \"HoodieRealtimeRecordReader can only work on HoodieRealtimeFileSplit and not with \" + split);\n\n    return new HoodieRealtimeRecordReader((HoodieRealtimeFileSplit) split, jobConf,\n        super.getRecordReader(split, jobConf, reporter));\n  }\n","realPath":"hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieHFileRealtimeInputFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":72,"status":"N"},{"authorDate":"2020-11-03 12:00:12","commitOrder":9,"curCode":"  public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSplit split, final JobConf jobConf,\n                                                                   final Reporter reporter) throws IOException {\n    \r\n    ValidationUtils.checkArgument(split instanceof RealtimeSplit,\n        \"HoodieRealtimeRecordReader can only work on RealtimeSplit and not with \" + split);\n    RealtimeSplit realtimeSplit = (RealtimeSplit) split;\n    addProjectionToJobConf(realtimeSplit, jobConf);\n    LOG.info(\"Creating record reader with readCols :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n        + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n    return new HoodieRealtimeRecordReader(realtimeSplit, jobConf,\n        super.getRecordReader(split, jobConf, reporter));\n  }\n","date":"2020-11-03 12:00:12","endLine":124,"groupId":"399","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"getRecordReader","params":"(finalInputSplitsplit@finalJobConfjobConf@finalReporterreporter)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/d8/f0a01a911edb9773d16e237dcbf07d1059597a.src","preCode":"  public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSplit split, final JobConf jobConf,\n                                                                   final Reporter reporter) throws IOException {\n    \r\n    \r\n    \r\n    \r\n    \r\n    if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {\n      synchronized (jobConf) {\n        LOG.info(\n            \"Before adding Hoodie columns, Projections :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n                + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n        if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          HoodieRealtimeInputFormatUtils.cleanProjectionColumnIds(jobConf);\n          HoodieRealtimeInputFormatUtils.addRequiredProjectionFields(jobConf);\n\n          this.conf = jobConf;\n          this.conf.set(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP, \"true\");\n        }\n      }\n    }\n\n    LOG.info(\"Creating record reader with readCols :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n        + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n\n    \r\n    ValidationUtils.checkArgument(split instanceof RealtimeSplit,\n        \"HoodieRealtimeRecordReader can only work on RealtimeSplit and not with \" + split);\n\n    return new HoodieRealtimeRecordReader((RealtimeSplit) split, jobConf,\n        super.getRecordReader(split, jobConf, reporter));\n  }\n","realPath":"hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":113,"status":"M"}],"commitId":"5f5c15b0d9f807dc7f9112a5220c3daeb01f0cb8","commitMessage":"@@@[HUDI-892] RealtimeParquetInputFormat skip adding projection columns if there are no log files (#2190)\n\n* [HUDI-892] RealtimeParquetInputFormat skip adding projection columns if there are no log files\n* [HUDI-892]  for test\n* [HUDI-892]  fix bug generate array from split\n* [HUDI-892] revert test log","date":"2020-11-03 12:00:12","modifiedFileCount":"3","status":"M","submitter":"lw0090"},{"authorTime":"2020-11-03 12:00:12","codes":[{"authorDate":"2021-08-02 21:45:09","commitOrder":10,"curCode":"  public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSplit split, final JobConf jobConf,\n      final Reporter reporter) throws IOException {\n    \r\n    \r\n    \r\n    \r\n    \r\n    if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {\n      synchronized (jobConf) {\n        LOG.info(\n            \"Before adding Hoodie columns, Projections :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n                + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n        if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          HoodieRealtimeInputFormatUtils.cleanProjectionColumnIds(jobConf);\n          HoodieRealtimeInputFormatUtils.addRequiredProjectionFields(jobConf, Option.empty());\n\n          this.conf = jobConf;\n          this.conf.set(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP, \"true\");\n        }\n      }\n    }\n\n    LOG.info(\"Creating record reader with readCols :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n        + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n    \r\n    ValidationUtils.checkArgument(split instanceof HoodieRealtimeFileSplit,\n        \"HoodieRealtimeRecordReader can only work on HoodieRealtimeFileSplit and not with \" + split);\n\n    return new HoodieRealtimeRecordReader((HoodieRealtimeFileSplit) split, jobConf,\n        super.getRecordReader(split, jobConf, reporter));\n  }\n","date":"2021-08-02 21:45:09","endLine":111,"groupId":"1071","id":5,"instanceNumber":1,"isCurCommit":1,"methodName":"getRecordReader","params":"(finalInputSplitsplit@finalJobConfjobConf@finalReporterreporter)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/63/728e38f1c7a15851d5d8d956aa749f0a82463b.src","preCode":"  public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSplit split, final JobConf jobConf,\n      final Reporter reporter) throws IOException {\n    \r\n    \r\n    \r\n    \r\n    \r\n    if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {\n      synchronized (jobConf) {\n        LOG.info(\n            \"Before adding Hoodie columns, Projections :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n                + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n        if (jobConf.get(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP) == null) {\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          \r\n          HoodieRealtimeInputFormatUtils.cleanProjectionColumnIds(jobConf);\n          HoodieRealtimeInputFormatUtils.addRequiredProjectionFields(jobConf);\n\n          this.conf = jobConf;\n          this.conf.set(HoodieInputFormatUtils.HOODIE_READ_COLUMNS_PROP, \"true\");\n        }\n      }\n    }\n\n    LOG.info(\"Creating record reader with readCols :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n        + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n    \r\n    ValidationUtils.checkArgument(split instanceof HoodieRealtimeFileSplit,\n        \"HoodieRealtimeRecordReader can only work on HoodieRealtimeFileSplit and not with \" + split);\n\n    return new HoodieRealtimeRecordReader((HoodieRealtimeFileSplit) split, jobConf,\n        super.getRecordReader(split, jobConf, reporter));\n  }\n","realPath":"hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieHFileRealtimeInputFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":74,"status":"M"},{"authorDate":"2020-11-03 12:00:12","commitOrder":10,"curCode":"  public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSplit split, final JobConf jobConf,\n                                                                   final Reporter reporter) throws IOException {\n    \r\n    ValidationUtils.checkArgument(split instanceof RealtimeSplit,\n        \"HoodieRealtimeRecordReader can only work on RealtimeSplit and not with \" + split);\n    RealtimeSplit realtimeSplit = (RealtimeSplit) split;\n    addProjectionToJobConf(realtimeSplit, jobConf);\n    LOG.info(\"Creating record reader with readCols :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n        + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n    return new HoodieRealtimeRecordReader(realtimeSplit, jobConf,\n        super.getRecordReader(split, jobConf, reporter));\n  }\n","date":"2020-11-03 12:00:12","endLine":124,"groupId":"1071","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"getRecordReader","params":"(finalInputSplitsplit@finalJobConfjobConf@finalReporterreporter)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/d8/f0a01a911edb9773d16e237dcbf07d1059597a.src","preCode":"  public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSplit split, final JobConf jobConf,\n                                                                   final Reporter reporter) throws IOException {\n    \r\n    ValidationUtils.checkArgument(split instanceof RealtimeSplit,\n        \"HoodieRealtimeRecordReader can only work on RealtimeSplit and not with \" + split);\n    RealtimeSplit realtimeSplit = (RealtimeSplit) split;\n    addProjectionToJobConf(realtimeSplit, jobConf);\n    LOG.info(\"Creating record reader with readCols :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)\n        + \", Ids :\" + jobConf.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));\n    return new HoodieRealtimeRecordReader(realtimeSplit, jobConf,\n        super.getRecordReader(split, jobConf, reporter));\n  }\n","realPath":"hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":113,"status":"N"}],"commitId":"fe508376faaa3c36e9f821f3d6fee731983798c4","commitMessage":"@@@[HUDI-2177][HUDI-2200] Adding virtual keys support for MOR table (#3315)\n\n","date":"2021-08-02 21:45:09","modifiedFileCount":"30","status":"M","submitter":"Sivabalan Narayanan"}]
