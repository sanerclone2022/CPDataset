[{"authorTime":"2020-12-25 22:43:34","codes":[{"authorDate":"2021-01-06 23:07:24","commitOrder":3,"curCode":"  public void testDataInternalWriter() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), STRUCT_TYPE);\n\n      int size = 10 + RANDOM.nextInt(1000);\n      \r\n      int batches = 5;\n      Dataset<Row> totalInputRows = null;\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n      Option<List<String>> fileNames = Option.of(new ArrayList<>());\n\n      \r\n      assertWriteStatuses(commitMetadata.getWriteStatuses(), batches, size, fileAbsPaths, fileNames);\n\n      \r\n      Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n      assertOutput(totalInputRows, result, instantTime, fileNames);\n    }\n  }\n","date":"2021-01-06 23:07:24","endLine":88,"groupId":"1872","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testDataInternalWriter","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ff/b649bd3970ad5f72990825371a4af8a99afd03.src","preCode":"  public void testDataInternalWriter() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), STRUCT_TYPE);\n\n      int size = 10 + RANDOM.nextInt(1000);\n      \r\n      int batches = 5;\n      Dataset<Row> totalInputRows = null;\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n      Option<List<String>> fileNames = Option.of(new ArrayList<>());\n\n      \r\n      assertWriteStatuses(commitMetadata.getWriteStatuses(), batches, size, fileAbsPaths, fileNames);\n\n      \r\n      Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n      assertOutput(totalInputRows, result, instantTime, fileNames);\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieBulkInsertDataInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":51,"status":"B"},{"authorDate":"2020-12-25 22:43:34","commitOrder":3,"curCode":"  public void testDataInternalWriter() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), RANDOM.nextLong(), STRUCT_TYPE);\n\n      int size = 10 + RANDOM.nextInt(1000);\n      \r\n      int batches = 5;\n      Dataset<Row> totalInputRows = null;\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      BaseWriterCommitMessage commitMetadata = (BaseWriterCommitMessage) writer.commit();\n      Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n      Option<List<String>> fileNames = Option.of(new ArrayList<>());\n\n      \r\n      assertWriteStatuses(commitMetadata.getWriteStatuses(), batches, size, fileAbsPaths, fileNames);\n\n      \r\n      Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n      assertOutput(totalInputRows, result, instantTime, fileNames);\n    }\n  }\n","date":"2020-12-25 22:43:34","endLine":87,"groupId":"1872","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testDataInternalWriter","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/0b/021abeb3b5fe2deced8f07df8f6bac397d52b6.src","preCode":"  public void testDataInternalWriter() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), RANDOM.nextLong(), STRUCT_TYPE);\n\n      int size = 10 + RANDOM.nextInt(1000);\n      \r\n      int batches = 5;\n      Dataset<Row> totalInputRows = null;\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      BaseWriterCommitMessage commitMetadata = (BaseWriterCommitMessage) writer.commit();\n      Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n      Option<List<String>> fileNames = Option.of(new ArrayList<>());\n\n      \r\n      assertWriteStatuses(commitMetadata.getWriteStatuses(), batches, size, fileAbsPaths, fileNames);\n\n      \r\n      Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n      assertOutput(totalInputRows, result, instantTime, fileNames);\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieBulkInsertDataInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":50,"status":"NB"}],"commitId":"b593f1062931a4d017ae8bd7dd42e47a8873a39f","commitMessage":"@@@[MINOR] Rename unit test package of hudi-spark3 from scala to java (#2411)\n\n","date":"2021-01-06 23:07:24","modifiedFileCount":"0","status":"M","submitter":"wangxianghu"},{"authorTime":"2021-07-07 23:15:25","codes":[{"authorDate":"2021-07-07 23:15:25","commitOrder":4,"curCode":"  public void testDataInternalWriter(boolean sorted) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), STRUCT_TYPE,\n          sorted);\n\n      int size = 10 + RANDOM.nextInt(1000);\n      \r\n      int batches = 5;\n      Dataset<Row> totalInputRows = null;\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n      Option<List<String>> fileNames = Option.of(new ArrayList<>());\n\n      \r\n      assertWriteStatuses(commitMetadata.getWriteStatuses(), batches, size, sorted, fileAbsPaths, fileNames);\n\n      \r\n      Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n      assertOutput(totalInputRows, result, instantTime, fileNames);\n    }\n  }\n","date":"2021-07-07 23:15:25","endLine":102,"groupId":"3705","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testDataInternalWriter","params":"(booleansorted)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/26/685fd8fc567b17298946a48330a93fd8f57ccd.src","preCode":"  public void testDataInternalWriter() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), STRUCT_TYPE);\n\n      int size = 10 + RANDOM.nextInt(1000);\n      \r\n      int batches = 5;\n      Dataset<Row> totalInputRows = null;\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n      Option<List<String>> fileNames = Option.of(new ArrayList<>());\n\n      \r\n      assertWriteStatuses(commitMetadata.getWriteStatuses(), batches, size, fileAbsPaths, fileNames);\n\n      \r\n      Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n      assertOutput(totalInputRows, result, instantTime, fileNames);\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieBulkInsertDataInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":64,"status":"M"},{"authorDate":"2021-07-07 23:15:25","commitOrder":4,"curCode":"  public void testDataInternalWriter(boolean sorted) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), RANDOM.nextLong(), STRUCT_TYPE,\n          sorted);\n\n      int size = 10 + RANDOM.nextInt(1000);\n      \r\n      int batches = 5;\n      Dataset<Row> totalInputRows = null;\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      BaseWriterCommitMessage commitMetadata = (BaseWriterCommitMessage) writer.commit();\n      Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n      Option<List<String>> fileNames = Option.of(new ArrayList<>());\n\n      \r\n      assertWriteStatuses(commitMetadata.getWriteStatuses(), batches, size, sorted, fileAbsPaths, fileNames);\n\n      \r\n      Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n      assertOutput(totalInputRows, result, instantTime, fileNames);\n    }\n  }\n","date":"2021-07-07 23:15:25","endLine":101,"groupId":"3705","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testDataInternalWriter","params":"(booleansorted)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/f7/7763f85d941ab01045bebe123f8d897cd18868.src","preCode":"  public void testDataInternalWriter() throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), RANDOM.nextLong(), STRUCT_TYPE);\n\n      int size = 10 + RANDOM.nextInt(1000);\n      \r\n      int batches = 5;\n      Dataset<Row> totalInputRows = null;\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      BaseWriterCommitMessage commitMetadata = (BaseWriterCommitMessage) writer.commit();\n      Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n      Option<List<String>> fileNames = Option.of(new ArrayList<>());\n\n      \r\n      assertWriteStatuses(commitMetadata.getWriteStatuses(), batches, size, fileAbsPaths, fileNames);\n\n      \r\n      Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n      assertOutput(totalInputRows, result, instantTime, fileNames);\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieBulkInsertDataInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":63,"status":"M"}],"commitId":"ea9e5d0e8b7557ef82631ac173d67f15bad13690","commitMessage":"@@@[HUDI-1104] Adding support for UserDefinedPartitioners and SortModes to BulkInsert with Rows (#3149)\n\n","date":"2021-07-07 23:15:25","modifiedFileCount":"21","status":"M","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-07-20 08:43:48","codes":[{"authorDate":"2021-07-20 08:43:48","commitOrder":5,"curCode":"  public void testDataInternalWriter(boolean sorted, boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000),\n          RANDOM.nextLong(), STRUCT_TYPE, populateMetaFields, sorted);\n\n      int size = 10 + RANDOM.nextInt(1000);\n      \r\n      int batches = 5;\n      Dataset<Row> totalInputRows = null;\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n      Option<List<String>> fileNames = Option.of(new ArrayList<>());\n\n      \r\n      assertWriteStatuses(commitMetadata.getWriteStatuses(), batches, size, sorted, fileAbsPaths, fileNames);\n\n      \r\n      Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n      assertOutput(totalInputRows, result, instantTime, fileNames, populateMetaFields);\n    }\n  }\n","date":"2021-07-20 08:43:48","endLine":111,"groupId":"3705","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testDataInternalWriter","params":"(booleansorted@booleanpopulateMetaFields)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/3b/7fb97ffa36657b4dd871d0576e1304fa94bd1c.src","preCode":"  public void testDataInternalWriter(boolean sorted) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), STRUCT_TYPE,\n          sorted);\n\n      int size = 10 + RANDOM.nextInt(1000);\n      \r\n      int batches = 5;\n      Dataset<Row> totalInputRows = null;\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n      Option<List<String>> fileNames = Option.of(new ArrayList<>());\n\n      \r\n      assertWriteStatuses(commitMetadata.getWriteStatuses(), batches, size, sorted, fileAbsPaths, fileNames);\n\n      \r\n      Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n      assertOutput(totalInputRows, result, instantTime, fileNames);\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieBulkInsertDataInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":73,"status":"M"},{"authorDate":"2021-07-20 08:43:48","commitOrder":5,"curCode":"  public void testDataInternalWriter(boolean sorted, boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), RANDOM.nextLong(),\n          STRUCT_TYPE, populateMetaFields, sorted);\n\n      int size = 10 + RANDOM.nextInt(1000);\n      \r\n      int batches = 5;\n      Dataset<Row> totalInputRows = null;\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      BaseWriterCommitMessage commitMetadata = (BaseWriterCommitMessage) writer.commit();\n      Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n      Option<List<String>> fileNames = Option.of(new ArrayList<>());\n\n      \r\n      assertWriteStatuses(commitMetadata.getWriteStatuses(), batches, size, sorted, fileAbsPaths, fileNames);\n\n      \r\n      Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n      assertOutput(totalInputRows, result, instantTime, fileNames, populateMetaFields);\n    }\n  }\n","date":"2021-07-20 08:43:48","endLine":110,"groupId":"3705","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testDataInternalWriter","params":"(booleansorted@booleanpopulateMetaFields)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/97/35379d45f2f18faae7e02ce8e6f12fef9308cc.src","preCode":"  public void testDataInternalWriter(boolean sorted) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), RANDOM.nextLong(), STRUCT_TYPE,\n          sorted);\n\n      int size = 10 + RANDOM.nextInt(1000);\n      \r\n      int batches = 5;\n      Dataset<Row> totalInputRows = null;\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      BaseWriterCommitMessage commitMetadata = (BaseWriterCommitMessage) writer.commit();\n      Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n      Option<List<String>> fileNames = Option.of(new ArrayList<>());\n\n      \r\n      assertWriteStatuses(commitMetadata.getWriteStatuses(), batches, size, sorted, fileAbsPaths, fileNames);\n\n      \r\n      Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n      assertOutput(totalInputRows, result, instantTime, fileNames);\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieBulkInsertDataInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":72,"status":"M"}],"commitId":"d5026e9a24850bdcce9f6df3686bf2235d7d01c4","commitMessage":"@@@[HUDI-2161] Adding support to disable meta columns with bulk insert operation (#3247)\n\n","date":"2021-07-20 08:43:48","modifiedFileCount":"39","status":"M","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-07-30 13:22:26","codes":[{"authorDate":"2021-07-30 13:22:26","commitOrder":6,"curCode":"  public void testDataInternalWriter(boolean sorted, boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    \r\n    for (int i = 0; i < 2; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000),\n          RANDOM.nextLong(), STRUCT_TYPE, populateMetaFields, sorted);\n\n      int size = 10 + RANDOM.nextInt(1000);\n      \r\n      int batches = 3;\n      Dataset<Row> totalInputRows = null;\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n      Option<List<String>> fileNames = Option.of(new ArrayList<>());\n\n      \r\n      assertWriteStatuses(commitMetadata.getWriteStatuses(), batches, size, sorted, fileAbsPaths, fileNames);\n\n      \r\n      Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n      assertOutput(totalInputRows, result, instantTime, fileNames, populateMetaFields);\n    }\n  }\n","date":"2021-07-30 13:22:26","endLine":111,"groupId":"10126","id":7,"instanceNumber":1,"isCurCommit":1,"methodName":"testDataInternalWriter","params":"(booleansorted@booleanpopulateMetaFields)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/a3/d0e3237202534f12b7053424feea99a7322074.src","preCode":"  public void testDataInternalWriter(boolean sorted, boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    \r\n    for (int i = 0; i < 5; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000),\n          RANDOM.nextLong(), STRUCT_TYPE, populateMetaFields, sorted);\n\n      int size = 10 + RANDOM.nextInt(1000);\n      \r\n      int batches = 5;\n      Dataset<Row> totalInputRows = null;\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      HoodieWriterCommitMessage commitMetadata = (HoodieWriterCommitMessage) writer.commit();\n      Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n      Option<List<String>> fileNames = Option.of(new ArrayList<>());\n\n      \r\n      assertWriteStatuses(commitMetadata.getWriteStatuses(), batches, size, sorted, fileAbsPaths, fileNames);\n\n      \r\n      Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n      assertOutput(totalInputRows, result, instantTime, fileNames, populateMetaFields);\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark3/src/test/java/org/apache/hudi/spark3/internal/TestHoodieBulkInsertDataInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":73,"status":"M"},{"authorDate":"2021-07-30 13:22:26","commitOrder":6,"curCode":"  public void testDataInternalWriter(boolean sorted, boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    \r\n    for (int i = 0; i < 2; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), RANDOM.nextLong(),\n          STRUCT_TYPE, populateMetaFields, sorted);\n\n      int size = 10 + RANDOM.nextInt(1000);\n      \r\n      int batches = 3;\n      Dataset<Row> totalInputRows = null;\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      BaseWriterCommitMessage commitMetadata = (BaseWriterCommitMessage) writer.commit();\n      Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n      Option<List<String>> fileNames = Option.of(new ArrayList<>());\n\n      \r\n      assertWriteStatuses(commitMetadata.getWriteStatuses(), batches, size, sorted, fileAbsPaths, fileNames);\n\n      \r\n      Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n      assertOutput(totalInputRows, result, instantTime, fileNames, populateMetaFields);\n    }\n  }\n","date":"2021-07-30 13:22:26","endLine":110,"groupId":"10126","id":8,"instanceNumber":2,"isCurCommit":1,"methodName":"testDataInternalWriter","params":"(booleansorted@booleanpopulateMetaFields)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/fd/943b72e35977421683476089648dbc09995b10.src","preCode":"  public void testDataInternalWriter(boolean sorted, boolean populateMetaFields) throws Exception {\n    \r\n    HoodieWriteConfig cfg = getWriteConfig(populateMetaFields);\n    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n    \r\n    for (int i = 0; i < 3; i++) {\n      String instantTime = \"00\" + i;\n      \r\n      HoodieBulkInsertDataInternalWriter writer = new HoodieBulkInsertDataInternalWriter(table, cfg, instantTime, RANDOM.nextInt(100000), RANDOM.nextLong(), RANDOM.nextLong(),\n          STRUCT_TYPE, populateMetaFields, sorted);\n\n      int size = 10 + RANDOM.nextInt(1000);\n      \r\n      int batches = 5;\n      Dataset<Row> totalInputRows = null;\n\n      for (int j = 0; j < batches; j++) {\n        String partitionPath = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS[j % 3];\n        Dataset<Row> inputRows = getRandomRows(sqlContext, size, partitionPath, false);\n        writeRows(inputRows, writer);\n        if (totalInputRows == null) {\n          totalInputRows = inputRows;\n        } else {\n          totalInputRows = totalInputRows.union(inputRows);\n        }\n      }\n\n      BaseWriterCommitMessage commitMetadata = (BaseWriterCommitMessage) writer.commit();\n      Option<List<String>> fileAbsPaths = Option.of(new ArrayList<>());\n      Option<List<String>> fileNames = Option.of(new ArrayList<>());\n\n      \r\n      assertWriteStatuses(commitMetadata.getWriteStatuses(), batches, size, sorted, fileAbsPaths, fileNames);\n\n      \r\n      Dataset<Row> result = sqlContext.read().parquet(fileAbsPaths.get().toArray(new String[0]));\n      assertOutput(totalInputRows, result, instantTime, fileNames, populateMetaFields);\n    }\n  }\n","realPath":"hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieBulkInsertDataInternalWriter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":72,"status":"M"}],"commitId":"7bdae69053afc5ef604a15806d78317cb976f2ce","commitMessage":"@@@[HUDI-2253] Refactoring few tests to reduce runningtime. DeltaStreamer and MultiDeltaStreamer tests. Bulk insert row writer tests (#3371)\n\nCo-authored-by: Sivabalan Narayanan <nsb@Sivabalans-MBP.attlocal.net>","date":"2021-07-30 13:22:26","modifiedFileCount":"6","status":"M","submitter":"Sivabalan Narayanan"}]
