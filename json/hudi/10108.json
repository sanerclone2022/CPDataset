[{"authorTime":"2020-05-14 06:37:03","codes":[{"authorDate":"2019-12-29 02:16:09","commitOrder":3,"curCode":"  private static int dataLoad(JavaSparkContext jsc, String command, String srcPath, String targetPath, String tableName,\n      String tableType, String rowKey, String partitionKey, int parallelism, String schemaFile, String sparkMemory,\n                              int retry, String propsFilePath, List<String> configs) {\n    Config cfg = new Config();\n    cfg.command = command;\n    cfg.srcPath = srcPath;\n    cfg.targetPath = targetPath;\n    cfg.tableName = tableName;\n    cfg.tableType = tableType;\n    cfg.rowKey = rowKey;\n    cfg.partitionKey = partitionKey;\n    cfg.parallelism = parallelism;\n    cfg.schemaFile = schemaFile;\n    cfg.propsFilePath = propsFilePath;\n    cfg.configs = configs;\n    jsc.getConf().set(\"spark.executor.memory\", sparkMemory);\n    return new HDFSParquetImporter(cfg).dataImport(jsc, retry);\n  }\n","date":"2020-01-04 08:00:57","endLine":190,"groupId":"1331","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"dataLoad","params":"(JavaSparkContextjsc@Stringcommand@StringsrcPath@StringtargetPath@StringtableName@StringtableType@StringrowKey@StringpartitionKey@intparallelism@StringschemaFile@StringsparkMemory@intretry@StringpropsFilePath@List<String>configs)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/8f/f52fad9cc1a098bf33b5b4566f399edc0c007f.src","preCode":"  private static int dataLoad(JavaSparkContext jsc, String command, String srcPath, String targetPath, String tableName,\n      String tableType, String rowKey, String partitionKey, int parallelism, String schemaFile, String sparkMemory,\n                              int retry, String propsFilePath, List<String> configs) {\n    Config cfg = new Config();\n    cfg.command = command;\n    cfg.srcPath = srcPath;\n    cfg.targetPath = targetPath;\n    cfg.tableName = tableName;\n    cfg.tableType = tableType;\n    cfg.rowKey = rowKey;\n    cfg.partitionKey = partitionKey;\n    cfg.parallelism = parallelism;\n    cfg.schemaFile = schemaFile;\n    cfg.propsFilePath = propsFilePath;\n    cfg.configs = configs;\n    jsc.getConf().set(\"spark.executor.memory\", sparkMemory);\n    return new HDFSParquetImporter(cfg).dataImport(jsc, retry);\n  }\n","realPath":"hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":173,"status":"NB"},{"authorDate":"2020-05-14 06:37:03","commitOrder":3,"curCode":"  private HDFSParquetImporter.Config getHDFSParquetImporterConfig(String srcPath, String targetPath, String tableName,\n      String tableType, String rowKey, String partitionKey, int parallelism, String schemaFile) {\n    HDFSParquetImporter.Config cfg = new HDFSParquetImporter.Config();\n    cfg.srcPath = srcPath;\n    cfg.targetPath = targetPath;\n    cfg.tableName = tableName;\n    cfg.tableType = tableType;\n    cfg.rowKey = rowKey;\n    cfg.partitionKey = partitionKey;\n    cfg.parallelism = parallelism;\n    cfg.schemaFile = schemaFile;\n    return cfg;\n  }\n","date":"2020-05-14 06:37:03","endLine":376,"groupId":"1331","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"getHDFSParquetImporterConfig","params":"(StringsrcPath@StringtargetPath@StringtableName@StringtableType@StringrowKey@StringpartitionKey@intparallelism@StringschemaFile)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/f7/38cdfdece690b977ec31850ebdf81d1d20a181.src","preCode":"  private HDFSParquetImporter.Config getHDFSParquetImporterConfig(String srcPath, String targetPath, String tableName,\n      String tableType, String rowKey, String partitionKey, int parallelism, String schemaFile) {\n    HDFSParquetImporter.Config cfg = new HDFSParquetImporter.Config();\n    cfg.srcPath = srcPath;\n    cfg.targetPath = targetPath;\n    cfg.tableName = tableName;\n    cfg.tableType = tableType;\n    cfg.rowKey = rowKey;\n    cfg.partitionKey = partitionKey;\n    cfg.parallelism = parallelism;\n    cfg.schemaFile = schemaFile;\n    return cfg;\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":364,"status":"B"}],"commitId":"0d4848b68b625a17d05b38864a84a6cc71189bfa","commitMessage":"@@@[HUDI-811] Restructure test packages (#1607)\n\n* restructure hudi-spark tests\n* restructure hudi-timeline-service tests\n* restructure hudi-hadoop-mr hudi-utilities tests\n* restructure hudi-hive-sync tests","date":"2020-05-14 06:37:03","modifiedFileCount":"11","status":"M","submitter":"Raymond Xu"},{"authorTime":"2020-05-14 19:15:49","codes":[{"authorDate":"2020-05-14 19:15:49","commitOrder":4,"curCode":"  private static int dataLoad(JavaSparkContext jsc, String command, String srcPath, String targetPath, String tableName,\n      String tableType, String rowKey, String partitionKey, int parallelism, String schemaFile,\n      int retry, String propsFilePath, List<String> configs) {\n    Config cfg = new Config();\n    cfg.command = command;\n    cfg.srcPath = srcPath;\n    cfg.targetPath = targetPath;\n    cfg.tableName = tableName;\n    cfg.tableType = tableType;\n    cfg.rowKey = rowKey;\n    cfg.partitionKey = partitionKey;\n    cfg.parallelism = parallelism;\n    cfg.schemaFile = schemaFile;\n    cfg.propsFilePath = propsFilePath;\n    cfg.configs = configs;\n    return new HDFSParquetImporter(cfg).dataImport(jsc, retry);\n  }\n","date":"2020-05-14 19:15:49","endLine":195,"groupId":"10108","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"dataLoad","params":"(JavaSparkContextjsc@Stringcommand@StringsrcPath@StringtargetPath@StringtableName@StringtableType@StringrowKey@StringpartitionKey@intparallelism@StringschemaFile@intretry@StringpropsFilePath@List<String>configs)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/be/9d7ddf8ebfa380b9a07924d1e1cdefadbb6811.src","preCode":"  private static int dataLoad(JavaSparkContext jsc, String command, String srcPath, String targetPath, String tableName,\n      String tableType, String rowKey, String partitionKey, int parallelism, String schemaFile, String sparkMemory,\n      int retry, String propsFilePath, List<String> configs) {\n    Config cfg = new Config();\n    cfg.command = command;\n    cfg.srcPath = srcPath;\n    cfg.targetPath = targetPath;\n    cfg.tableName = tableName;\n    cfg.tableType = tableType;\n    cfg.rowKey = rowKey;\n    cfg.partitionKey = partitionKey;\n    cfg.parallelism = parallelism;\n    cfg.schemaFile = schemaFile;\n    cfg.propsFilePath = propsFilePath;\n    cfg.configs = configs;\n    jsc.getConf().set(\"spark.executor.memory\", sparkMemory);\n    return new HDFSParquetImporter(cfg).dataImport(jsc, retry);\n  }\n","realPath":"hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":179,"status":"M"},{"authorDate":"2020-05-14 19:15:49","commitOrder":4,"curCode":"  public HDFSParquetImporter.Config getHDFSParquetImporterConfig(String srcPath, String targetPath, String tableName,\n      String tableType, String rowKey, String partitionKey, int parallelism, String schemaFile) {\n    HDFSParquetImporter.Config cfg = new HDFSParquetImporter.Config();\n    cfg.srcPath = srcPath;\n    cfg.targetPath = targetPath;\n    cfg.tableName = tableName;\n    cfg.tableType = tableType;\n    cfg.rowKey = rowKey;\n    cfg.partitionKey = partitionKey;\n    cfg.parallelism = parallelism;\n    cfg.schemaFile = schemaFile;\n    return cfg;\n  }\n","date":"2020-05-14 19:15:49","endLine":376,"groupId":"10108","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"getHDFSParquetImporterConfig","params":"(StringsrcPath@StringtargetPath@StringtableName@StringtableType@StringrowKey@StringpartitionKey@intparallelism@StringschemaFile)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/a8/474f670aec9287d2fb774ab52b609cb92ec63d.src","preCode":"  private HDFSParquetImporter.Config getHDFSParquetImporterConfig(String srcPath, String targetPath, String tableName,\n      String tableType, String rowKey, String partitionKey, int parallelism, String schemaFile) {\n    HDFSParquetImporter.Config cfg = new HDFSParquetImporter.Config();\n    cfg.srcPath = srcPath;\n    cfg.targetPath = targetPath;\n    cfg.tableName = tableName;\n    cfg.tableType = tableType;\n    cfg.rowKey = rowKey;\n    cfg.partitionKey = partitionKey;\n    cfg.parallelism = parallelism;\n    cfg.schemaFile = schemaFile;\n    return cfg;\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":364,"status":"M"}],"commitId":"3a2fe13fcb7c168f8ff023e3bdb6ae482b400316","commitMessage":"@@@[HUDI-701] Add unit test for HDFSParquetImportCommand (#1574)\n\n","date":"2020-05-14 19:15:49","modifiedFileCount":"3","status":"M","submitter":"hongdd"}]
