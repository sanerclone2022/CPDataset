[{"authorTime":"2020-12-22 11:18:18","codes":[{"authorDate":"2020-10-02 05:25:29","commitOrder":2,"curCode":"  protected JavaRDD<WriteStatus> compact(String compactionInstantTime, boolean shouldComplete) {\n    HoodieSparkTable<T> table = HoodieSparkTable.create(config, context);\n    HoodieTimeline pendingCompactionTimeline = table.getActiveTimeline().filterPendingCompactionTimeline();\n    HoodieInstant inflightInstant = HoodieTimeline.getCompactionInflightInstant(compactionInstantTime);\n    if (pendingCompactionTimeline.containsInstant(inflightInstant)) {\n      rollbackInflightCompaction(inflightInstant, table);\n      table.getMetaClient().reloadActiveTimeline();\n    }\n    compactionTimer = metrics.getCompactionCtx();\n    HoodieWriteMetadata<JavaRDD<WriteStatus>> compactionMetadata = table.compact(context, compactionInstantTime);\n    JavaRDD<WriteStatus> statuses = compactionMetadata.getWriteStatuses();\n    if (shouldComplete && compactionMetadata.getCommitMetadata().isPresent()) {\n      completeCompaction(compactionMetadata.getCommitMetadata().get(), statuses, table, compactionInstantTime);\n    }\n    return statuses;\n  }\n","date":"2020-10-02 05:25:29","endLine":297,"groupId":"1307","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"compact","params":"(StringcompactionInstantTime@booleanshouldComplete)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/56/f06898abba2ed1f4e291f65b581c9426fcb1b3.src","preCode":"  protected JavaRDD<WriteStatus> compact(String compactionInstantTime, boolean shouldComplete) {\n    HoodieSparkTable<T> table = HoodieSparkTable.create(config, context);\n    HoodieTimeline pendingCompactionTimeline = table.getActiveTimeline().filterPendingCompactionTimeline();\n    HoodieInstant inflightInstant = HoodieTimeline.getCompactionInflightInstant(compactionInstantTime);\n    if (pendingCompactionTimeline.containsInstant(inflightInstant)) {\n      rollbackInflightCompaction(inflightInstant, table);\n      table.getMetaClient().reloadActiveTimeline();\n    }\n    compactionTimer = metrics.getCompactionCtx();\n    HoodieWriteMetadata<JavaRDD<WriteStatus>> compactionMetadata = table.compact(context, compactionInstantTime);\n    JavaRDD<WriteStatus> statuses = compactionMetadata.getWriteStatuses();\n    if (shouldComplete && compactionMetadata.getCommitMetadata().isPresent()) {\n      completeCompaction(compactionMetadata.getCommitMetadata().get(), statuses, table, compactionInstantTime);\n    }\n    return statuses;\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":282,"status":"NB"},{"authorDate":"2020-12-22 11:18:18","commitOrder":2,"curCode":"  public HoodieWriteMetadata<JavaRDD<WriteStatus>> cluster(String clusteringInstant, boolean shouldComplete) {\n    HoodieSparkTable<T> table = HoodieSparkTable.create(config, context);\n    HoodieTimeline pendingClusteringTimeline = table.getActiveTimeline().filterPendingReplaceTimeline();\n    HoodieInstant inflightInstant = HoodieTimeline.getReplaceCommitInflightInstant(clusteringInstant);\n    if (pendingClusteringTimeline.containsInstant(inflightInstant)) {\n      rollbackInflightClustering(inflightInstant, table);\n      table.getMetaClient().reloadActiveTimeline();\n    }\n    clusteringTimer = metrics.getClusteringCtx();\n    LOG.info(\"Starting clustering at \" + clusteringInstant);\n    HoodieWriteMetadata<JavaRDD<WriteStatus>> clusteringMetadata = table.cluster(context, clusteringInstant);\n    JavaRDD<WriteStatus> statuses = clusteringMetadata.getWriteStatuses();\n    if (shouldComplete && clusteringMetadata.getCommitMetadata().isPresent()) {\n      completeClustering((HoodieReplaceCommitMetadata) clusteringMetadata.getCommitMetadata().get(), statuses, table, clusteringInstant);\n    }\n    return clusteringMetadata;\n  }\n","date":"2020-12-22 11:18:18","endLine":336,"groupId":"5717","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"cluster","params":"(StringclusteringInstant@booleanshouldComplete)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/18/f5309eee0b1adfb6a48612169483449f76ea68.src","preCode":"  public HoodieWriteMetadata<JavaRDD<WriteStatus>> cluster(String clusteringInstant, boolean shouldComplete) {\n    HoodieSparkTable<T> table = HoodieSparkTable.create(config, context);\n    HoodieTimeline pendingClusteringTimeline = table.getActiveTimeline().filterPendingReplaceTimeline();\n    HoodieInstant inflightInstant = HoodieTimeline.getReplaceCommitInflightInstant(clusteringInstant);\n    if (pendingClusteringTimeline.containsInstant(inflightInstant)) {\n      rollbackInflightClustering(inflightInstant, table);\n      table.getMetaClient().reloadActiveTimeline();\n    }\n    clusteringTimer = metrics.getClusteringCtx();\n    LOG.info(\"Starting clustering at \" + clusteringInstant);\n    HoodieWriteMetadata<JavaRDD<WriteStatus>> clusteringMetadata = table.cluster(context, clusteringInstant);\n    JavaRDD<WriteStatus> statuses = clusteringMetadata.getWriteStatuses();\n    if (shouldComplete && clusteringMetadata.getCommitMetadata().isPresent()) {\n      completeClustering((HoodieReplaceCommitMetadata) clusteringMetadata.getCommitMetadata().get(), statuses, table, clusteringInstant);\n    }\n    return clusteringMetadata;\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":320,"status":"B"}],"commitId":"959afb8ba4cadf0fe09ddd6d67f9bf38bea13050","commitMessage":"@@@Merge pull request #2263 from satishkotha/sk/clustering\n\n[HUDI-1075] Implement simple clustering strategies to create and run ClusteringPlan","date":"2020-12-22 11:18:18","modifiedFileCount":"28","status":"M","submitter":"satishkotha"},{"authorTime":"2021-03-17 07:43:53","codes":[{"authorDate":"2021-03-17 07:43:53","commitOrder":3,"curCode":"  protected JavaRDD<WriteStatus> compact(String compactionInstantTime, boolean shouldComplete) {\n    HoodieSparkTable<T> table = HoodieSparkTable.create(config, context);\n    preWrite(compactionInstantTime, WriteOperationType.COMPACT, table.getMetaClient());\n    HoodieTimeline pendingCompactionTimeline = table.getActiveTimeline().filterPendingCompactionTimeline();\n    HoodieInstant inflightInstant = HoodieTimeline.getCompactionInflightInstant(compactionInstantTime);\n    if (pendingCompactionTimeline.containsInstant(inflightInstant)) {\n      rollbackInflightCompaction(inflightInstant, table);\n      table.getMetaClient().reloadActiveTimeline();\n    }\n    compactionTimer = metrics.getCompactionCtx();\n    HoodieWriteMetadata<JavaRDD<WriteStatus>> compactionMetadata = table.compact(context, compactionInstantTime);\n    JavaRDD<WriteStatus> statuses = compactionMetadata.getWriteStatuses();\n    if (shouldComplete && compactionMetadata.getCommitMetadata().isPresent()) {\n      completeTableService(TableServiceType.COMPACT, compactionMetadata.getCommitMetadata().get(), statuses, table, compactionInstantTime);\n    }\n    return statuses;\n  }\n","date":"2021-03-17 07:43:53","endLine":332,"groupId":"10752","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"compact","params":"(StringcompactionInstantTime@booleanshouldComplete)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/8a/0eee59a5486669cfd8a2d6d79abb6f149c2c19.src","preCode":"  protected JavaRDD<WriteStatus> compact(String compactionInstantTime, boolean shouldComplete) {\n    HoodieSparkTable<T> table = HoodieSparkTable.create(config, context);\n    HoodieTimeline pendingCompactionTimeline = table.getActiveTimeline().filterPendingCompactionTimeline();\n    HoodieInstant inflightInstant = HoodieTimeline.getCompactionInflightInstant(compactionInstantTime);\n    if (pendingCompactionTimeline.containsInstant(inflightInstant)) {\n      rollbackInflightCompaction(inflightInstant, table);\n      table.getMetaClient().reloadActiveTimeline();\n    }\n    compactionTimer = metrics.getCompactionCtx();\n    HoodieWriteMetadata<JavaRDD<WriteStatus>> compactionMetadata = table.compact(context, compactionInstantTime);\n    JavaRDD<WriteStatus> statuses = compactionMetadata.getWriteStatuses();\n    if (shouldComplete && compactionMetadata.getCommitMetadata().isPresent()) {\n      completeCompaction(compactionMetadata.getCommitMetadata().get(), statuses, table, compactionInstantTime);\n    }\n    return statuses;\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":316,"status":"M"},{"authorDate":"2021-03-17 07:43:53","commitOrder":3,"curCode":"  public HoodieWriteMetadata<JavaRDD<WriteStatus>> cluster(String clusteringInstant, boolean shouldComplete) {\n    HoodieSparkTable<T> table = HoodieSparkTable.create(config, context);\n    preWrite(clusteringInstant, WriteOperationType.CLUSTER, table.getMetaClient());\n    HoodieTimeline pendingClusteringTimeline = table.getActiveTimeline().filterPendingReplaceTimeline();\n    HoodieInstant inflightInstant = HoodieTimeline.getReplaceCommitInflightInstant(clusteringInstant);\n    if (pendingClusteringTimeline.containsInstant(inflightInstant)) {\n      rollbackInflightClustering(inflightInstant, table);\n      table.getMetaClient().reloadActiveTimeline();\n    }\n    clusteringTimer = metrics.getClusteringCtx();\n    LOG.info(\"Starting clustering at \" + clusteringInstant);\n    HoodieWriteMetadata<JavaRDD<WriteStatus>> clusteringMetadata = table.cluster(context, clusteringInstant);\n    JavaRDD<WriteStatus> statuses = clusteringMetadata.getWriteStatuses();\n    \r\n    if (shouldComplete && clusteringMetadata.getCommitMetadata().isPresent()) {\n      completeTableService(TableServiceType.CLUSTER, clusteringMetadata.getCommitMetadata().get(), statuses, table, clusteringInstant);\n    }\n    return clusteringMetadata;\n  }\n","date":"2021-03-17 07:43:53","endLine":353,"groupId":"10752","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"cluster","params":"(StringclusteringInstant@booleanshouldComplete)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/8a/0eee59a5486669cfd8a2d6d79abb6f149c2c19.src","preCode":"  public HoodieWriteMetadata<JavaRDD<WriteStatus>> cluster(String clusteringInstant, boolean shouldComplete) {\n    HoodieSparkTable<T> table = HoodieSparkTable.create(config, context);\n    HoodieTimeline pendingClusteringTimeline = table.getActiveTimeline().filterPendingReplaceTimeline();\n    HoodieInstant inflightInstant = HoodieTimeline.getReplaceCommitInflightInstant(clusteringInstant);\n    if (pendingClusteringTimeline.containsInstant(inflightInstant)) {\n      rollbackInflightClustering(inflightInstant, table);\n      table.getMetaClient().reloadActiveTimeline();\n    }\n    clusteringTimer = metrics.getClusteringCtx();\n    LOG.info(\"Starting clustering at \" + clusteringInstant);\n    HoodieWriteMetadata<JavaRDD<WriteStatus>> clusteringMetadata = table.cluster(context, clusteringInstant);\n    JavaRDD<WriteStatus> statuses = clusteringMetadata.getWriteStatuses();\n    if (shouldComplete && clusteringMetadata.getCommitMetadata().isPresent()) {\n      completeClustering((HoodieReplaceCommitMetadata) clusteringMetadata.getCommitMetadata().get(), statuses, table, clusteringInstant);\n    }\n    return clusteringMetadata;\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":335,"status":"M"}],"commitId":"74241947c123c860a1b0344f25cef316440a70d6","commitMessage":"@@@[HUDI-845] Added locking capability to allow multiple writers (#2374)\n\n* [HUDI-845] Added locking capability to allow multiple writers\n1. Added LockProvider API for pluggable lock methodologies\n2. Added Resolution Strategy API to allow for pluggable conflict resolution\n3. Added TableService client API to schedule table services\n4. Added Transaction Manager for wrapping actions within transactions","date":"2021-03-17 07:43:53","modifiedFileCount":"48","status":"M","submitter":"n3nash"}]
