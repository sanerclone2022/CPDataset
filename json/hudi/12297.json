[{"authorTime":"2019-08-12 08:48:17","codes":[{"authorDate":"2019-08-12 08:48:17","commitOrder":1,"curCode":"  public HoodieCleaner(Config cfg, JavaSparkContext jssc) throws IOException {\n    this.cfg = cfg;\n    this.jssc = jssc;\n    this.fs = FSUtils.getFs(cfg.basePath, jssc.hadoopConfiguration());\n    this.props = cfg.propsFilePath == null ? UtilHelpers.buildProperties(cfg.configs) :\n        UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getConfig();\n    log.info(\"Creating Cleaner with configs : \" + props.toString());\n  }\n","date":"2019-08-12 08:48:17","endLine":68,"groupId":"328","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"HoodieCleaner","params":"(Configcfg@JavaSparkContextjssc)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ae/e1ca436524376bb41696d01de3aab4fcdf4900.src","preCode":"  public HoodieCleaner(Config cfg, JavaSparkContext jssc) throws IOException {\n    this.cfg = cfg;\n    this.jssc = jssc;\n    this.fs = FSUtils.getFs(cfg.basePath, jssc.hadoopConfiguration());\n    this.props = cfg.propsFilePath == null ? UtilHelpers.buildProperties(cfg.configs) :\n        UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getConfig();\n    log.info(\"Creating Cleaner with configs : \" + props.toString());\n  }\n","realPath":"hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCleaner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":61,"status":"B"},{"authorDate":"2019-08-12 08:48:17","commitOrder":1,"curCode":"  public int dataImport(JavaSparkContext jsc, int retry) throws Exception {\n    this.fs = FSUtils.getFs(cfg.targetPath, jsc.hadoopConfiguration());\n    int ret = -1;\n    try {\n      \r\n      if (fs.exists(new Path(cfg.targetPath))) {\n        throw new HoodieIOException(String.format(\"Make sure %s is not present.\", cfg.targetPath));\n      }\n      do {\n        ret = dataImport(jsc);\n      } while (ret != 0 && retry-- > 0);\n    } catch (Throwable t) {\n      logger.error(t);\n    }\n    return ret;\n  }\n","date":"2019-08-12 08:48:17","endLine":116,"groupId":"328","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"dataImport","params":"(JavaSparkContextjsc@intretry)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/6a/f7d3165dde6cb425d8dd20f8e43a7818c8a0aa.src","preCode":"  public int dataImport(JavaSparkContext jsc, int retry) throws Exception {\n    this.fs = FSUtils.getFs(cfg.targetPath, jsc.hadoopConfiguration());\n    int ret = -1;\n    try {\n      \r\n      if (fs.exists(new Path(cfg.targetPath))) {\n        throw new HoodieIOException(String.format(\"Make sure %s is not present.\", cfg.targetPath));\n      }\n      do {\n        ret = dataImport(jsc);\n      } while (ret != 0 && retry-- > 0);\n    } catch (Throwable t) {\n      logger.error(t);\n    }\n    return ret;\n  }\n","realPath":"hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":101,"status":"B"}],"commitId":"a4f9d7575f39bb79089714049ffea12ba5f25ec8","commitMessage":"@@@HUDI-123 Rename code packages/constants to org.apache.hudi (#830)\n\n- Rename com.uber.hoodie to org.apache.hudi\n- Flag to pass com.uber.hoodie Input formats for hoodie-sync\n- Works with HUDI demo. \n- Also tested for backwards compatibility with datasets built by com.uber.hoodie packages\n- Migration guide : https://cwiki.apache.org/confluence/display/HUDI/Migration+Guide+From+com.uber.hoodie+to+org.apache.hudi","date":"2019-08-12 08:48:17","modifiedFileCount":"0","status":"B","submitter":"Balaji Varadarajan"},{"authorTime":"2019-11-03 14:12:44","codes":[{"authorDate":"2019-08-12 08:48:17","commitOrder":2,"curCode":"  public HoodieCleaner(Config cfg, JavaSparkContext jssc) throws IOException {\n    this.cfg = cfg;\n    this.jssc = jssc;\n    this.fs = FSUtils.getFs(cfg.basePath, jssc.hadoopConfiguration());\n    this.props = cfg.propsFilePath == null ? UtilHelpers.buildProperties(cfg.configs) :\n        UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getConfig();\n    log.info(\"Creating Cleaner with configs : \" + props.toString());\n  }\n","date":"2019-08-12 08:48:17","endLine":68,"groupId":"328","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"HoodieCleaner","params":"(Configcfg@JavaSparkContextjssc)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ae/e1ca436524376bb41696d01de3aab4fcdf4900.src","preCode":"  public HoodieCleaner(Config cfg, JavaSparkContext jssc) throws IOException {\n    this.cfg = cfg;\n    this.jssc = jssc;\n    this.fs = FSUtils.getFs(cfg.basePath, jssc.hadoopConfiguration());\n    this.props = cfg.propsFilePath == null ? UtilHelpers.buildProperties(cfg.configs) :\n        UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getConfig();\n    log.info(\"Creating Cleaner with configs : \" + props.toString());\n  }\n","realPath":"hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCleaner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":61,"status":"N"},{"authorDate":"2019-11-03 14:12:44","commitOrder":2,"curCode":"  public int dataImport(JavaSparkContext jsc, int retry) {\n    this.fs = FSUtils.getFs(cfg.targetPath, jsc.hadoopConfiguration());\n    this.props = cfg.propsFilePath == null ? UtilHelpers.buildProperties(cfg.configs)\n        : UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getConfig();\n    log.info(\"Starting data import with configs : \" + props.toString());\n    int ret = -1;\n    try {\n      \r\n      if (fs.exists(new Path(cfg.targetPath))) {\n        throw new HoodieIOException(String.format(\"Make sure %s is not present.\", cfg.targetPath));\n      }\n      do {\n        ret = dataImport(jsc);\n      } while (ret != 0 && retry-- > 0);\n    } catch (Throwable t) {\n      log.error(t);\n    }\n    return ret;\n  }\n","date":"2019-11-03 14:12:44","endLine":117,"groupId":"328","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"dataImport","params":"(JavaSparkContextjsc@intretry)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/34/552bb0b31a43566bb84b481fdd932727572642.src","preCode":"  public int dataImport(JavaSparkContext jsc, int retry) throws Exception {\n    this.fs = FSUtils.getFs(cfg.targetPath, jsc.hadoopConfiguration());\n    int ret = -1;\n    try {\n      \r\n      if (fs.exists(new Path(cfg.targetPath))) {\n        throw new HoodieIOException(String.format(\"Make sure %s is not present.\", cfg.targetPath));\n      }\n      do {\n        ret = dataImport(jsc);\n      } while (ret != 0 && retry-- > 0);\n    } catch (Throwable t) {\n      logger.error(t);\n    }\n    return ret;\n  }\n","realPath":"hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":99,"status":"M"}],"commitId":"91740635b2cf02ef9e34a603dea31bd246f5b436","commitMessage":"@@@[HUDI-321] Support bulkinsert in HDFSParquetImporter (#987)\n\n- Add bulk insert feature\n- Fix some minor issues","date":"2019-11-03 14:12:44","modifiedFileCount":"2","status":"M","submitter":"Raymond Xu"},{"authorTime":"2019-12-10 19:23:38","codes":[{"authorDate":"2019-12-10 19:23:38","commitOrder":3,"curCode":"  public HoodieCleaner(Config cfg, JavaSparkContext jssc) throws IOException {\n    this.cfg = cfg;\n    this.jssc = jssc;\n    this.fs = FSUtils.getFs(cfg.basePath, jssc.hadoopConfiguration());\n    this.props = cfg.propsFilePath == null ? UtilHelpers.buildProperties(cfg.configs)\n        : UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getConfig();\n    LOG.info(\"Creating Cleaner with configs : \" + props.toString());\n  }\n","date":"2019-12-10 19:23:38","endLine":70,"groupId":"328","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"HoodieCleaner","params":"(Configcfg@JavaSparkContextjssc)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/91/85d9730244c0c8b3719e003a4fde6067b64e5f.src","preCode":"  public HoodieCleaner(Config cfg, JavaSparkContext jssc) throws IOException {\n    this.cfg = cfg;\n    this.jssc = jssc;\n    this.fs = FSUtils.getFs(cfg.basePath, jssc.hadoopConfiguration());\n    this.props = cfg.propsFilePath == null ? UtilHelpers.buildProperties(cfg.configs)\n        : UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getConfig();\n    log.info(\"Creating Cleaner with configs : \" + props.toString());\n  }\n","realPath":"hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCleaner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":63,"status":"M"},{"authorDate":"2019-12-10 19:23:38","commitOrder":3,"curCode":"  public int dataImport(JavaSparkContext jsc, int retry) {\n    this.fs = FSUtils.getFs(cfg.targetPath, jsc.hadoopConfiguration());\n    this.props = cfg.propsFilePath == null ? UtilHelpers.buildProperties(cfg.configs)\n        : UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getConfig();\n    LOG.info(\"Starting data import with configs : \" + props.toString());\n    int ret = -1;\n    try {\n      \r\n      if (fs.exists(new Path(cfg.targetPath))) {\n        throw new HoodieIOException(String.format(\"Make sure %s is not present.\", cfg.targetPath));\n      }\n      do {\n        ret = dataImport(jsc);\n      } while (ret != 0 && retry-- > 0);\n    } catch (Throwable t) {\n      LOG.error(t);\n    }\n    return ret;\n  }\n","date":"2019-12-10 19:23:38","endLine":120,"groupId":"328","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"dataImport","params":"(JavaSparkContextjsc@intretry)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/4a/a72d0ab9ae619d27d5bbad0af7ba06ee22ca1d.src","preCode":"  public int dataImport(JavaSparkContext jsc, int retry) {\n    this.fs = FSUtils.getFs(cfg.targetPath, jsc.hadoopConfiguration());\n    this.props = cfg.propsFilePath == null ? UtilHelpers.buildProperties(cfg.configs)\n        : UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getConfig();\n    log.info(\"Starting data import with configs : \" + props.toString());\n    int ret = -1;\n    try {\n      \r\n      if (fs.exists(new Path(cfg.targetPath))) {\n        throw new HoodieIOException(String.format(\"Make sure %s is not present.\", cfg.targetPath));\n      }\n      do {\n        ret = dataImport(jsc);\n      } while (ret != 0 && retry-- > 0);\n    } catch (Throwable t) {\n      log.error(t);\n    }\n    return ret;\n  }\n","realPath":"hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":102,"status":"M"}],"commitId":"d447e2d7518fc6ee87f1620542e45f1454a5ba0f","commitMessage":"@@@[checkstyle] Unify LOG form (#1092)\n\n","date":"2019-12-10 19:23:38","modifiedFileCount":"100","status":"M","submitter":"lamber-ken"},{"authorTime":"2019-12-10 19:23:38","codes":[{"authorDate":"2020-02-02 18:03:44","commitOrder":4,"curCode":"  public HoodieCleaner(Config cfg, JavaSparkContext jssc) {\n    this.cfg = cfg;\n    this.jssc = jssc;\n    \r\n\r\n\n    FileSystem fs = FSUtils.getFs(cfg.basePath, jssc.hadoopConfiguration());\n    this.props = cfg.propsFilePath == null ? UtilHelpers.buildProperties(cfg.configs)\n        : UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getConfig();\n    LOG.info(\"Creating Cleaner with configs : \" + props.toString());\n  }\n","date":"2020-02-02 18:03:44","endLine":67,"groupId":"328","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"HoodieCleaner","params":"(Configcfg@JavaSparkContextjssc)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/11/f44e1438ac8174c50109988a4c203387db0d24.src","preCode":"  public HoodieCleaner(Config cfg, JavaSparkContext jssc) throws IOException {\n    this.cfg = cfg;\n    this.jssc = jssc;\n    this.fs = FSUtils.getFs(cfg.basePath, jssc.hadoopConfiguration());\n    this.props = cfg.propsFilePath == null ? UtilHelpers.buildProperties(cfg.configs)\n        : UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getConfig();\n    LOG.info(\"Creating Cleaner with configs : \" + props.toString());\n  }\n","realPath":"hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCleaner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":57,"status":"M"},{"authorDate":"2019-12-10 19:23:38","commitOrder":4,"curCode":"  public int dataImport(JavaSparkContext jsc, int retry) {\n    this.fs = FSUtils.getFs(cfg.targetPath, jsc.hadoopConfiguration());\n    this.props = cfg.propsFilePath == null ? UtilHelpers.buildProperties(cfg.configs)\n        : UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getConfig();\n    LOG.info(\"Starting data import with configs : \" + props.toString());\n    int ret = -1;\n    try {\n      \r\n      if (fs.exists(new Path(cfg.targetPath))) {\n        throw new HoodieIOException(String.format(\"Make sure %s is not present.\", cfg.targetPath));\n      }\n      do {\n        ret = dataImport(jsc);\n      } while (ret != 0 && retry-- > 0);\n    } catch (Throwable t) {\n      LOG.error(t);\n    }\n    return ret;\n  }\n","date":"2019-12-10 19:23:38","endLine":120,"groupId":"328","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"dataImport","params":"(JavaSparkContextjsc@intretry)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/4a/a72d0ab9ae619d27d5bbad0af7ba06ee22ca1d.src","preCode":"  public int dataImport(JavaSparkContext jsc, int retry) {\n    this.fs = FSUtils.getFs(cfg.targetPath, jsc.hadoopConfiguration());\n    this.props = cfg.propsFilePath == null ? UtilHelpers.buildProperties(cfg.configs)\n        : UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getConfig();\n    LOG.info(\"Starting data import with configs : \" + props.toString());\n    int ret = -1;\n    try {\n      \r\n      if (fs.exists(new Path(cfg.targetPath))) {\n        throw new HoodieIOException(String.format(\"Make sure %s is not present.\", cfg.targetPath));\n      }\n      do {\n        ret = dataImport(jsc);\n      } while (ret != 0 && retry-- > 0);\n    } catch (Throwable t) {\n      LOG.error(t);\n    }\n    return ret;\n  }\n","realPath":"hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":102,"status":"N"}],"commitId":"5b7bb142dc6712c41fd8ada208ab3186369431f9","commitMessage":"@@@[HUDI-583] Code Cleanup.  remove redundant code.  and other changes (#1237)\n\n","date":"2020-02-02 18:03:44","modifiedFileCount":"65","status":"M","submitter":"Suneel Marthi"},{"authorTime":"2020-04-21 14:21:30","codes":[{"authorDate":"2020-02-02 18:03:44","commitOrder":5,"curCode":"  public HoodieCleaner(Config cfg, JavaSparkContext jssc) {\n    this.cfg = cfg;\n    this.jssc = jssc;\n    \r\n\r\n\n    FileSystem fs = FSUtils.getFs(cfg.basePath, jssc.hadoopConfiguration());\n    this.props = cfg.propsFilePath == null ? UtilHelpers.buildProperties(cfg.configs)\n        : UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getConfig();\n    LOG.info(\"Creating Cleaner with configs : \" + props.toString());\n  }\n","date":"2020-02-02 18:03:44","endLine":67,"groupId":"12297","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"HoodieCleaner","params":"(Configcfg@JavaSparkContextjssc)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/11/f44e1438ac8174c50109988a4c203387db0d24.src","preCode":"  public HoodieCleaner(Config cfg, JavaSparkContext jssc) {\n    this.cfg = cfg;\n    this.jssc = jssc;\n    \r\n\r\n\n    FileSystem fs = FSUtils.getFs(cfg.basePath, jssc.hadoopConfiguration());\n    this.props = cfg.propsFilePath == null ? UtilHelpers.buildProperties(cfg.configs)\n        : UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getConfig();\n    LOG.info(\"Creating Cleaner with configs : \" + props.toString());\n  }\n","realPath":"hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCleaner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":57,"status":"N"},{"authorDate":"2020-04-21 14:21:30","commitOrder":5,"curCode":"  public int dataImport(JavaSparkContext jsc, int retry) {\n    this.fs = FSUtils.getFs(cfg.targetPath, jsc.hadoopConfiguration());\n    this.props = cfg.propsFilePath == null ? UtilHelpers.buildProperties(cfg.configs)\n        : UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getConfig();\n    LOG.info(\"Starting data import with configs : \" + props.toString());\n    int ret = -1;\n    try {\n      \r\n      if (fs.exists(new Path(cfg.targetPath)) && !isUpsert()) {\n        throw new HoodieIOException(String.format(\"Make sure %s is not present.\", cfg.targetPath));\n      }\n      do {\n        ret = dataImport(jsc);\n      } while (ret != 0 && retry-- > 0);\n    } catch (Throwable t) {\n      LOG.error(t);\n    }\n    return ret;\n  }\n","date":"2020-04-21 14:21:30","endLine":125,"groupId":"12297","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"dataImport","params":"(JavaSparkContextjsc@intretry)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/4b/efaec88d3f477797d6b4232fa8e8736664c970.src","preCode":"  public int dataImport(JavaSparkContext jsc, int retry) {\n    this.fs = FSUtils.getFs(cfg.targetPath, jsc.hadoopConfiguration());\n    this.props = cfg.propsFilePath == null ? UtilHelpers.buildProperties(cfg.configs)\n        : UtilHelpers.readConfig(fs, new Path(cfg.propsFilePath), cfg.configs).getConfig();\n    LOG.info(\"Starting data import with configs : \" + props.toString());\n    int ret = -1;\n    try {\n      \r\n      if (fs.exists(new Path(cfg.targetPath))) {\n        throw new HoodieIOException(String.format(\"Make sure %s is not present.\", cfg.targetPath));\n      }\n      do {\n        ret = dataImport(jsc);\n      } while (ret != 0 && retry-- > 0);\n    } catch (Throwable t) {\n      LOG.error(t);\n    }\n    return ret;\n  }\n","realPath":"hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":107,"status":"M"}],"commitId":"84dd9047d3902650d7ff5bc95b9789d6880ca8e2","commitMessage":"@@@[HUDI-789]Adjust logic of upsert in HDFSParquetImporter (#1511)\n\n","date":"2020-04-21 14:21:30","modifiedFileCount":"2","status":"M","submitter":"hongdd"}]
