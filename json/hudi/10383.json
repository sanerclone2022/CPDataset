[{"authorTime":"2021-07-30 14:25:05","codes":[{"authorDate":"2021-07-30 14:25:05","commitOrder":3,"curCode":"  void testStreamWriteAndReadFromSpecifiedCommit(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    options.put(FlinkOptions.READ_AS_STREAMING.key(), \"true\");\n    options.put(FlinkOptions.TABLE_TYPE.key(), tableType.name());\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    String firstCommit = TestUtils.getFirstCommit(tempFile.getAbsolutePath());\n    options.put(FlinkOptions.READ_STREAMING_START_COMMIT.key(), firstCommit);\n    streamTableEnv.executeSql(\"drop table t1\");\n    hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    streamTableEnv.executeSql(hoodieTableDDL);\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT);\n  }\n","date":"2021-07-30 14:25:05","endLine":124,"groupId":"198","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testStreamWriteAndReadFromSpecifiedCommit","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/c1/813dca44526c524e82c835ffb425218706aca0.src","preCode":"  void testStreamWriteAndReadFromSpecifiedCommit(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    options.put(FlinkOptions.READ_AS_STREAMING.key(), \"true\");\n    options.put(FlinkOptions.TABLE_TYPE.key(), tableType.name());\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    String firstCommit = TestUtils.getFirstCommit(tempFile.getAbsolutePath());\n    options.put(FlinkOptions.READ_STREAMING_START_COMMIT.key(), firstCommit);\n    streamTableEnv.executeSql(\"drop table t1\");\n    hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    streamTableEnv.executeSql(hoodieTableDDL);\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":98,"status":"B"},{"authorDate":"2021-07-30 14:25:05","commitOrder":3,"curCode":"  void testStreamWriteAndRead(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    options.put(FlinkOptions.READ_AS_STREAMING.key(), \"true\");\n    options.put(FlinkOptions.TABLE_TYPE.key(), tableType.name());\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    \r\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n  }\n","date":"2021-07-30 14:25:05","endLine":150,"groupId":"3901","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testStreamWriteAndRead","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/c1/813dca44526c524e82c835ffb425218706aca0.src","preCode":"  void testStreamWriteAndRead(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    options.put(FlinkOptions.READ_AS_STREAMING.key(), \"true\");\n    options.put(FlinkOptions.TABLE_TYPE.key(), tableType.name());\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    \r\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":128,"status":"MB"}],"commitId":"8b19ec9ca07070a9819502e82091dd14d559ef94","commitMessage":"@@@[HUDI-2252] Default consumes from the latest instant for flink streaming reader (#3368)\n\n","date":"2021-07-30 14:25:05","modifiedFileCount":"4","status":"M","submitter":"swuferhong"},{"authorTime":"2021-08-19 23:21:20","codes":[{"authorDate":"2021-08-19 23:21:20","commitOrder":4,"curCode":"  void testStreamWriteAndReadFromSpecifiedCommit(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, \"true\")\n        .option(FlinkOptions.TABLE_TYPE, tableType.name())\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    String firstCommit = TestUtils.getFirstCommit(tempFile.getAbsolutePath());\n    streamTableEnv.executeSql(\"drop table t1\");\n    hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, \"true\")\n        .option(FlinkOptions.TABLE_TYPE, tableType.name())\n        .option(FlinkOptions.READ_STREAMING_START_COMMIT, firstCommit)\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT);\n  }\n","date":"2021-08-19 23:21:20","endLine":127,"groupId":"782","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testStreamWriteAndReadFromSpecifiedCommit","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9e/ffdcc8c67f59ad51cfd9a0e5ed56ecd41b530e.src","preCode":"  void testStreamWriteAndReadFromSpecifiedCommit(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    options.put(FlinkOptions.READ_AS_STREAMING.key(), \"true\");\n    options.put(FlinkOptions.TABLE_TYPE.key(), tableType.name());\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    String firstCommit = TestUtils.getFirstCommit(tempFile.getAbsolutePath());\n    options.put(FlinkOptions.READ_STREAMING_START_COMMIT.key(), firstCommit);\n    streamTableEnv.executeSql(\"drop table t1\");\n    hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    streamTableEnv.executeSql(hoodieTableDDL);\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":97,"status":"M"},{"authorDate":"2021-08-19 23:21:20","commitOrder":4,"curCode":"  void testStreamWriteAndRead(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, \"true\")\n        .option(FlinkOptions.TABLE_TYPE, tableType.name())\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    \r\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n  }\n","date":"2021-08-19 23:21:20","endLine":153,"groupId":"2947","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testStreamWriteAndRead","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9e/ffdcc8c67f59ad51cfd9a0e5ed56ecd41b530e.src","preCode":"  void testStreamWriteAndRead(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    options.put(FlinkOptions.READ_AS_STREAMING.key(), \"true\");\n    options.put(FlinkOptions.TABLE_TYPE.key(), tableType.name());\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    \r\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":131,"status":"M"}],"commitId":"9762e4c08c0ff953cc62e72b1295db4fd4c002c5","commitMessage":"@@@[MINOR] Some cosmetic changes for Flink (#3503)\n\n","date":"2021-08-19 23:21:20","modifiedFileCount":"6","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-08-19 23:21:20","codes":[{"authorDate":"2021-08-28 20:16:54","commitOrder":5,"curCode":"  void testStreamWriteAndReadFromSpecifiedCommit(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, \"true\")\n        .option(FlinkOptions.TABLE_TYPE, tableType.name())\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    String firstCommit = TestUtils.getFirstCommit(tempFile.getAbsolutePath());\n    streamTableEnv.executeSql(\"drop table t1\");\n    hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, \"true\")\n        .option(FlinkOptions.TABLE_TYPE, tableType.name())\n        .option(FlinkOptions.READ_STREAMING_START_COMMIT, firstCommit)\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT);\n\n    streamTableEnv.getConfig().getConfiguration()\n        .setBoolean(\"table.dynamic-table-options.enabled\", true);\n    \r\n    List<Row> rows3 = execSelectSql(streamTableEnv,\n        \"select * from t1/*+options('read.streaming.start-commit'='earliest')*/\", 10);\n    assertRowsEquals(rows3, TestData.DATA_SET_SOURCE_INSERT);\n  }\n","date":"2021-08-28 20:16:54","endLine":133,"groupId":"782","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testStreamWriteAndReadFromSpecifiedCommit","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/26/e0be6eeb7f27fe9f52c03056b855aaaea344f4.src","preCode":"  void testStreamWriteAndReadFromSpecifiedCommit(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, \"true\")\n        .option(FlinkOptions.TABLE_TYPE, tableType.name())\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    String firstCommit = TestUtils.getFirstCommit(tempFile.getAbsolutePath());\n    streamTableEnv.executeSql(\"drop table t1\");\n    hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, \"true\")\n        .option(FlinkOptions.TABLE_TYPE, tableType.name())\n        .option(FlinkOptions.READ_STREAMING_START_COMMIT, firstCommit)\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":96,"status":"M"},{"authorDate":"2021-08-19 23:21:20","commitOrder":5,"curCode":"  void testStreamWriteAndRead(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, \"true\")\n        .option(FlinkOptions.TABLE_TYPE, tableType.name())\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    \r\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n  }\n","date":"2021-08-19 23:21:20","endLine":153,"groupId":"2947","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testStreamWriteAndRead","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9e/ffdcc8c67f59ad51cfd9a0e5ed56ecd41b530e.src","preCode":"  void testStreamWriteAndRead(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, \"true\")\n        .option(FlinkOptions.TABLE_TYPE, tableType.name())\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    \r\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":131,"status":"N"}],"commitId":"57668d02a0aa723dd4b2245dc7659fe18113eb59","commitMessage":"@@@[HUDI-2371] Improvement flink streaming reader (#3552)\n\n- Support reading empty table\n- Fix filtering by partition path\n- Support reading from earliest commit","date":"2021-08-28 20:16:54","modifiedFileCount":"9","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-09-11 13:17:16","codes":[{"authorDate":"2021-09-11 13:17:16","commitOrder":6,"curCode":"  void testStreamWriteAndReadFromSpecifiedCommit(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, true)\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    String firstCommit = TestUtils.getFirstCommit(tempFile.getAbsolutePath());\n    streamTableEnv.executeSql(\"drop table t1\");\n    hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, true)\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .option(FlinkOptions.READ_STREAMING_START_COMMIT, firstCommit)\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT);\n\n    streamTableEnv.getConfig().getConfiguration()\n        .setBoolean(\"table.dynamic-table-options.enabled\", true);\n    \r\n    List<Row> rows3 = execSelectSql(streamTableEnv,\n        \"select * from t1/*+options('read.streaming.start-commit'='earliest')*/\", 10);\n    assertRowsEquals(rows3, TestData.DATA_SET_SOURCE_INSERT);\n  }\n","date":"2021-09-11 13:17:16","endLine":133,"groupId":"4103","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testStreamWriteAndReadFromSpecifiedCommit","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/5b/e603f7838e5195aa26e8cf17398d597ca9ee2f.src","preCode":"  void testStreamWriteAndReadFromSpecifiedCommit(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, \"true\")\n        .option(FlinkOptions.TABLE_TYPE, tableType.name())\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    String firstCommit = TestUtils.getFirstCommit(tempFile.getAbsolutePath());\n    streamTableEnv.executeSql(\"drop table t1\");\n    hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, \"true\")\n        .option(FlinkOptions.TABLE_TYPE, tableType.name())\n        .option(FlinkOptions.READ_STREAMING_START_COMMIT, firstCommit)\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT);\n\n    streamTableEnv.getConfig().getConfiguration()\n        .setBoolean(\"table.dynamic-table-options.enabled\", true);\n    \r\n    List<Row> rows3 = execSelectSql(streamTableEnv,\n        \"select * from t1/*+options('read.streaming.start-commit'='earliest')*/\", 10);\n    assertRowsEquals(rows3, TestData.DATA_SET_SOURCE_INSERT);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":96,"status":"M"},{"authorDate":"2021-09-11 13:17:16","commitOrder":6,"curCode":"  void testStreamWriteAndRead(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, true)\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    \r\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n  }\n","date":"2021-09-11 13:17:16","endLine":159,"groupId":"4105","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testStreamWriteAndRead","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/5b/e603f7838e5195aa26e8cf17398d597ca9ee2f.src","preCode":"  void testStreamWriteAndRead(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, \"true\")\n        .option(FlinkOptions.TABLE_TYPE, tableType.name())\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    \r\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":137,"status":"M"}],"commitId":"b30c5bdaef77aee9f564ac24f80f5c364014bb17","commitMessage":"@@@[HUDI-2412] Add timestamp based partitioning for flink writer (#3638)\n\n","date":"2021-09-11 13:17:16","modifiedFileCount":"11","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-09-11 13:17:16","codes":[{"authorDate":"2021-09-19 09:06:46","commitOrder":7,"curCode":"  void testStreamWriteAndReadFromSpecifiedCommit(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, true)\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    String firstCommit = TestUtils.getFirstCommit(tempFile.getAbsolutePath());\n    streamTableEnv.executeSql(\"drop table t1\");\n    hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, true)\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .option(FlinkOptions.READ_START_COMMIT, firstCommit)\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT);\n\n    streamTableEnv.getConfig().getConfiguration()\n        .setBoolean(\"table.dynamic-table-options.enabled\", true);\n    \r\n    List<Row> rows3 = execSelectSql(streamTableEnv,\n        \"select * from t1/*+options('read.streaming.start-commit'='earliest')*/\", 10);\n    assertRowsEquals(rows3, TestData.DATA_SET_SOURCE_INSERT);\n  }\n","date":"2021-09-19 09:06:46","endLine":133,"groupId":"4103","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testStreamWriteAndReadFromSpecifiedCommit","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/db/7111b1f795f5656ebb0c6f1b97ff4f9f0bc83c.src","preCode":"  void testStreamWriteAndReadFromSpecifiedCommit(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, true)\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    String firstCommit = TestUtils.getFirstCommit(tempFile.getAbsolutePath());\n    streamTableEnv.executeSql(\"drop table t1\");\n    hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, true)\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .option(FlinkOptions.READ_STREAMING_START_COMMIT, firstCommit)\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT);\n\n    streamTableEnv.getConfig().getConfiguration()\n        .setBoolean(\"table.dynamic-table-options.enabled\", true);\n    \r\n    List<Row> rows3 = execSelectSql(streamTableEnv,\n        \"select * from t1/*+options('read.streaming.start-commit'='earliest')*/\", 10);\n    assertRowsEquals(rows3, TestData.DATA_SET_SOURCE_INSERT);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":96,"status":"M"},{"authorDate":"2021-09-11 13:17:16","commitOrder":7,"curCode":"  void testStreamWriteAndRead(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, true)\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    \r\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n  }\n","date":"2021-09-11 13:17:16","endLine":159,"groupId":"4105","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testStreamWriteAndRead","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/5b/e603f7838e5195aa26e8cf17398d597ca9ee2f.src","preCode":"  void testStreamWriteAndRead(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, true)\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    \r\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":137,"status":"N"}],"commitId":"3354fac42f9a2c4dbc8ac73ca4749160e9b9459b","commitMessage":"@@@[HUDI-2449] Incremental read for Flink (#3686)\n\n","date":"2021-09-19 09:06:46","modifiedFileCount":"15","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-09-11 13:17:16","codes":[{"authorDate":"2021-09-24 23:44:01","commitOrder":8,"curCode":"  void testStreamWriteAndReadFromSpecifiedCommit(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, true)\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    String firstCommit = TestUtils.getFirstCommit(tempFile.getAbsolutePath());\n    streamTableEnv.executeSql(\"drop table t1\");\n    hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, true)\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .option(FlinkOptions.READ_START_COMMIT, firstCommit)\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT);\n\n    streamTableEnv.getConfig().getConfiguration()\n        .setBoolean(\"table.dynamic-table-options.enabled\", true);\n    \r\n    List<Row> rows3 = execSelectSql(streamTableEnv,\n        \"select * from t1/*+options('read.start-commit'='earliest')*/\", 10);\n    assertRowsEquals(rows3, TestData.DATA_SET_SOURCE_INSERT);\n  }\n","date":"2021-09-24 23:44:01","endLine":133,"groupId":"10383","id":11,"instanceNumber":1,"isCurCommit":1,"methodName":"testStreamWriteAndReadFromSpecifiedCommit","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/a5/812aa581481abb6ec7fb2db89cc1e200d7976c.src","preCode":"  void testStreamWriteAndReadFromSpecifiedCommit(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, true)\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    String firstCommit = TestUtils.getFirstCommit(tempFile.getAbsolutePath());\n    streamTableEnv.executeSql(\"drop table t1\");\n    hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, true)\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .option(FlinkOptions.READ_START_COMMIT, firstCommit)\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT);\n\n    streamTableEnv.getConfig().getConfiguration()\n        .setBoolean(\"table.dynamic-table-options.enabled\", true);\n    \r\n    List<Row> rows3 = execSelectSql(streamTableEnv,\n        \"select * from t1/*+options('read.streaming.start-commit'='earliest')*/\", 10);\n    assertRowsEquals(rows3, TestData.DATA_SET_SOURCE_INSERT);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":96,"status":"M"},{"authorDate":"2021-09-11 13:17:16","commitOrder":8,"curCode":"  void testStreamWriteAndRead(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, true)\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    \r\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n  }\n","date":"2021-09-11 13:17:16","endLine":159,"groupId":"10383","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testStreamWriteAndRead","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/5b/e603f7838e5195aa26e8cf17398d597ca9ee2f.src","preCode":"  void testStreamWriteAndRead(HoodieTableType tableType) throws Exception {\n    \r\n    String createSource = TestConfigurations.getFileSourceDDL(\"source\");\n    streamTableEnv.executeSql(createSource);\n\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.READ_AS_STREAMING, true)\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .end();\n    streamTableEnv.executeSql(hoodieTableDDL);\n    String insertInto = \"insert into t1 select * from source\";\n    execInsertSql(streamTableEnv, insertInto);\n\n    \r\n    List<Row> rows = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n\n    \r\n    execInsertSql(streamTableEnv, insertInto);\n    List<Row> rows2 = execSelectSql(streamTableEnv, \"select * from t1\", 10);\n    assertRowsEquals(rows2, TestData.DATA_SET_SOURCE_INSERT_LATEST_COMMIT);\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":137,"status":"N"}],"commitId":"31a301f0aa955450e52c28ee5857526034d523f0","commitMessage":"@@@[HUDI-2485] Consume as mini-batch for flink stream reader (#3710)\n\n","date":"2021-09-24 23:44:01","modifiedFileCount":"5","status":"M","submitter":"Danny Chan"}]
