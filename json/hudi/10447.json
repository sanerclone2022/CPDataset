[{"authorTime":"2021-08-28 20:16:54","codes":[{"authorDate":"2021-08-28 20:16:54","commitOrder":1,"curCode":"  private InputFormat<RowData, ?> getBatchInputFormat() {\n    \r\n    if (!partitionKeys.isEmpty() && getOrFetchPartitions().isEmpty()) {\n      return new CollectionInputFormat<>(Collections.emptyList(), null);\n    }\n\n    final Path[] paths = getReadPaths();\n    if (paths.length == 0) {\n      return new CollectionInputFormat<>(Collections.emptyList(), null);\n    }\n\n    final Schema tableAvroSchema = getTableAvroSchema();\n    final DataType rowDataType = AvroSchemaConverter.convertToDataType(tableAvroSchema);\n    final RowType rowType = (RowType) rowDataType.getLogicalType();\n    final RowType requiredRowType = (RowType) getProducedDataType().notNull().getLogicalType();\n\n    final String queryType = this.conf.getString(FlinkOptions.QUERY_TYPE);\n    switch (queryType) {\n      case FlinkOptions.QUERY_TYPE_SNAPSHOT:\n        final HoodieTableType tableType = HoodieTableType.valueOf(this.conf.getString(FlinkOptions.TABLE_TYPE));\n        switch (tableType) {\n          case MERGE_ON_READ:\n            final List<MergeOnReadInputSplit> inputSplits = buildFileIndex(paths);\n            if (inputSplits.size() == 0) {\n              \r\n              LOG.warn(\"No input splits generate for MERGE_ON_READ input format, returns empty collection instead\");\n              return new CollectionInputFormat<>(Collections.emptyList(), null);\n            }\n            final MergeOnReadTableState hoodieTableState = new MergeOnReadTableState(\n                rowType,\n                requiredRowType,\n                tableAvroSchema.toString(),\n                AvroSchemaConverter.convertToSchema(requiredRowType).toString(),\n                inputSplits,\n                conf.getString(FlinkOptions.RECORD_KEY_FIELD).split(\",\"));\n            return MergeOnReadInputFormat.builder()\n                .config(this.conf)\n                .paths(FilePathUtils.toFlinkPaths(paths))\n                .tableState(hoodieTableState)\n                \r\n                \r\n                .fieldTypes(rowDataType.getChildren())\n                .defaultPartName(conf.getString(FlinkOptions.PARTITION_DEFAULT_NAME))\n                .limit(this.limit)\n                .emitDelete(false)\n                .build();\n          case COPY_ON_WRITE:\n            FileInputFormat<RowData> format = new CopyOnWriteInputFormat(\n                FilePathUtils.toFlinkPaths(paths),\n                this.schema.getColumnNames().toArray(new String[0]),\n                this.schema.getColumnDataTypes().toArray(new DataType[0]),\n                this.requiredPos,\n                this.conf.getString(FlinkOptions.PARTITION_DEFAULT_NAME),\n                this.limit == NO_LIMIT_CONSTANT ? Long.MAX_VALUE : this.limit, \r\n                getParquetConf(this.conf, this.hadoopConf),\n                this.conf.getBoolean(FlinkOptions.UTC_TIMEZONE)\n            );\n            format.setFilesFilter(new LatestFileFilter(this.hadoopConf));\n            return format;\n          default:\n            throw new HoodieException(\"Unexpected table type: \" + this.conf.getString(FlinkOptions.TABLE_TYPE));\n        }\n      case FlinkOptions.QUERY_TYPE_READ_OPTIMIZED:\n        FileInputFormat<RowData> format = new CopyOnWriteInputFormat(\n            FilePathUtils.toFlinkPaths(paths),\n            this.schema.getColumnNames().toArray(new String[0]),\n            this.schema.getColumnDataTypes().toArray(new DataType[0]),\n            this.requiredPos,\n            \"default\",\n            this.limit == NO_LIMIT_CONSTANT ? Long.MAX_VALUE : this.limit, \r\n            getParquetConf(this.conf, this.hadoopConf),\n            this.conf.getBoolean(FlinkOptions.UTC_TIMEZONE)\n        );\n        format.setFilesFilter(new LatestFileFilter(this.hadoopConf));\n        return format;\n      default:\n        String errMsg = String.format(\"Invalid query type : '%s', options ['%s', '%s'] are supported now\", queryType,\n            FlinkOptions.QUERY_TYPE_SNAPSHOT, FlinkOptions.QUERY_TYPE_READ_OPTIMIZED);\n        throw new HoodieException(errMsg);\n    }\n  }\n","date":"2021-08-28 20:16:54","endLine":400,"groupId":"2423","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"getBatchInputFormat","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/fc/4239442db1684c43ee5912fce5614042efad8d.src","preCode":"  private InputFormat<RowData, ?> getBatchInputFormat() {\n    \r\n    if (!partitionKeys.isEmpty() && getOrFetchPartitions().isEmpty()) {\n      return new CollectionInputFormat<>(Collections.emptyList(), null);\n    }\n\n    final Path[] paths = getReadPaths();\n    if (paths.length == 0) {\n      return new CollectionInputFormat<>(Collections.emptyList(), null);\n    }\n\n    final Schema tableAvroSchema = getTableAvroSchema();\n    final DataType rowDataType = AvroSchemaConverter.convertToDataType(tableAvroSchema);\n    final RowType rowType = (RowType) rowDataType.getLogicalType();\n    final RowType requiredRowType = (RowType) getProducedDataType().notNull().getLogicalType();\n\n    final String queryType = this.conf.getString(FlinkOptions.QUERY_TYPE);\n    switch (queryType) {\n      case FlinkOptions.QUERY_TYPE_SNAPSHOT:\n        final HoodieTableType tableType = HoodieTableType.valueOf(this.conf.getString(FlinkOptions.TABLE_TYPE));\n        switch (tableType) {\n          case MERGE_ON_READ:\n            final List<MergeOnReadInputSplit> inputSplits = buildFileIndex(paths);\n            if (inputSplits.size() == 0) {\n              \r\n              LOG.warn(\"No input splits generate for MERGE_ON_READ input format, returns empty collection instead\");\n              return new CollectionInputFormat<>(Collections.emptyList(), null);\n            }\n            final MergeOnReadTableState hoodieTableState = new MergeOnReadTableState(\n                rowType,\n                requiredRowType,\n                tableAvroSchema.toString(),\n                AvroSchemaConverter.convertToSchema(requiredRowType).toString(),\n                inputSplits,\n                conf.getString(FlinkOptions.RECORD_KEY_FIELD).split(\",\"));\n            return MergeOnReadInputFormat.builder()\n                .config(this.conf)\n                .paths(FilePathUtils.toFlinkPaths(paths))\n                .tableState(hoodieTableState)\n                \r\n                \r\n                .fieldTypes(rowDataType.getChildren())\n                .defaultPartName(conf.getString(FlinkOptions.PARTITION_DEFAULT_NAME))\n                .limit(this.limit)\n                .emitDelete(false)\n                .build();\n          case COPY_ON_WRITE:\n            FileInputFormat<RowData> format = new CopyOnWriteInputFormat(\n                FilePathUtils.toFlinkPaths(paths),\n                this.schema.getColumnNames().toArray(new String[0]),\n                this.schema.getColumnDataTypes().toArray(new DataType[0]),\n                this.requiredPos,\n                this.conf.getString(FlinkOptions.PARTITION_DEFAULT_NAME),\n                this.limit == NO_LIMIT_CONSTANT ? Long.MAX_VALUE : this.limit, \r\n                getParquetConf(this.conf, this.hadoopConf),\n                this.conf.getBoolean(FlinkOptions.UTC_TIMEZONE)\n            );\n            format.setFilesFilter(new LatestFileFilter(this.hadoopConf));\n            return format;\n          default:\n            throw new HoodieException(\"Unexpected table type: \" + this.conf.getString(FlinkOptions.TABLE_TYPE));\n        }\n      case FlinkOptions.QUERY_TYPE_READ_OPTIMIZED:\n        FileInputFormat<RowData> format = new CopyOnWriteInputFormat(\n            FilePathUtils.toFlinkPaths(paths),\n            this.schema.getColumnNames().toArray(new String[0]),\n            this.schema.getColumnDataTypes().toArray(new DataType[0]),\n            this.requiredPos,\n            \"default\",\n            this.limit == NO_LIMIT_CONSTANT ? Long.MAX_VALUE : this.limit, \r\n            getParquetConf(this.conf, this.hadoopConf),\n            this.conf.getBoolean(FlinkOptions.UTC_TIMEZONE)\n        );\n        format.setFilesFilter(new LatestFileFilter(this.hadoopConf));\n        return format;\n      default:\n        String errMsg = String.format(\"Invalid query type : '%s', options ['%s', '%s'] are supported now\", queryType,\n            FlinkOptions.QUERY_TYPE_SNAPSHOT, FlinkOptions.QUERY_TYPE_READ_OPTIMIZED);\n        throw new HoodieException(errMsg);\n    }\n  }\n","realPath":"hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSource.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":320,"status":"B"},{"authorDate":"2021-08-28 20:16:54","commitOrder":1,"curCode":"  private InputFormat<RowData, ?> getStreamInputFormat() {\n    \r\n    Schema tableAvroSchema = this.metaClient == null ? inferSchemaFromDdl() : getTableAvroSchema();\n    final DataType rowDataType = AvroSchemaConverter.convertToDataType(tableAvroSchema);\n    final RowType rowType = (RowType) rowDataType.getLogicalType();\n    final RowType requiredRowType = (RowType) getProducedDataType().notNull().getLogicalType();\n\n    final String queryType = this.conf.getString(FlinkOptions.QUERY_TYPE);\n    org.apache.flink.core.fs.Path[] paths = new org.apache.flink.core.fs.Path[0];\n    if (FlinkOptions.QUERY_TYPE_SNAPSHOT.equals(queryType)) {\n      final HoodieTableType tableType = HoodieTableType.valueOf(this.conf.getString(FlinkOptions.TABLE_TYPE));\n      switch (tableType) {\n        case MERGE_ON_READ:\n          final MergeOnReadTableState hoodieTableState = new MergeOnReadTableState(\n              rowType,\n              requiredRowType,\n              tableAvroSchema.toString(),\n              AvroSchemaConverter.convertToSchema(requiredRowType).toString(),\n              Collections.emptyList(),\n              conf.getString(FlinkOptions.RECORD_KEY_FIELD).split(\",\"));\n          return MergeOnReadInputFormat.builder()\n              .config(this.conf)\n              .paths(paths)\n              .tableState(hoodieTableState)\n              \r\n              \r\n              .fieldTypes(rowDataType.getChildren())\n              .defaultPartName(conf.getString(FlinkOptions.PARTITION_DEFAULT_NAME))\n              .limit(this.limit)\n              .emitDelete(true)\n              .build();\n        case COPY_ON_WRITE:\n          final MergeOnReadTableState hoodieTableState2 = new MergeOnReadTableState(\n              rowType,\n              requiredRowType,\n              tableAvroSchema.toString(),\n              AvroSchemaConverter.convertToSchema(requiredRowType).toString(),\n              Collections.emptyList(),\n              conf.getString(FlinkOptions.RECORD_KEY_FIELD).split(\",\"));\n          return MergeOnReadInputFormat.builder()\n              .config(this.conf)\n              .paths(paths)\n              .tableState(hoodieTableState2)\n              \r\n              \r\n              .fieldTypes(rowDataType.getChildren())\n              .defaultPartName(conf.getString(FlinkOptions.PARTITION_DEFAULT_NAME))\n              .limit(this.limit)\n              .build();\n        default:\n          throw new HoodieException(\"Unexpected table type: \" + this.conf.getString(FlinkOptions.TABLE_TYPE));\n      }\n    }\n    String errMsg = String.format(\"Invalid query type : '%s', options ['%s'] are supported now\", queryType,\n        FlinkOptions.QUERY_TYPE_SNAPSHOT);\n    throw new HoodieException(errMsg);\n  }\n","date":"2021-08-28 20:16:54","endLine":458,"groupId":"5542","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"getStreamInputFormat","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/fc/4239442db1684c43ee5912fce5614042efad8d.src","preCode":"  private InputFormat<RowData, ?> getStreamInputFormat() {\n    \r\n    Schema tableAvroSchema = this.metaClient == null ? inferSchemaFromDdl() : getTableAvroSchema();\n    final DataType rowDataType = AvroSchemaConverter.convertToDataType(tableAvroSchema);\n    final RowType rowType = (RowType) rowDataType.getLogicalType();\n    final RowType requiredRowType = (RowType) getProducedDataType().notNull().getLogicalType();\n\n    final String queryType = this.conf.getString(FlinkOptions.QUERY_TYPE);\n    org.apache.flink.core.fs.Path[] paths = new org.apache.flink.core.fs.Path[0];\n    if (FlinkOptions.QUERY_TYPE_SNAPSHOT.equals(queryType)) {\n      final HoodieTableType tableType = HoodieTableType.valueOf(this.conf.getString(FlinkOptions.TABLE_TYPE));\n      switch (tableType) {\n        case MERGE_ON_READ:\n          final MergeOnReadTableState hoodieTableState = new MergeOnReadTableState(\n              rowType,\n              requiredRowType,\n              tableAvroSchema.toString(),\n              AvroSchemaConverter.convertToSchema(requiredRowType).toString(),\n              Collections.emptyList(),\n              conf.getString(FlinkOptions.RECORD_KEY_FIELD).split(\",\"));\n          return MergeOnReadInputFormat.builder()\n              .config(this.conf)\n              .paths(paths)\n              .tableState(hoodieTableState)\n              \r\n              \r\n              .fieldTypes(rowDataType.getChildren())\n              .defaultPartName(conf.getString(FlinkOptions.PARTITION_DEFAULT_NAME))\n              .limit(this.limit)\n              .emitDelete(true)\n              .build();\n        case COPY_ON_WRITE:\n          final MergeOnReadTableState hoodieTableState2 = new MergeOnReadTableState(\n              rowType,\n              requiredRowType,\n              tableAvroSchema.toString(),\n              AvroSchemaConverter.convertToSchema(requiredRowType).toString(),\n              Collections.emptyList(),\n              conf.getString(FlinkOptions.RECORD_KEY_FIELD).split(\",\"));\n          return MergeOnReadInputFormat.builder()\n              .config(this.conf)\n              .paths(paths)\n              .tableState(hoodieTableState2)\n              \r\n              \r\n              .fieldTypes(rowDataType.getChildren())\n              .defaultPartName(conf.getString(FlinkOptions.PARTITION_DEFAULT_NAME))\n              .limit(this.limit)\n              .build();\n        default:\n          throw new HoodieException(\"Unexpected table type: \" + this.conf.getString(FlinkOptions.TABLE_TYPE));\n      }\n    }\n    String errMsg = String.format(\"Invalid query type : '%s', options ['%s'] are supported now\", queryType,\n        FlinkOptions.QUERY_TYPE_SNAPSHOT);\n    throw new HoodieException(errMsg);\n  }\n","realPath":"hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSource.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":402,"status":"B"}],"commitId":"57668d02a0aa723dd4b2245dc7659fe18113eb59","commitMessage":"@@@[HUDI-2371] Improvement flink streaming reader (#3552)\n\n- Support reading empty table\n- Fix filtering by partition path\n- Support reading from earliest commit","date":"2021-08-28 20:16:54","modifiedFileCount":"9","status":"B","submitter":"Danny Chan"},{"authorTime":"2021-09-19 09:06:46","codes":[{"authorDate":"2021-09-19 09:06:46","commitOrder":2,"curCode":"  private InputFormat<RowData, ?> getBatchInputFormat() {\n    final Schema tableAvroSchema = getTableAvroSchema();\n    final DataType rowDataType = AvroSchemaConverter.convertToDataType(tableAvroSchema);\n    final RowType rowType = (RowType) rowDataType.getLogicalType();\n    final RowType requiredRowType = (RowType) getProducedDataType().notNull().getLogicalType();\n\n    final String queryType = this.conf.getString(FlinkOptions.QUERY_TYPE);\n    switch (queryType) {\n      case FlinkOptions.QUERY_TYPE_SNAPSHOT:\n        final HoodieTableType tableType = HoodieTableType.valueOf(this.conf.getString(FlinkOptions.TABLE_TYPE));\n        switch (tableType) {\n          case MERGE_ON_READ:\n            final List<MergeOnReadInputSplit> inputSplits = buildFileIndex();\n            if (inputSplits.size() == 0) {\n              \r\n              LOG.warn(\"No input splits generate for MERGE_ON_READ input format, returns empty collection instead\");\n              return EMPTY_INPUT_FORMAT;\n            }\n            return mergeOnReadInputFormat(rowType, requiredRowType, tableAvroSchema,\n                rowDataType, inputSplits, false);\n          case COPY_ON_WRITE:\n            return baseFileOnlyInputFormat();\n          default:\n            throw new HoodieException(\"Unexpected table type: \" + this.conf.getString(FlinkOptions.TABLE_TYPE));\n        }\n      case FlinkOptions.QUERY_TYPE_READ_OPTIMIZED:\n        return baseFileOnlyInputFormat();\n      case FlinkOptions.QUERY_TYPE_INCREMENTAL:\n        IncrementalInputSplits incrementalInputSplits = IncrementalInputSplits.builder()\n            .conf(conf).path(FilePathUtils.toFlinkPath(path))\n            .maxCompactionMemoryInBytes(maxCompactionMemoryInBytes)\n            .requiredPartitions(getRequiredPartitionPaths()).build();\n        final IncrementalInputSplits.Result result = incrementalInputSplits.inputSplits(metaClient, hadoopConf);\n        if (result.isEmpty()) {\n          \r\n          LOG.warn(\"No input splits generate for incremental read, returns empty collection instead\");\n          return new CollectionInputFormat<>(Collections.emptyList(), null);\n        }\n        return mergeOnReadInputFormat(rowType, requiredRowType, tableAvroSchema,\n            rowDataType, result.getInputSplits(), false);\n      default:\n        String errMsg = String.format(\"Invalid query type : '%s', options ['%s', '%s', '%s'] are supported now\", queryType,\n            FlinkOptions.QUERY_TYPE_SNAPSHOT, FlinkOptions.QUERY_TYPE_READ_OPTIMIZED, FlinkOptions.QUERY_TYPE_INCREMENTAL);\n        throw new HoodieException(errMsg);\n    }\n  }\n","date":"2021-09-19 09:06:46","endLine":372,"groupId":"0","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"getBatchInputFormat","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/04/94143a1a01be261db406402f6ca5c6fee927a0.src","preCode":"  private InputFormat<RowData, ?> getBatchInputFormat() {\n    \r\n    if (!partitionKeys.isEmpty() && getOrFetchPartitions().isEmpty()) {\n      return new CollectionInputFormat<>(Collections.emptyList(), null);\n    }\n\n    final Path[] paths = getReadPaths();\n    if (paths.length == 0) {\n      return new CollectionInputFormat<>(Collections.emptyList(), null);\n    }\n\n    final Schema tableAvroSchema = getTableAvroSchema();\n    final DataType rowDataType = AvroSchemaConverter.convertToDataType(tableAvroSchema);\n    final RowType rowType = (RowType) rowDataType.getLogicalType();\n    final RowType requiredRowType = (RowType) getProducedDataType().notNull().getLogicalType();\n\n    final String queryType = this.conf.getString(FlinkOptions.QUERY_TYPE);\n    switch (queryType) {\n      case FlinkOptions.QUERY_TYPE_SNAPSHOT:\n        final HoodieTableType tableType = HoodieTableType.valueOf(this.conf.getString(FlinkOptions.TABLE_TYPE));\n        switch (tableType) {\n          case MERGE_ON_READ:\n            final List<MergeOnReadInputSplit> inputSplits = buildFileIndex(paths);\n            if (inputSplits.size() == 0) {\n              \r\n              LOG.warn(\"No input splits generate for MERGE_ON_READ input format, returns empty collection instead\");\n              return new CollectionInputFormat<>(Collections.emptyList(), null);\n            }\n            final MergeOnReadTableState hoodieTableState = new MergeOnReadTableState(\n                rowType,\n                requiredRowType,\n                tableAvroSchema.toString(),\n                AvroSchemaConverter.convertToSchema(requiredRowType).toString(),\n                inputSplits,\n                conf.getString(FlinkOptions.RECORD_KEY_FIELD).split(\",\"));\n            return MergeOnReadInputFormat.builder()\n                .config(this.conf)\n                .paths(FilePathUtils.toFlinkPaths(paths))\n                .tableState(hoodieTableState)\n                \r\n                \r\n                .fieldTypes(rowDataType.getChildren())\n                .defaultPartName(conf.getString(FlinkOptions.PARTITION_DEFAULT_NAME))\n                .limit(this.limit)\n                .emitDelete(false)\n                .build();\n          case COPY_ON_WRITE:\n            FileInputFormat<RowData> format = new CopyOnWriteInputFormat(\n                FilePathUtils.toFlinkPaths(paths),\n                this.schema.getColumnNames().toArray(new String[0]),\n                this.schema.getColumnDataTypes().toArray(new DataType[0]),\n                this.requiredPos,\n                this.conf.getString(FlinkOptions.PARTITION_DEFAULT_NAME),\n                this.limit == NO_LIMIT_CONSTANT ? Long.MAX_VALUE : this.limit, \r\n                getParquetConf(this.conf, this.hadoopConf),\n                this.conf.getBoolean(FlinkOptions.UTC_TIMEZONE)\n            );\n            format.setFilesFilter(new LatestFileFilter(this.hadoopConf));\n            return format;\n          default:\n            throw new HoodieException(\"Unexpected table type: \" + this.conf.getString(FlinkOptions.TABLE_TYPE));\n        }\n      case FlinkOptions.QUERY_TYPE_READ_OPTIMIZED:\n        FileInputFormat<RowData> format = new CopyOnWriteInputFormat(\n            FilePathUtils.toFlinkPaths(paths),\n            this.schema.getColumnNames().toArray(new String[0]),\n            this.schema.getColumnDataTypes().toArray(new DataType[0]),\n            this.requiredPos,\n            \"default\",\n            this.limit == NO_LIMIT_CONSTANT ? Long.MAX_VALUE : this.limit, \r\n            getParquetConf(this.conf, this.hadoopConf),\n            this.conf.getBoolean(FlinkOptions.UTC_TIMEZONE)\n        );\n        format.setFilesFilter(new LatestFileFilter(this.hadoopConf));\n        return format;\n      default:\n        String errMsg = String.format(\"Invalid query type : '%s', options ['%s', '%s'] are supported now\", queryType,\n            FlinkOptions.QUERY_TYPE_SNAPSHOT, FlinkOptions.QUERY_TYPE_READ_OPTIMIZED);\n        throw new HoodieException(errMsg);\n    }\n  }\n","realPath":"hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSource.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":327,"status":"M"},{"authorDate":"2021-09-19 09:06:46","commitOrder":2,"curCode":"  private InputFormat<RowData, ?> getStreamInputFormat() {\n    \r\n    Schema tableAvroSchema = this.metaClient == null ? inferSchemaFromDdl() : getTableAvroSchema();\n    final DataType rowDataType = AvroSchemaConverter.convertToDataType(tableAvroSchema);\n    final RowType rowType = (RowType) rowDataType.getLogicalType();\n    final RowType requiredRowType = (RowType) getProducedDataType().notNull().getLogicalType();\n\n    final String queryType = this.conf.getString(FlinkOptions.QUERY_TYPE);\n    if (FlinkOptions.QUERY_TYPE_SNAPSHOT.equals(queryType)) {\n      final HoodieTableType tableType = HoodieTableType.valueOf(this.conf.getString(FlinkOptions.TABLE_TYPE));\n      boolean emitDelete = tableType == HoodieTableType.MERGE_ON_READ;\n      return mergeOnReadInputFormat(rowType, requiredRowType, tableAvroSchema,\n          rowDataType, Collections.emptyList(), emitDelete);\n    }\n    String errMsg = String.format(\"Invalid query type : '%s', options ['%s'] are supported now\", queryType,\n        FlinkOptions.QUERY_TYPE_SNAPSHOT);\n    throw new HoodieException(errMsg);\n  }\n","date":"2021-09-19 09:06:46","endLine":391,"groupId":"5300","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"getStreamInputFormat","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/04/94143a1a01be261db406402f6ca5c6fee927a0.src","preCode":"  private InputFormat<RowData, ?> getStreamInputFormat() {\n    \r\n    Schema tableAvroSchema = this.metaClient == null ? inferSchemaFromDdl() : getTableAvroSchema();\n    final DataType rowDataType = AvroSchemaConverter.convertToDataType(tableAvroSchema);\n    final RowType rowType = (RowType) rowDataType.getLogicalType();\n    final RowType requiredRowType = (RowType) getProducedDataType().notNull().getLogicalType();\n\n    final String queryType = this.conf.getString(FlinkOptions.QUERY_TYPE);\n    org.apache.flink.core.fs.Path[] paths = new org.apache.flink.core.fs.Path[0];\n    if (FlinkOptions.QUERY_TYPE_SNAPSHOT.equals(queryType)) {\n      final HoodieTableType tableType = HoodieTableType.valueOf(this.conf.getString(FlinkOptions.TABLE_TYPE));\n      switch (tableType) {\n        case MERGE_ON_READ:\n          final MergeOnReadTableState hoodieTableState = new MergeOnReadTableState(\n              rowType,\n              requiredRowType,\n              tableAvroSchema.toString(),\n              AvroSchemaConverter.convertToSchema(requiredRowType).toString(),\n              Collections.emptyList(),\n              conf.getString(FlinkOptions.RECORD_KEY_FIELD).split(\",\"));\n          return MergeOnReadInputFormat.builder()\n              .config(this.conf)\n              .paths(paths)\n              .tableState(hoodieTableState)\n              \r\n              \r\n              .fieldTypes(rowDataType.getChildren())\n              .defaultPartName(conf.getString(FlinkOptions.PARTITION_DEFAULT_NAME))\n              .limit(this.limit)\n              .emitDelete(true)\n              .build();\n        case COPY_ON_WRITE:\n          final MergeOnReadTableState hoodieTableState2 = new MergeOnReadTableState(\n              rowType,\n              requiredRowType,\n              tableAvroSchema.toString(),\n              AvroSchemaConverter.convertToSchema(requiredRowType).toString(),\n              Collections.emptyList(),\n              conf.getString(FlinkOptions.RECORD_KEY_FIELD).split(\",\"));\n          return MergeOnReadInputFormat.builder()\n              .config(this.conf)\n              .paths(paths)\n              .tableState(hoodieTableState2)\n              \r\n              \r\n              .fieldTypes(rowDataType.getChildren())\n              .defaultPartName(conf.getString(FlinkOptions.PARTITION_DEFAULT_NAME))\n              .limit(this.limit)\n              .build();\n        default:\n          throw new HoodieException(\"Unexpected table type: \" + this.conf.getString(FlinkOptions.TABLE_TYPE));\n      }\n    }\n    String errMsg = String.format(\"Invalid query type : '%s', options ['%s'] are supported now\", queryType,\n        FlinkOptions.QUERY_TYPE_SNAPSHOT);\n    throw new HoodieException(errMsg);\n  }\n","realPath":"hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSource.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":374,"status":"M"}],"commitId":"3354fac42f9a2c4dbc8ac73ca4749160e9b9459b","commitMessage":"@@@[HUDI-2449] Incremental read for Flink (#3686)\n\n","date":"2021-09-19 09:06:46","modifiedFileCount":"15","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-09-19 09:06:46","codes":[{"authorDate":"2021-09-22 12:18:02","commitOrder":3,"curCode":"  private InputFormat<RowData, ?> getBatchInputFormat() {\n    final Schema tableAvroSchema = getTableAvroSchema();\n    final DataType rowDataType = AvroSchemaConverter.convertToDataType(tableAvroSchema);\n    final RowType rowType = (RowType) rowDataType.getLogicalType();\n    final RowType requiredRowType = (RowType) getProducedDataType().notNull().getLogicalType();\n\n    final String queryType = this.conf.getString(FlinkOptions.QUERY_TYPE);\n    switch (queryType) {\n      case FlinkOptions.QUERY_TYPE_SNAPSHOT:\n        final HoodieTableType tableType = HoodieTableType.valueOf(this.conf.getString(FlinkOptions.TABLE_TYPE));\n        switch (tableType) {\n          case MERGE_ON_READ:\n            final List<MergeOnReadInputSplit> inputSplits = buildFileIndex();\n            if (inputSplits.size() == 0) {\n              \r\n              LOG.warn(\"No input splits generate for MERGE_ON_READ input format, returns empty collection instead\");\n              return InputFormats.EMPTY_INPUT_FORMAT;\n            }\n            return mergeOnReadInputFormat(rowType, requiredRowType, tableAvroSchema,\n                rowDataType, inputSplits, false);\n          case COPY_ON_WRITE:\n            return baseFileOnlyInputFormat();\n          default:\n            throw new HoodieException(\"Unexpected table type: \" + this.conf.getString(FlinkOptions.TABLE_TYPE));\n        }\n      case FlinkOptions.QUERY_TYPE_READ_OPTIMIZED:\n        return baseFileOnlyInputFormat();\n      case FlinkOptions.QUERY_TYPE_INCREMENTAL:\n        IncrementalInputSplits incrementalInputSplits = IncrementalInputSplits.builder()\n            .conf(conf).path(FilePathUtils.toFlinkPath(path))\n            .maxCompactionMemoryInBytes(maxCompactionMemoryInBytes)\n            .requiredPartitions(getRequiredPartitionPaths()).build();\n        final IncrementalInputSplits.Result result = incrementalInputSplits.inputSplits(metaClient, hadoopConf);\n        if (result.isEmpty()) {\n          \r\n          LOG.warn(\"No input splits generate for incremental read, returns empty collection instead\");\n          return InputFormats.EMPTY_INPUT_FORMAT;\n        }\n        return mergeOnReadInputFormat(rowType, requiredRowType, tableAvroSchema,\n            rowDataType, result.getInputSplits(), false);\n      default:\n        String errMsg = String.format(\"Invalid query type : '%s', options ['%s', '%s', '%s'] are supported now\", queryType,\n            FlinkOptions.QUERY_TYPE_SNAPSHOT, FlinkOptions.QUERY_TYPE_READ_OPTIMIZED, FlinkOptions.QUERY_TYPE_INCREMENTAL);\n        throw new HoodieException(errMsg);\n    }\n  }\n","date":"2021-09-22 12:18:02","endLine":369,"groupId":"10447","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"getBatchInputFormat","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/6e/f608bc713b83f76aef9bd8b064e328cebb61b3.src","preCode":"  private InputFormat<RowData, ?> getBatchInputFormat() {\n    final Schema tableAvroSchema = getTableAvroSchema();\n    final DataType rowDataType = AvroSchemaConverter.convertToDataType(tableAvroSchema);\n    final RowType rowType = (RowType) rowDataType.getLogicalType();\n    final RowType requiredRowType = (RowType) getProducedDataType().notNull().getLogicalType();\n\n    final String queryType = this.conf.getString(FlinkOptions.QUERY_TYPE);\n    switch (queryType) {\n      case FlinkOptions.QUERY_TYPE_SNAPSHOT:\n        final HoodieTableType tableType = HoodieTableType.valueOf(this.conf.getString(FlinkOptions.TABLE_TYPE));\n        switch (tableType) {\n          case MERGE_ON_READ:\n            final List<MergeOnReadInputSplit> inputSplits = buildFileIndex();\n            if (inputSplits.size() == 0) {\n              \r\n              LOG.warn(\"No input splits generate for MERGE_ON_READ input format, returns empty collection instead\");\n              return EMPTY_INPUT_FORMAT;\n            }\n            return mergeOnReadInputFormat(rowType, requiredRowType, tableAvroSchema,\n                rowDataType, inputSplits, false);\n          case COPY_ON_WRITE:\n            return baseFileOnlyInputFormat();\n          default:\n            throw new HoodieException(\"Unexpected table type: \" + this.conf.getString(FlinkOptions.TABLE_TYPE));\n        }\n      case FlinkOptions.QUERY_TYPE_READ_OPTIMIZED:\n        return baseFileOnlyInputFormat();\n      case FlinkOptions.QUERY_TYPE_INCREMENTAL:\n        IncrementalInputSplits incrementalInputSplits = IncrementalInputSplits.builder()\n            .conf(conf).path(FilePathUtils.toFlinkPath(path))\n            .maxCompactionMemoryInBytes(maxCompactionMemoryInBytes)\n            .requiredPartitions(getRequiredPartitionPaths()).build();\n        final IncrementalInputSplits.Result result = incrementalInputSplits.inputSplits(metaClient, hadoopConf);\n        if (result.isEmpty()) {\n          \r\n          LOG.warn(\"No input splits generate for incremental read, returns empty collection instead\");\n          return new CollectionInputFormat<>(Collections.emptyList(), null);\n        }\n        return mergeOnReadInputFormat(rowType, requiredRowType, tableAvroSchema,\n            rowDataType, result.getInputSplits(), false);\n      default:\n        String errMsg = String.format(\"Invalid query type : '%s', options ['%s', '%s', '%s'] are supported now\", queryType,\n            FlinkOptions.QUERY_TYPE_SNAPSHOT, FlinkOptions.QUERY_TYPE_READ_OPTIMIZED, FlinkOptions.QUERY_TYPE_INCREMENTAL);\n        throw new HoodieException(errMsg);\n    }\n  }\n","realPath":"hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSource.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":324,"status":"M"},{"authorDate":"2021-09-19 09:06:46","commitOrder":3,"curCode":"  private InputFormat<RowData, ?> getStreamInputFormat() {\n    \r\n    Schema tableAvroSchema = this.metaClient == null ? inferSchemaFromDdl() : getTableAvroSchema();\n    final DataType rowDataType = AvroSchemaConverter.convertToDataType(tableAvroSchema);\n    final RowType rowType = (RowType) rowDataType.getLogicalType();\n    final RowType requiredRowType = (RowType) getProducedDataType().notNull().getLogicalType();\n\n    final String queryType = this.conf.getString(FlinkOptions.QUERY_TYPE);\n    if (FlinkOptions.QUERY_TYPE_SNAPSHOT.equals(queryType)) {\n      final HoodieTableType tableType = HoodieTableType.valueOf(this.conf.getString(FlinkOptions.TABLE_TYPE));\n      boolean emitDelete = tableType == HoodieTableType.MERGE_ON_READ;\n      return mergeOnReadInputFormat(rowType, requiredRowType, tableAvroSchema,\n          rowDataType, Collections.emptyList(), emitDelete);\n    }\n    String errMsg = String.format(\"Invalid query type : '%s', options ['%s'] are supported now\", queryType,\n        FlinkOptions.QUERY_TYPE_SNAPSHOT);\n    throw new HoodieException(errMsg);\n  }\n","date":"2021-09-19 09:06:46","endLine":391,"groupId":"10447","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"getStreamInputFormat","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/04/94143a1a01be261db406402f6ca5c6fee927a0.src","preCode":"  private InputFormat<RowData, ?> getStreamInputFormat() {\n    \r\n    Schema tableAvroSchema = this.metaClient == null ? inferSchemaFromDdl() : getTableAvroSchema();\n    final DataType rowDataType = AvroSchemaConverter.convertToDataType(tableAvroSchema);\n    final RowType rowType = (RowType) rowDataType.getLogicalType();\n    final RowType requiredRowType = (RowType) getProducedDataType().notNull().getLogicalType();\n\n    final String queryType = this.conf.getString(FlinkOptions.QUERY_TYPE);\n    if (FlinkOptions.QUERY_TYPE_SNAPSHOT.equals(queryType)) {\n      final HoodieTableType tableType = HoodieTableType.valueOf(this.conf.getString(FlinkOptions.TABLE_TYPE));\n      boolean emitDelete = tableType == HoodieTableType.MERGE_ON_READ;\n      return mergeOnReadInputFormat(rowType, requiredRowType, tableAvroSchema,\n          rowDataType, Collections.emptyList(), emitDelete);\n    }\n    String errMsg = String.format(\"Invalid query type : '%s', options ['%s'] are supported now\", queryType,\n        FlinkOptions.QUERY_TYPE_SNAPSHOT);\n    throw new HoodieException(errMsg);\n  }\n","realPath":"hudi-flink/src/main/java/org/apache/hudi/table/HoodieTableSource.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":374,"status":"N"}],"commitId":"e813dae36d5fa1f584e221e24e0919e43f7d0503","commitMessage":"@@@[MINOR] Cosmetic changes for flink (#3701)\n\n","date":"2021-09-22 12:18:02","modifiedFileCount":"7","status":"M","submitter":"Danny Chan"}]
