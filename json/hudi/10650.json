[{"authorTime":"2021-08-11 22:39:48","codes":[{"authorDate":"2021-08-05 07:08:50","commitOrder":2,"curCode":"  public void testSync(HoodieTableType tableType) throws Exception {\n    init(tableType);\n    HoodieSparkEngineContext engineContext = new HoodieSparkEngineContext(jsc);\n\n    String newCommitTime;\n    List<HoodieRecord> records;\n    List<WriteStatus> writeStatuses;\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, false))) {\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateInserts(newCommitTime, 5);\n      client.startCommitWithTime(newCommitTime);\n      writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateInserts(newCommitTime, 5);\n      client.startCommitWithTime(newCommitTime);\n      writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateInserts(newCommitTime, 5);\n      writeStatuses = client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      validateMetadata(client);\n      assertTrue(metadata(client).isInSync());\n    }\n\n    \r\n    String restoreToInstant;\n    String inflightActionTimestamp;\n    String beforeInflightActionTimestamp;\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, false))) {\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUniqueUpdates(newCommitTime, 5);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUpdates(newCommitTime, 10);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      if (metaClient.getTableType() == HoodieTableType.MERGE_ON_READ) {\n        newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n        client.scheduleCompactionAtInstant(newCommitTime, Option.empty());\n        client.compact(newCommitTime);\n        assertTrue(metadata(client).isInSync());\n      }\n\n      \r\n      restoreToInstant = newCommitTime;\n      if (metaClient.getTableType() == HoodieTableType.COPY_ON_WRITE) {\n        client.savepoint(\"hoodie\", \"metadata test\");\n        assertTrue(metadata(client).isInSync());\n      }\n\n      \r\n      inflightActionTimestamp = HoodieActiveTimeline.createNewInstantTime();\n      beforeInflightActionTimestamp = newCommitTime;\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateDeletes(newCommitTime, 5);\n      JavaRDD<HoodieKey> deleteKeys = jsc.parallelize(records, 1).map(r -> r.getKey());\n      client.startCommitWithTime(newCommitTime);\n      client.delete(deleteKeys, newCommitTime);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.clean(newCommitTime);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUniqueUpdates(newCommitTime, 10);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime, HoodieTimeline.REPLACE_COMMIT_ACTION);\n      records = dataGen.generateInserts(newCommitTime, 5);\n      HoodieWriteResult replaceResult = client.insertOverwrite(jsc.parallelize(records, 1), newCommitTime);\n      writeStatuses = replaceResult.getWriteStatuses().collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n    }\n\n    \r\n    \r\n    Path inflightCleanPath = new Path(metaClient.getMetaPath(), HoodieTimeline.makeInflightCleanerFileName(inflightActionTimestamp));\n    fs.create(inflightCleanPath).close();\n\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      client.syncTableMetadata();\n\n      \r\n      HoodieBackedTableMetadataWriter writer =\n          (HoodieBackedTableMetadataWriter) SparkHoodieBackedTableMetadataWriter.create(hadoopConf, client.getConfig(), context);\n      assertEquals(writer.getLatestSyncedInstantTime().get(), beforeInflightActionTimestamp);\n\n      \r\n      HoodieTableMetadata metadata  = HoodieTableMetadata.create(context, client.getConfig().getMetadataConfig(),\n          client.getConfig().getBasePath(), FileSystemViewStorageConfig.FILESYSTEM_VIEW_SPILLABLE_DIR.defaultValue());\n      assertEquals(metadata.getSyncedInstantTime().get(), newCommitTime);\n\n      \r\n      fs.delete(inflightCleanPath, false);\n      client.syncTableMetadata();\n\n      writer =\n          (HoodieBackedTableMetadataWriter)SparkHoodieBackedTableMetadataWriter.create(hadoopConf, client.getConfig(), context);\n      assertEquals(writer.getLatestSyncedInstantTime().get(), newCommitTime);\n\n      \r\n      metadata  = HoodieTableMetadata.create(context, client.getConfig().getMetadataConfig(),\n          client.getConfig().getBasePath(), FileSystemViewStorageConfig.FILESYSTEM_VIEW_SPILLABLE_DIR.defaultValue());\n      assertEquals(metadata.getSyncedInstantTime().get(), newCommitTime);\n    }\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      client.restoreToInstant(restoreToInstant);\n      assertFalse(metadata(client).isInSync());\n\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      client.syncTableMetadata();\n\n      validateMetadata(client);\n      assertTrue(metadata(client).isInSync());\n    }\n  }\n","date":"2021-08-05 07:08:50","endLine":643,"groupId":"6001","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testSync","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/51/3bac14b2efdcb0328ee45ad486ac6a240e4b05.src","preCode":"  public void testSync(HoodieTableType tableType) throws Exception {\n    init(tableType);\n    HoodieSparkEngineContext engineContext = new HoodieSparkEngineContext(jsc);\n\n    String newCommitTime;\n    List<HoodieRecord> records;\n    List<WriteStatus> writeStatuses;\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, false))) {\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateInserts(newCommitTime, 5);\n      client.startCommitWithTime(newCommitTime);\n      writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateInserts(newCommitTime, 5);\n      client.startCommitWithTime(newCommitTime);\n      writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateInserts(newCommitTime, 5);\n      writeStatuses = client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      validateMetadata(client);\n      assertTrue(metadata(client).isInSync());\n    }\n\n    \r\n    String restoreToInstant;\n    String inflightActionTimestamp;\n    String beforeInflightActionTimestamp;\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, false))) {\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUniqueUpdates(newCommitTime, 5);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUpdates(newCommitTime, 10);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      if (metaClient.getTableType() == HoodieTableType.MERGE_ON_READ) {\n        newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n        client.scheduleCompactionAtInstant(newCommitTime, Option.empty());\n        client.compact(newCommitTime);\n        assertTrue(metadata(client).isInSync());\n      }\n\n      \r\n      restoreToInstant = newCommitTime;\n      if (metaClient.getTableType() == HoodieTableType.COPY_ON_WRITE) {\n        client.savepoint(\"hoodie\", \"metadata test\");\n        assertTrue(metadata(client).isInSync());\n      }\n\n      \r\n      inflightActionTimestamp = HoodieActiveTimeline.createNewInstantTime();\n      beforeInflightActionTimestamp = newCommitTime;\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateDeletes(newCommitTime, 5);\n      JavaRDD<HoodieKey> deleteKeys = jsc.parallelize(records, 1).map(r -> r.getKey());\n      client.startCommitWithTime(newCommitTime);\n      client.delete(deleteKeys, newCommitTime);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.clean(newCommitTime);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUniqueUpdates(newCommitTime, 10);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime, HoodieTimeline.REPLACE_COMMIT_ACTION);\n      records = dataGen.generateInserts(newCommitTime, 5);\n      HoodieWriteResult replaceResult = client.insertOverwrite(jsc.parallelize(records, 1), newCommitTime);\n      writeStatuses = replaceResult.getWriteStatuses().collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n    }\n\n    \r\n    \r\n    Path inflightCleanPath = new Path(metaClient.getMetaPath(), HoodieTimeline.makeInflightCleanerFileName(inflightActionTimestamp));\n    fs.create(inflightCleanPath).close();\n\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      client.syncTableMetadata();\n\n      \r\n      HoodieBackedTableMetadataWriter writer =\n          (HoodieBackedTableMetadataWriter) SparkHoodieBackedTableMetadataWriter.create(hadoopConf, client.getConfig(), context);\n      assertEquals(writer.getLatestSyncedInstantTime().get(), beforeInflightActionTimestamp);\n\n      \r\n      HoodieTableMetadata metadata  = HoodieTableMetadata.create(context, client.getConfig().getMetadataConfig(),\n          client.getConfig().getBasePath(), FileSystemViewStorageConfig.FILESYSTEM_VIEW_SPILLABLE_DIR.defaultValue());\n      assertEquals(metadata.getSyncedInstantTime().get(), newCommitTime);\n\n      \r\n      fs.delete(inflightCleanPath, false);\n      client.syncTableMetadata();\n\n      writer =\n          (HoodieBackedTableMetadataWriter)SparkHoodieBackedTableMetadataWriter.create(hadoopConf, client.getConfig(), context);\n      assertEquals(writer.getLatestSyncedInstantTime().get(), newCommitTime);\n\n      \r\n      metadata  = HoodieTableMetadata.create(context, client.getConfig().getMetadataConfig(),\n          client.getConfig().getBasePath(), FileSystemViewStorageConfig.FILESYSTEM_VIEW_SPILLABLE_DIR.defaultValue());\n      assertEquals(metadata.getSyncedInstantTime().get(), newCommitTime);\n    }\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      client.restoreToInstant(restoreToInstant);\n      assertFalse(metadata(client).isInSync());\n\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      client.syncTableMetadata();\n\n      validateMetadata(client);\n      assertTrue(metadata(client).isInSync());\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":491,"status":"NB"},{"authorDate":"2021-08-11 22:39:48","commitOrder":2,"curCode":"  public void testMetdataTableCommitFailure() throws Exception {\n    init(HoodieTableType.COPY_ON_WRITE);\n    HoodieSparkEngineContext engineContext = new HoodieSparkEngineContext(jsc);\n\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      String newCommitTime = \"001\";\n      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n      client.startCommitWithTime(newCommitTime);\n      List<WriteStatus> writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      \r\n      newCommitTime = \"002\";\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateInserts(newCommitTime, 20);\n      writeStatuses = client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    \r\n    HoodieTableMetaClient metadataMetaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(metadataTableBasePath).build();\n    HoodieActiveTimeline timeline = metadataMetaClient.getActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n\n    \r\n    FileCreateUtils.deleteDeltaCommit(metadataTableBasePath, \"002\");\n    timeline = metadataMetaClient.reloadActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(true, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n\n    \r\n    String latestCommitTime = HoodieActiveTimeline.createNewInstantTime();\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      String newCommitTime = \"003\";\n      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n      client.startCommitWithTime(newCommitTime);\n      client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n\n      records = dataGen.generateInserts(latestCommitTime, 20);\n      client.startCommitWithTime(latestCommitTime);\n      List<WriteStatus> writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), latestCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    timeline = metadataMetaClient.reloadActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, latestCommitTime)));\n    assertTrue(timeline.getRollbackTimeline().countInstants() == 1);\n  }\n","date":"2021-08-11 22:39:48","endLine":917,"groupId":"1716","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testMetdataTableCommitFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e1/2c8bc29e08c1016c0c4954e0fe66da749ad266.src","preCode":"  public void testMetdataTableCommitFailure() throws Exception {\n    init(HoodieTableType.COPY_ON_WRITE);\n    HoodieSparkEngineContext engineContext = new HoodieSparkEngineContext(jsc);\n\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      String newCommitTime = \"001\";\n      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n      client.startCommitWithTime(newCommitTime);\n      List<WriteStatus> writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      \r\n      newCommitTime = \"002\";\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateInserts(newCommitTime, 20);\n      writeStatuses = client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    \r\n    HoodieTableMetaClient metadataMetaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(metadataTableBasePath).build();\n    HoodieActiveTimeline timeline = metadataMetaClient.getActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n\n    \r\n    FileCreateUtils.deleteDeltaCommit(metadataTableBasePath, \"002\");\n    timeline = metadataMetaClient.reloadActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(true, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n\n    \r\n    String latestCommitTime = HoodieActiveTimeline.createNewInstantTime();\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      String newCommitTime = \"003\";\n      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n      client.startCommitWithTime(newCommitTime);\n      client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n\n      records = dataGen.generateInserts(latestCommitTime, 20);\n      client.startCommitWithTime(latestCommitTime);\n      List<WriteStatus> writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), latestCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    timeline = metadataMetaClient.reloadActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, latestCommitTime)));\n    assertTrue(timeline.getRollbackTimeline().countInstants() == 1);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":866,"status":"B"}],"commitId":"aa11989eade1bd268d83e6aafe15f2ea60be67d2","commitMessage":"@@@[HUDI-2286] Handle the case of failed deltacommit on the metadata table. (#3428)\n\nA failed deltacommit on the metadata table will be automatically rolled back. Assuming the failed commit was \"t10\".  the rollback will happen the next time at \"t11\". Post rollback.  when we try to sync the dataset to the metadata table.  we should look for all unsynched instants including t11. Current code ignores t11 since the latest commit timestamp on metadata table is t11 (due to rollback).","date":"2021-08-11 22:39:48","modifiedFileCount":"3","status":"M","submitter":"Prashant Wason"},{"authorTime":"2021-08-11 22:39:48","codes":[{"authorDate":"2021-08-14 12:23:34","commitOrder":3,"curCode":"  public void testSync(HoodieTableType tableType) throws Exception {\n    init(tableType);\n    HoodieSparkEngineContext engineContext = new HoodieSparkEngineContext(jsc);\n\n    String newCommitTime;\n    List<HoodieRecord> records;\n    List<WriteStatus> writeStatuses;\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, false))) {\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateInserts(newCommitTime, 5);\n      client.startCommitWithTime(newCommitTime);\n      writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateInserts(newCommitTime, 5);\n      client.startCommitWithTime(newCommitTime);\n      writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateInserts(newCommitTime, 5);\n      writeStatuses = client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      validateMetadata(client);\n      assertTrue(metadata(client).isInSync());\n    }\n\n    \r\n    String restoreToInstant;\n    String inflightActionTimestamp;\n    String beforeInflightActionTimestamp;\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, false))) {\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUniqueUpdates(newCommitTime, 5);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUpdates(newCommitTime, 10);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      if (metaClient.getTableType() == HoodieTableType.MERGE_ON_READ) {\n        newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n        client.scheduleCompactionAtInstant(newCommitTime, Option.empty());\n        client.compact(newCommitTime);\n        assertTrue(metadata(client).isInSync());\n      }\n\n      \r\n      restoreToInstant = newCommitTime;\n      if (metaClient.getTableType() == HoodieTableType.COPY_ON_WRITE) {\n        client.savepoint(\"hoodie\", \"metadata test\");\n        assertTrue(metadata(client).isInSync());\n      }\n\n      \r\n      inflightActionTimestamp = HoodieActiveTimeline.createNewInstantTime();\n      beforeInflightActionTimestamp = newCommitTime;\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateDeletes(newCommitTime, 5);\n      JavaRDD<HoodieKey> deleteKeys = jsc.parallelize(records, 1).map(r -> r.getKey());\n      client.startCommitWithTime(newCommitTime);\n      client.delete(deleteKeys, newCommitTime);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.clean(newCommitTime);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUniqueUpdates(newCommitTime, 10);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime, HoodieTimeline.REPLACE_COMMIT_ACTION);\n      records = dataGen.generateInserts(newCommitTime, 5);\n      HoodieWriteResult replaceResult = client.insertOverwrite(jsc.parallelize(records, 1), newCommitTime);\n      writeStatuses = replaceResult.getWriteStatuses().collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n    }\n\n    \r\n    \r\n    Path inflightCleanPath = new Path(metaClient.getMetaPath(), HoodieTimeline.makeInflightCleanerFileName(inflightActionTimestamp));\n    fs.create(inflightCleanPath).close();\n\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      client.syncTableMetadata();\n\n      \r\n      HoodieBackedTableMetadataWriter writer =\n          (HoodieBackedTableMetadataWriter) SparkHoodieBackedTableMetadataWriter.create(hadoopConf, client.getConfig(), context);\n      assertEquals(writer.getMetadataReader().getUpdateTime().get(), beforeInflightActionTimestamp);\n\n      \r\n      HoodieTableMetadata metadata  = HoodieTableMetadata.create(context, client.getConfig().getMetadataConfig(),\n          client.getConfig().getBasePath(), FileSystemViewStorageConfig.FILESYSTEM_VIEW_SPILLABLE_DIR.defaultValue());\n      assertEquals(((HoodieBackedTableMetadata)metadata).getReaderTime().get(), newCommitTime);\n\n      \r\n      fs.delete(inflightCleanPath, false);\n      client.syncTableMetadata();\n\n      writer =\n          (HoodieBackedTableMetadataWriter)SparkHoodieBackedTableMetadataWriter.create(hadoopConf, client.getConfig(), context);\n      assertEquals(writer.getMetadataReader().getUpdateTime().get(), newCommitTime);\n\n      \r\n      metadata  = HoodieTableMetadata.create(context, client.getConfig().getMetadataConfig(),\n          client.getConfig().getBasePath(), FileSystemViewStorageConfig.FILESYSTEM_VIEW_SPILLABLE_DIR.defaultValue());\n      assertEquals(writer.getMetadataReader().getUpdateTime().get(), newCommitTime);\n    }\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      client.restoreToInstant(restoreToInstant);\n      assertFalse(metadata(client).isInSync());\n\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      client.syncTableMetadata();\n\n      validateMetadata(client);\n      assertTrue(metadata(client).isInSync());\n    }\n  }\n","date":"2021-08-14 12:23:34","endLine":833,"groupId":"6001","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testSync","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e7/8c2c8ea8e856a72105852a7c77ff9fcacec2c5.src","preCode":"  public void testSync(HoodieTableType tableType) throws Exception {\n    init(tableType);\n    HoodieSparkEngineContext engineContext = new HoodieSparkEngineContext(jsc);\n\n    String newCommitTime;\n    List<HoodieRecord> records;\n    List<WriteStatus> writeStatuses;\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, false))) {\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateInserts(newCommitTime, 5);\n      client.startCommitWithTime(newCommitTime);\n      writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateInserts(newCommitTime, 5);\n      client.startCommitWithTime(newCommitTime);\n      writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateInserts(newCommitTime, 5);\n      writeStatuses = client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      validateMetadata(client);\n      assertTrue(metadata(client).isInSync());\n    }\n\n    \r\n    String restoreToInstant;\n    String inflightActionTimestamp;\n    String beforeInflightActionTimestamp;\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, false))) {\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUniqueUpdates(newCommitTime, 5);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUpdates(newCommitTime, 10);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      if (metaClient.getTableType() == HoodieTableType.MERGE_ON_READ) {\n        newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n        client.scheduleCompactionAtInstant(newCommitTime, Option.empty());\n        client.compact(newCommitTime);\n        assertTrue(metadata(client).isInSync());\n      }\n\n      \r\n      restoreToInstant = newCommitTime;\n      if (metaClient.getTableType() == HoodieTableType.COPY_ON_WRITE) {\n        client.savepoint(\"hoodie\", \"metadata test\");\n        assertTrue(metadata(client).isInSync());\n      }\n\n      \r\n      inflightActionTimestamp = HoodieActiveTimeline.createNewInstantTime();\n      beforeInflightActionTimestamp = newCommitTime;\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateDeletes(newCommitTime, 5);\n      JavaRDD<HoodieKey> deleteKeys = jsc.parallelize(records, 1).map(r -> r.getKey());\n      client.startCommitWithTime(newCommitTime);\n      client.delete(deleteKeys, newCommitTime);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.clean(newCommitTime);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUniqueUpdates(newCommitTime, 10);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime, HoodieTimeline.REPLACE_COMMIT_ACTION);\n      records = dataGen.generateInserts(newCommitTime, 5);\n      HoodieWriteResult replaceResult = client.insertOverwrite(jsc.parallelize(records, 1), newCommitTime);\n      writeStatuses = replaceResult.getWriteStatuses().collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n    }\n\n    \r\n    \r\n    Path inflightCleanPath = new Path(metaClient.getMetaPath(), HoodieTimeline.makeInflightCleanerFileName(inflightActionTimestamp));\n    fs.create(inflightCleanPath).close();\n\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      client.syncTableMetadata();\n\n      \r\n      HoodieBackedTableMetadataWriter writer =\n          (HoodieBackedTableMetadataWriter) SparkHoodieBackedTableMetadataWriter.create(hadoopConf, client.getConfig(), context);\n      assertEquals(writer.getLatestSyncedInstantTime().get(), beforeInflightActionTimestamp);\n\n      \r\n      HoodieTableMetadata metadata  = HoodieTableMetadata.create(context, client.getConfig().getMetadataConfig(),\n          client.getConfig().getBasePath(), FileSystemViewStorageConfig.FILESYSTEM_VIEW_SPILLABLE_DIR.defaultValue());\n      assertEquals(metadata.getSyncedInstantTime().get(), newCommitTime);\n\n      \r\n      fs.delete(inflightCleanPath, false);\n      client.syncTableMetadata();\n\n      writer =\n          (HoodieBackedTableMetadataWriter)SparkHoodieBackedTableMetadataWriter.create(hadoopConf, client.getConfig(), context);\n      assertEquals(writer.getLatestSyncedInstantTime().get(), newCommitTime);\n\n      \r\n      metadata  = HoodieTableMetadata.create(context, client.getConfig().getMetadataConfig(),\n          client.getConfig().getBasePath(), FileSystemViewStorageConfig.FILESYSTEM_VIEW_SPILLABLE_DIR.defaultValue());\n      assertEquals(metadata.getSyncedInstantTime().get(), newCommitTime);\n    }\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      client.restoreToInstant(restoreToInstant);\n      assertFalse(metadata(client).isInSync());\n\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      client.syncTableMetadata();\n\n      validateMetadata(client);\n      assertTrue(metadata(client).isInSync());\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":681,"status":"M"},{"authorDate":"2021-08-11 22:39:48","commitOrder":3,"curCode":"  public void testMetdataTableCommitFailure() throws Exception {\n    init(HoodieTableType.COPY_ON_WRITE);\n    HoodieSparkEngineContext engineContext = new HoodieSparkEngineContext(jsc);\n\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      String newCommitTime = \"001\";\n      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n      client.startCommitWithTime(newCommitTime);\n      List<WriteStatus> writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      \r\n      newCommitTime = \"002\";\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateInserts(newCommitTime, 20);\n      writeStatuses = client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    \r\n    HoodieTableMetaClient metadataMetaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(metadataTableBasePath).build();\n    HoodieActiveTimeline timeline = metadataMetaClient.getActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n\n    \r\n    FileCreateUtils.deleteDeltaCommit(metadataTableBasePath, \"002\");\n    timeline = metadataMetaClient.reloadActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(true, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n\n    \r\n    String latestCommitTime = HoodieActiveTimeline.createNewInstantTime();\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      String newCommitTime = \"003\";\n      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n      client.startCommitWithTime(newCommitTime);\n      client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n\n      records = dataGen.generateInserts(latestCommitTime, 20);\n      client.startCommitWithTime(latestCommitTime);\n      List<WriteStatus> writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), latestCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    timeline = metadataMetaClient.reloadActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, latestCommitTime)));\n    assertTrue(timeline.getRollbackTimeline().countInstants() == 1);\n  }\n","date":"2021-08-11 22:39:48","endLine":917,"groupId":"1716","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testMetdataTableCommitFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e1/2c8bc29e08c1016c0c4954e0fe66da749ad266.src","preCode":"  public void testMetdataTableCommitFailure() throws Exception {\n    init(HoodieTableType.COPY_ON_WRITE);\n    HoodieSparkEngineContext engineContext = new HoodieSparkEngineContext(jsc);\n\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      String newCommitTime = \"001\";\n      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n      client.startCommitWithTime(newCommitTime);\n      List<WriteStatus> writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      \r\n      newCommitTime = \"002\";\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateInserts(newCommitTime, 20);\n      writeStatuses = client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    \r\n    HoodieTableMetaClient metadataMetaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(metadataTableBasePath).build();\n    HoodieActiveTimeline timeline = metadataMetaClient.getActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n\n    \r\n    FileCreateUtils.deleteDeltaCommit(metadataTableBasePath, \"002\");\n    timeline = metadataMetaClient.reloadActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(true, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n\n    \r\n    String latestCommitTime = HoodieActiveTimeline.createNewInstantTime();\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      String newCommitTime = \"003\";\n      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n      client.startCommitWithTime(newCommitTime);\n      client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n\n      records = dataGen.generateInserts(latestCommitTime, 20);\n      client.startCommitWithTime(latestCommitTime);\n      List<WriteStatus> writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), latestCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    timeline = metadataMetaClient.reloadActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, latestCommitTime)));\n    assertTrue(timeline.getRollbackTimeline().countInstants() == 1);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":866,"status":"N"}],"commitId":"8eed4406949992086fe36b8da0029de98f443588","commitMessage":"@@@[HUDI-2119] Ensure the rolled-back instance was previously synced to the Metadata Table when syncing a Rollback Instant. (#3210)\n\n* [HUDI-2119] Ensure the rolled-back instance was previously synced to the Metadata Table when syncing a Rollback Instant.\n\nIf the rolled-back instant was synced to the Metadata Table.  a corresponding deltacommit with the same timestamp should have been created on the Metadata Table timeline. To ensure we can always perfomr this check.  the Metadata Table instants should not be archived until their corresponding instants are present in the dataset timeline. But ensuring this requires a large number of instants to be kept on the metadata table.\n\nIn this change.  the metadata table will keep atleast the number of instants that the main dataset is keeping. If the instant being rolled back was before the metadata table timeline.  the code will throw an exception and the metadata table will have to be re-bootstrapped. This should be a very rare occurance and should occur only when the dataset is being repaired by rolling back multiple commits or restoring to an much older time.\n\n* Fixed checkstyle\n\n* Improvements from review comments.\n\nFixed  checkstyle\nReplaced explicit null check with Option.ofNullable\nRemoved redundant function getSynedInstantTime\n\n* Renamed getSyncedInstantTime and getSyncedInstantTimeForReader.\n\nSync is confusing so renamed to getUpdateTime() and getReaderTime().\n\n* Removed getReaderTime which is only for testing as the same method can be accessed during testing differently without making it part of the public interface.\n\n* Fix compilation error\n\n* Reverting changes to HoodieMetadataFileSystemView\n\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2021-08-14 12:23:34","modifiedFileCount":"13","status":"M","submitter":"Prashant Wason"},{"authorTime":"2021-08-11 22:39:48","codes":[{"authorDate":"2021-08-20 04:36:40","commitOrder":4,"curCode":"  public void testSync(HoodieTableType tableType) throws Exception {\n    init(tableType);\n    HoodieSparkEngineContext engineContext = new HoodieSparkEngineContext(jsc);\n\n    String newCommitTime;\n    List<HoodieRecord> records;\n    List<WriteStatus> writeStatuses;\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, false))) {\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateInserts(newCommitTime, 5);\n      client.startCommitWithTime(newCommitTime);\n      writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateInserts(newCommitTime, 5);\n      client.startCommitWithTime(newCommitTime);\n      writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateInserts(newCommitTime, 5);\n      writeStatuses = client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      validateMetadata(client);\n      assertTrue(metadata(client).isInSync());\n    }\n\n    \r\n    String restoreToInstant;\n    String inflightActionTimestamp;\n    String beforeInflightActionTimestamp;\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, false))) {\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUniqueUpdates(newCommitTime, 5);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUpdates(newCommitTime, 10);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      if (metaClient.getTableType() == HoodieTableType.MERGE_ON_READ) {\n        newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n        client.scheduleCompactionAtInstant(newCommitTime, Option.empty());\n        client.compact(newCommitTime);\n        assertTrue(metadata(client).isInSync());\n      }\n\n      \r\n      restoreToInstant = newCommitTime;\n      if (metaClient.getTableType() == HoodieTableType.COPY_ON_WRITE) {\n        client.savepoint(\"hoodie\", \"metadata test\");\n        assertTrue(metadata(client).isInSync());\n      }\n\n      \r\n      inflightActionTimestamp = HoodieActiveTimeline.createNewInstantTime();\n      beforeInflightActionTimestamp = newCommitTime;\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateDeletes(newCommitTime, 5);\n      JavaRDD<HoodieKey> deleteKeys = jsc.parallelize(records, 1).map(r -> r.getKey());\n      client.startCommitWithTime(newCommitTime);\n      client.delete(deleteKeys, newCommitTime);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.clean(newCommitTime);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUniqueUpdates(newCommitTime, 10);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime, HoodieTimeline.REPLACE_COMMIT_ACTION);\n      records = dataGen.generateInserts(newCommitTime, 5);\n      HoodieWriteResult replaceResult = client.insertOverwrite(jsc.parallelize(records, 1), newCommitTime);\n      writeStatuses = replaceResult.getWriteStatuses().collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n    }\n\n    \r\n    \r\n    Path inflightCleanPath = new Path(metaClient.getMetaPath(), HoodieTimeline.makeInflightCleanerFileName(inflightActionTimestamp));\n    fs.create(inflightCleanPath).close();\n\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      client.syncTableMetadata();\n\n      \r\n      HoodieBackedTableMetadataWriter writer =\n          (HoodieBackedTableMetadataWriter) SparkHoodieBackedTableMetadataWriter.create(hadoopConf, client.getConfig(), context);\n      assertEquals(writer.getMetadataReader().getUpdateTime().get(), beforeInflightActionTimestamp);\n\n      \r\n      HoodieTableMetadata metadata = HoodieTableMetadata.create(context, client.getConfig().getMetadataConfig(),\n          client.getConfig().getBasePath(), FileSystemViewStorageConfig.SPILLABLE_DIR.defaultValue());\n      assertEquals(((HoodieBackedTableMetadata)metadata).getReaderTime().get(), newCommitTime);\n\n      \r\n      fs.delete(inflightCleanPath, false);\n      client.syncTableMetadata();\n\n      writer =\n          (HoodieBackedTableMetadataWriter)SparkHoodieBackedTableMetadataWriter.create(hadoopConf, client.getConfig(), context);\n      assertEquals(writer.getMetadataReader().getUpdateTime().get(), newCommitTime);\n\n      \r\n      metadata = HoodieTableMetadata.create(context, client.getConfig().getMetadataConfig(),\n          client.getConfig().getBasePath(), FileSystemViewStorageConfig.SPILLABLE_DIR.defaultValue());\n      assertEquals(writer.getMetadataReader().getUpdateTime().get(), newCommitTime);\n    }\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      client.restoreToInstant(restoreToInstant);\n      assertFalse(metadata(client).isInSync());\n\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      client.syncTableMetadata();\n\n      validateMetadata(client);\n      assertTrue(metadata(client).isInSync());\n    }\n  }\n","date":"2021-08-20 04:36:40","endLine":834,"groupId":"6001","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testSync","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/be/c7ee4a8b3a2e6a61b528045b0873193a979e5b.src","preCode":"  public void testSync(HoodieTableType tableType) throws Exception {\n    init(tableType);\n    HoodieSparkEngineContext engineContext = new HoodieSparkEngineContext(jsc);\n\n    String newCommitTime;\n    List<HoodieRecord> records;\n    List<WriteStatus> writeStatuses;\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, false))) {\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateInserts(newCommitTime, 5);\n      client.startCommitWithTime(newCommitTime);\n      writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateInserts(newCommitTime, 5);\n      client.startCommitWithTime(newCommitTime);\n      writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateInserts(newCommitTime, 5);\n      writeStatuses = client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      validateMetadata(client);\n      assertTrue(metadata(client).isInSync());\n    }\n\n    \r\n    String restoreToInstant;\n    String inflightActionTimestamp;\n    String beforeInflightActionTimestamp;\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, false))) {\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUniqueUpdates(newCommitTime, 5);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUpdates(newCommitTime, 10);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      if (metaClient.getTableType() == HoodieTableType.MERGE_ON_READ) {\n        newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n        client.scheduleCompactionAtInstant(newCommitTime, Option.empty());\n        client.compact(newCommitTime);\n        assertTrue(metadata(client).isInSync());\n      }\n\n      \r\n      restoreToInstant = newCommitTime;\n      if (metaClient.getTableType() == HoodieTableType.COPY_ON_WRITE) {\n        client.savepoint(\"hoodie\", \"metadata test\");\n        assertTrue(metadata(client).isInSync());\n      }\n\n      \r\n      inflightActionTimestamp = HoodieActiveTimeline.createNewInstantTime();\n      beforeInflightActionTimestamp = newCommitTime;\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateDeletes(newCommitTime, 5);\n      JavaRDD<HoodieKey> deleteKeys = jsc.parallelize(records, 1).map(r -> r.getKey());\n      client.startCommitWithTime(newCommitTime);\n      client.delete(deleteKeys, newCommitTime);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.clean(newCommitTime);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUniqueUpdates(newCommitTime, 10);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime, HoodieTimeline.REPLACE_COMMIT_ACTION);\n      records = dataGen.generateInserts(newCommitTime, 5);\n      HoodieWriteResult replaceResult = client.insertOverwrite(jsc.parallelize(records, 1), newCommitTime);\n      writeStatuses = replaceResult.getWriteStatuses().collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n    }\n\n    \r\n    \r\n    Path inflightCleanPath = new Path(metaClient.getMetaPath(), HoodieTimeline.makeInflightCleanerFileName(inflightActionTimestamp));\n    fs.create(inflightCleanPath).close();\n\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      client.syncTableMetadata();\n\n      \r\n      HoodieBackedTableMetadataWriter writer =\n          (HoodieBackedTableMetadataWriter) SparkHoodieBackedTableMetadataWriter.create(hadoopConf, client.getConfig(), context);\n      assertEquals(writer.getMetadataReader().getUpdateTime().get(), beforeInflightActionTimestamp);\n\n      \r\n      HoodieTableMetadata metadata  = HoodieTableMetadata.create(context, client.getConfig().getMetadataConfig(),\n          client.getConfig().getBasePath(), FileSystemViewStorageConfig.FILESYSTEM_VIEW_SPILLABLE_DIR.defaultValue());\n      assertEquals(((HoodieBackedTableMetadata)metadata).getReaderTime().get(), newCommitTime);\n\n      \r\n      fs.delete(inflightCleanPath, false);\n      client.syncTableMetadata();\n\n      writer =\n          (HoodieBackedTableMetadataWriter)SparkHoodieBackedTableMetadataWriter.create(hadoopConf, client.getConfig(), context);\n      assertEquals(writer.getMetadataReader().getUpdateTime().get(), newCommitTime);\n\n      \r\n      metadata  = HoodieTableMetadata.create(context, client.getConfig().getMetadataConfig(),\n          client.getConfig().getBasePath(), FileSystemViewStorageConfig.FILESYSTEM_VIEW_SPILLABLE_DIR.defaultValue());\n      assertEquals(writer.getMetadataReader().getUpdateTime().get(), newCommitTime);\n    }\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      client.restoreToInstant(restoreToInstant);\n      assertFalse(metadata(client).isInSync());\n\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      client.syncTableMetadata();\n\n      validateMetadata(client);\n      assertTrue(metadata(client).isInSync());\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":682,"status":"M"},{"authorDate":"2021-08-11 22:39:48","commitOrder":4,"curCode":"  public void testMetdataTableCommitFailure() throws Exception {\n    init(HoodieTableType.COPY_ON_WRITE);\n    HoodieSparkEngineContext engineContext = new HoodieSparkEngineContext(jsc);\n\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      String newCommitTime = \"001\";\n      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n      client.startCommitWithTime(newCommitTime);\n      List<WriteStatus> writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      \r\n      newCommitTime = \"002\";\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateInserts(newCommitTime, 20);\n      writeStatuses = client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    \r\n    HoodieTableMetaClient metadataMetaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(metadataTableBasePath).build();\n    HoodieActiveTimeline timeline = metadataMetaClient.getActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n\n    \r\n    FileCreateUtils.deleteDeltaCommit(metadataTableBasePath, \"002\");\n    timeline = metadataMetaClient.reloadActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(true, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n\n    \r\n    String latestCommitTime = HoodieActiveTimeline.createNewInstantTime();\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      String newCommitTime = \"003\";\n      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n      client.startCommitWithTime(newCommitTime);\n      client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n\n      records = dataGen.generateInserts(latestCommitTime, 20);\n      client.startCommitWithTime(latestCommitTime);\n      List<WriteStatus> writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), latestCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    timeline = metadataMetaClient.reloadActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, latestCommitTime)));\n    assertTrue(timeline.getRollbackTimeline().countInstants() == 1);\n  }\n","date":"2021-08-11 22:39:48","endLine":917,"groupId":"1716","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testMetdataTableCommitFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e1/2c8bc29e08c1016c0c4954e0fe66da749ad266.src","preCode":"  public void testMetdataTableCommitFailure() throws Exception {\n    init(HoodieTableType.COPY_ON_WRITE);\n    HoodieSparkEngineContext engineContext = new HoodieSparkEngineContext(jsc);\n\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      String newCommitTime = \"001\";\n      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n      client.startCommitWithTime(newCommitTime);\n      List<WriteStatus> writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      \r\n      newCommitTime = \"002\";\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateInserts(newCommitTime, 20);\n      writeStatuses = client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    \r\n    HoodieTableMetaClient metadataMetaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(metadataTableBasePath).build();\n    HoodieActiveTimeline timeline = metadataMetaClient.getActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n\n    \r\n    FileCreateUtils.deleteDeltaCommit(metadataTableBasePath, \"002\");\n    timeline = metadataMetaClient.reloadActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(true, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n\n    \r\n    String latestCommitTime = HoodieActiveTimeline.createNewInstantTime();\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      String newCommitTime = \"003\";\n      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n      client.startCommitWithTime(newCommitTime);\n      client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n\n      records = dataGen.generateInserts(latestCommitTime, 20);\n      client.startCommitWithTime(latestCommitTime);\n      List<WriteStatus> writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), latestCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    timeline = metadataMetaClient.reloadActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, latestCommitTime)));\n    assertTrue(timeline.getRollbackTimeline().countInstants() == 1);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":866,"status":"N"}],"commitId":"c350d05dd3301f14fa9d688746c9de2416db3f11","commitMessage":"@@@Restore 0.8.0 config keys with deprecated annotation (#3506)\n\nCo-authored-by: Sagar Sumit <sagarsumit09@gmail.com>\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2021-08-20 04:36:40","modifiedFileCount":"109","status":"M","submitter":"Udit Mehrotra"},{"authorTime":"2021-09-24 03:40:11","codes":[{"authorDate":"2021-09-24 03:40:11","commitOrder":5,"curCode":"  public void testSync(HoodieTableType tableType) throws Exception {\n    init(tableType);\n    \r\n    writeConfig = getWriteConfigBuilder(true, false, false).build();\n    testTable.doWriteOperation(\"001\", BULK_INSERT, asList(\"p1\", \"p2\"), asList(\"p1\", \"p2\"), 1);\n    testTable.doWriteOperation(\"002\", BULK_INSERT, asList(\"p1\", \"p2\"), 1);\n    \r\n    testTable.doWriteOperation(\"003\", INSERT, asList(\"p1\", \"p2\"), 1);\n    syncAndValidate(testTable, emptyList(), true, true, true);\n    \r\n    testTable.doWriteOperation(\"004\", UPSERT, asList(\"p1\", \"p2\"), 1);\n    testTable.doWriteOperation(\"005\", UPSERT, singletonList(\"p3\"), asList(\"p1\", \"p2\", \"p3\"), 3);\n    syncAndValidate(testTable, emptyList(), false, true, true);\n\n    \r\n    if (MERGE_ON_READ.equals(tableType)) {\n      testTable = testTable.doCompaction(\"006\", asList(\"p1\", \"p2\"));\n      syncAndValidate(testTable, emptyList(), false, true, true);\n    }\n\n    \r\n    testTable.doWriteOperation(\"008\", UPSERT, asList(\"p1\", \"p2\", \"p3\"), 2);\n    syncAndValidate(testTable, emptyList(), false, true, true);\n\n    \r\n    if (COPY_ON_WRITE.equals(tableType)) {\n      testTable.doSavepoint(\"008\");\n      syncAndValidate(testTable, emptyList(), false, true, true);\n    }\n\n    \r\n    testTable.doWriteOperation(\"009\", DELETE, emptyList(), asList(\"p1\", \"p2\", \"p3\"), 2);\n    syncAndValidate(testTable, emptyList(), false, true, true);\n\n    \r\n    testTable.doCleanBasedOnCommits(\"010\", asList(\"001\", \"002\"));\n    syncAndValidate(testTable, emptyList(), false, true, true);\n\n    \r\n    testTable.doWriteOperation(\"011\", UPSERT, asList(\"p1\", \"p2\", \"p3\"), 2);\n    syncAndValidate(testTable, emptyList(), false, true, true);\n\n    \r\n    testTable.doCluster(\"012\", new HashMap<>());\n    syncAndValidate(testTable, emptyList(), false, true, true);\n\n    \r\n    \r\n    HoodieCommitMetadata inflightCommitMeta = testTable.doWriteOperation(\"007\", UPSERT, emptyList(),\n        asList(\"p1\", \"p2\", \"p3\"), 2, false, true);\n    \r\n    testTable.doWriteOperation(\"013\", UPSERT, emptyList(), asList(\"p1\", \"p2\", \"p3\"), 2);\n    \r\n    \r\n    syncAndValidate(testTable, singletonList(\"007\"), true, true, false);\n    \r\n    testTable.moveInflightCommitToComplete(\"007\", inflightCommitMeta);\n    syncTableMetadata(writeConfig);\n    \r\n    testTable.doWriteOperation(\"014\", UPSERT, emptyList(), asList(\"p1\", \"p2\", \"p3\"), 2);\n    syncAndValidate(testTable, emptyList(), true, true, true);\n\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n  }\n","date":"2021-09-24 03:40:11","endLine":542,"groupId":"10650","id":7,"instanceNumber":1,"isCurCommit":1,"methodName":"testSync","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/80/3c5b9d86af16690bab6643ba769283c3e22524.src","preCode":"  public void testSync(HoodieTableType tableType) throws Exception {\n    init(tableType);\n    HoodieSparkEngineContext engineContext = new HoodieSparkEngineContext(jsc);\n\n    String newCommitTime;\n    List<HoodieRecord> records;\n    List<WriteStatus> writeStatuses;\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, false))) {\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateInserts(newCommitTime, 5);\n      client.startCommitWithTime(newCommitTime);\n      writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateInserts(newCommitTime, 5);\n      client.startCommitWithTime(newCommitTime);\n      writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateInserts(newCommitTime, 5);\n      writeStatuses = client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      validateMetadata(client);\n      assertTrue(metadata(client).isInSync());\n    }\n\n    \r\n    String restoreToInstant;\n    String inflightActionTimestamp;\n    String beforeInflightActionTimestamp;\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, false))) {\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUniqueUpdates(newCommitTime, 5);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUpdates(newCommitTime, 10);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      if (metaClient.getTableType() == HoodieTableType.MERGE_ON_READ) {\n        newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n        client.scheduleCompactionAtInstant(newCommitTime, Option.empty());\n        client.compact(newCommitTime);\n        assertTrue(metadata(client).isInSync());\n      }\n\n      \r\n      restoreToInstant = newCommitTime;\n      if (metaClient.getTableType() == HoodieTableType.COPY_ON_WRITE) {\n        client.savepoint(\"hoodie\", \"metadata test\");\n        assertTrue(metadata(client).isInSync());\n      }\n\n      \r\n      inflightActionTimestamp = HoodieActiveTimeline.createNewInstantTime();\n      beforeInflightActionTimestamp = newCommitTime;\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      records = dataGen.generateDeletes(newCommitTime, 5);\n      JavaRDD<HoodieKey> deleteKeys = jsc.parallelize(records, 1).map(r -> r.getKey());\n      client.startCommitWithTime(newCommitTime);\n      client.delete(deleteKeys, newCommitTime);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.clean(newCommitTime);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateUniqueUpdates(newCommitTime, 10);\n      writeStatuses = client.upsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n\n      \r\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime, HoodieTimeline.REPLACE_COMMIT_ACTION);\n      records = dataGen.generateInserts(newCommitTime, 5);\n      HoodieWriteResult replaceResult = client.insertOverwrite(jsc.parallelize(records, 1), newCommitTime);\n      writeStatuses = replaceResult.getWriteStatuses().collect();\n      assertNoWriteErrors(writeStatuses);\n      assertTrue(metadata(client).isInSync());\n    }\n\n    \r\n    \r\n    Path inflightCleanPath = new Path(metaClient.getMetaPath(), HoodieTimeline.makeInflightCleanerFileName(inflightActionTimestamp));\n    fs.create(inflightCleanPath).close();\n\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      client.syncTableMetadata();\n\n      \r\n      HoodieBackedTableMetadataWriter writer =\n          (HoodieBackedTableMetadataWriter) SparkHoodieBackedTableMetadataWriter.create(hadoopConf, client.getConfig(), context);\n      assertEquals(writer.getMetadataReader().getUpdateTime().get(), beforeInflightActionTimestamp);\n\n      \r\n      HoodieTableMetadata metadata = HoodieTableMetadata.create(context, client.getConfig().getMetadataConfig(),\n          client.getConfig().getBasePath(), FileSystemViewStorageConfig.SPILLABLE_DIR.defaultValue());\n      assertEquals(((HoodieBackedTableMetadata)metadata).getReaderTime().get(), newCommitTime);\n\n      \r\n      fs.delete(inflightCleanPath, false);\n      client.syncTableMetadata();\n\n      writer =\n          (HoodieBackedTableMetadataWriter)SparkHoodieBackedTableMetadataWriter.create(hadoopConf, client.getConfig(), context);\n      assertEquals(writer.getMetadataReader().getUpdateTime().get(), newCommitTime);\n\n      \r\n      metadata = HoodieTableMetadata.create(context, client.getConfig().getMetadataConfig(),\n          client.getConfig().getBasePath(), FileSystemViewStorageConfig.SPILLABLE_DIR.defaultValue());\n      assertEquals(writer.getMetadataReader().getUpdateTime().get(), newCommitTime);\n    }\n\n    \r\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      client.restoreToInstant(restoreToInstant);\n      assertFalse(metadata(client).isInSync());\n\n      newCommitTime = HoodieActiveTimeline.createNewInstantTime();\n      client.startCommitWithTime(newCommitTime);\n      client.syncTableMetadata();\n\n      validateMetadata(client);\n      assertTrue(metadata(client).isInSync());\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":468,"status":"M"},{"authorDate":"2021-09-24 03:40:11","commitOrder":5,"curCode":"  public void testMetdataTableCommitFailure(HoodieTableType tableType) throws Exception {\n    init(tableType);\n    testTable.doWriteOperation(\"001\", INSERT, asList(\"p1\", \"p2\"), asList(\"p1\", \"p2\"), 2, true);\n    syncTableMetadata(writeConfig);\n    testTable.doWriteOperation(\"002\", INSERT, asList(\"p1\", \"p2\"), 2, true);\n    syncTableMetadata(writeConfig);\n\n    \r\n    HoodieTableMetaClient metadataMetaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(metadataTableBasePath).build();\n    HoodieActiveTimeline timeline = metadataMetaClient.getActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n\n    \r\n    FileCreateUtils.deleteDeltaCommit(metadataTableBasePath, \"002\");\n    timeline = metadataMetaClient.reloadActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(true, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n\n    \r\n    testTable.doWriteOperation(\"003\", BULK_INSERT, singletonList(\"p3\"), asList(\"p1\", \"p2\", \"p3\"), 2);\n    syncTableMetadata(writeConfig);\n\n    timeline = metadataMetaClient.reloadActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"003\")));\n    assertEquals(1, timeline.getRollbackTimeline().countInstants());\n  }\n","date":"2021-09-24 03:40:11","endLine":716,"groupId":"10650","id":8,"instanceNumber":2,"isCurCommit":1,"methodName":"testMetdataTableCommitFailure","params":"(HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/80/3c5b9d86af16690bab6643ba769283c3e22524.src","preCode":"  public void testMetdataTableCommitFailure() throws Exception {\n    init(HoodieTableType.COPY_ON_WRITE);\n    HoodieSparkEngineContext engineContext = new HoodieSparkEngineContext(jsc);\n\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      \r\n      String newCommitTime = \"001\";\n      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n      client.startCommitWithTime(newCommitTime);\n      List<WriteStatus> writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n\n      \r\n      newCommitTime = \"002\";\n      client.startCommitWithTime(newCommitTime);\n      records = dataGen.generateInserts(newCommitTime, 20);\n      writeStatuses = client.insert(jsc.parallelize(records, 1), newCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    \r\n    HoodieTableMetaClient metadataMetaClient = HoodieTableMetaClient.builder().setConf(hadoopConf).setBasePath(metadataTableBasePath).build();\n    HoodieActiveTimeline timeline = metadataMetaClient.getActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n\n    \r\n    FileCreateUtils.deleteDeltaCommit(metadataTableBasePath, \"002\");\n    timeline = metadataMetaClient.reloadActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(true, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n\n    \r\n    String latestCommitTime = HoodieActiveTimeline.createNewInstantTime();\n    try (SparkRDDWriteClient client = new SparkRDDWriteClient(engineContext, getWriteConfig(true, true))) {\n      String newCommitTime = \"003\";\n      List<HoodieRecord> records = dataGen.generateInserts(newCommitTime, 20);\n      client.startCommitWithTime(newCommitTime);\n      client.bulkInsert(jsc.parallelize(records, 1), newCommitTime).collect();\n\n      records = dataGen.generateInserts(latestCommitTime, 20);\n      client.startCommitWithTime(latestCommitTime);\n      List<WriteStatus> writeStatuses = client.bulkInsert(jsc.parallelize(records, 1), latestCommitTime).collect();\n      assertNoWriteErrors(writeStatuses);\n    }\n\n    timeline = metadataMetaClient.reloadActiveTimeline();\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"001\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, \"002\")));\n    assertTrue(timeline.containsInstant(new HoodieInstant(false, HoodieTimeline.DELTA_COMMIT_ACTION, latestCommitTime)));\n    assertTrue(timeline.getRollbackTimeline().countInstants() == 1);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/functional/TestHoodieBackedMetadata.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":688,"status":"M"}],"commitId":"eeafd24f4c3a65e107867e30eb499b4aec69d7e5","commitMessage":"@@@[HUDI-2395] Metadata tests rewrite (#3695)\n\n- Added commit metadata infra to test table so that we can test entire metadata using test table itself. These tests don't care about the contents of files as such and hence we should be able to test all code paths for metadata using test table.\n\nCo-authored-by: Sivabalan Narayanan <n.siva.b@gmail.com>","date":"2021-09-24 03:40:11","modifiedFileCount":"7","status":"M","submitter":"Sagar Sumit"}]
