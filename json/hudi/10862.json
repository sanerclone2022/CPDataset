[{"authorTime":"2021-05-08 14:27:56","codes":[{"authorDate":"2021-01-28 02:09:51","commitOrder":3,"curCode":"  protected HoodieMergeHandle getUpdateHandle(String partitionPath, String fileId, Iterator<HoodieRecord<T>> recordItr) {\n    if (table.requireSortedRecords()) {\n      return new HoodieSortedMergeHandle<>(config, instantTime, (HoodieSparkTable) table, recordItr, partitionPath, fileId, taskContextSupplier);\n    } else if (!WriteOperationType.isChangingRecords(operationType) && config.allowDuplicateInserts()) {\n      return new HoodieConcatHandle<>(config, instantTime, table, recordItr, partitionPath, fileId, taskContextSupplier);\n    } else {\n      return new HoodieMergeHandle<>(config, instantTime, table, recordItr, partitionPath, fileId, taskContextSupplier);\n    }\n  }\n","date":"2021-01-28 02:09:51","endLine":329,"groupId":"2654","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"getUpdateHandle","params":"(StringpartitionPath@StringfileId@Iterator<HoodieRecord<T>>recordItr)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/5a/4d79c7e63e86112f278e277952219a1639c043.src","preCode":"  protected HoodieMergeHandle getUpdateHandle(String partitionPath, String fileId, Iterator<HoodieRecord<T>> recordItr) {\n    if (table.requireSortedRecords()) {\n      return new HoodieSortedMergeHandle<>(config, instantTime, (HoodieSparkTable) table, recordItr, partitionPath, fileId, taskContextSupplier);\n    } else if (!WriteOperationType.isChangingRecords(operationType) && config.allowDuplicateInserts()) {\n      return new HoodieConcatHandle<>(config, instantTime, table, recordItr, partitionPath, fileId, taskContextSupplier);\n    } else {\n      return new HoodieMergeHandle<>(config, instantTime, table, recordItr, partitionPath, fileId, taskContextSupplier);\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":321,"status":"NB"},{"authorDate":"2021-05-08 14:27:56","commitOrder":3,"curCode":"  public FlinkMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T, I, K, O> hoodieTable,\n                          Iterator<HoodieRecord<T>> recordItr, String partitionPath, String fileId,\n                          TaskContextSupplier taskContextSupplier) {\n    super(config, instantTime, hoodieTable, recordItr, partitionPath, fileId, taskContextSupplier);\n    if (rolloverPaths == null) {\n      \r\n      rolloverPaths = new ArrayList<>();\n    }\n    \r\n    if (getAttemptId() > 0) {\n      deleteInvalidDataFile(getAttemptId() - 1);\n    }\n  }\n","date":"2021-05-08 14:27:56","endLine":89,"groupId":"382","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"FlinkMergeHandle","params":"(HoodieWriteConfigconfig@StringinstantTime@HoodieTable<T@I@K@O>hoodieTable@Iterator<HoodieRecord<T>>recordItr@StringpartitionPath@StringfileId@TaskContextSuppliertaskContextSupplier)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/02/3ee5f7483224090033f7a4f51569ab2fb5c41b.src","preCode":"  public FlinkMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T, I, K, O> hoodieTable,\n                          Iterator<HoodieRecord<T>> recordItr, String partitionPath, String fileId,\n                          TaskContextSupplier taskContextSupplier) {\n    super(config, instantTime, hoodieTable, recordItr, partitionPath, fileId, taskContextSupplier);\n    if (rolloverPaths == null) {\n      \r\n      rolloverPaths = new ArrayList<>();\n    }\n    \r\n    if (getAttemptId() > 0) {\n      deleteInvalidDataFile(getAttemptId() - 1);\n    }\n  }\n","realPath":"hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/FlinkMergeHandle.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":77,"status":"B"}],"commitId":"bfbf993cbe3f1e3fab93095f4342ed17423efab5","commitMessage":"@@@[HUDI-1878] Add max memory option for flink writer task (#2920)\n\nAlso removes the rate limiter because it has the similar functionality. \nmodify the create and merge handle cleans the retry files automatically.","date":"2021-05-08 14:27:56","modifiedFileCount":"10","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-07-27 05:21:04","codes":[{"authorDate":"2021-07-27 05:21:04","commitOrder":4,"curCode":"  protected HoodieMergeHandle getUpdateHandle(String partitionPath, String fileId, Iterator<HoodieRecord<T>> recordItr) {\n    if (table.requireSortedRecords()) {\n      return new HoodieSortedMergeHandle<>(config, instantTime, (HoodieSparkTable) table, recordItr, partitionPath, fileId, taskContextSupplier,\n          keyGeneratorOpt);\n    } else if (!WriteOperationType.isChangingRecords(operationType) && config.allowDuplicateInserts()) {\n      return new HoodieConcatHandle<>(config, instantTime, table, recordItr, partitionPath, fileId, taskContextSupplier, keyGeneratorOpt);\n    } else {\n      return new HoodieMergeHandle<>(config, instantTime, table, recordItr, partitionPath, fileId, taskContextSupplier, keyGeneratorOpt);\n    }\n  }\n","date":"2021-07-27 05:21:04","endLine":354,"groupId":"10862","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"getUpdateHandle","params":"(StringpartitionPath@StringfileId@Iterator<HoodieRecord<T>>recordItr)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/50/f3d68a881bb69fce5cb9d863f682a2f7bf2651.src","preCode":"  protected HoodieMergeHandle getUpdateHandle(String partitionPath, String fileId, Iterator<HoodieRecord<T>> recordItr) {\n    if (table.requireSortedRecords()) {\n      return new HoodieSortedMergeHandle<>(config, instantTime, (HoodieSparkTable) table, recordItr, partitionPath, fileId, taskContextSupplier);\n    } else if (!WriteOperationType.isChangingRecords(operationType) && config.allowDuplicateInserts()) {\n      return new HoodieConcatHandle<>(config, instantTime, table, recordItr, partitionPath, fileId, taskContextSupplier);\n    } else {\n      return new HoodieMergeHandle<>(config, instantTime, table, recordItr, partitionPath, fileId, taskContextSupplier);\n    }\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":345,"status":"M"},{"authorDate":"2021-07-27 05:21:04","commitOrder":4,"curCode":"  public FlinkMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T, I, K, O> hoodieTable,\n                          Iterator<HoodieRecord<T>> recordItr, String partitionPath, String fileId,\n                          TaskContextSupplier taskContextSupplier) {\n    super(config, instantTime, hoodieTable, recordItr, partitionPath, fileId, taskContextSupplier, Option.empty());\n    if (rolloverPaths == null) {\n      \r\n      rolloverPaths = new ArrayList<>();\n    }\n    \r\n    if (getAttemptId() > 0) {\n      deleteInvalidDataFile(getAttemptId() - 1);\n    }\n  }\n","date":"2021-07-27 05:21:04","endLine":78,"groupId":"10862","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"FlinkMergeHandle","params":"(HoodieWriteConfigconfig@StringinstantTime@HoodieTable<T@I@K@O>hoodieTable@Iterator<HoodieRecord<T>>recordItr@StringpartitionPath@StringfileId@TaskContextSuppliertaskContextSupplier)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/55/844a44f7d16670d91f35a31bebc5b592ca5c85.src","preCode":"  public FlinkMergeHandle(HoodieWriteConfig config, String instantTime, HoodieTable<T, I, K, O> hoodieTable,\n                          Iterator<HoodieRecord<T>> recordItr, String partitionPath, String fileId,\n                          TaskContextSupplier taskContextSupplier) {\n    super(config, instantTime, hoodieTable, recordItr, partitionPath, fileId, taskContextSupplier);\n    if (rolloverPaths == null) {\n      \r\n      rolloverPaths = new ArrayList<>();\n    }\n    \r\n    if (getAttemptId() > 0) {\n      deleteInvalidDataFile(getAttemptId() - 1);\n    }\n  }\n","realPath":"hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/io/FlinkMergeHandle.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":66,"status":"M"}],"commitId":"61148c1c43c9ff5ba04b6c174e9e2a006db9e7b3","commitMessage":"@@@[HUDI-2176.  2178.  2179] Adding virtual key support to COW table (#3306)\n\n","date":"2021-07-27 05:21:04","modifiedFileCount":"42","status":"M","submitter":"Sivabalan Narayanan"}]
