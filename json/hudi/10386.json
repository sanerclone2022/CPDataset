[{"authorTime":"2021-04-21 20:07:27","codes":[{"authorDate":"2021-04-16 11:40:53","commitOrder":2,"curCode":"  void testUpsertWithMiniBatches(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    options.put(FlinkOptions.WRITE_BATCH_SIZE.key(), \"0.001\");\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par1'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par1'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par1'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par1')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[id1,Sophia,18,1970-01-01T00:00:05,par1]\");\n  }\n","date":"2021-04-16 11:40:53","endLine":353,"groupId":"5600","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpsertWithMiniBatches","params":"(ExecModeexecMode)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/29/a7d7d0ae44a0bb3ff867caf4a31993d85d9cce.src","preCode":"  void testUpsertWithMiniBatches(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    options.put(FlinkOptions.WRITE_BATCH_SIZE.key(), \"0.001\");\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par1'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par1'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par1'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par1')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[id1,Sophia,18,1970-01-01T00:00:05,par1]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":329,"status":"NB"},{"authorDate":"2021-04-21 20:07:27","commitOrder":2,"curCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = \"create table t1(\\n\"\n        + \"  uuid varchar(20),\\n\"\n        + \"  name varchar(10),\\n\"\n        + \"  age int,\\n\"\n        + \"  ts timestamp(3),\\n\"\n        + \"  `partition` varchar(20),\\n\"\n        + \"  PRIMARY KEY(uuid) NOT ENFORCED\\n\"\n        + \")\\n\"\n        + \"with (\\n\"\n        + \"  'connector' = 'hudi',\\n\"\n        + \"  'path' = '\" + tempFile.getAbsolutePath() + \"'\\n\"\n        + \")\";\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[id1,Sophia,18,1970-01-01T00:00:05,par5]\");\n  }\n","date":"2021-04-21 20:07:27","endLine":389,"groupId":"5601","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testWriteNonPartitionedTable","params":"(ExecModeexecMode)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/fe/652c5ffe74fd8523ceeec8845065e66162b189.src","preCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = \"create table t1(\\n\"\n        + \"  uuid varchar(20),\\n\"\n        + \"  name varchar(10),\\n\"\n        + \"  age int,\\n\"\n        + \"  ts timestamp(3),\\n\"\n        + \"  `partition` varchar(20),\\n\"\n        + \"  PRIMARY KEY(uuid) NOT ENFORCED\\n\"\n        + \")\\n\"\n        + \"with (\\n\"\n        + \"  'connector' = 'hudi',\\n\"\n        + \"  'path' = '\" + tempFile.getAbsolutePath() + \"'\\n\"\n        + \")\";\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[id1,Sophia,18,1970-01-01T00:00:05,par5]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":357,"status":"B"}],"commitId":"ac3589f00659985c39ef29e5edd089279f6c2f70","commitMessage":"@@@[HUDI-1814] Non partitioned table for Flink writer (#2859)\n\n","date":"2021-04-21 20:07:27","modifiedFileCount":"4","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-04-21 20:07:27","codes":[{"authorDate":"2021-04-25 23:06:53","commitOrder":3,"curCode":"  void testUpsertWithMiniBatches(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    options.put(FlinkOptions.WRITE_BUCKET_SIZE.key(), \"0.001\");\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par1'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par1'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par1'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par1')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[id1,Sophia,18,1970-01-01T00:00:05,par1]\");\n  }\n","date":"2021-04-25 23:06:53","endLine":353,"groupId":"5600","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpsertWithMiniBatches","params":"(ExecModeexecMode)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/2f/2dcb26bf17a1150e98d91e37bf4a75d170efbd.src","preCode":"  void testUpsertWithMiniBatches(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    options.put(FlinkOptions.WRITE_BATCH_SIZE.key(), \"0.001\");\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par1'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par1'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par1'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par1')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[id1,Sophia,18,1970-01-01T00:00:05,par1]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":329,"status":"M"},{"authorDate":"2021-04-21 20:07:27","commitOrder":3,"curCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = \"create table t1(\\n\"\n        + \"  uuid varchar(20),\\n\"\n        + \"  name varchar(10),\\n\"\n        + \"  age int,\\n\"\n        + \"  ts timestamp(3),\\n\"\n        + \"  `partition` varchar(20),\\n\"\n        + \"  PRIMARY KEY(uuid) NOT ENFORCED\\n\"\n        + \")\\n\"\n        + \"with (\\n\"\n        + \"  'connector' = 'hudi',\\n\"\n        + \"  'path' = '\" + tempFile.getAbsolutePath() + \"'\\n\"\n        + \")\";\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[id1,Sophia,18,1970-01-01T00:00:05,par5]\");\n  }\n","date":"2021-04-21 20:07:27","endLine":389,"groupId":"5601","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testWriteNonPartitionedTable","params":"(ExecModeexecMode)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/fe/652c5ffe74fd8523ceeec8845065e66162b189.src","preCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = \"create table t1(\\n\"\n        + \"  uuid varchar(20),\\n\"\n        + \"  name varchar(10),\\n\"\n        + \"  age int,\\n\"\n        + \"  ts timestamp(3),\\n\"\n        + \"  `partition` varchar(20),\\n\"\n        + \"  PRIMARY KEY(uuid) NOT ENFORCED\\n\"\n        + \")\\n\"\n        + \"with (\\n\"\n        + \"  'connector' = 'hudi',\\n\"\n        + \"  'path' = '\" + tempFile.getAbsolutePath() + \"'\\n\"\n        + \")\";\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[id1,Sophia,18,1970-01-01T00:00:05,par5]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":357,"status":"N"}],"commitId":"1b27259b530225e3c76fda684f888cad78045d3c","commitMessage":"@@@[HUDI-1844] Add option to flush when total buckets memory exceeds the threshold (#2877)\n\nCurrent code supports flushing as per-bucket memory usage.  while the\nbuckets may still take too much memory for bootstrap from history data.\n\nWhen the threshold hits.  flush out half of the buckets with bigger\nbuffer size.","date":"2021-04-25 23:06:53","modifiedFileCount":"4","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-04-21 20:07:27","codes":[{"authorDate":"2021-04-29 20:32:10","commitOrder":4,"curCode":"  void testUpsertWithMiniBatches(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    options.put(FlinkOptions.WRITE_BATCH_SIZE.key(), \"0.001\");\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par1'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par1'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par1'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par1')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[id1,Sophia,18,1970-01-01T00:00:05,par1]\");\n  }\n","date":"2021-04-29 20:32:10","endLine":353,"groupId":"5600","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpsertWithMiniBatches","params":"(ExecModeexecMode)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/fe/652c5ffe74fd8523ceeec8845065e66162b189.src","preCode":"  void testUpsertWithMiniBatches(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    options.put(FlinkOptions.WRITE_BUCKET_SIZE.key(), \"0.001\");\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par1'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par1'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par1'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par1')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[id1,Sophia,18,1970-01-01T00:00:05,par1]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":329,"status":"M"},{"authorDate":"2021-04-21 20:07:27","commitOrder":4,"curCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = \"create table t1(\\n\"\n        + \"  uuid varchar(20),\\n\"\n        + \"  name varchar(10),\\n\"\n        + \"  age int,\\n\"\n        + \"  ts timestamp(3),\\n\"\n        + \"  `partition` varchar(20),\\n\"\n        + \"  PRIMARY KEY(uuid) NOT ENFORCED\\n\"\n        + \")\\n\"\n        + \"with (\\n\"\n        + \"  'connector' = 'hudi',\\n\"\n        + \"  'path' = '\" + tempFile.getAbsolutePath() + \"'\\n\"\n        + \")\";\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[id1,Sophia,18,1970-01-01T00:00:05,par5]\");\n  }\n","date":"2021-04-21 20:07:27","endLine":389,"groupId":"5601","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testWriteNonPartitionedTable","params":"(ExecModeexecMode)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/fe/652c5ffe74fd8523ceeec8845065e66162b189.src","preCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = \"create table t1(\\n\"\n        + \"  uuid varchar(20),\\n\"\n        + \"  name varchar(10),\\n\"\n        + \"  age int,\\n\"\n        + \"  ts timestamp(3),\\n\"\n        + \"  `partition` varchar(20),\\n\"\n        + \"  PRIMARY KEY(uuid) NOT ENFORCED\\n\"\n        + \")\\n\"\n        + \"with (\\n\"\n        + \"  'connector' = 'hudi',\\n\"\n        + \"  'path' = '\" + tempFile.getAbsolutePath() + \"'\\n\"\n        + \")\";\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[id1,Sophia,18,1970-01-01T00:00:05,par5]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":357,"status":"N"}],"commitId":"6e9c5dd76548911abd346fad61a0dd6d3c623af9","commitMessage":"@@@[HUDI-1863] Add rate limiter to Flink writer to avoid OOM for bootstrap (#2891)\n\n","date":"2021-04-29 20:32:10","modifiedFileCount":"6","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-08-16 18:14:05","codes":[{"authorDate":"2021-08-16 18:14:05","commitOrder":5,"curCode":"  void testUpsertWithMiniBatches(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    options.put(FlinkOptions.WRITE_BATCH_SIZE.key(), \"0.001\");\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par1'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par1'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par1'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par1')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par1]]\");\n  }\n","date":"2021-08-16 18:14:05","endLine":433,"groupId":"5600","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpsertWithMiniBatches","params":"(ExecModeexecMode)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/07/64f5586ea801d7feba77591330d849b66102fc.src","preCode":"  void testUpsertWithMiniBatches(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    options.put(FlinkOptions.WRITE_BATCH_SIZE.key(), \"0.001\");\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par1'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par1'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par1'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par1')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[id1,Sophia,18,1970-01-01T00:00:05,par1]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":409,"status":"M"},{"authorDate":"2021-08-16 18:14:05","commitOrder":5,"curCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = \"create table t1(\\n\"\n        + \"  uuid varchar(20),\\n\"\n        + \"  name varchar(10),\\n\"\n        + \"  age int,\\n\"\n        + \"  ts timestamp(3),\\n\"\n        + \"  `partition` varchar(20),\\n\"\n        + \"  PRIMARY KEY(uuid) NOT ENFORCED\\n\"\n        + \")\\n\"\n        + \"with (\\n\"\n        + \"  'connector' = 'hudi',\\n\"\n        + \"  'path' = '\" + tempFile.getAbsolutePath() + \"'\\n\"\n        + \")\";\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\");\n  }\n","date":"2021-08-16 18:14:05","endLine":469,"groupId":"5601","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testWriteNonPartitionedTable","params":"(ExecModeexecMode)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/07/64f5586ea801d7feba77591330d849b66102fc.src","preCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = \"create table t1(\\n\"\n        + \"  uuid varchar(20),\\n\"\n        + \"  name varchar(10),\\n\"\n        + \"  age int,\\n\"\n        + \"  ts timestamp(3),\\n\"\n        + \"  `partition` varchar(20),\\n\"\n        + \"  PRIMARY KEY(uuid) NOT ENFORCED\\n\"\n        + \")\\n\"\n        + \"with (\\n\"\n        + \"  'connector' = 'hudi',\\n\"\n        + \"  'path' = '\" + tempFile.getAbsolutePath() + \"'\\n\"\n        + \")\";\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[id1,Sophia,18,1970-01-01T00:00:05,par5]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":437,"status":"M"}],"commitId":"66f951322a3872073b86896fa5c10b51a0f6e4ab","commitMessage":"@@@[HUDI-2191] Bump flink version to 1.13.1 (#3291)\n\n","date":"2021-08-16 18:14:05","modifiedFileCount":"17","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-08-19 17:15:26","codes":[{"authorDate":"2021-08-16 18:14:05","commitOrder":6,"curCode":"  void testUpsertWithMiniBatches(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    options.put(FlinkOptions.WRITE_BATCH_SIZE.key(), \"0.001\");\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par1'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par1'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par1'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par1')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par1]]\");\n  }\n","date":"2021-08-16 18:14:05","endLine":433,"groupId":"5600","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpsertWithMiniBatches","params":"(ExecModeexecMode)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/07/64f5586ea801d7feba77591330d849b66102fc.src","preCode":"  void testUpsertWithMiniBatches(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    options.put(FlinkOptions.WRITE_BATCH_SIZE.key(), \"0.001\");\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par1'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par1'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par1'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par1')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par1]]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":409,"status":"N"},{"authorDate":"2021-08-19 17:15:26","commitOrder":6,"curCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options, false);\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\");\n  }\n","date":"2021-08-19 17:15:26","endLine":510,"groupId":"0","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testWriteNonPartitionedTable","params":"(ExecModeexecMode)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/99/effba3e6f09d7321e176a048413d962b7d0809.src","preCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = \"create table t1(\\n\"\n        + \"  uuid varchar(20),\\n\"\n        + \"  name varchar(10),\\n\"\n        + \"  age int,\\n\"\n        + \"  ts timestamp(3),\\n\"\n        + \"  `partition` varchar(20),\\n\"\n        + \"  PRIMARY KEY(uuid) NOT ENFORCED\\n\"\n        + \")\\n\"\n        + \"with (\\n\"\n        + \"  'connector' = 'hudi',\\n\"\n        + \"  'path' = '\" + tempFile.getAbsolutePath() + \"'\\n\"\n        + \")\";\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":487,"status":"M"}],"commitId":"1fed44af84b4726d40c57f1dad012c8e4a510f91","commitMessage":"@@@[HUDI-2316] Support Flink batch upsert (#3494)\n\n","date":"2021-08-19 17:15:26","modifiedFileCount":"6","status":"M","submitter":"swuferhong"},{"authorTime":"2021-08-19 23:21:20","codes":[{"authorDate":"2021-08-19 23:21:20","commitOrder":7,"curCode":"  void testUpsertWithMiniBatches(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.WRITE_BATCH_SIZE, \"0.001\")\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par1'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par1'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par1'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par1')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par1]]\");\n  }\n","date":"2021-08-19 23:21:20","endLine":489,"groupId":"789","id":11,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpsertWithMiniBatches","params":"(ExecModeexecMode)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9e/ffdcc8c67f59ad51cfd9a0e5ed56ecd41b530e.src","preCode":"  void testUpsertWithMiniBatches(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    options.put(FlinkOptions.WRITE_BATCH_SIZE.key(), \"0.001\");\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options);\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par1'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par1'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par1'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par1')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par1]]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":465,"status":"M"},{"authorDate":"2021-08-19 23:21:20","commitOrder":7,"curCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .withPartition(false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\");\n  }\n","date":"2021-08-19 23:21:20","endLine":517,"groupId":"2976","id":12,"instanceNumber":2,"isCurCommit":0,"methodName":"testWriteNonPartitionedTable","params":"(ExecModeexecMode)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9e/ffdcc8c67f59ad51cfd9a0e5ed56ecd41b530e.src","preCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    Map<String, String> options = new HashMap<>();\n    options.put(FlinkOptions.PATH.key(), tempFile.getAbsolutePath());\n    String hoodieTableDDL = TestConfigurations.getCreateHoodieTableDDL(\"t1\", options, false);\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":493,"status":"M"}],"commitId":"9762e4c08c0ff953cc62e72b1295db4fd4c002c5","commitMessage":"@@@[MINOR] Some cosmetic changes for Flink (#3503)\n\n","date":"2021-08-19 23:21:20","modifiedFileCount":"6","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-09-11 13:17:16","codes":[{"authorDate":"2021-08-19 23:21:20","commitOrder":8,"curCode":"  void testUpsertWithMiniBatches(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.WRITE_BATCH_SIZE, \"0.001\")\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par1'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par1'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par1'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par1')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par1]]\");\n  }\n","date":"2021-08-19 23:21:20","endLine":489,"groupId":"789","id":13,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpsertWithMiniBatches","params":"(ExecModeexecMode)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9e/ffdcc8c67f59ad51cfd9a0e5ed56ecd41b530e.src","preCode":"  void testUpsertWithMiniBatches(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.WRITE_BATCH_SIZE, \"0.001\")\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par1'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par1'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par1'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par1')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par1]]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":465,"status":"N"},{"authorDate":"2021-09-11 13:17:16","commitOrder":8,"curCode":"  void testWriteNonPartitionedTable(ExecMode execMode, HoodieTableType tableType) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .withPartition(false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\");\n  }\n","date":"2021-09-11 13:17:16","endLine":586,"groupId":"2976","id":14,"instanceNumber":2,"isCurCommit":0,"methodName":"testWriteNonPartitionedTable","params":"(ExecModeexecMode@HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/5b/e603f7838e5195aa26e8cf17398d597ca9ee2f.src","preCode":"  void testWriteNonPartitionedTable(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .withPartition(false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":561,"status":"M"}],"commitId":"b30c5bdaef77aee9f564ac24f80f5c364014bb17","commitMessage":"@@@[HUDI-2412] Add timestamp based partitioning for flink writer (#3638)\n\n","date":"2021-09-11 13:17:16","modifiedFileCount":"11","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-09-15 12:04:46","codes":[{"authorDate":"2021-08-19 23:21:20","commitOrder":9,"curCode":"  void testUpsertWithMiniBatches(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.WRITE_BATCH_SIZE, \"0.001\")\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par1'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par1'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par1'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par1')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par1]]\");\n  }\n","date":"2021-08-19 23:21:20","endLine":489,"groupId":"10386","id":15,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpsertWithMiniBatches","params":"(ExecModeexecMode)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9e/ffdcc8c67f59ad51cfd9a0e5ed56ecd41b530e.src","preCode":"  void testUpsertWithMiniBatches(ExecMode execMode) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.WRITE_BATCH_SIZE, \"0.001\")\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par1'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par1'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par1'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par1')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par1]]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":465,"status":"N"},{"authorDate":"2021-09-15 12:04:46","commitOrder":9,"curCode":"  void testWriteNonPartitionedTable(ExecMode execMode, HoodieTableType tableType) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .noPartition()\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\");\n  }\n","date":"2021-09-15 12:04:46","endLine":586,"groupId":"10386","id":16,"instanceNumber":2,"isCurCommit":0,"methodName":"testWriteNonPartitionedTable","params":"(ExecModeexecMode@HoodieTableTypetableType)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9d/0bcabac6aaad159803c08004869e0ff2db0462.src","preCode":"  void testWriteNonPartitionedTable(ExecMode execMode, HoodieTableType tableType) {\n    TableEnvironment tableEnv = execMode == ExecMode.BATCH ? batchTableEnv : streamTableEnv;\n    String hoodieTableDDL = sql(\"t1\")\n        .option(FlinkOptions.PATH, tempFile.getAbsolutePath())\n        .option(FlinkOptions.TABLE_TYPE, tableType)\n        .withPartition(false)\n        .end();\n    tableEnv.executeSql(hoodieTableDDL);\n\n    final String insertInto1 = \"insert into t1 values\\n\"\n        + \"('id1','Danny',23,TIMESTAMP '1970-01-01 00:00:01','par1')\";\n\n    execInsertSql(tableEnv, insertInto1);\n\n    final String insertInto2 = \"insert into t1 values\\n\"\n        + \"('id1','Stephen',33,TIMESTAMP '1970-01-01 00:00:02','par2'),\\n\"\n        + \"('id1','Julian',53,TIMESTAMP '1970-01-01 00:00:03','par3'),\\n\"\n        + \"('id1','Fabian',31,TIMESTAMP '1970-01-01 00:00:04','par4'),\\n\"\n        + \"('id1','Sophia',18,TIMESTAMP '1970-01-01 00:00:05','par5')\";\n\n    execInsertSql(tableEnv, insertInto2);\n\n    List<Row> result = CollectionUtil.iterableToList(\n        () -> tableEnv.sqlQuery(\"select * from t1\").execute().collect());\n    assertRowsEquals(result, \"[+I[id1, Sophia, 18, 1970-01-01T00:00:05, par5]]\");\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/table/HoodieDataSourceITCase.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":561,"status":"M"}],"commitId":"627f20f9c54a87dded3350b73a6327f5b95632f6","commitMessage":"@@@[HUDI-2430] Make decimal compatible with hudi for flink writer (#3658)\n\n","date":"2021-09-15 12:04:46","modifiedFileCount":"5","status":"M","submitter":"Danny Chan"}]
