[{"authorTime":"2019-08-12 08:48:17","codes":[{"authorDate":"2020-10-02 05:25:29","commitOrder":2,"curCode":"  protected void initDFS() throws IOException {\n    FileSystem.closeAll();\n    hdfsTestService = new HdfsTestService();\n    dfsCluster = hdfsTestService.start(true);\n\n    \r\n    dfs = dfsCluster.getFileSystem();\n    dfsBasePath = dfs.getWorkingDirectory().toString();\n    this.basePath = dfsBasePath;\n    this.hadoopConf = dfs.getConf();\n    dfs.mkdirs(new Path(dfsBasePath));\n  }\n","date":"2020-10-02 05:25:29","endLine":274,"groupId":"2461","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"initDFS","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/b1/0781e3b8ec9dac667537fcea8e44ee8c577868.src","preCode":"  protected void initDFS() throws IOException {\n    FileSystem.closeAll();\n    hdfsTestService = new HdfsTestService();\n    dfsCluster = hdfsTestService.start(true);\n\n    \r\n    dfs = dfsCluster.getFileSystem();\n    dfsBasePath = dfs.getWorkingDirectory().toString();\n    this.basePath = dfsBasePath;\n    this.hadoopConf = dfs.getConf();\n    dfs.mkdirs(new Path(dfsBasePath));\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestHarness.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":263,"status":"B"},{"authorDate":"2019-08-12 08:48:17","commitOrder":2,"curCode":"  public static void initClass() throws Exception {\n    hdfsTestService = new HdfsTestService();\n    dfsCluster = hdfsTestService.start(true);\n    \r\n    dfs = dfsCluster.getFileSystem();\n    dfsBasePath = dfs.getWorkingDirectory().toString();\n    dfs.mkdirs(new Path(dfsBasePath));\n\n    \r\n    Path filePath = new Path(dfsBasePath + \"/t1.props\");\n    writePropertiesFile(filePath, new String[]{\n        \"\", \"#comment\", \"abc\",\r\n        \"int.prop=123\", \"double.prop=113.4\", \"string.prop=str\", \"boolean.prop=true\", \"long.prop=1354354354\"\n    });\n\n    filePath = new Path(dfsBasePath + \"/t2.props\");\n    writePropertiesFile(filePath, new String[]{\n        \"string.prop=ignored\", \"include=t1.props\"\n    });\n\n    filePath = new Path(dfsBasePath + \"/t3.props\");\n    writePropertiesFile(filePath, new String[]{\n        \"double.prop=838.3\", \"include = t2.props\", \"double.prop=243.4\", \"string.prop=t3.value\"\n    });\n\n    filePath = new Path(dfsBasePath + \"/t4.props\");\n    writePropertiesFile(filePath, new String[]{\n        \"double.prop=838.3\", \"include = t4.props\"\n    });\n  }\n","date":"2019-08-12 08:48:17","endLine":75,"groupId":"2461","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"initClass","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/36/d37a686f543437e2f51ea9559e8ef94a548847.src","preCode":"  public static void initClass() throws Exception {\n    hdfsTestService = new HdfsTestService();\n    dfsCluster = hdfsTestService.start(true);\n    \r\n    dfs = dfsCluster.getFileSystem();\n    dfsBasePath = dfs.getWorkingDirectory().toString();\n    dfs.mkdirs(new Path(dfsBasePath));\n\n    \r\n    Path filePath = new Path(dfsBasePath + \"/t1.props\");\n    writePropertiesFile(filePath, new String[]{\n        \"\", \"#comment\", \"abc\",\r\n        \"int.prop=123\", \"double.prop=113.4\", \"string.prop=str\", \"boolean.prop=true\", \"long.prop=1354354354\"\n    });\n\n    filePath = new Path(dfsBasePath + \"/t2.props\");\n    writePropertiesFile(filePath, new String[]{\n        \"string.prop=ignored\", \"include=t1.props\"\n    });\n\n    filePath = new Path(dfsBasePath + \"/t3.props\");\n    writePropertiesFile(filePath, new String[]{\n        \"double.prop=838.3\", \"include = t2.props\", \"double.prop=243.4\", \"string.prop=t3.value\"\n    });\n\n    filePath = new Path(dfsBasePath + \"/t4.props\");\n    writePropertiesFile(filePath, new String[]{\n        \"double.prop=838.3\", \"include = t4.props\"\n    });\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/util/TestDFSPropertiesConfiguration.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":46,"status":"NB"}],"commitId":"1f7add92916c37b05be270d9c75a9042134ec506","commitMessage":"@@@[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)\n\n- This change breaks `hudi-client` into `hudi-client-common` and `hudi-spark-client` modules \n- Simple usages of Spark using jsc.parallelize() has been redone using EngineContext#map.  EngineContext#flatMap etc\n- Code changes in the PR.  break classes into `BaseXYZ` parent classes with no spark dependencies living in `hudi-client-common`\n- Classes on `hudi-spark-client` are named `SparkXYZ` extending the parent classes with all the Spark dependencies\n- To simplify/cleanup.  HoodieIndex#fetchRecordLocation has been removed and its usages in tests replaced with alternatives\n\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2020-10-02 05:25:29","modifiedFileCount":"31","status":"M","submitter":"Mathieu"},{"authorTime":"2019-08-12 08:48:17","codes":[{"authorDate":"2021-03-08 17:36:03","commitOrder":3,"curCode":"  protected void initDFS() throws IOException {\n    hdfsTestService = new HdfsTestService();\n    dfsCluster = hdfsTestService.start(true);\n\n    \r\n    dfs = dfsCluster.getFileSystem();\n    dfsBasePath = dfs.getWorkingDirectory().toString();\n    this.basePath = dfsBasePath;\n    this.hadoopConf = dfs.getConf();\n    dfs.mkdirs(new Path(dfsBasePath));\n  }\n","date":"2021-03-08 17:36:03","endLine":264,"groupId":"10622","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"initDFS","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/72/b277128162decbe0b596b722a6c9f571fcd3de.src","preCode":"  protected void initDFS() throws IOException {\n    FileSystem.closeAll();\n    hdfsTestService = new HdfsTestService();\n    dfsCluster = hdfsTestService.start(true);\n\n    \r\n    dfs = dfsCluster.getFileSystem();\n    dfsBasePath = dfs.getWorkingDirectory().toString();\n    this.basePath = dfsBasePath;\n    this.hadoopConf = dfs.getConf();\n    dfs.mkdirs(new Path(dfsBasePath));\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestHarness.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":254,"status":"M"},{"authorDate":"2019-08-12 08:48:17","commitOrder":3,"curCode":"  public static void initClass() throws Exception {\n    hdfsTestService = new HdfsTestService();\n    dfsCluster = hdfsTestService.start(true);\n    \r\n    dfs = dfsCluster.getFileSystem();\n    dfsBasePath = dfs.getWorkingDirectory().toString();\n    dfs.mkdirs(new Path(dfsBasePath));\n\n    \r\n    Path filePath = new Path(dfsBasePath + \"/t1.props\");\n    writePropertiesFile(filePath, new String[]{\n        \"\", \"#comment\", \"abc\",\r\n        \"int.prop=123\", \"double.prop=113.4\", \"string.prop=str\", \"boolean.prop=true\", \"long.prop=1354354354\"\n    });\n\n    filePath = new Path(dfsBasePath + \"/t2.props\");\n    writePropertiesFile(filePath, new String[]{\n        \"string.prop=ignored\", \"include=t1.props\"\n    });\n\n    filePath = new Path(dfsBasePath + \"/t3.props\");\n    writePropertiesFile(filePath, new String[]{\n        \"double.prop=838.3\", \"include = t2.props\", \"double.prop=243.4\", \"string.prop=t3.value\"\n    });\n\n    filePath = new Path(dfsBasePath + \"/t4.props\");\n    writePropertiesFile(filePath, new String[]{\n        \"double.prop=838.3\", \"include = t4.props\"\n    });\n  }\n","date":"2019-08-12 08:48:17","endLine":75,"groupId":"10622","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"initClass","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/36/d37a686f543437e2f51ea9559e8ef94a548847.src","preCode":"  public static void initClass() throws Exception {\n    hdfsTestService = new HdfsTestService();\n    dfsCluster = hdfsTestService.start(true);\n    \r\n    dfs = dfsCluster.getFileSystem();\n    dfsBasePath = dfs.getWorkingDirectory().toString();\n    dfs.mkdirs(new Path(dfsBasePath));\n\n    \r\n    Path filePath = new Path(dfsBasePath + \"/t1.props\");\n    writePropertiesFile(filePath, new String[]{\n        \"\", \"#comment\", \"abc\",\r\n        \"int.prop=123\", \"double.prop=113.4\", \"string.prop=str\", \"boolean.prop=true\", \"long.prop=1354354354\"\n    });\n\n    filePath = new Path(dfsBasePath + \"/t2.props\");\n    writePropertiesFile(filePath, new String[]{\n        \"string.prop=ignored\", \"include=t1.props\"\n    });\n\n    filePath = new Path(dfsBasePath + \"/t3.props\");\n    writePropertiesFile(filePath, new String[]{\n        \"double.prop=838.3\", \"include = t2.props\", \"double.prop=243.4\", \"string.prop=t3.value\"\n    });\n\n    filePath = new Path(dfsBasePath + \"/t4.props\");\n    writePropertiesFile(filePath, new String[]{\n        \"double.prop=838.3\", \"include = t4.props\"\n    });\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/util/TestDFSPropertiesConfiguration.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":46,"status":"N"}],"commitId":"d3a451611cc01a65cc0f305c72c04d64af0e7d38","commitMessage":"@@@[MINOR] HoodieClientTestHarness close resources in AfterAll phase (#2646)\n\nParameterized test case like `org.apache.hudi.table.upgrade.TestUpgradeDowngrade#testUpgrade` incurs flakiness when org.apache.hadoop.fs.FileSystem#closeAll is invoked at BeforeEach; it should be invoked in AfterAll instead.","date":"2021-03-08 17:36:03","modifiedFileCount":"1","status":"M","submitter":"Raymond Xu"}]
