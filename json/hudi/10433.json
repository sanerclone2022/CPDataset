[{"authorTime":"2021-03-10 22:44:06","codes":[{"authorDate":"2021-03-10 22:44:06","commitOrder":1,"curCode":"  public void testConsumeFromLatestCommit() throws Exception {\n    TestData.writeData(TestData.DATA_SET_ONE, conf);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().noneMatch(split -> split.getInstantRange().isPresent()),\n          \"No instants should have range limit\");\n\n      Thread.sleep(1000L);\n\n      \r\n      latch = new CountDownLatch(4);\n      sourceContext.reset(latch);\n\n      \r\n      TestData.writeData(TestData.DATA_SET_TWO, conf);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n\n      \r\n      function.close();\n    }\n  }\n","date":"2021-03-10 22:44:06","endLine":109,"groupId":"4687","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testConsumeFromLatestCommit","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/f0/2a28c0ffe0b3a700f82c2425bf7e26faf24969.src","preCode":"  public void testConsumeFromLatestCommit() throws Exception {\n    TestData.writeData(TestData.DATA_SET_ONE, conf);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().noneMatch(split -> split.getInstantRange().isPresent()),\n          \"No instants should have range limit\");\n\n      Thread.sleep(1000L);\n\n      \r\n      latch = new CountDownLatch(4);\n      sourceContext.reset(latch);\n\n      \r\n      TestData.writeData(TestData.DATA_SET_TWO, conf);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n\n      \r\n      function.close();\n    }\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/source/TestStreamReadMonitoringFunction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":73,"status":"B"},{"authorDate":"2021-03-10 22:44:06","commitOrder":1,"curCode":"  public void testConsumeFromSpecifiedCommit() throws Exception {\n    \r\n    \r\n    TestData.writeData(TestData.DATA_SET_ONE, conf);\n    TestData.writeData(TestData.DATA_SET_TWO, conf);\n    String specifiedCommit = TestUtils.getLatestCommit(tempFile.getAbsolutePath());\n    conf.setString(FlinkOptions.READ_STREAMING_START_COMMIT, specifiedCommit);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getLatestCommit().equals(specifiedCommit)),\n          \"All the splits should be with specified instant time\");\n\n      \r\n      function.close();\n    }\n  }\n","date":"2021-03-10 22:44:06","endLine":140,"groupId":"1544","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testConsumeFromSpecifiedCommit","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/f0/2a28c0ffe0b3a700f82c2425bf7e26faf24969.src","preCode":"  public void testConsumeFromSpecifiedCommit() throws Exception {\n    \r\n    \r\n    TestData.writeData(TestData.DATA_SET_ONE, conf);\n    TestData.writeData(TestData.DATA_SET_TWO, conf);\n    String specifiedCommit = TestUtils.getLatestCommit(tempFile.getAbsolutePath());\n    conf.setString(FlinkOptions.READ_STREAMING_START_COMMIT, specifiedCommit);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getLatestCommit().equals(specifiedCommit)),\n          \"All the splits should be with specified instant time\");\n\n      \r\n      function.close();\n    }\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/source/TestStreamReadMonitoringFunction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":112,"status":"B"}],"commitId":"2fdae6835ce3fcad3111205d2373a69b34788483","commitMessage":"@@@[HUDI-1663] Streaming read for Flink MOR table (#2640)\n\nSupports two read modes:\n* Read the full data set starting from the latest commit instant and\n  subsequent incremental data set\n* Read data set that starts from a specified commit instant","date":"2021-03-10 22:44:06","modifiedFileCount":"13","status":"B","submitter":"Danny Chan"},{"authorTime":"2021-03-11 19:44:06","codes":[{"authorDate":"2021-03-11 19:44:06","commitOrder":2,"curCode":"  public void testConsumeFromLatestCommit() throws Exception {\n    TestData.writeData(TestData.DATA_SET_INSERT, conf);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().noneMatch(split -> split.getInstantRange().isPresent()),\n          \"No instants should have range limit\");\n\n      Thread.sleep(1000L);\n\n      \r\n      latch = new CountDownLatch(4);\n      sourceContext.reset(latch);\n\n      \r\n      TestData.writeData(TestData.DATA_SET_UPDATE_INSERT, conf);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n\n      \r\n      function.close();\n    }\n  }\n","date":"2021-03-11 19:44:06","endLine":109,"groupId":"4687","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testConsumeFromLatestCommit","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/47/33fa07b673bb8f902e45c1a1fd5369f881fd83.src","preCode":"  public void testConsumeFromLatestCommit() throws Exception {\n    TestData.writeData(TestData.DATA_SET_ONE, conf);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().noneMatch(split -> split.getInstantRange().isPresent()),\n          \"No instants should have range limit\");\n\n      Thread.sleep(1000L);\n\n      \r\n      latch = new CountDownLatch(4);\n      sourceContext.reset(latch);\n\n      \r\n      TestData.writeData(TestData.DATA_SET_TWO, conf);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n\n      \r\n      function.close();\n    }\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/source/TestStreamReadMonitoringFunction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":73,"status":"M"},{"authorDate":"2021-03-11 19:44:06","commitOrder":2,"curCode":"  public void testConsumeFromSpecifiedCommit() throws Exception {\n    \r\n    \r\n    TestData.writeData(TestData.DATA_SET_INSERT, conf);\n    TestData.writeData(TestData.DATA_SET_UPDATE_INSERT, conf);\n    String specifiedCommit = TestUtils.getLatestCommit(tempFile.getAbsolutePath());\n    conf.setString(FlinkOptions.READ_STREAMING_START_COMMIT, specifiedCommit);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getLatestCommit().equals(specifiedCommit)),\n          \"All the splits should be with specified instant time\");\n\n      \r\n      function.close();\n    }\n  }\n","date":"2021-03-11 19:44:06","endLine":140,"groupId":"1544","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testConsumeFromSpecifiedCommit","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/47/33fa07b673bb8f902e45c1a1fd5369f881fd83.src","preCode":"  public void testConsumeFromSpecifiedCommit() throws Exception {\n    \r\n    \r\n    TestData.writeData(TestData.DATA_SET_ONE, conf);\n    TestData.writeData(TestData.DATA_SET_TWO, conf);\n    String specifiedCommit = TestUtils.getLatestCommit(tempFile.getAbsolutePath());\n    conf.setString(FlinkOptions.READ_STREAMING_START_COMMIT, specifiedCommit);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getLatestCommit().equals(specifiedCommit)),\n          \"All the splits should be with specified instant time\");\n\n      \r\n      function.close();\n    }\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/source/TestStreamReadMonitoringFunction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":112,"status":"M"}],"commitId":"12ff562d2bd77b6dc8676eac79cbd631185dac3c","commitMessage":"@@@[HUDI-1678] Row level delete for Flink sink (#2659)\n\n","date":"2021-03-11 19:44:06","modifiedFileCount":"8","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-03-11 19:44:06","codes":[{"authorDate":"2021-07-30 14:25:05","commitOrder":3,"curCode":"  public void testConsumeFromLatestCommit() throws Exception {\n    \r\n    TestData.writeData(TestData.DATA_SET_INSERT, conf);\n    TestData.writeData(TestData.DATA_SET_UPDATE_INSERT, conf);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n      String latestCommit = TestUtils.getLatestCommit(tempFile.getAbsolutePath());\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getLatestCommit().equals(latestCommit)),\n          \"All the splits should be with latestCommit instant time\");\n\n      \r\n      function.close();\n    }\n  }\n","date":"2021-07-30 14:25:05","endLine":99,"groupId":"4687","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testConsumeFromLatestCommit","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/f1/45744465330a165b6cccdd63e902234c53ae10.src","preCode":"  public void testConsumeFromLatestCommit() throws Exception {\n    TestData.writeData(TestData.DATA_SET_INSERT, conf);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().noneMatch(split -> split.getInstantRange().isPresent()),\n          \"No instants should have range limit\");\n\n      Thread.sleep(1000L);\n\n      \r\n      latch = new CountDownLatch(4);\n      sourceContext.reset(latch);\n\n      \r\n      TestData.writeData(TestData.DATA_SET_UPDATE_INSERT, conf);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n\n      \r\n      function.close();\n    }\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/source/TestStreamReadMonitoringFunction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":72,"status":"M"},{"authorDate":"2021-03-11 19:44:06","commitOrder":3,"curCode":"  public void testConsumeFromSpecifiedCommit() throws Exception {\n    \r\n    \r\n    TestData.writeData(TestData.DATA_SET_INSERT, conf);\n    TestData.writeData(TestData.DATA_SET_UPDATE_INSERT, conf);\n    String specifiedCommit = TestUtils.getLatestCommit(tempFile.getAbsolutePath());\n    conf.setString(FlinkOptions.READ_STREAMING_START_COMMIT, specifiedCommit);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getLatestCommit().equals(specifiedCommit)),\n          \"All the splits should be with specified instant time\");\n\n      \r\n      function.close();\n    }\n  }\n","date":"2021-03-11 19:44:06","endLine":140,"groupId":"1544","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testConsumeFromSpecifiedCommit","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/47/33fa07b673bb8f902e45c1a1fd5369f881fd83.src","preCode":"  public void testConsumeFromSpecifiedCommit() throws Exception {\n    \r\n    \r\n    TestData.writeData(TestData.DATA_SET_INSERT, conf);\n    TestData.writeData(TestData.DATA_SET_UPDATE_INSERT, conf);\n    String specifiedCommit = TestUtils.getLatestCommit(tempFile.getAbsolutePath());\n    conf.setString(FlinkOptions.READ_STREAMING_START_COMMIT, specifiedCommit);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getLatestCommit().equals(specifiedCommit)),\n          \"All the splits should be with specified instant time\");\n\n      \r\n      function.close();\n    }\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/source/TestStreamReadMonitoringFunction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":112,"status":"N"}],"commitId":"8b19ec9ca07070a9819502e82091dd14d559ef94","commitMessage":"@@@[HUDI-2252] Default consumes from the latest instant for flink streaming reader (#3368)\n\n","date":"2021-07-30 14:25:05","modifiedFileCount":"4","status":"M","submitter":"swuferhong"},{"authorTime":"2021-09-19 09:06:46","codes":[{"authorDate":"2021-07-30 14:25:05","commitOrder":4,"curCode":"  public void testConsumeFromLatestCommit() throws Exception {\n    \r\n    TestData.writeData(TestData.DATA_SET_INSERT, conf);\n    TestData.writeData(TestData.DATA_SET_UPDATE_INSERT, conf);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n      String latestCommit = TestUtils.getLatestCommit(tempFile.getAbsolutePath());\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getLatestCommit().equals(latestCommit)),\n          \"All the splits should be with latestCommit instant time\");\n\n      \r\n      function.close();\n    }\n  }\n","date":"2021-07-30 14:25:05","endLine":99,"groupId":"10433","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testConsumeFromLatestCommit","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/f1/45744465330a165b6cccdd63e902234c53ae10.src","preCode":"  public void testConsumeFromLatestCommit() throws Exception {\n    \r\n    TestData.writeData(TestData.DATA_SET_INSERT, conf);\n    TestData.writeData(TestData.DATA_SET_UPDATE_INSERT, conf);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n      String latestCommit = TestUtils.getLatestCommit(tempFile.getAbsolutePath());\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getLatestCommit().equals(latestCommit)),\n          \"All the splits should be with latestCommit instant time\");\n\n      \r\n      function.close();\n    }\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/source/TestStreamReadMonitoringFunction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":72,"status":"N"},{"authorDate":"2021-09-19 09:06:46","commitOrder":4,"curCode":"  public void testConsumeFromSpecifiedCommit() throws Exception {\n    \r\n    \r\n    TestData.writeData(TestData.DATA_SET_INSERT, conf);\n    TestData.writeData(TestData.DATA_SET_UPDATE_INSERT, conf);\n    String specifiedCommit = TestUtils.getLatestCommit(tempFile.getAbsolutePath());\n    conf.setString(FlinkOptions.READ_START_COMMIT, specifiedCommit);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getLatestCommit().equals(specifiedCommit)),\n          \"All the splits should be with specified instant time\");\n\n      \r\n      function.close();\n    }\n  }\n","date":"2021-09-19 09:06:46","endLine":169,"groupId":"10433","id":8,"instanceNumber":2,"isCurCommit":1,"methodName":"testConsumeFromSpecifiedCommit","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/36/87e9d7cee4d3ac79c5c4ac11d56784e8365478.src","preCode":"  public void testConsumeFromSpecifiedCommit() throws Exception {\n    \r\n    \r\n    TestData.writeData(TestData.DATA_SET_INSERT, conf);\n    TestData.writeData(TestData.DATA_SET_UPDATE_INSERT, conf);\n    String specifiedCommit = TestUtils.getLatestCommit(tempFile.getAbsolutePath());\n    conf.setString(FlinkOptions.READ_STREAMING_START_COMMIT, specifiedCommit);\n    StreamReadMonitoringFunction function = TestUtils.getMonitorFunc(conf);\n    try (AbstractStreamOperatorTestHarness<MergeOnReadInputSplit> harness = createHarness(function)) {\n      harness.setup();\n      harness.open();\n\n      CountDownLatch latch = new CountDownLatch(4);\n      CollectingSourceContext sourceContext = new CollectingSourceContext(latch);\n\n      runAsync(sourceContext, function);\n\n      assertTrue(latch.await(WAIT_TIME_MILLIS, TimeUnit.MILLISECONDS), \"Should finish splits generation\");\n      assertThat(\"Should produce the expected splits\",\n          sourceContext.getPartitionPaths(), is(\"par1,par2,par3,par4\"));\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getInstantRange().isPresent()),\n          \"All the instants should have range limit\");\n      assertTrue(sourceContext.splits.stream().allMatch(split -> split.getLatestCommit().equals(specifiedCommit)),\n          \"All the splits should be with specified instant time\");\n\n      \r\n      function.close();\n    }\n  }\n","realPath":"hudi-flink/src/test/java/org/apache/hudi/source/TestStreamReadMonitoringFunction.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":141,"status":"M"}],"commitId":"3354fac42f9a2c4dbc8ac73ca4749160e9b9459b","commitMessage":"@@@[HUDI-2449] Incremental read for Flink (#3686)\n\n","date":"2021-09-19 09:06:46","modifiedFileCount":"15","status":"M","submitter":"Danny Chan"}]
