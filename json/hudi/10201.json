[{"authorTime":"2020-11-01 12:15:41","codes":[{"authorDate":"2020-11-01 12:15:41","commitOrder":4,"curCode":"  public JavaRDD<GenericRecord> generateUpdates(Config config) throws IOException {\n    if (deltaOutputConfig.getDeltaOutputMode() == DeltaOutputMode.DFS) {\n      JavaRDD<GenericRecord> inserts = null;\n      if (config.getNumRecordsInsert() > 0) {\n        inserts = generateInserts(config);\n      }\n      DeltaInputReader deltaInputReader = null;\n      JavaRDD<GenericRecord> adjustedRDD = null;\n      if (config.getNumUpsertPartitions() != 0) {\n        if (config.getNumUpsertPartitions() < 0) {\n          \r\n          deltaInputReader = new DFSAvroDeltaInputReader(sparkSession, schemaStr,\n              ((DFSDeltaConfig) deltaOutputConfig).getDeltaBasePath(), Option.empty(), Option.empty());\n          adjustedRDD = deltaInputReader.read(config.getNumRecordsUpsert());\n          adjustedRDD = adjustRDDToGenerateExactNumUpdates(adjustedRDD, jsc, config.getNumRecordsUpsert());\n        } else {\n          deltaInputReader =\n              new DFSHoodieDatasetInputReader(jsc, ((DFSDeltaConfig) deltaOutputConfig).getDatasetOutputPath(),\n                  schemaStr);\n          if (config.getFractionUpsertPerFile() > 0) {\n            adjustedRDD = deltaInputReader.read(config.getNumUpsertPartitions(), config.getNumUpsertFiles(),\n                config.getFractionUpsertPerFile());\n          } else {\n            adjustedRDD = deltaInputReader.read(config.getNumUpsertPartitions(), config.getNumUpsertFiles(), config\n                .getNumRecordsUpsert());\n          }\n        }\n\n        \r\n        int numPartition = Math.min(deltaOutputConfig.getInputParallelism(),\n            Math.max(1, config.getNumUpsertPartitions()));\n        log.info(\"Repartitioning records into \" + numPartition + \" partitions for updates\");\n        adjustedRDD = adjustedRDD.repartition(numPartition);\n        log.info(\"Repartitioning records done for updates\");\n\n        UpdateConverter converter = new UpdateConverter(schemaStr, config.getRecordSize(),\n            partitionPathFieldNames, recordRowKeyFieldNames);\n        JavaRDD<GenericRecord> updates = converter.convert(adjustedRDD);\n        updates.persist(StorageLevel.DISK_ONLY());\n        if (inserts == null) {\n          inserts = updates;\n        } else {\n          inserts = inserts.union(updates);\n        }\n      }\n      return inserts;\n      \r\n    } else {\n      throw new IllegalArgumentException(\"Other formats are not supported at the moment\");\n    }\n  }\n","date":"2020-11-01 12:15:41","endLine":186,"groupId":"959","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"generateUpdates","params":"(Configconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/53/af8eb74068e77c094d59a1a50e9f2dcbe62a5c.src","preCode":"  public JavaRDD<GenericRecord> generateUpdates(Config config) throws IOException {\n    if (deltaOutputConfig.getDeltaOutputMode() == DeltaOutputMode.DFS) {\n      JavaRDD<GenericRecord> inserts = null;\n      if (config.getNumRecordsInsert() > 0) {\n        inserts = generateInserts(config);\n      }\n      DeltaInputReader deltaInputReader = null;\n      JavaRDD<GenericRecord> adjustedRDD = null;\n      if (config.getNumUpsertPartitions() != 0) {\n        if (config.getNumUpsertPartitions() < 0) {\n          \r\n          deltaInputReader = new DFSAvroDeltaInputReader(sparkSession, schemaStr,\n              ((DFSDeltaConfig) deltaOutputConfig).getDeltaBasePath(), Option.empty(), Option.empty());\n          adjustedRDD = deltaInputReader.read(config.getNumRecordsUpsert());\n          adjustedRDD = adjustRDDToGenerateExactNumUpdates(adjustedRDD, jsc, config.getNumRecordsUpsert());\n        } else {\n          deltaInputReader =\n              new DFSHoodieDatasetInputReader(jsc, ((DFSDeltaConfig) deltaOutputConfig).getDatasetOutputPath(),\n                  schemaStr);\n          if (config.getFractionUpsertPerFile() > 0) {\n            adjustedRDD = deltaInputReader.read(config.getNumUpsertPartitions(), config.getNumUpsertFiles(),\n                config.getFractionUpsertPerFile());\n          } else {\n            adjustedRDD = deltaInputReader.read(config.getNumUpsertPartitions(), config.getNumUpsertFiles(), config\n                .getNumRecordsUpsert());\n          }\n        }\n\n        \r\n        int numPartition = Math.min(deltaOutputConfig.getInputParallelism(),\n            Math.max(1, config.getNumUpsertPartitions()));\n        log.info(\"Repartitioning records into \" + numPartition + \" partitions for updates\");\n        adjustedRDD = adjustedRDD.repartition(numPartition);\n        log.info(\"Repartitioning records done for updates\");\n\n        UpdateConverter converter = new UpdateConverter(schemaStr, config.getRecordSize(),\n            partitionPathFieldNames, recordRowKeyFieldNames);\n        JavaRDD<GenericRecord> updates = converter.convert(adjustedRDD);\n        updates.persist(StorageLevel.DISK_ONLY());\n        if (inserts == null) {\n          inserts = updates;\n        } else {\n          inserts = inserts.union(updates);\n        }\n      }\n      return inserts;\n      \r\n    } else {\n      throw new IllegalArgumentException(\"Other formats are not supported at the moment\");\n    }\n  }\n","realPath":"hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/DeltaGenerator.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":136,"status":"MB"},{"authorDate":"2020-11-01 12:15:41","commitOrder":4,"curCode":"  public JavaRDD<GenericRecord> generateDeletes(Config config) throws IOException {\n    if (deltaOutputConfig.getDeltaOutputMode() == DeltaOutputMode.DFS) {\n      DeltaInputReader deltaInputReader = null;\n      JavaRDD<GenericRecord> adjustedRDD = null;\n\n      if (config.getNumDeletePartitions() < 1) {\n        \r\n        deltaInputReader = new DFSAvroDeltaInputReader(sparkSession, schemaStr,\n            ((DFSDeltaConfig) deltaOutputConfig).getDeltaBasePath(), Option.empty(), Option.empty());\n        adjustedRDD = deltaInputReader.read(config.getNumRecordsDelete());\n        adjustedRDD = adjustRDDToGenerateExactNumUpdates(adjustedRDD, jsc, config.getNumRecordsDelete());\n      } else {\n        deltaInputReader =\n            new DFSHoodieDatasetInputReader(jsc, ((DFSDeltaConfig) deltaOutputConfig).getDatasetOutputPath(),\n                schemaStr);\n        if (config.getFractionUpsertPerFile() > 0) {\n          adjustedRDD = deltaInputReader.read(config.getNumDeletePartitions(), config.getNumUpsertFiles(),\n              config.getFractionUpsertPerFile());\n        } else {\n          adjustedRDD = deltaInputReader.read(config.getNumDeletePartitions(), config.getNumUpsertFiles(), config\n              .getNumRecordsDelete());\n        }\n      }\n      log.info(\"Repartitioning records for delete\");\n      \r\n      adjustedRDD = adjustedRDD.repartition(jsc.defaultParallelism());\n      Converter converter = new DeleteConverter(schemaStr, config.getRecordSize());\n      JavaRDD<GenericRecord> deletes = converter.convert(adjustedRDD);\n      deletes.persist(StorageLevel.DISK_ONLY());\n      return deletes;\n    } else {\n      throw new IllegalArgumentException(\"Other formats are not supported at the moment\");\n    }\n  }\n","date":"2020-11-01 12:15:41","endLine":221,"groupId":"966","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"generateDeletes","params":"(Configconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/53/af8eb74068e77c094d59a1a50e9f2dcbe62a5c.src","preCode":"  public JavaRDD<GenericRecord> generateDeletes(Config config) throws IOException {\n    if (deltaOutputConfig.getDeltaOutputMode() == DeltaOutputMode.DFS) {\n      DeltaInputReader deltaInputReader = null;\n      JavaRDD<GenericRecord> adjustedRDD = null;\n\n      if (config.getNumDeletePartitions() < 1) {\n        \r\n        deltaInputReader = new DFSAvroDeltaInputReader(sparkSession, schemaStr,\n            ((DFSDeltaConfig) deltaOutputConfig).getDeltaBasePath(), Option.empty(), Option.empty());\n        adjustedRDD = deltaInputReader.read(config.getNumRecordsDelete());\n        adjustedRDD = adjustRDDToGenerateExactNumUpdates(adjustedRDD, jsc, config.getNumRecordsDelete());\n      } else {\n        deltaInputReader =\n            new DFSHoodieDatasetInputReader(jsc, ((DFSDeltaConfig) deltaOutputConfig).getDatasetOutputPath(),\n                schemaStr);\n        if (config.getFractionUpsertPerFile() > 0) {\n          adjustedRDD = deltaInputReader.read(config.getNumDeletePartitions(), config.getNumUpsertFiles(),\n              config.getFractionUpsertPerFile());\n        } else {\n          adjustedRDD = deltaInputReader.read(config.getNumDeletePartitions(), config.getNumUpsertFiles(), config\n              .getNumRecordsDelete());\n        }\n      }\n      log.info(\"Repartitioning records for delete\");\n      \r\n      adjustedRDD = adjustedRDD.repartition(jsc.defaultParallelism());\n      Converter converter = new DeleteConverter(schemaStr, config.getRecordSize());\n      JavaRDD<GenericRecord> deletes = converter.convert(adjustedRDD);\n      deletes.persist(StorageLevel.DISK_ONLY());\n      return deletes;\n    } else {\n      throw new IllegalArgumentException(\"Other formats are not supported at the moment\");\n    }\n  }\n","realPath":"hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/DeltaGenerator.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":188,"status":"B"}],"commitId":"a205dd10faba0a83dcb39a12abb6f744b5224992","commitMessage":"@@@[HUDI-1338] Adding Delete support to test suite framework (#2172)\n\n- Adding Delete support to test suite. \n         Added DeleteNode \n         Added support to generate delete records \n","date":"2020-11-01 12:15:41","modifiedFileCount":"7","status":"M","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-09-13 23:53:13","codes":[{"authorDate":"2021-09-13 23:53:13","commitOrder":5,"curCode":"  public JavaRDD<GenericRecord> generateUpdates(Config config) throws IOException {\n    if (deltaOutputConfig.getDeltaOutputMode() == DeltaOutputMode.DFS) {\n      JavaRDD<GenericRecord> inserts = null;\n      if (config.getNumRecordsInsert() > 0) {\n        inserts = generateInserts(config);\n      }\n      DeltaInputReader deltaInputReader = null;\n      JavaRDD<GenericRecord> adjustedRDD = null;\n      if (config.getNumUpsertPartitions() != 0) {\n        if (config.getNumUpsertPartitions() < 0) {\n          \r\n          deltaInputReader = new DFSAvroDeltaInputReader(sparkSession, schemaStr,\n              ((DFSDeltaConfig) deltaOutputConfig).getDeltaBasePath(), Option.empty(), Option.empty());\n          adjustedRDD = deltaInputReader.read(config.getNumRecordsUpsert());\n          adjustedRDD = adjustRDDToGenerateExactNumUpdates(adjustedRDD, jsc, config.getNumRecordsUpsert());\n        } else {\n          deltaInputReader =\n              new DFSHoodieDatasetInputReader(jsc, ((DFSDeltaConfig) deltaOutputConfig).getDatasetOutputPath(),\n                  schemaStr);\n          if (config.getFractionUpsertPerFile() > 0) {\n            adjustedRDD = deltaInputReader.read(config.getNumUpsertPartitions(), config.getNumUpsertFiles(),\n                config.getFractionUpsertPerFile());\n          } else {\n            adjustedRDD = deltaInputReader.read(config.getNumUpsertPartitions(), config.getNumUpsertFiles(), config\n                .getNumRecordsUpsert());\n          }\n        }\n\n        \r\n        int numPartition = Math.min(deltaOutputConfig.getInputParallelism(),\n            Math.max(1, config.getNumUpsertPartitions()));\n        log.info(\"Repartitioning records into \" + numPartition + \" partitions for updates\");\n        adjustedRDD = adjustedRDD.repartition(numPartition);\n        log.info(\"Repartitioning records done for updates\");\n        UpdateConverter converter = new UpdateConverter(schemaStr, config.getRecordSize(),\n            partitionPathFieldNames, recordRowKeyFieldNames);\n        JavaRDD<GenericRecord> convertedRecords = converter.convert(adjustedRDD);\n        JavaRDD<GenericRecord> updates = convertedRecords.map(record -> {\n          record.put(SchemaUtils.SOURCE_ORDERING_FIELD, batchId);\n          return record;\n        });\n        updates.persist(StorageLevel.DISK_ONLY());\n        if (inserts == null) {\n          inserts = updates;\n        } else {\n          inserts = inserts.union(updates);\n        }\n      }\n      return inserts;\n      \r\n    } else {\n      throw new IllegalArgumentException(\"Other formats are not supported at the moment\");\n    }\n  }\n","date":"2021-09-13 23:53:13","endLine":192,"groupId":"10201","id":3,"instanceNumber":1,"isCurCommit":1,"methodName":"generateUpdates","params":"(Configconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/6d/5bc4ffedeca9741e7d1cebde8996ad37d0cb48.src","preCode":"  public JavaRDD<GenericRecord> generateUpdates(Config config) throws IOException {\n    if (deltaOutputConfig.getDeltaOutputMode() == DeltaOutputMode.DFS) {\n      JavaRDD<GenericRecord> inserts = null;\n      if (config.getNumRecordsInsert() > 0) {\n        inserts = generateInserts(config);\n      }\n      DeltaInputReader deltaInputReader = null;\n      JavaRDD<GenericRecord> adjustedRDD = null;\n      if (config.getNumUpsertPartitions() != 0) {\n        if (config.getNumUpsertPartitions() < 0) {\n          \r\n          deltaInputReader = new DFSAvroDeltaInputReader(sparkSession, schemaStr,\n              ((DFSDeltaConfig) deltaOutputConfig).getDeltaBasePath(), Option.empty(), Option.empty());\n          adjustedRDD = deltaInputReader.read(config.getNumRecordsUpsert());\n          adjustedRDD = adjustRDDToGenerateExactNumUpdates(adjustedRDD, jsc, config.getNumRecordsUpsert());\n        } else {\n          deltaInputReader =\n              new DFSHoodieDatasetInputReader(jsc, ((DFSDeltaConfig) deltaOutputConfig).getDatasetOutputPath(),\n                  schemaStr);\n          if (config.getFractionUpsertPerFile() > 0) {\n            adjustedRDD = deltaInputReader.read(config.getNumUpsertPartitions(), config.getNumUpsertFiles(),\n                config.getFractionUpsertPerFile());\n          } else {\n            adjustedRDD = deltaInputReader.read(config.getNumUpsertPartitions(), config.getNumUpsertFiles(), config\n                .getNumRecordsUpsert());\n          }\n        }\n\n        \r\n        int numPartition = Math.min(deltaOutputConfig.getInputParallelism(),\n            Math.max(1, config.getNumUpsertPartitions()));\n        log.info(\"Repartitioning records into \" + numPartition + \" partitions for updates\");\n        adjustedRDD = adjustedRDD.repartition(numPartition);\n        log.info(\"Repartitioning records done for updates\");\n        UpdateConverter converter = new UpdateConverter(schemaStr, config.getRecordSize(),\n            partitionPathFieldNames, recordRowKeyFieldNames);\n        JavaRDD<GenericRecord> updates = converter.convert(adjustedRDD);\n        updates.persist(StorageLevel.DISK_ONLY());\n        if (inserts == null) {\n          inserts = updates;\n        } else {\n          inserts = inserts.union(updates);\n        }\n      }\n      return inserts;\n      \r\n    } else {\n      throw new IllegalArgumentException(\"Other formats are not supported at the moment\");\n    }\n  }\n","realPath":"hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/DeltaGenerator.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":139,"status":"M"},{"authorDate":"2021-09-13 23:53:13","commitOrder":5,"curCode":"  public JavaRDD<GenericRecord> generateDeletes(Config config) throws IOException {\n    if (deltaOutputConfig.getDeltaOutputMode() == DeltaOutputMode.DFS) {\n      DeltaInputReader deltaInputReader = null;\n      JavaRDD<GenericRecord> adjustedRDD = null;\n\n      if (config.getNumDeletePartitions() < 1) {\n        \r\n        deltaInputReader = new DFSAvroDeltaInputReader(sparkSession, schemaStr,\n            ((DFSDeltaConfig) deltaOutputConfig).getDeltaBasePath(), Option.empty(), Option.empty());\n        adjustedRDD = deltaInputReader.read(config.getNumRecordsDelete());\n        adjustedRDD = adjustRDDToGenerateExactNumUpdates(adjustedRDD, jsc, config.getNumRecordsDelete());\n      } else {\n        deltaInputReader =\n            new DFSHoodieDatasetInputReader(jsc, ((DFSDeltaConfig) deltaOutputConfig).getDatasetOutputPath(),\n                schemaStr);\n        if (config.getFractionUpsertPerFile() > 0) {\n          adjustedRDD = deltaInputReader.read(config.getNumDeletePartitions(), config.getNumUpsertFiles(),\n              config.getFractionUpsertPerFile());\n        } else {\n          adjustedRDD = deltaInputReader.read(config.getNumDeletePartitions(), config.getNumUpsertFiles(), config\n              .getNumRecordsDelete());\n        }\n      }\n\n      log.info(\"Repartitioning records for delete\");\n      \r\n      adjustedRDD = adjustedRDD.repartition(jsc.defaultParallelism());\n      Converter converter = new DeleteConverter(schemaStr, config.getRecordSize());\n      JavaRDD<GenericRecord> convertedRecords = converter.convert(adjustedRDD);\n      JavaRDD<GenericRecord> deletes = convertedRecords.map(record -> {\n        record.put(SchemaUtils.SOURCE_ORDERING_FIELD, batchId);\n        return record;\n      });\n      deletes.persist(StorageLevel.DISK_ONLY());\n      return deletes;\n    } else {\n      throw new IllegalArgumentException(\"Other formats are not supported at the moment\");\n    }\n  }\n","date":"2021-09-13 23:53:13","endLine":232,"groupId":"10201","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"generateDeletes","params":"(Configconfig)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/6d/5bc4ffedeca9741e7d1cebde8996ad37d0cb48.src","preCode":"  public JavaRDD<GenericRecord> generateDeletes(Config config) throws IOException {\n    if (deltaOutputConfig.getDeltaOutputMode() == DeltaOutputMode.DFS) {\n      DeltaInputReader deltaInputReader = null;\n      JavaRDD<GenericRecord> adjustedRDD = null;\n\n      if (config.getNumDeletePartitions() < 1) {\n        \r\n        deltaInputReader = new DFSAvroDeltaInputReader(sparkSession, schemaStr,\n            ((DFSDeltaConfig) deltaOutputConfig).getDeltaBasePath(), Option.empty(), Option.empty());\n        adjustedRDD = deltaInputReader.read(config.getNumRecordsDelete());\n        adjustedRDD = adjustRDDToGenerateExactNumUpdates(adjustedRDD, jsc, config.getNumRecordsDelete());\n      } else {\n        deltaInputReader =\n            new DFSHoodieDatasetInputReader(jsc, ((DFSDeltaConfig) deltaOutputConfig).getDatasetOutputPath(),\n                schemaStr);\n        if (config.getFractionUpsertPerFile() > 0) {\n          adjustedRDD = deltaInputReader.read(config.getNumDeletePartitions(), config.getNumUpsertFiles(),\n              config.getFractionUpsertPerFile());\n        } else {\n          adjustedRDD = deltaInputReader.read(config.getNumDeletePartitions(), config.getNumUpsertFiles(), config\n              .getNumRecordsDelete());\n        }\n      }\n      log.info(\"Repartitioning records for delete\");\n      \r\n      adjustedRDD = adjustedRDD.repartition(jsc.defaultParallelism());\n      Converter converter = new DeleteConverter(schemaStr, config.getRecordSize());\n      JavaRDD<GenericRecord> deletes = converter.convert(adjustedRDD);\n      deletes.persist(StorageLevel.DISK_ONLY());\n      return deletes;\n    } else {\n      throw new IllegalArgumentException(\"Other formats are not supported at the moment\");\n    }\n  }\n","realPath":"hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/DeltaGenerator.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":194,"status":"M"}],"commitId":"5d60491f5b76ef0f77174d71567d0673d9315bcd","commitMessage":"@@@[HUDI-2388] Add DAG nodes for Spark SQL in integration test suite (#3583)\n\n- Fixed validation in integ test suite for both deltastreamer and write client path. \n\nCo-authored-by: Sivabalan Narayanan <n.siva.b@gmail.com>","date":"2021-09-13 23:53:13","modifiedFileCount":"6","status":"M","submitter":"Y Ethan Guo"}]
