[{"authorTime":"2020-10-02 05:25:29","codes":[{"authorDate":"2020-08-06 12:34:55","commitOrder":2,"curCode":"  private static HoodieLogFile generateLogData(Path parquetFilePath, boolean isLogSchemaSimple)\n      throws IOException, InterruptedException, URISyntaxException {\n    Schema schema = getTestDataSchema(isLogSchemaSimple);\n    HoodieBaseFile dataFile = new HoodieBaseFile(fileSystem.getFileStatus(parquetFilePath));\n    \r\n    Writer logWriter = HoodieLogFormat.newWriterBuilder().onParentPath(parquetFilePath.getParent())\n        .withFileExtension(HoodieLogFile.DELTA_EXTENSION).withFileId(dataFile.getFileId())\n        .overBaseCommit(dataFile.getCommitTime()).withFs(fileSystem).build();\n    List<IndexedRecord> records = (isLogSchemaSimple ? SchemaTestUtil.generateTestRecords(0, 100)\n        : SchemaTestUtil.generateEvolvedTestRecords(100, 100));\n    Map<HeaderMetadataType, String> header = new HashMap<>(2);\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, dataFile.getCommitTime());\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records, header);\n    logWriter.appendBlock(dataBlock);\n    logWriter.close();\n    return logWriter.getLogFile();\n  }\n","date":"2020-08-06 12:34:55","endLine":326,"groupId":"1495","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"generateLogData","params":"(PathparquetFilePath@booleanisLogSchemaSimple)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/d0/d1b667aea20b9592b845de9833ace73a3ed40a.src","preCode":"  private static HoodieLogFile generateLogData(Path parquetFilePath, boolean isLogSchemaSimple)\n      throws IOException, InterruptedException, URISyntaxException {\n    Schema schema = getTestDataSchema(isLogSchemaSimple);\n    HoodieBaseFile dataFile = new HoodieBaseFile(fileSystem.getFileStatus(parquetFilePath));\n    \r\n    Writer logWriter = HoodieLogFormat.newWriterBuilder().onParentPath(parquetFilePath.getParent())\n        .withFileExtension(HoodieLogFile.DELTA_EXTENSION).withFileId(dataFile.getFileId())\n        .overBaseCommit(dataFile.getCommitTime()).withFs(fileSystem).build();\n    List<IndexedRecord> records = (isLogSchemaSimple ? SchemaTestUtil.generateTestRecords(0, 100)\n        : SchemaTestUtil.generateEvolvedTestRecords(100, 100));\n    Map<HeaderMetadataType, String> header = new HashMap<>(2);\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, dataFile.getCommitTime());\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records, header);\n    logWriter.appendBlock(dataBlock);\n    logWriter.close();\n    return logWriter.getLogFile();\n  }\n","realPath":"hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":309,"status":"NB"},{"authorDate":"2020-10-02 05:25:29","commitOrder":2,"curCode":"  private void writeToFile(Schema wrapperSchema, List<IndexedRecord> records) throws Exception {\n    if (records.size() > 0) {\n      Map<HeaderMetadataType, String> header = new HashMap<>();\n      header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, wrapperSchema.toString());\n      HoodieAvroDataBlock block = new HoodieAvroDataBlock(records, header);\n      this.writer = writer.appendBlock(block);\n      records.clear();\n    }\n  }\n","date":"2020-10-02 05:25:29","endLine":342,"groupId":"4415","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"writeToFile","params":"(SchemawrapperSchema@List<IndexedRecord>records)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/af/2c2782e57d33688f1f73831432138bfb6ba09a.src","preCode":"  private void writeToFile(Schema wrapperSchema, List<IndexedRecord> records) throws Exception {\n    if (records.size() > 0) {\n      Map<HeaderMetadataType, String> header = new HashMap<>();\n      header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, wrapperSchema.toString());\n      HoodieAvroDataBlock block = new HoodieAvroDataBlock(records, header);\n      this.writer = writer.appendBlock(block);\n      records.clear();\n    }\n  }\n","realPath":"hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":334,"status":"B"}],"commitId":"1f7add92916c37b05be270d9c75a9042134ec506","commitMessage":"@@@[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)\n\n- This change breaks `hudi-client` into `hudi-client-common` and `hudi-spark-client` modules \n- Simple usages of Spark using jsc.parallelize() has been redone using EngineContext#map.  EngineContext#flatMap etc\n- Code changes in the PR.  break classes into `BaseXYZ` parent classes with no spark dependencies living in `hudi-client-common`\n- Classes on `hudi-spark-client` are named `SparkXYZ` extending the parent classes with all the Spark dependencies\n- To simplify/cleanup.  HoodieIndex#fetchRecordLocation has been removed and its usages in tests replaced with alternatives\n\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2020-10-02 05:25:29","modifiedFileCount":"31","status":"M","submitter":"Mathieu"},{"authorTime":"2020-12-31 06:22:15","codes":[{"authorDate":"2020-08-06 12:34:55","commitOrder":3,"curCode":"  private static HoodieLogFile generateLogData(Path parquetFilePath, boolean isLogSchemaSimple)\n      throws IOException, InterruptedException, URISyntaxException {\n    Schema schema = getTestDataSchema(isLogSchemaSimple);\n    HoodieBaseFile dataFile = new HoodieBaseFile(fileSystem.getFileStatus(parquetFilePath));\n    \r\n    Writer logWriter = HoodieLogFormat.newWriterBuilder().onParentPath(parquetFilePath.getParent())\n        .withFileExtension(HoodieLogFile.DELTA_EXTENSION).withFileId(dataFile.getFileId())\n        .overBaseCommit(dataFile.getCommitTime()).withFs(fileSystem).build();\n    List<IndexedRecord> records = (isLogSchemaSimple ? SchemaTestUtil.generateTestRecords(0, 100)\n        : SchemaTestUtil.generateEvolvedTestRecords(100, 100));\n    Map<HeaderMetadataType, String> header = new HashMap<>(2);\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, dataFile.getCommitTime());\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records, header);\n    logWriter.appendBlock(dataBlock);\n    logWriter.close();\n    return logWriter.getLogFile();\n  }\n","date":"2020-08-06 12:34:55","endLine":326,"groupId":"10357","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"generateLogData","params":"(PathparquetFilePath@booleanisLogSchemaSimple)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/d0/d1b667aea20b9592b845de9833ace73a3ed40a.src","preCode":"  private static HoodieLogFile generateLogData(Path parquetFilePath, boolean isLogSchemaSimple)\n      throws IOException, InterruptedException, URISyntaxException {\n    Schema schema = getTestDataSchema(isLogSchemaSimple);\n    HoodieBaseFile dataFile = new HoodieBaseFile(fileSystem.getFileStatus(parquetFilePath));\n    \r\n    Writer logWriter = HoodieLogFormat.newWriterBuilder().onParentPath(parquetFilePath.getParent())\n        .withFileExtension(HoodieLogFile.DELTA_EXTENSION).withFileId(dataFile.getFileId())\n        .overBaseCommit(dataFile.getCommitTime()).withFs(fileSystem).build();\n    List<IndexedRecord> records = (isLogSchemaSimple ? SchemaTestUtil.generateTestRecords(0, 100)\n        : SchemaTestUtil.generateEvolvedTestRecords(100, 100));\n    Map<HeaderMetadataType, String> header = new HashMap<>(2);\n    header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, dataFile.getCommitTime());\n    header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());\n    HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records, header);\n    logWriter.appendBlock(dataBlock);\n    logWriter.close();\n    return logWriter.getLogFile();\n  }\n","realPath":"hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":309,"status":"N"},{"authorDate":"2020-12-31 06:22:15","commitOrder":3,"curCode":"  private void writeToFile(Schema wrapperSchema, List<IndexedRecord> records) throws Exception {\n    if (records.size() > 0) {\n      Map<HeaderMetadataType, String> header = new HashMap<>();\n      header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, wrapperSchema.toString());\n      HoodieAvroDataBlock block = new HoodieAvroDataBlock(records, header);\n      writer.appendBlock(block);\n      records.clear();\n    }\n  }\n","date":"2020-12-31 06:22:15","endLine":341,"groupId":"10357","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"writeToFile","params":"(SchemawrapperSchema@List<IndexedRecord>records)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e2/4ae73e38389c979809db90c14fc06077a354a1.src","preCode":"  private void writeToFile(Schema wrapperSchema, List<IndexedRecord> records) throws Exception {\n    if (records.size() > 0) {\n      Map<HeaderMetadataType, String> header = new HashMap<>();\n      header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, wrapperSchema.toString());\n      HoodieAvroDataBlock block = new HoodieAvroDataBlock(records, header);\n      this.writer = writer.appendBlock(block);\n      records.clear();\n    }\n  }\n","realPath":"hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTimelineArchiveLog.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":333,"status":"M"}],"commitId":"605b617cfa9a25bff48e618beccfaa7b4eaeee56","commitMessage":"@@@[HUDI-1434] fix incorrect log file path in HoodieWriteStat (#2300)\n\n* [HUDI-1434] fix incorrect log file path in HoodieWriteStat\n\n* HoodieWriteHandle#close() returns a list of WriteStatus objs\n\n* Handle rolled-over log files and return a WriteStatus per log file written\n\n - Combined data and delete block logging into a single call\n - Lazily initialize and manage write status based on returned AppendResult\n - Use FSUtils.getFileSize() to set final file size.  consistent with other handles\n - Added tests around returned values in AppendResult\n - Added validation of the file sizes returned in write stat\n\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2020-12-31 06:22:15","modifiedFileCount":"27","status":"M","submitter":"Gary Li"}]
