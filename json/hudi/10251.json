[{"authorTime":"2021-03-17 07:43:53","codes":[{"authorDate":"2021-01-10 09:30:16","commitOrder":6,"curCode":"  private void testUpsertsContinuousMode(HoodieTableType tableType, String tempDir) throws Exception {\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    int totalRecords = 3000;\n    \r\n    HoodieDeltaStreamer.Config cfg = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT);\n    cfg.continuousMode = true;\n    cfg.tableType = tableType.name();\n    cfg.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfg.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ds = new HoodieDeltaStreamer(cfg, jsc);\n    deltaStreamerTestRunner(ds, cfg, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(5, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(2, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(5, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n  }\n","date":"2021-01-10 09:30:16","endLine":654,"groupId":"4069","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpsertsContinuousMode","params":"(HoodieTableTypetableType@StringtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ee/ef8ed791413ec1b52cad15338a6da4b9f64f7c.src","preCode":"  private void testUpsertsContinuousMode(HoodieTableType tableType, String tempDir) throws Exception {\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    int totalRecords = 3000;\n    \r\n    HoodieDeltaStreamer.Config cfg = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT);\n    cfg.continuousMode = true;\n    cfg.tableType = tableType.name();\n    cfg.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfg.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ds = new HoodieDeltaStreamer(cfg, jsc);\n    deltaStreamerTestRunner(ds, cfg, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(5, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(2, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(5, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":632,"status":"NB"},{"authorDate":"2021-03-17 07:43:53","commitOrder":6,"curCode":"  private void testLatestCheckpointCarryOverWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.writer.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.writer.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadataForFirstInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n\n    \r\n    HoodieCommitMetadata commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointBeforeParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.writer.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.writer.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.write.meta.key.prefixes\", CHECKPOINT_KEY);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n\n    \r\n    \r\n    cfgBackfillJob.checkpoint = commitMetadataForFirstInstant.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n    backfillJob.sync();\n\n    \r\n    timeline = meta.getActiveTimeline().reload().getCommitsTimeline().filterCompletedInstants();\n    commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointAfterParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n    Assertions.assertEquals(lastCheckpointBeforeParallelBackfill, lastCheckpointAfterParallelBackfill);\n  }\n","date":"2021-03-17 07:43:53","endLine":895,"groupId":"1268","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testLatestCheckpointCarryOverWithMultipleWriters","params":"(HoodieTableTypetableType@StringtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/8f/3e045217cd7c7f09d92f32e296bc40b344c0a5.src","preCode":"  private void testLatestCheckpointCarryOverWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.writer.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.writer.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadataForFirstInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n\n    \r\n    HoodieCommitMetadata commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointBeforeParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.writer.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.writer.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.write.meta.key.prefixes\", CHECKPOINT_KEY);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n\n    \r\n    \r\n    cfgBackfillJob.checkpoint = commitMetadataForFirstInstant.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n    backfillJob.sync();\n\n    \r\n    timeline = meta.getActiveTimeline().reload().getCommitsTimeline().filterCompletedInstants();\n    commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointAfterParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n    Assertions.assertEquals(lastCheckpointBeforeParallelBackfill, lastCheckpointAfterParallelBackfill);\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":827,"status":"B"}],"commitId":"74241947c123c860a1b0344f25cef316440a70d6","commitMessage":"@@@[HUDI-845] Added locking capability to allow multiple writers (#2374)\n\n* [HUDI-845] Added locking capability to allow multiple writers\n1. Added LockProvider API for pluggable lock methodologies\n2. Added Resolution Strategy API to allow for pluggable conflict resolution\n3. Added TableService client API to schedule table services\n4. Added Transaction Manager for wrapping actions within transactions","date":"2021-03-17 07:43:53","modifiedFileCount":"48","status":"M","submitter":"n3nash"},{"authorTime":"2021-03-24 17:24:02","codes":[{"authorDate":"2021-01-10 09:30:16","commitOrder":7,"curCode":"  private void testUpsertsContinuousMode(HoodieTableType tableType, String tempDir) throws Exception {\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    int totalRecords = 3000;\n    \r\n    HoodieDeltaStreamer.Config cfg = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT);\n    cfg.continuousMode = true;\n    cfg.tableType = tableType.name();\n    cfg.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfg.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ds = new HoodieDeltaStreamer(cfg, jsc);\n    deltaStreamerTestRunner(ds, cfg, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(5, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(2, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(5, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n  }\n","date":"2021-01-10 09:30:16","endLine":654,"groupId":"4069","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpsertsContinuousMode","params":"(HoodieTableTypetableType@StringtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/ee/ef8ed791413ec1b52cad15338a6da4b9f64f7c.src","preCode":"  private void testUpsertsContinuousMode(HoodieTableType tableType, String tempDir) throws Exception {\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    int totalRecords = 3000;\n    \r\n    HoodieDeltaStreamer.Config cfg = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT);\n    cfg.continuousMode = true;\n    cfg.tableType = tableType.name();\n    cfg.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfg.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ds = new HoodieDeltaStreamer(cfg, jsc);\n    deltaStreamerTestRunner(ds, cfg, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(5, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(2, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(5, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":632,"status":"N"},{"authorDate":"2021-03-24 17:24:02","commitOrder":7,"curCode":"  private void testLatestCheckpointCarryOverWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadataForFirstInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n\n    \r\n    HoodieCommitMetadata commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointBeforeParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.write.meta.key.prefixes\", CHECKPOINT_KEY);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n\n    \r\n    \r\n    cfgBackfillJob.checkpoint = commitMetadataForFirstInstant.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n    backfillJob.sync();\n\n    \r\n    timeline = meta.getActiveTimeline().reload().getCommitsTimeline().filterCompletedInstants();\n    commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointAfterParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n    Assertions.assertEquals(lastCheckpointBeforeParallelBackfill, lastCheckpointAfterParallelBackfill);\n  }\n","date":"2021-03-24 17:24:02","endLine":896,"groupId":"1268","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testLatestCheckpointCarryOverWithMultipleWriters","params":"(HoodieTableTypetableType@StringtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/cb/59ce7b1d3804b8f705b994b23b300ba16a238a.src","preCode":"  private void testLatestCheckpointCarryOverWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.writer.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.writer.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadataForFirstInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n\n    \r\n    HoodieCommitMetadata commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointBeforeParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.writer.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.writer.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.write.meta.key.prefixes\", CHECKPOINT_KEY);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n\n    \r\n    \r\n    cfgBackfillJob.checkpoint = commitMetadataForFirstInstant.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n    backfillJob.sync();\n\n    \r\n    timeline = meta.getActiveTimeline().reload().getCommitsTimeline().filterCompletedInstants();\n    commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointAfterParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n    Assertions.assertEquals(lastCheckpointBeforeParallelBackfill, lastCheckpointAfterParallelBackfill);\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":828,"status":"M"}],"commitId":"01a1d7997bc8f3f82452ebf8e2e655143d00926c","commitMessage":"@@@[HUDI-1712] Rename & standardize config to match other configs (#2708)\n\n","date":"2021-03-24 17:24:02","modifiedFileCount":"3","status":"M","submitter":"n3nash"},{"authorTime":"2021-07-01 05:26:30","codes":[{"authorDate":"2021-07-01 05:26:30","commitOrder":8,"curCode":"  private void testUpsertsContinuousMode(HoodieTableType tableType, String tempDir) throws Exception {\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    int totalRecords = 3000;\n    \r\n    HoodieDeltaStreamer.Config cfg = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT);\n    cfg.continuousMode = true;\n    cfg.tableType = tableType.name();\n    cfg.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfg.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\n    HoodieDeltaStreamer ds = new HoodieDeltaStreamer(cfg, jsc);\n    deltaStreamerTestRunner(ds, cfg, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(5, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(2, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(5, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n  }\n","date":"2021-07-01 05:26:30","endLine":755,"groupId":"1266","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpsertsContinuousMode","params":"(HoodieTableTypetableType@StringtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/8a/2648dc4073b890af70b626df8d8a006f1e1490.src","preCode":"  private void testUpsertsContinuousMode(HoodieTableType tableType, String tempDir) throws Exception {\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    int totalRecords = 3000;\n    \r\n    HoodieDeltaStreamer.Config cfg = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT);\n    cfg.continuousMode = true;\n    cfg.tableType = tableType.name();\n    cfg.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfg.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ds = new HoodieDeltaStreamer(cfg, jsc);\n    deltaStreamerTestRunner(ds, cfg, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(5, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(2, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(5, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":733,"status":"M"},{"authorDate":"2021-07-01 05:26:30","commitOrder":8,"curCode":"  private void testLatestCheckpointCarryOverWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadataForFirstInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n\n    \r\n    HoodieCommitMetadata commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointBeforeParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.write.meta.key.prefixes\", CHECKPOINT_KEY);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n\n    \r\n    \r\n    cfgBackfillJob.checkpoint = commitMetadataForFirstInstant.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n    backfillJob.sync();\n\n    \r\n    timeline = meta.getActiveTimeline().reload().getCommitsTimeline().filterCompletedInstants();\n    commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointAfterParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n    Assertions.assertEquals(lastCheckpointBeforeParallelBackfill, lastCheckpointAfterParallelBackfill);\n  }\n","date":"2021-07-01 05:26:30","endLine":912,"groupId":"1268","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testLatestCheckpointCarryOverWithMultipleWriters","params":"(HoodieTableTypetableType@StringtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/8a/2648dc4073b890af70b626df8d8a006f1e1490.src","preCode":"  private void testLatestCheckpointCarryOverWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadataForFirstInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n\n    \r\n    HoodieCommitMetadata commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointBeforeParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.write.meta.key.prefixes\", CHECKPOINT_KEY);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n\n    \r\n    \r\n    cfgBackfillJob.checkpoint = commitMetadataForFirstInstant.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n    backfillJob.sync();\n\n    \r\n    timeline = meta.getActiveTimeline().reload().getCommitsTimeline().filterCompletedInstants();\n    commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointAfterParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n    Assertions.assertEquals(lastCheckpointBeforeParallelBackfill, lastCheckpointAfterParallelBackfill);\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":844,"status":"M"}],"commitId":"d412fb2fe642417460532044cac162bb68f4bec4","commitMessage":"@@@[HUDI-89] Add configOption & refactor all configs based on that (#2833)\n\nCo-authored-by: Wenning Ding <wenningd@amazon.com>","date":"2021-07-01 05:26:30","modifiedFileCount":"138","status":"M","submitter":"wenningd"},{"authorTime":"2021-08-13 11:31:04","codes":[{"authorDate":"2021-08-13 11:31:04","commitOrder":9,"curCode":"  private void testUpsertsContinuousMode(HoodieTableType tableType, String tempDir) throws Exception {\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    int totalRecords = 3000;\n    \r\n    HoodieDeltaStreamer.Config cfg = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT);\n    cfg.continuousMode = true;\n    cfg.tableType = tableType.name();\n    cfg.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfg.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN.key()));\n    HoodieDeltaStreamer ds = new HoodieDeltaStreamer(cfg, jsc);\n    deltaStreamerTestRunner(ds, cfg, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(5, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(2, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(5, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n  }\n","date":"2021-08-13 11:31:04","endLine":727,"groupId":"10251","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testUpsertsContinuousMode","params":"(HoodieTableTypetableType@StringtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/78/48679fedc95d91e43238401a487957dc139909.src","preCode":"  private void testUpsertsContinuousMode(HoodieTableType tableType, String tempDir) throws Exception {\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    int totalRecords = 3000;\n    \r\n    HoodieDeltaStreamer.Config cfg = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT);\n    cfg.continuousMode = true;\n    cfg.tableType = tableType.name();\n    cfg.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfg.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\n    HoodieDeltaStreamer ds = new HoodieDeltaStreamer(cfg, jsc);\n    deltaStreamerTestRunner(ds, cfg, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(5, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(2, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(5, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":705,"status":"M"},{"authorDate":"2021-08-13 11:31:04","commitOrder":9,"curCode":"  private void testLatestCheckpointCarryOverWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN.key()));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadataForFirstInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n\n    \r\n    HoodieCommitMetadata commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointBeforeParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.write.meta.key.prefixes\", CHECKPOINT_KEY);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n\n    \r\n    \r\n    cfgBackfillJob.checkpoint = commitMetadataForFirstInstant.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN.key()));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n    backfillJob.sync();\n\n    \r\n    timeline = meta.getActiveTimeline().reload().getCommitsTimeline().filterCompletedInstants();\n    commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointAfterParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n    Assertions.assertEquals(lastCheckpointBeforeParallelBackfill, lastCheckpointAfterParallelBackfill);\n  }\n","date":"2021-08-13 11:31:04","endLine":884,"groupId":"10251","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testLatestCheckpointCarryOverWithMultipleWriters","params":"(HoodieTableTypetableType@StringtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/78/48679fedc95d91e43238401a487957dc139909.src","preCode":"  private void testLatestCheckpointCarryOverWithMultipleWriters(HoodieTableType tableType, String tempDir) throws Exception {\n    \r\n    String tableBasePath = dfsBasePath + \"/\" + tempDir;\n    \r\n    TypedProperties props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n    \r\n    int totalRecords = 3000;\n\n    HoodieDeltaStreamer.Config cfgIngestionJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgIngestionJob.continuousMode = true;\n    cfgIngestionJob.tableType = tableType.name();\n    cfgIngestionJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgIngestionJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\n    HoodieDeltaStreamer ingestionJob = new HoodieDeltaStreamer(cfgIngestionJob, jsc);\n\n    \r\n    deltaStreamerTestRunner(ingestionJob, cfgIngestionJob, (r) -> {\n      if (tableType.equals(HoodieTableType.MERGE_ON_READ)) {\n        TestHelpers.assertAtleastNDeltaCommits(3, tableBasePath, dfs);\n        TestHelpers.assertAtleastNCompactionCommits(1, tableBasePath, dfs);\n      } else {\n        TestHelpers.assertAtleastNCompactionCommits(3, tableBasePath, dfs);\n      }\n      TestHelpers.assertRecordCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      TestHelpers.assertDistanceCount(totalRecords, tableBasePath + \"/*/*.parquet\", sqlContext);\n      return true;\n    });\n\n    \r\n    HoodieDeltaStreamer.Config cfgBackfillJob = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT,\n        Arrays.asList(TripsWithDistanceTransformer.class.getName()), PROPS_FILENAME_TEST_MULTI_WRITER, false);\n    cfgBackfillJob.continuousMode = false;\n    cfgBackfillJob.tableType = tableType.name();\n    HoodieTableMetaClient meta = HoodieTableMetaClient.builder().setConf(dfs.getConf()).setBasePath(tableBasePath).build();\n    HoodieTimeline timeline = meta.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n    HoodieCommitMetadata commitMetadataForFirstInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.firstInstant().get()).get(), HoodieCommitMetadata.class);\n\n    \r\n    HoodieCommitMetadata commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointBeforeParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n\n    \r\n    props = prepareMultiWriterProps(PROPS_FILENAME_TEST_MULTI_WRITER);\n    props.setProperty(\"hoodie.write.lock.provider\", \"org.apache.hudi.client.transaction.FileSystemBasedLockProviderTestClass\");\n    props.setProperty(\"hoodie.write.lock.filesystem.path\", tableBasePath);\n    props.setProperty(\"hoodie.write.meta.key.prefixes\", CHECKPOINT_KEY);\n    UtilitiesTestBase.Helpers.savePropsToDFS(props, dfs, dfsBasePath + \"/\" + PROPS_FILENAME_TEST_MULTI_WRITER);\n\n    \r\n    \r\n    cfgBackfillJob.checkpoint = commitMetadataForFirstInstant.getMetadata(CHECKPOINT_KEY);\n    cfgBackfillJob.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n    cfgBackfillJob.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP.key()));\n    HoodieDeltaStreamer backfillJob = new HoodieDeltaStreamer(cfgBackfillJob, jsc);\n    backfillJob.sync();\n\n    \r\n    timeline = meta.getActiveTimeline().reload().getCommitsTimeline().filterCompletedInstants();\n    commitMetadataForLastInstant = HoodieCommitMetadata\n        .fromBytes(timeline.getInstantDetails(timeline.lastInstant().get()).get(), HoodieCommitMetadata.class);\n    String lastCheckpointAfterParallelBackfill = commitMetadataForLastInstant.getMetadata(CHECKPOINT_KEY);\n    Assertions.assertEquals(lastCheckpointBeforeParallelBackfill, lastCheckpointAfterParallelBackfill);\n  }\n","realPath":"hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":816,"status":"M"}],"commitId":"0544d70d8f4204f4e5edfe9144c17f1ed221eb7c","commitMessage":"@@@[MINOR] Deprecate older configs (#3464)\n\nRename and deprecate props in HoodieWriteConfig\n\nRename and deprecate older props","date":"2021-08-13 11:31:04","modifiedFileCount":"38","status":"M","submitter":"Sagar Sumit"}]
