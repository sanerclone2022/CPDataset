[{"authorTime":"2020-04-09 13:50:36","codes":[{"authorDate":"2020-04-09 13:50:36","commitOrder":3,"curCode":"  public void testAddKey() throws IOException {\n    List<String> rowKeys = new ArrayList<>();\n    for (int i = 0; i < 1000; i++) {\n      rowKeys.add(UUID.randomUUID().toString());\n    }\n    String filePath = folder.getRoot() + \"/test.parquet\";\n    Schema schema = HoodieAvroUtils.getRecordKeySchema();\n    BloomFilter filter = BloomFilterFactory.createBloomFilter(\n        1000, 0.0001, 10000,\n        BloomFilterTypeCode.SIMPLE.name());\n    HoodieAvroWriteSupport writeSupport = new HoodieAvroWriteSupport(\n        new AvroSchemaConverter().convert(schema), schema, filter);\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, rowKey);\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","date":"2020-04-09 23:18:02","endLine":68,"groupId":"2364","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testAddKey","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/43/4807db813da4cd062c2fdf95812a186a854c35.src","preCode":"  public void testAddKey() throws IOException {\n    List<String> rowKeys = new ArrayList<>();\n    for (int i = 0; i < 1000; i++) {\n      rowKeys.add(UUID.randomUUID().toString());\n    }\n    String filePath = folder.getRoot() + \"/test.parquet\";\n    Schema schema = HoodieAvroUtils.getRecordKeySchema();\n    BloomFilter filter = BloomFilterFactory.createBloomFilter(\n        1000, 0.0001, 10000,\n        BloomFilterTypeCode.SIMPLE.name());\n    HoodieAvroWriteSupport writeSupport = new HoodieAvroWriteSupport(\n        new AvroSchemaConverter().convert(schema), schema, filter);\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, rowKey);\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/avro/TestHoodieAvroWriteSupport.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":47,"status":"B"},{"authorDate":"2020-04-09 13:50:36","commitOrder":3,"curCode":"  private void writeParquetFile(String filePath, List<String> rowKeys) throws Exception {\n    \r\n    Schema schema = HoodieAvroUtils.getRecordKeySchema();\n    BloomFilter filter = BloomFilterFactory\n        .createBloomFilter(1000, 0.0001, 10000, bloomFilterTypeToTest);\n    HoodieAvroWriteSupport writeSupport =\n        new HoodieAvroWriteSupport(new AvroSchemaConverter().convert(schema), schema, filter);\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, rowKey);\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","date":"2020-04-09 23:18:02","endLine":146,"groupId":"2364","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"writeParquetFile","params":"(StringfilePath@List<String>rowKeys)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/58/c4580de4b260906b3165efb6b2bb51b435d3e6.src","preCode":"  private void writeParquetFile(String filePath, List<String> rowKeys) throws Exception {\n    \r\n    Schema schema = HoodieAvroUtils.getRecordKeySchema();\n    BloomFilter filter = BloomFilterFactory\n        .createBloomFilter(1000, 0.0001, 10000, bloomFilterTypeToTest);\n    HoodieAvroWriteSupport writeSupport =\n        new HoodieAvroWriteSupport(new AvroSchemaConverter().convert(schema), schema, filter);\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, rowKey);\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/util/TestParquetUtils.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":130,"status":"MB"}],"commitId":"996f7612323729d3ab4bedc1b0c7754aded2dc00","commitMessage":"@@@Trying git merge --squash\n","date":"2020-04-09 23:18:02","modifiedFileCount":"2","status":"M","submitter":"Abhishek Modi"},{"authorTime":"2020-04-09 13:50:36","codes":[{"authorDate":"2020-04-16 03:35:01","commitOrder":4,"curCode":"  public void testAddKey(@TempDir java.nio.file.Path tempDir) throws IOException {\n    List<String> rowKeys = new ArrayList<>();\n    for (int i = 0; i < 1000; i++) {\n      rowKeys.add(UUID.randomUUID().toString());\n    }\n    String filePath = tempDir.resolve(\"test.parquet\").toAbsolutePath().toString();\n    Schema schema = HoodieAvroUtils.getRecordKeySchema();\n    BloomFilter filter = BloomFilterFactory.createBloomFilter(\n        1000, 0.0001, 10000,\n        BloomFilterTypeCode.SIMPLE.name());\n    HoodieAvroWriteSupport writeSupport = new HoodieAvroWriteSupport(\n        new AvroSchemaConverter().convert(schema), schema, filter);\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, rowKey);\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","date":"2020-04-16 03:35:01","endLine":65,"groupId":"2364","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testAddKey","params":"(@TempDirjava.nio.file.PathtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/72/f1453d550342bdad3042de76fc66cadba687ae.src","preCode":"  public void testAddKey() throws IOException {\n    List<String> rowKeys = new ArrayList<>();\n    for (int i = 0; i < 1000; i++) {\n      rowKeys.add(UUID.randomUUID().toString());\n    }\n    String filePath = folder.getRoot() + \"/test.parquet\";\n    Schema schema = HoodieAvroUtils.getRecordKeySchema();\n    BloomFilter filter = BloomFilterFactory.createBloomFilter(\n        1000, 0.0001, 10000,\n        BloomFilterTypeCode.SIMPLE.name());\n    HoodieAvroWriteSupport writeSupport = new HoodieAvroWriteSupport(\n        new AvroSchemaConverter().convert(schema), schema, filter);\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, rowKey);\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/avro/TestHoodieAvroWriteSupport.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":44,"status":"M"},{"authorDate":"2020-04-09 13:50:36","commitOrder":4,"curCode":"  private void writeParquetFile(String filePath, List<String> rowKeys) throws Exception {\n    \r\n    Schema schema = HoodieAvroUtils.getRecordKeySchema();\n    BloomFilter filter = BloomFilterFactory\n        .createBloomFilter(1000, 0.0001, 10000, bloomFilterTypeToTest);\n    HoodieAvroWriteSupport writeSupport =\n        new HoodieAvroWriteSupport(new AvroSchemaConverter().convert(schema), schema, filter);\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, rowKey);\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","date":"2020-04-09 23:18:02","endLine":146,"groupId":"2364","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"writeParquetFile","params":"(StringfilePath@List<String>rowKeys)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/58/c4580de4b260906b3165efb6b2bb51b435d3e6.src","preCode":"  private void writeParquetFile(String filePath, List<String> rowKeys) throws Exception {\n    \r\n    Schema schema = HoodieAvroUtils.getRecordKeySchema();\n    BloomFilter filter = BloomFilterFactory\n        .createBloomFilter(1000, 0.0001, 10000, bloomFilterTypeToTest);\n    HoodieAvroWriteSupport writeSupport =\n        new HoodieAvroWriteSupport(new AvroSchemaConverter().convert(schema), schema, filter);\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, rowKey);\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/util/TestParquetUtils.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":130,"status":"N"}],"commitId":"d65efe659d36dfb80be16a6389c5aba2f8701c39","commitMessage":"@@@[HUDI-780] Migrate test cases to Junit 5 (#1504)\n\n","date":"2020-04-16 03:35:01","modifiedFileCount":"14","status":"M","submitter":"Raymond Xu"},{"authorTime":"2020-05-06 19:15:20","codes":[{"authorDate":"2020-04-16 03:35:01","commitOrder":5,"curCode":"  public void testAddKey(@TempDir java.nio.file.Path tempDir) throws IOException {\n    List<String> rowKeys = new ArrayList<>();\n    for (int i = 0; i < 1000; i++) {\n      rowKeys.add(UUID.randomUUID().toString());\n    }\n    String filePath = tempDir.resolve(\"test.parquet\").toAbsolutePath().toString();\n    Schema schema = HoodieAvroUtils.getRecordKeySchema();\n    BloomFilter filter = BloomFilterFactory.createBloomFilter(\n        1000, 0.0001, 10000,\n        BloomFilterTypeCode.SIMPLE.name());\n    HoodieAvroWriteSupport writeSupport = new HoodieAvroWriteSupport(\n        new AvroSchemaConverter().convert(schema), schema, filter);\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, rowKey);\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","date":"2020-04-16 03:35:01","endLine":65,"groupId":"2364","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testAddKey","params":"(@TempDirjava.nio.file.PathtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/72/f1453d550342bdad3042de76fc66cadba687ae.src","preCode":"  public void testAddKey(@TempDir java.nio.file.Path tempDir) throws IOException {\n    List<String> rowKeys = new ArrayList<>();\n    for (int i = 0; i < 1000; i++) {\n      rowKeys.add(UUID.randomUUID().toString());\n    }\n    String filePath = tempDir.resolve(\"test.parquet\").toAbsolutePath().toString();\n    Schema schema = HoodieAvroUtils.getRecordKeySchema();\n    BloomFilter filter = BloomFilterFactory.createBloomFilter(\n        1000, 0.0001, 10000,\n        BloomFilterTypeCode.SIMPLE.name());\n    HoodieAvroWriteSupport writeSupport = new HoodieAvroWriteSupport(\n        new AvroSchemaConverter().convert(schema), schema, filter);\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, rowKey);\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/avro/TestHoodieAvroWriteSupport.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":44,"status":"N"},{"authorDate":"2020-05-06 19:15:20","commitOrder":5,"curCode":"  private void writeParquetFile(String typeCode, String filePath, List<String> rowKeys) throws Exception {\n    \r\n    Schema schema = HoodieAvroUtils.getRecordKeySchema();\n    BloomFilter filter = BloomFilterFactory\n        .createBloomFilter(1000, 0.0001, 10000, typeCode);\n    HoodieAvroWriteSupport writeSupport =\n        new HoodieAvroWriteSupport(new AvroSchemaConverter().convert(schema), schema, filter);\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, rowKey);\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","date":"2020-05-06 19:15:20","endLine":139,"groupId":"2364","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"writeParquetFile","params":"(StringtypeCode@StringfilePath@List<String>rowKeys)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/2d/2084d5a2f706db813a82d4d4ab25830d6c5cee.src","preCode":"  private void writeParquetFile(String filePath, List<String> rowKeys) throws Exception {\n    \r\n    Schema schema = HoodieAvroUtils.getRecordKeySchema();\n    BloomFilter filter = BloomFilterFactory\n        .createBloomFilter(1000, 0.0001, 10000, bloomFilterTypeToTest);\n    HoodieAvroWriteSupport writeSupport =\n        new HoodieAvroWriteSupport(new AvroSchemaConverter().convert(schema), schema, filter);\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, rowKey);\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/util/TestParquetUtils.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":123,"status":"M"}],"commitId":"366bb10d8c4fe98424f09a6cf6f4aee7716451a4","commitMessage":"@@@[HUDI-812] Migrate hudi common tests to JUnit 5 (#1590)\n\n* [HUDI-812] Migrate hudi-common tests to JUnit 5","date":"2020-05-06 19:15:20","modifiedFileCount":"38","status":"M","submitter":"Raymond Xu"},{"authorTime":"2020-05-18 09:32:24","codes":[{"authorDate":"2020-04-16 03:35:01","commitOrder":6,"curCode":"  public void testAddKey(@TempDir java.nio.file.Path tempDir) throws IOException {\n    List<String> rowKeys = new ArrayList<>();\n    for (int i = 0; i < 1000; i++) {\n      rowKeys.add(UUID.randomUUID().toString());\n    }\n    String filePath = tempDir.resolve(\"test.parquet\").toAbsolutePath().toString();\n    Schema schema = HoodieAvroUtils.getRecordKeySchema();\n    BloomFilter filter = BloomFilterFactory.createBloomFilter(\n        1000, 0.0001, 10000,\n        BloomFilterTypeCode.SIMPLE.name());\n    HoodieAvroWriteSupport writeSupport = new HoodieAvroWriteSupport(\n        new AvroSchemaConverter().convert(schema), schema, filter);\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, rowKey);\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","date":"2020-04-16 03:35:01","endLine":65,"groupId":"2364","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testAddKey","params":"(@TempDirjava.nio.file.PathtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/72/f1453d550342bdad3042de76fc66cadba687ae.src","preCode":"  public void testAddKey(@TempDir java.nio.file.Path tempDir) throws IOException {\n    List<String> rowKeys = new ArrayList<>();\n    for (int i = 0; i < 1000; i++) {\n      rowKeys.add(UUID.randomUUID().toString());\n    }\n    String filePath = tempDir.resolve(\"test.parquet\").toAbsolutePath().toString();\n    Schema schema = HoodieAvroUtils.getRecordKeySchema();\n    BloomFilter filter = BloomFilterFactory.createBloomFilter(\n        1000, 0.0001, 10000,\n        BloomFilterTypeCode.SIMPLE.name());\n    HoodieAvroWriteSupport writeSupport = new HoodieAvroWriteSupport(\n        new AvroSchemaConverter().convert(schema), schema, filter);\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, rowKey);\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/avro/TestHoodieAvroWriteSupport.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":44,"status":"N"},{"authorDate":"2020-05-18 09:32:24","commitOrder":6,"curCode":"  private void writeParquetFile(String typeCode, String filePath, List<String> rowKeys, Schema schema, boolean addPartitionPathField, String partitionPath) throws Exception {\n    \r\n    BloomFilter filter = BloomFilterFactory\n        .createBloomFilter(1000, 0.0001, 10000, typeCode);\n    HoodieAvroWriteSupport writeSupport =\n        new HoodieAvroWriteSupport(new AvroSchemaConverter().convert(schema), schema, filter);\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, rowKey);\n      if (addPartitionPathField) {\n        rec.put(HoodieRecord.PARTITION_PATH_METADATA_FIELD, partitionPath);\n      }\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","date":"2020-05-18 09:32:24","endLine":172,"groupId":"2364","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"writeParquetFile","params":"(StringtypeCode@StringfilePath@List<String>rowKeys@Schemaschema@booleanaddPartitionPathField@StringpartitionPath)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/15/b160292f4932b549e882e5479548075e24f09e.src","preCode":"  private void writeParquetFile(String typeCode, String filePath, List<String> rowKeys) throws Exception {\n    \r\n    Schema schema = HoodieAvroUtils.getRecordKeySchema();\n    BloomFilter filter = BloomFilterFactory\n        .createBloomFilter(1000, 0.0001, 10000, typeCode);\n    HoodieAvroWriteSupport writeSupport =\n        new HoodieAvroWriteSupport(new AvroSchemaConverter().convert(schema), schema, filter);\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, rowKey);\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/util/TestParquetUtils.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":154,"status":"M"}],"commitId":"29edf4b3b8ade64ec7822d6b7b2a125d5ca781c4","commitMessage":"@@@[HUDI-407] Adding Simple Index to Hoodie. (#1402)\n\nThis index finds the location by joining incoming records with records from base files.","date":"2020-05-18 09:32:24","modifiedFileCount":"11","status":"M","submitter":"Sivabalan Narayanan"},{"authorTime":"2021-07-27 05:21:04","codes":[{"authorDate":"2021-07-27 05:21:04","commitOrder":7,"curCode":"  public void testAddKey(@TempDir java.nio.file.Path tempDir) throws IOException {\n    List<String> rowKeys = new ArrayList<>();\n    for (int i = 0; i < 1000; i++) {\n      rowKeys.add(UUID.randomUUID().toString());\n    }\n    String filePath = tempDir.resolve(\"test.parquet\").toAbsolutePath().toString();\n    Schema schema = HoodieAvroUtils.getRecordKeySchema();\n    BloomFilter filter = BloomFilterFactory.createBloomFilter(\n        1000, 0.0001, 10000,\n        BloomFilterTypeCode.SIMPLE.name());\n    HoodieAvroWriteSupport writeSupport = new HoodieAvroWriteSupport(\n        new AvroSchemaConverter().convert(schema), schema, Option.of(filter));\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, rowKey);\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","date":"2021-07-27 05:21:04","endLine":66,"groupId":"10875","id":9,"instanceNumber":1,"isCurCommit":1,"methodName":"testAddKey","params":"(@TempDirjava.nio.file.PathtempDir)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/16/a77c145cf296fb1e555f50587f603c9d8c07e6.src","preCode":"  public void testAddKey(@TempDir java.nio.file.Path tempDir) throws IOException {\n    List<String> rowKeys = new ArrayList<>();\n    for (int i = 0; i < 1000; i++) {\n      rowKeys.add(UUID.randomUUID().toString());\n    }\n    String filePath = tempDir.resolve(\"test.parquet\").toAbsolutePath().toString();\n    Schema schema = HoodieAvroUtils.getRecordKeySchema();\n    BloomFilter filter = BloomFilterFactory.createBloomFilter(\n        1000, 0.0001, 10000,\n        BloomFilterTypeCode.SIMPLE.name());\n    HoodieAvroWriteSupport writeSupport = new HoodieAvroWriteSupport(\n        new AvroSchemaConverter().convert(schema), schema, filter);\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, rowKey);\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/avro/TestHoodieAvroWriteSupport.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":45,"status":"M"},{"authorDate":"2021-07-27 05:21:04","commitOrder":7,"curCode":"  private void writeParquetFile(String typeCode, String filePath, List<String> rowKeys, Schema schema, boolean addPartitionPathField, String partitionPathValue,\n                                boolean useMetaFields, String recordFieldName, String partitionFieldName) throws Exception {\n    \r\n    BloomFilter filter = BloomFilterFactory\n        .createBloomFilter(1000, 0.0001, 10000, typeCode);\n    HoodieAvroWriteSupport writeSupport =\n        new HoodieAvroWriteSupport(new AvroSchemaConverter().convert(schema), schema, Option.of(filter));\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(useMetaFields ? HoodieRecord.RECORD_KEY_METADATA_FIELD : recordFieldName, rowKey);\n      if (addPartitionPathField) {\n        rec.put(useMetaFields ? HoodieRecord.PARTITION_PATH_METADATA_FIELD : partitionFieldName, partitionPathValue);\n      }\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","date":"2021-07-27 05:21:04","endLine":225,"groupId":"10875","id":10,"instanceNumber":2,"isCurCommit":1,"methodName":"writeParquetFile","params":"(StringtypeCode@StringfilePath@List<String>rowKeys@Schemaschema@booleanaddPartitionPathField@StringpartitionPathValue@booleanuseMetaFields@StringrecordFieldName@StringpartitionFieldName)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e0/7c0fad3d24e1cafe48cc7ee15f28f693e9a073.src","preCode":"  private void writeParquetFile(String typeCode, String filePath, List<String> rowKeys, Schema schema, boolean addPartitionPathField, String partitionPath) throws Exception {\n    \r\n    BloomFilter filter = BloomFilterFactory\n        .createBloomFilter(1000, 0.0001, 10000, typeCode);\n    HoodieAvroWriteSupport writeSupport =\n        new HoodieAvroWriteSupport(new AvroSchemaConverter().convert(schema), schema, filter);\n    ParquetWriter writer = new ParquetWriter(new Path(filePath), writeSupport, CompressionCodecName.GZIP,\n        120 * 1024 * 1024, ParquetWriter.DEFAULT_PAGE_SIZE);\n    for (String rowKey : rowKeys) {\n      GenericRecord rec = new GenericData.Record(schema);\n      rec.put(HoodieRecord.RECORD_KEY_METADATA_FIELD, rowKey);\n      if (addPartitionPathField) {\n        rec.put(HoodieRecord.PARTITION_PATH_METADATA_FIELD, partitionPath);\n      }\n      writer.write(rec);\n      writeSupport.add(rowKey);\n    }\n    writer.close();\n  }\n","realPath":"hudi-common/src/test/java/org/apache/hudi/common/util/TestParquetUtils.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":206,"status":"M"}],"commitId":"61148c1c43c9ff5ba04b6c174e9e2a006db9e7b3","commitMessage":"@@@[HUDI-2176.  2178.  2179] Adding virtual key support to COW table (#3306)\n\n","date":"2021-07-27 05:21:04","modifiedFileCount":"42","status":"M","submitter":"Sivabalan Narayanan"}]
