[{"authorTime":"2020-10-02 05:25:29","codes":[{"authorDate":"2020-10-02 05:25:29","commitOrder":1,"curCode":"  public void testKeepLatestFileVersionsMOR() throws Exception {\n\n    HoodieWriteConfig config =\n        HoodieWriteConfig.newBuilder().withPath(basePath).withAssumeDatePartitioning(true)\n            .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS).retainFileVersions(1).build())\n            .build();\n\n    HoodieTableMetaClient metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient);\n    String p0 = \"2020/01/01\";\n\n    \r\n    String file1P0 = testTable.addDeltaCommit(\"000\").withBaseFilesInPartitions(p0).get(p0);\n    testTable.forDeltaCommit(\"000\")\n        .withLogFile(p0, file1P0, 1)\n        .withLogFile(p0, file1P0, 2);\n\n    \r\n    testTable.addDeltaCommit(\"001\")\n        .withBaseFilesInPartition(p0, file1P0)\n        .withLogFile(p0, file1P0, 3);\n\n    List<HoodieCleanStat> hoodieCleanStats = runCleaner(config);\n    assertEquals(3,\n        getCleanStat(hoodieCleanStats, p0).getSuccessDeleteFiles()\n            .size(), \"Must clean three files, one parquet and 2 log files\");\n    assertFalse(testTable.baseFileExists(p0, \"000\", file1P0));\n    assertFalse(testTable.logFilesExist(p0, \"000\", file1P0, 1, 2));\n    assertTrue(testTable.baseFileExists(p0, \"001\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"001\", file1P0, 3));\n  }\n","date":"2020-10-02 05:25:29","endLine":646,"groupId":"4833","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testKeepLatestFileVersionsMOR","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/73/0e1ef01acf331061c6f69fcf5963de57f09433.src","preCode":"  public void testKeepLatestFileVersionsMOR() throws Exception {\n\n    HoodieWriteConfig config =\n        HoodieWriteConfig.newBuilder().withPath(basePath).withAssumeDatePartitioning(true)\n            .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS).retainFileVersions(1).build())\n            .build();\n\n    HoodieTableMetaClient metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient);\n    String p0 = \"2020/01/01\";\n\n    \r\n    String file1P0 = testTable.addDeltaCommit(\"000\").withBaseFilesInPartitions(p0).get(p0);\n    testTable.forDeltaCommit(\"000\")\n        .withLogFile(p0, file1P0, 1)\n        .withLogFile(p0, file1P0, 2);\n\n    \r\n    testTable.addDeltaCommit(\"001\")\n        .withBaseFilesInPartition(p0, file1P0)\n        .withLogFile(p0, file1P0, 3);\n\n    List<HoodieCleanStat> hoodieCleanStats = runCleaner(config);\n    assertEquals(3,\n        getCleanStat(hoodieCleanStats, p0).getSuccessDeleteFiles()\n            .size(), \"Must clean three files, one parquet and 2 log files\");\n    assertFalse(testTable.baseFileExists(p0, \"000\", file1P0));\n    assertFalse(testTable.logFilesExist(p0, \"000\", file1P0, 1, 2));\n    assertTrue(testTable.baseFileExists(p0, \"001\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"001\", file1P0, 3));\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":615,"status":"B"},{"authorDate":"2020-10-02 05:25:29","commitOrder":1,"curCode":"  public void testKeepLatestCommitsWithPendingCompactions() throws IOException {\n    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath).withAssumeDatePartitioning(true)\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n            .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS).retainCommits(2).build())\n        .build();\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    testPendingCompactions(config, 48, 18, false);\n  }\n","date":"2020-10-02 05:25:29","endLine":1039,"groupId":"4571","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testKeepLatestCommitsWithPendingCompactions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/73/0e1ef01acf331061c6f69fcf5963de57f09433.src","preCode":"  public void testKeepLatestCommitsWithPendingCompactions() throws IOException {\n    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath).withAssumeDatePartitioning(true)\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n            .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS).retainCommits(2).build())\n        .build();\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    testPendingCompactions(config, 48, 18, false);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":1024,"status":"B"}],"commitId":"1f7add92916c37b05be270d9c75a9042134ec506","commitMessage":"@@@[HUDI-1089] Refactor hudi-client to support multi-engine (#1827)\n\n- This change breaks `hudi-client` into `hudi-client-common` and `hudi-spark-client` modules \n- Simple usages of Spark using jsc.parallelize() has been redone using EngineContext#map.  EngineContext#flatMap etc\n- Code changes in the PR.  break classes into `BaseXYZ` parent classes with no spark dependencies living in `hudi-client-common`\n- Classes on `hudi-spark-client` are named `SparkXYZ` extending the parent classes with all the Spark dependencies\n- To simplify/cleanup.  HoodieIndex#fetchRecordLocation has been removed and its usages in tests replaced with alternatives\n\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2020-10-02 05:25:29","modifiedFileCount":"31","status":"B","submitter":"Mathieu"},{"authorTime":"2020-10-12 14:39:10","codes":[{"authorDate":"2020-10-12 14:39:10","commitOrder":2,"curCode":"  public void testKeepLatestFileVersionsMOR() throws Exception {\n\n    HoodieWriteConfig config =\n        HoodieWriteConfig.newBuilder().withPath(basePath).withAssumeDatePartitioning(true)\n            .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS).retainFileVersions(1).build())\n            .build();\n\n    HoodieTableMetaClient metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient);\n    String p0 = \"2020/01/01\";\n\n    \r\n    String file1P0 = testTable.addDeltaCommit(\"000\").getFileIdsWithBaseFilesInPartitions(p0).get(p0);\n    testTable.forDeltaCommit(\"000\")\n        .withLogFile(p0, file1P0, 1)\n        .withLogFile(p0, file1P0, 2);\n\n    \r\n    testTable.addDeltaCommit(\"001\")\n        .withBaseFilesInPartition(p0, file1P0)\n        .withLogFile(p0, file1P0, 3);\n\n    List<HoodieCleanStat> hoodieCleanStats = runCleaner(config);\n    assertEquals(3,\n        getCleanStat(hoodieCleanStats, p0).getSuccessDeleteFiles()\n            .size(), \"Must clean three files, one parquet and 2 log files\");\n    assertFalse(testTable.baseFileExists(p0, \"000\", file1P0));\n    assertFalse(testTable.logFilesExist(p0, \"000\", file1P0, 1, 2));\n    assertTrue(testTable.baseFileExists(p0, \"001\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"001\", file1P0, 3));\n  }\n","date":"2020-10-12 14:39:10","endLine":645,"groupId":"4833","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testKeepLatestFileVersionsMOR","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/00/f1ea00ea94bf95a807008e252d2a524e4b64ab.src","preCode":"  public void testKeepLatestFileVersionsMOR() throws Exception {\n\n    HoodieWriteConfig config =\n        HoodieWriteConfig.newBuilder().withPath(basePath).withAssumeDatePartitioning(true)\n            .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS).retainFileVersions(1).build())\n            .build();\n\n    HoodieTableMetaClient metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient);\n    String p0 = \"2020/01/01\";\n\n    \r\n    String file1P0 = testTable.addDeltaCommit(\"000\").withBaseFilesInPartitions(p0).get(p0);\n    testTable.forDeltaCommit(\"000\")\n        .withLogFile(p0, file1P0, 1)\n        .withLogFile(p0, file1P0, 2);\n\n    \r\n    testTable.addDeltaCommit(\"001\")\n        .withBaseFilesInPartition(p0, file1P0)\n        .withLogFile(p0, file1P0, 3);\n\n    List<HoodieCleanStat> hoodieCleanStats = runCleaner(config);\n    assertEquals(3,\n        getCleanStat(hoodieCleanStats, p0).getSuccessDeleteFiles()\n            .size(), \"Must clean three files, one parquet and 2 log files\");\n    assertFalse(testTable.baseFileExists(p0, \"000\", file1P0));\n    assertFalse(testTable.logFilesExist(p0, \"000\", file1P0, 1, 2));\n    assertTrue(testTable.baseFileExists(p0, \"001\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"001\", file1P0, 3));\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":614,"status":"M"},{"authorDate":"2020-10-12 14:39:10","commitOrder":2,"curCode":"  public void testKeepLatestCommitsWithPendingCompactions() throws Exception {\n    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath).withAssumeDatePartitioning(true)\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n            .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS).retainCommits(2).build())\n        .build();\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    testPendingCompactions(config, 48, 18, false);\n  }\n","date":"2020-10-12 14:39:10","endLine":1038,"groupId":"4571","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testKeepLatestCommitsWithPendingCompactions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/00/f1ea00ea94bf95a807008e252d2a524e4b64ab.src","preCode":"  public void testKeepLatestCommitsWithPendingCompactions() throws IOException {\n    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath).withAssumeDatePartitioning(true)\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n            .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS).retainCommits(2).build())\n        .build();\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    testPendingCompactions(config, 48, 18, false);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":1023,"status":"M"}],"commitId":"c5e10d668f9366f29bdf7721f7efe4140782527b","commitMessage":"@@@[HUDI-995] Migrate HoodieTestUtils APIs to HoodieTestTable (#2167)\n\nRemove APIs in `HoodieTestUtils`\n- `createCommitFiles`\n- `createDataFile`\n- `createNewLogFile`\n- `createCompactionRequest`\n\nMigrated usages in `TestCleaner#testPendingCompactions`.\n\nAlso improved some API names in `HoodieTestTable`.","date":"2020-10-12 14:39:10","modifiedFileCount":"6","status":"M","submitter":"Raymond Xu"},{"authorTime":"2021-01-20 13:20:28","codes":[{"authorDate":"2021-01-20 13:20:28","commitOrder":3,"curCode":"  public void testKeepLatestFileVersionsMOR() throws Exception {\n\n    HoodieWriteConfig config =\n        HoodieWriteConfig.newBuilder().withPath(basePath)\n            .withMetadataConfig(HoodieMetadataConfig.newBuilder().withAssumeDatePartitioning(true).build())\n            .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS).retainFileVersions(1).build())\n            .build();\n\n    HoodieTableMetaClient metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient);\n    String p0 = \"2020/01/01\";\n\n    \r\n    String file1P0 = testTable.addDeltaCommit(\"000\").getFileIdsWithBaseFilesInPartitions(p0).get(p0);\n    testTable.forDeltaCommit(\"000\")\n        .withLogFile(p0, file1P0, 1)\n        .withLogFile(p0, file1P0, 2);\n\n    \r\n    testTable.addDeltaCommit(\"001\")\n        .withBaseFilesInPartition(p0, file1P0)\n        .withLogFile(p0, file1P0, 3);\n\n    List<HoodieCleanStat> hoodieCleanStats = runCleaner(config);\n    assertEquals(3,\n        getCleanStat(hoodieCleanStats, p0).getSuccessDeleteFiles()\n            .size(), \"Must clean three files, one parquet and 2 log files\");\n    assertFalse(testTable.baseFileExists(p0, \"000\", file1P0));\n    assertFalse(testTable.logFilesExist(p0, \"000\", file1P0, 1, 2));\n    assertTrue(testTable.baseFileExists(p0, \"001\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"001\", file1P0, 3));\n  }\n","date":"2021-01-20 13:20:28","endLine":649,"groupId":"4833","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testKeepLatestFileVersionsMOR","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/4f/ff08abfbcef6651c952fa0d168eff4cfa8682c.src","preCode":"  public void testKeepLatestFileVersionsMOR() throws Exception {\n\n    HoodieWriteConfig config =\n        HoodieWriteConfig.newBuilder().withPath(basePath).withAssumeDatePartitioning(true)\n            .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS).retainFileVersions(1).build())\n            .build();\n\n    HoodieTableMetaClient metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient);\n    String p0 = \"2020/01/01\";\n\n    \r\n    String file1P0 = testTable.addDeltaCommit(\"000\").getFileIdsWithBaseFilesInPartitions(p0).get(p0);\n    testTable.forDeltaCommit(\"000\")\n        .withLogFile(p0, file1P0, 1)\n        .withLogFile(p0, file1P0, 2);\n\n    \r\n    testTable.addDeltaCommit(\"001\")\n        .withBaseFilesInPartition(p0, file1P0)\n        .withLogFile(p0, file1P0, 3);\n\n    List<HoodieCleanStat> hoodieCleanStats = runCleaner(config);\n    assertEquals(3,\n        getCleanStat(hoodieCleanStats, p0).getSuccessDeleteFiles()\n            .size(), \"Must clean three files, one parquet and 2 log files\");\n    assertFalse(testTable.baseFileExists(p0, \"000\", file1P0));\n    assertFalse(testTable.logFilesExist(p0, \"000\", file1P0, 1, 2));\n    assertTrue(testTable.baseFileExists(p0, \"001\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"001\", file1P0, 3));\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":617,"status":"M"},{"authorDate":"2021-01-20 13:20:28","commitOrder":3,"curCode":"  public void testKeepLatestCommitsWithPendingCompactions() throws Exception {\n    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath)\n        .withMetadataConfig(HoodieMetadataConfig.newBuilder().withAssumeDatePartitioning(true).build())\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n            .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS).retainCommits(2).build())\n        .build();\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    testPendingCompactions(config, 48, 18, false);\n  }\n","date":"2021-01-20 13:20:28","endLine":1192,"groupId":"4571","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testKeepLatestCommitsWithPendingCompactions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/4f/ff08abfbcef6651c952fa0d168eff4cfa8682c.src","preCode":"  public void testKeepLatestCommitsWithPendingCompactions() throws Exception {\n    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath).withAssumeDatePartitioning(true)\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n            .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS).retainCommits(2).build())\n        .build();\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    testPendingCompactions(config, 48, 18, false);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":1176,"status":"M"}],"commitId":"5ca0625b277efa3a73d2ae0fbdfa4c6163f312d2","commitMessage":"@@@[HUDI 1308] Harden RFC-15 Implementation based on production testing (#2441)\n\nAddresses leaks.  perf degradation observed during testing. These were regressions from the original rfc-15 PoC implementation.\n\n* Pass a single instance of HoodieTableMetadata everywhere\n* Fix tests and add config for enabling metrics\n - Removed special casing of assumeDatePartitioning inside FSUtils#getAllPartitionPaths()\n - Consequently.  IOException is never thrown and many files had to be adjusted\n- More diligent handling of open file handles in metadata table\n - Added config for controlling reuse of connections\n - Added config for turning off fallback to listing.  so we can see tests fail\n - Changed all ipf listing code to cache/amortize the open/close for better performance\n - Timelineserver also reuses connections.  for better performance\n - Without timelineserver.  when metadata table is opened from executors.  reuse is not allowed\n - HoodieMetadataConfig passed into HoodieTableMetadata#create as argument.\n -  Fix TestHoodieBackedTableMetadata#testSync","date":"2021-01-20 13:20:28","modifiedFileCount":"53","status":"M","submitter":"vinoth chandar"},{"authorTime":"2021-01-20 13:20:28","codes":[{"authorDate":"2021-05-12 01:01:45","commitOrder":4,"curCode":"  public void testKeepLatestFileVersionsMOR() throws Exception {\n\n    HoodieWriteConfig config =\n        HoodieWriteConfig.newBuilder().withPath(basePath)\n            .withMetadataConfig(HoodieMetadataConfig.newBuilder().withAssumeDatePartitioning(true).build())\n            .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS).retainFileVersions(1).build())\n            .build();\n\n    HoodieTableMetaClient metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient);\n    String p0 = \"2020/01/01\";\n\n    \r\n    String file1P0 = testTable.addDeltaCommit(\"000\").getFileIdsWithBaseFilesInPartitions(p0).get(p0);\n    testTable.forDeltaCommit(\"000\")\n        .withLogFile(p0, file1P0, 1)\n        .withLogFile(p0, file1P0, 2);\n\n    \r\n    testTable.addDeltaCommit(\"001\")\n        .withBaseFilesInPartition(p0, file1P0)\n        .withLogFile(p0, file1P0, 3);\n\n    List<HoodieCleanStat> hoodieCleanStats = runCleaner(config);\n    assertEquals(3,\n        getCleanStat(hoodieCleanStats, p0).getSuccessDeleteFiles()\n            .size(), \"Must clean three files, one base and 2 log files\");\n    assertFalse(testTable.baseFileExists(p0, \"000\", file1P0));\n    assertFalse(testTable.logFilesExist(p0, \"000\", file1P0, 1, 2));\n    assertTrue(testTable.baseFileExists(p0, \"001\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"001\", file1P0, 3));\n  }\n","date":"2021-05-12 01:01:45","endLine":762,"groupId":"10535","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testKeepLatestFileVersionsMOR","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/cb/37ed4bcf2732df46a8ab47fda68fa27b9d1582.src","preCode":"  public void testKeepLatestFileVersionsMOR() throws Exception {\n\n    HoodieWriteConfig config =\n        HoodieWriteConfig.newBuilder().withPath(basePath)\n            .withMetadataConfig(HoodieMetadataConfig.newBuilder().withAssumeDatePartitioning(true).build())\n            .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n                .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS).retainFileVersions(1).build())\n            .build();\n\n    HoodieTableMetaClient metaClient = HoodieTestUtils.init(hadoopConf, basePath, HoodieTableType.MERGE_ON_READ);\n    HoodieTestTable testTable = HoodieTestTable.of(metaClient);\n    String p0 = \"2020/01/01\";\n\n    \r\n    String file1P0 = testTable.addDeltaCommit(\"000\").getFileIdsWithBaseFilesInPartitions(p0).get(p0);\n    testTable.forDeltaCommit(\"000\")\n        .withLogFile(p0, file1P0, 1)\n        .withLogFile(p0, file1P0, 2);\n\n    \r\n    testTable.addDeltaCommit(\"001\")\n        .withBaseFilesInPartition(p0, file1P0)\n        .withLogFile(p0, file1P0, 3);\n\n    List<HoodieCleanStat> hoodieCleanStats = runCleaner(config);\n    assertEquals(3,\n        getCleanStat(hoodieCleanStats, p0).getSuccessDeleteFiles()\n            .size(), \"Must clean three files, one parquet and 2 log files\");\n    assertFalse(testTable.baseFileExists(p0, \"000\", file1P0));\n    assertFalse(testTable.logFilesExist(p0, \"000\", file1P0, 1, 2));\n    assertTrue(testTable.baseFileExists(p0, \"001\", file1P0));\n    assertTrue(testTable.logFileExists(p0, \"001\", file1P0, 3));\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":730,"status":"M"},{"authorDate":"2021-01-20 13:20:28","commitOrder":4,"curCode":"  public void testKeepLatestCommitsWithPendingCompactions() throws Exception {\n    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath)\n        .withMetadataConfig(HoodieMetadataConfig.newBuilder().withAssumeDatePartitioning(true).build())\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n            .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS).retainCommits(2).build())\n        .build();\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    testPendingCompactions(config, 48, 18, false);\n  }\n","date":"2021-01-20 13:20:28","endLine":1192,"groupId":"10535","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testKeepLatestCommitsWithPendingCompactions","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/4f/ff08abfbcef6651c952fa0d168eff4cfa8682c.src","preCode":"  public void testKeepLatestCommitsWithPendingCompactions() throws Exception {\n    HoodieWriteConfig config = HoodieWriteConfig.newBuilder().withPath(basePath)\n        .withMetadataConfig(HoodieMetadataConfig.newBuilder().withAssumeDatePartitioning(true).build())\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder()\n            .withCleanerPolicy(HoodieCleaningPolicy.KEEP_LATEST_COMMITS).retainCommits(2).build())\n        .build();\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    \r\n    testPendingCompactions(config, 48, 18, false);\n  }\n","realPath":"hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/TestCleaner.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":1176,"status":"N"}],"commitId":"be9db2c4f5a570fcaa555618b34ad11109ed6b00","commitMessage":"@@@[HUDI-1055] Remove hardcoded parquet in tests (#2740)\n\n* Remove hardcoded parquet in tests\n* Use DataFileUtils.getInstance\n* Renaming DataFileUtils to BaseFileUtils\n\nCo-authored-by: Vinoth Chandar <vinoth@apache.org>","date":"2021-05-12 01:01:45","modifiedFileCount":"40","status":"M","submitter":"TeRS-K"}]
