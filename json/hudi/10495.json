[{"authorTime":"2020-11-18 17:57:11","codes":[{"authorDate":"2020-10-02 05:25:29","commitOrder":2,"curCode":"  public Iterator<List<WriteStatus>> handleInsert(String idPfx, Iterator<HoodieRecord<T>> recordItr)\n      throws Exception {\n    \r\n    if (!recordItr.hasNext()) {\n      LOG.info(\"Empty partition\");\n      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();\n    }\n    return new SparkLazyInsertIterable(recordItr, true, config, instantTime, table, idPfx,\n        taskContextSupplier, new CreateHandleFactory<>());\n  }\n","date":"2020-10-02 05:25:29","endLine":329,"groupId":"154","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"handleInsert","params":"(StringidPfx@Iterator<HoodieRecord<T>>recordItr)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/36/cca8cd1403c5fe6ced6d0b42aa6684081520d7.src","preCode":"  public Iterator<List<WriteStatus>> handleInsert(String idPfx, Iterator<HoodieRecord<T>> recordItr)\n      throws Exception {\n    \r\n    if (!recordItr.hasNext()) {\n      LOG.info(\"Empty partition\");\n      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();\n    }\n    return new SparkLazyInsertIterable(recordItr, true, config, instantTime, table, idPfx,\n        taskContextSupplier, new CreateHandleFactory<>());\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":320,"status":"NB"},{"authorDate":"2020-11-18 17:57:11","commitOrder":2,"curCode":"  public Iterator<List<WriteStatus>> handleInsert(String idPfx, Iterator<HoodieRecord<T>> recordItr)\n      throws Exception {\n    \r\n    if (!recordItr.hasNext()) {\n      LOG.info(\"Empty partition\");\n      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();\n    }\n    return new FlinkLazyInsertIterable<>(recordItr, true, config, instantTime, table, idPfx,\n        taskContextSupplier, new CreateHandleFactory<>());\n  }\n","date":"2020-11-18 17:57:11","endLine":312,"groupId":"3976","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"handleInsert","params":"(StringidPfx@Iterator<HoodieRecord<T>>recordItr)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/0b/98b11049a24b3056bf1f49981561179611d6fe.src","preCode":"  public Iterator<List<WriteStatus>> handleInsert(String idPfx, Iterator<HoodieRecord<T>> recordItr)\n      throws Exception {\n    \r\n    if (!recordItr.hasNext()) {\n      LOG.info(\"Empty partition\");\n      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();\n    }\n    return new FlinkLazyInsertIterable<>(recordItr, true, config, instantTime, table, idPfx,\n        taskContextSupplier, new CreateHandleFactory<>());\n  }\n","realPath":"hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/BaseFlinkCommitActionExecutor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":303,"status":"B"}],"commitId":"4d05680038752077ceaebef261b66a5afc761e10","commitMessage":"@@@[HUDI-1327] Introduce base implemetation of hudi-flink-client (#2176)\n\n","date":"2020-11-18 17:57:11","modifiedFileCount":"6","status":"M","submitter":"wangxianghu"},{"authorTime":"2021-02-06 22:03:52","codes":[{"authorDate":"2020-10-02 05:25:29","commitOrder":3,"curCode":"  public Iterator<List<WriteStatus>> handleInsert(String idPfx, Iterator<HoodieRecord<T>> recordItr)\n      throws Exception {\n    \r\n    if (!recordItr.hasNext()) {\n      LOG.info(\"Empty partition\");\n      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();\n    }\n    return new SparkLazyInsertIterable(recordItr, true, config, instantTime, table, idPfx,\n        taskContextSupplier, new CreateHandleFactory<>());\n  }\n","date":"2020-10-02 05:25:29","endLine":329,"groupId":"154","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"handleInsert","params":"(StringidPfx@Iterator<HoodieRecord<T>>recordItr)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/36/cca8cd1403c5fe6ced6d0b42aa6684081520d7.src","preCode":"  public Iterator<List<WriteStatus>> handleInsert(String idPfx, Iterator<HoodieRecord<T>> recordItr)\n      throws Exception {\n    \r\n    if (!recordItr.hasNext()) {\n      LOG.info(\"Empty partition\");\n      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();\n    }\n    return new SparkLazyInsertIterable(recordItr, true, config, instantTime, table, idPfx,\n        taskContextSupplier, new CreateHandleFactory<>());\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":320,"status":"N"},{"authorDate":"2021-02-06 22:03:52","commitOrder":3,"curCode":"  public Iterator<List<WriteStatus>> handleInsert(String idPfx, Iterator<HoodieRecord<T>> recordItr)\n      throws Exception {\n    \r\n    if (!recordItr.hasNext()) {\n      LOG.info(\"Empty partition\");\n      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();\n    }\n    return new FlinkLazyInsertIterable<>(recordItr, true, config, instantTime, table, idPfx,\n        taskContextSupplier, new FlinkCreateHandleFactory<>());\n  }\n","date":"2021-02-06 22:03:52","endLine":246,"groupId":"3976","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"handleInsert","params":"(StringidPfx@Iterator<HoodieRecord<T>>recordItr)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/04/4f841d276156fb15db71acef307560286c9023.src","preCode":"  public Iterator<List<WriteStatus>> handleInsert(String idPfx, Iterator<HoodieRecord<T>> recordItr)\n      throws Exception {\n    \r\n    if (!recordItr.hasNext()) {\n      LOG.info(\"Empty partition\");\n      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();\n    }\n    return new FlinkLazyInsertIterable<>(recordItr, true, config, instantTime, table, idPfx,\n        taskContextSupplier, new CreateHandleFactory<>());\n  }\n","realPath":"hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/BaseFlinkCommitActionExecutor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":237,"status":"M"}],"commitId":"4c5b6923ccfaaa6616a934a3f690b1a795a42d41","commitMessage":"@@@[HUDI-1557] Make Flink write pipeline write task scalable (#2506)\n\nThis is the #step 2 of RFC-24:\nhttps://cwiki.apache.org/confluence/display/HUDI/RFC+-+24%3A+Hoodie+Flink+Writer+Proposal\n\nThis PR introduce a BucketAssigner that assigns bucket ID (partition\npath & fileID) for each stream record.\n\nThere is no need to look up index and partition the records anymore in\nthe following pipeline for these records. \nwe actually decide the write target location before the write and each\nrecord computes its location when the BucketAssigner receives it.  thus. \nthe indexing is with streaming style.\n\nComputing locations for a batch of records all at a time is resource\nconsuming so a pressure to the engine. \nwe should avoid that in streaming system.","date":"2021-02-06 22:03:52","modifiedFileCount":"25","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-02-17 15:24:50","codes":[{"authorDate":"2020-10-02 05:25:29","commitOrder":4,"curCode":"  public Iterator<List<WriteStatus>> handleInsert(String idPfx, Iterator<HoodieRecord<T>> recordItr)\n      throws Exception {\n    \r\n    if (!recordItr.hasNext()) {\n      LOG.info(\"Empty partition\");\n      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();\n    }\n    return new SparkLazyInsertIterable(recordItr, true, config, instantTime, table, idPfx,\n        taskContextSupplier, new CreateHandleFactory<>());\n  }\n","date":"2020-10-02 05:25:29","endLine":329,"groupId":"154","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"handleInsert","params":"(StringidPfx@Iterator<HoodieRecord<T>>recordItr)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/36/cca8cd1403c5fe6ced6d0b42aa6684081520d7.src","preCode":"  public Iterator<List<WriteStatus>> handleInsert(String idPfx, Iterator<HoodieRecord<T>> recordItr)\n      throws Exception {\n    \r\n    if (!recordItr.hasNext()) {\n      LOG.info(\"Empty partition\");\n      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();\n    }\n    return new SparkLazyInsertIterable(recordItr, true, config, instantTime, table, idPfx,\n        taskContextSupplier, new CreateHandleFactory<>());\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":320,"status":"N"},{"authorDate":"2021-02-17 15:24:50","commitOrder":4,"curCode":"  public Iterator<List<WriteStatus>> handleInsert(String idPfx, Iterator<HoodieRecord<T>> recordItr)\n      throws Exception {\n    \r\n    if (!recordItr.hasNext()) {\n      LOG.info(\"Empty partition\");\n      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();\n    }\n    return new FlinkLazyInsertIterable<>(recordItr, true, config, instantTime, table, idPfx,\n        taskContextSupplier, new ExplicitCreateHandleFactory<>(writeHandle));\n  }\n","date":"2021-02-17 15:24:50","endLine":269,"groupId":"3976","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"handleInsert","params":"(StringidPfx@Iterator<HoodieRecord<T>>recordItr)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/9b/6dcd6335c7ce7627b9d56338b3931bf9abe79b.src","preCode":"  public Iterator<List<WriteStatus>> handleInsert(String idPfx, Iterator<HoodieRecord<T>> recordItr)\n      throws Exception {\n    \r\n    if (!recordItr.hasNext()) {\n      LOG.info(\"Empty partition\");\n      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();\n    }\n    return new FlinkLazyInsertIterable<>(recordItr, true, config, instantTime, table, idPfx,\n        taskContextSupplier, new FlinkCreateHandleFactory<>());\n  }\n","realPath":"hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/BaseFlinkCommitActionExecutor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":260,"status":"M"}],"commitId":"5d2491d10c70e4e5fc9b7aeb62cc64bcaaf6043f","commitMessage":"@@@[HUDI-1598] Write as minor batches during one checkpoint interval for the new writer (#2553)\n\n","date":"2021-02-17 15:24:50","modifiedFileCount":"22","status":"M","submitter":"Danny Chan"},{"authorTime":"2021-03-01 12:29:41","codes":[{"authorDate":"2020-10-02 05:25:29","commitOrder":5,"curCode":"  public Iterator<List<WriteStatus>> handleInsert(String idPfx, Iterator<HoodieRecord<T>> recordItr)\n      throws Exception {\n    \r\n    if (!recordItr.hasNext()) {\n      LOG.info(\"Empty partition\");\n      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();\n    }\n    return new SparkLazyInsertIterable(recordItr, true, config, instantTime, table, idPfx,\n        taskContextSupplier, new CreateHandleFactory<>());\n  }\n","date":"2020-10-02 05:25:29","endLine":329,"groupId":"10495","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"handleInsert","params":"(StringidPfx@Iterator<HoodieRecord<T>>recordItr)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/36/cca8cd1403c5fe6ced6d0b42aa6684081520d7.src","preCode":"  public Iterator<List<WriteStatus>> handleInsert(String idPfx, Iterator<HoodieRecord<T>> recordItr)\n      throws Exception {\n    \r\n    if (!recordItr.hasNext()) {\n      LOG.info(\"Empty partition\");\n      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();\n    }\n    return new SparkLazyInsertIterable(recordItr, true, config, instantTime, table, idPfx,\n        taskContextSupplier, new CreateHandleFactory<>());\n  }\n","realPath":"hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":320,"status":"N"},{"authorDate":"2021-03-01 12:29:41","commitOrder":5,"curCode":"  public Iterator<List<WriteStatus>> handleInsert(String idPfx, Iterator<HoodieRecord<T>> recordItr)\n      throws Exception {\n    \r\n    if (!recordItr.hasNext()) {\n      LOG.info(\"Empty partition\");\n      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();\n    }\n    return new FlinkLazyInsertIterable<>(recordItr, true, config, instantTime, table, idPfx,\n        taskContextSupplier, new ExplicitWriteHandleFactory<>(writeHandle));\n  }\n","date":"2021-03-01 12:29:41","endLine":260,"groupId":"10495","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"handleInsert","params":"(StringidPfx@Iterator<HoodieRecord<T>>recordItr)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-hudi-10-0.7/blobInfo/CC_OUT/blobs/e0/bbc25efd52e67a168395392fec0fad181fc46b.src","preCode":"  public Iterator<List<WriteStatus>> handleInsert(String idPfx, Iterator<HoodieRecord<T>> recordItr)\n      throws Exception {\n    \r\n    if (!recordItr.hasNext()) {\n      LOG.info(\"Empty partition\");\n      return Collections.singletonList((List<WriteStatus>) Collections.EMPTY_LIST).iterator();\n    }\n    return new FlinkLazyInsertIterable<>(recordItr, true, config, instantTime, table, idPfx,\n        taskContextSupplier, new ExplicitCreateHandleFactory<>(writeHandle));\n  }\n","realPath":"hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/table/action/commit/BaseFlinkCommitActionExecutor.java","repoName":"hudi","snippetEndLine":0,"snippetStartLine":0,"startLine":251,"status":"M"}],"commitId":"7a11de12764d8f68f296c6e68a22822318bfbefa","commitMessage":"@@@[HUDI-1632] Supports merge on read write mode for Flink writer (#2593)\n\nAlso supports async compaction with pluggable strategies.","date":"2021-03-01 12:29:41","modifiedFileCount":"20","status":"M","submitter":"Danny Chan"}]
