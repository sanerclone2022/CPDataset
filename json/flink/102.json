[{"authorTime":"2016-10-07 06:21:42","codes":[{"authorDate":"2016-10-07 06:21:42","commitOrder":1,"curCode":"\tpublic void testFileReadingOperatorWithIngestionTime() throws Exception {\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tTypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);\n\n\t\tfinal long watermarkInterval = 10;\n\n\t\tContinuousFileReaderOperator<String> reader = new ContinuousFileReaderOperator<>(format);\n\t\tfinal OneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester =\n\t\t\tnew OneInputStreamOperatorTestHarness<>(reader);\n\n\t\ttester.getExecutionConfig().setAutoWatermarkInterval(watermarkInterval);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.IngestionTime);\n\t\treader.setOutputType(typeInfo, tester.getExecutionConfig());\n\n\t\ttester.open();\n\n\t\tAssert.assertEquals(TimeCharacteristic.IngestionTime, tester.getTimeCharacteristic());\n\n\t\t\r\n\n\t\tConcurrentLinkedQueue<Object> output = tester.getOutput();\n\n\t\ttester.setProcessingTime(201);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(200, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(301);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(300, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(401);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(400, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(501);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(500, ((Watermark) output.poll()).getTimestamp());\n\n\t\tAssert.assertTrue(output.isEmpty());\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\treader.getRuntimeContext().getNumberOfParallelSubtasks());\n\n\t\t\r\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\n\t\tlong lastSeenWatermark = Long.MIN_VALUE;\n\t\tint lineCounter = 0;\t\r\n\t\tint watermarkCounter = 0;\n\n\t\tfor (FileInputSplit split: splits) {\n\n\t\t\t\r\n\t\t\tlong nextTimestamp = tester.getProcessingTime() + watermarkInterval;\n\t\t\ttester.setProcessingTime(nextTimestamp);\n\n\t\t\t\r\n\t\t\ttester.processElement(new StreamRecord<>(\n\t\t\t\tnew TimestampedFileInputSplit(modTimes.get(split.getPath().getName()),\n\t\t\t\t\tsplit.getSplitNumber(), split.getPath(), split.getStart(),\n\t\t\t\t\tsplit.getLength(), split.getHostnames())));\n\n\t\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\twhile (tester.getOutput().isEmpty() || tester.getOutput().size() != (LINES_PER_FILE + 1)) {\n\t\t\t\tThread.sleep(10);\n\t\t\t}\n\n\t\t\t\r\n\t\t\tfor (Object line: tester.getOutput()) {\n\n\t\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\t\t\t\t\tlineCounter++;\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp, element.getTimestamp());\n\n\t\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\t\tif (content == null) {\n\t\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t\t}\n\t\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t\t} else if (line instanceof Watermark) {\n\t\t\t\t\tlong watermark = ((Watermark) line).getTimestamp();\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp - (nextTimestamp % watermarkInterval), watermark);\n\t\t\t\t\tAssert.assertTrue(watermark > lastSeenWatermark);\n\t\t\t\t\twatermarkCounter++;\n\n\t\t\t\t\tlastSeenWatermark = watermark;\n\t\t\t\t} else {\n\t\t\t\t\tAssert.fail(\"Unknown element in the list.\");\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t\r\n\t\t\ttester.getOutput().clear();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE, lineCounter);\n\n\t\t\r\n\t\tAssert.assertEquals(splits.length, watermarkCounter);\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\n\t\t\r\n\t\tAssert.assertEquals(1, tester.getOutput().size());\n\t\tAssert.assertTrue(tester.getOutput().peek() instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) tester.getOutput().poll()).getTimestamp());\n\n\t\t\r\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\t}\n","date":"2016-10-27 20:22:02","endLine":268,"groupId":"28240","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testFileReadingOperatorWithIngestionTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/02/83f68178013da87b98a2223506d11540630117.src","preCode":"\tpublic void testFileReadingOperatorWithIngestionTime() throws Exception {\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tTypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);\n\n\t\tfinal long watermarkInterval = 10;\n\n\t\tContinuousFileReaderOperator<String> reader = new ContinuousFileReaderOperator<>(format);\n\t\tfinal OneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester =\n\t\t\tnew OneInputStreamOperatorTestHarness<>(reader);\n\n\t\ttester.getExecutionConfig().setAutoWatermarkInterval(watermarkInterval);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.IngestionTime);\n\t\treader.setOutputType(typeInfo, tester.getExecutionConfig());\n\n\t\ttester.open();\n\n\t\tAssert.assertEquals(TimeCharacteristic.IngestionTime, tester.getTimeCharacteristic());\n\n\t\t\r\n\n\t\tConcurrentLinkedQueue<Object> output = tester.getOutput();\n\n\t\ttester.setProcessingTime(201);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(200, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(301);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(300, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(401);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(400, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(501);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(500, ((Watermark) output.poll()).getTimestamp());\n\n\t\tAssert.assertTrue(output.isEmpty());\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\treader.getRuntimeContext().getNumberOfParallelSubtasks());\n\n\t\t\r\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\n\t\tlong lastSeenWatermark = Long.MIN_VALUE;\n\t\tint lineCounter = 0;\t\r\n\t\tint watermarkCounter = 0;\n\n\t\tfor (FileInputSplit split: splits) {\n\n\t\t\t\r\n\t\t\tlong nextTimestamp = tester.getProcessingTime() + watermarkInterval;\n\t\t\ttester.setProcessingTime(nextTimestamp);\n\n\t\t\t\r\n\t\t\ttester.processElement(new StreamRecord<>(\n\t\t\t\tnew TimestampedFileInputSplit(modTimes.get(split.getPath().getName()),\n\t\t\t\t\tsplit.getSplitNumber(), split.getPath(), split.getStart(),\n\t\t\t\t\tsplit.getLength(), split.getHostnames())));\n\n\t\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\twhile (tester.getOutput().isEmpty() || tester.getOutput().size() != (LINES_PER_FILE + 1)) {\n\t\t\t\tThread.sleep(10);\n\t\t\t}\n\n\t\t\t\r\n\t\t\tfor (Object line: tester.getOutput()) {\n\n\t\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\t\t\t\t\tlineCounter++;\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp, element.getTimestamp());\n\n\t\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\t\tif (content == null) {\n\t\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t\t}\n\t\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t\t} else if (line instanceof Watermark) {\n\t\t\t\t\tlong watermark = ((Watermark) line).getTimestamp();\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp - (nextTimestamp % watermarkInterval), watermark);\n\t\t\t\t\tAssert.assertTrue(watermark > lastSeenWatermark);\n\t\t\t\t\twatermarkCounter++;\n\n\t\t\t\t\tlastSeenWatermark = watermark;\n\t\t\t\t} else {\n\t\t\t\t\tAssert.fail(\"Unknown element in the list.\");\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t\r\n\t\t\ttester.getOutput().clear();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE, lineCounter);\n\n\t\t\r\n\t\tAssert.assertEquals(splits.length, watermarkCounter);\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\n\t\t\r\n\t\tAssert.assertEquals(1, tester.getOutput().size());\n\t\tAssert.assertTrue(tester.getOutput().peek() instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) tester.getOutput().poll()).getTimestamp());\n\n\t\t\r\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":113,"status":"B"},{"authorDate":"2016-10-07 06:21:42","commitOrder":1,"curCode":"\tpublic void testFileReadingOperatorWithEventTime() throws Exception {\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\tfilesCreated.add(file.f0);\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tTypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);\n\n\t\tContinuousFileReaderOperator<String> reader = new ContinuousFileReaderOperator<>(format);\n\t\treader.setOutputType(typeInfo, new ExecutionConfig());\n\n\t\tOneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester =\n\t\t\tnew OneInputStreamOperatorTestHarness<>(reader);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.EventTime);\n\t\ttester.open();\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\treader.getRuntimeContext().getNumberOfParallelSubtasks());\n\n\t\t\r\n\t\tfor (FileInputSplit split: splits) {\n\t\t\ttester.processElement(new StreamRecord<>(new TimestampedFileInputSplit(\n\t\t\t\tmodTimes.get(split.getPath().getName()), split.getSplitNumber(), split.getPath(),\n\t\t\t\tsplit.getStart(), split.getLength(), split.getHostnames())));\n\t\t}\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\t\r\n\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE + 1, tester.getOutput().size());\n\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\t\tObject lastElement = null;\n\t\tfor (Object line: tester.getOutput()) {\n\t\t\tlastElement = line;\n\n\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\n\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\tif (content == null) {\n\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t}\n\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t}\n\t\t}\n\n\t\t\r\n\t\tAssert.assertTrue(lastElement instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) lastElement).getTimestamp());\n\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t}\n","date":"2016-10-27 20:22:02","endLine":361,"groupId":"28241","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testFileReadingOperatorWithEventTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/02/83f68178013da87b98a2223506d11540630117.src","preCode":"\tpublic void testFileReadingOperatorWithEventTime() throws Exception {\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\tfilesCreated.add(file.f0);\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tTypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);\n\n\t\tContinuousFileReaderOperator<String> reader = new ContinuousFileReaderOperator<>(format);\n\t\treader.setOutputType(typeInfo, new ExecutionConfig());\n\n\t\tOneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester =\n\t\t\tnew OneInputStreamOperatorTestHarness<>(reader);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.EventTime);\n\t\ttester.open();\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\treader.getRuntimeContext().getNumberOfParallelSubtasks());\n\n\t\t\r\n\t\tfor (FileInputSplit split: splits) {\n\t\t\ttester.processElement(new StreamRecord<>(new TimestampedFileInputSplit(\n\t\t\t\tmodTimes.get(split.getPath().getName()), split.getSplitNumber(), split.getPath(),\n\t\t\t\tsplit.getStart(), split.getLength(), split.getHostnames())));\n\t\t}\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\t\r\n\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE + 1, tester.getOutput().size());\n\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\t\tObject lastElement = null;\n\t\tfor (Object line: tester.getOutput()) {\n\t\t\tlastElement = line;\n\n\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\n\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\tif (content == null) {\n\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t}\n\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t}\n\t\t}\n\n\t\t\r\n\t\tAssert.assertTrue(lastElement instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) lastElement).getTimestamp());\n\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":271,"status":"B"}],"commitId":"b410c393c960f55c09fadd4f22732d06f801b938","commitMessage":"@@@[FLINK-4800] Introduce the TimestampedFileInputSplit for Continuous File Processing\n\nThis commit mainly introduces the TimestampedFileInputSplit. \nwhich extends the class FileInputSplit and also contains:\ni) the modification time of the file it belongs to and also.  and\nii) when checkpointing.  the point the reader is currently reading\n    from in the split the reader.\n\nThis will be useful for rescaling. With this addition.  the\nContinuousFileMonitoringFunction sends TimestampedFileInputSplit\nto the Readers.  and the Readers' state now contain only\nTimestampedFileInputSplit.\n\nIn addition.  it refactors the code of the ContinuousFileMonitoringFunction\nand that of the ContinuousFileReaderOperator along with the related\ntests.\n\nThis closes #2618.\n","date":"2016-10-27 20:22:02","modifiedFileCount":"5","status":"B","submitter":"kl0u"},{"authorTime":"2017-02-10 22:14:34","codes":[{"authorDate":"2017-02-10 22:14:34","commitOrder":2,"curCode":"\tpublic void testFileReadingOperatorWithIngestionTime() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tTypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);\n\n\t\tfinal long watermarkInterval = 10;\n\n\t\tContinuousFileReaderOperator<String> reader = new ContinuousFileReaderOperator<>(format);\n\t\tfinal OneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester =\n\t\t\tnew OneInputStreamOperatorTestHarness<>(reader);\n\n\t\ttester.getExecutionConfig().setAutoWatermarkInterval(watermarkInterval);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.IngestionTime);\n\t\treader.setOutputType(typeInfo, tester.getExecutionConfig());\n\n\t\ttester.open();\n\n\t\tAssert.assertEquals(TimeCharacteristic.IngestionTime, tester.getTimeCharacteristic());\n\n\t\t\r\n\n\t\tConcurrentLinkedQueue<Object> output = tester.getOutput();\n\n\t\ttester.setProcessingTime(201);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(200, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(301);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(300, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(401);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(400, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(501);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(500, ((Watermark) output.poll()).getTimestamp());\n\n\t\tAssert.assertTrue(output.isEmpty());\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\treader.getRuntimeContext().getNumberOfParallelSubtasks());\n\n\t\t\r\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\n\t\tlong lastSeenWatermark = Long.MIN_VALUE;\n\t\tint lineCounter = 0;\t\r\n\t\tint watermarkCounter = 0;\n\n\t\tfor (FileInputSplit split: splits) {\n\n\t\t\t\r\n\t\t\tlong nextTimestamp = tester.getProcessingTime() + watermarkInterval;\n\t\t\ttester.setProcessingTime(nextTimestamp);\n\n\t\t\t\r\n\t\t\ttester.processElement(new StreamRecord<>(\n\t\t\t\tnew TimestampedFileInputSplit(modTimes.get(split.getPath().getName()),\n\t\t\t\t\tsplit.getSplitNumber(), split.getPath(), split.getStart(),\n\t\t\t\t\tsplit.getLength(), split.getHostnames())));\n\n\t\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\twhile (tester.getOutput().isEmpty() || tester.getOutput().size() != (LINES_PER_FILE + 1)) {\n\t\t\t\tThread.sleep(10);\n\t\t\t}\n\n\t\t\t\r\n\t\t\tfor (Object line: tester.getOutput()) {\n\n\t\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\t\t\t\t\tlineCounter++;\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp, element.getTimestamp());\n\n\t\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\t\tif (content == null) {\n\t\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t\t}\n\t\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t\t} else if (line instanceof Watermark) {\n\t\t\t\t\tlong watermark = ((Watermark) line).getTimestamp();\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp - (nextTimestamp % watermarkInterval), watermark);\n\t\t\t\t\tAssert.assertTrue(watermark > lastSeenWatermark);\n\t\t\t\t\twatermarkCounter++;\n\n\t\t\t\t\tlastSeenWatermark = watermark;\n\t\t\t\t} else {\n\t\t\t\t\tAssert.fail(\"Unknown element in the list.\");\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t\r\n\t\t\ttester.getOutput().clear();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE, lineCounter);\n\n\t\t\r\n\t\tAssert.assertEquals(splits.length, watermarkCounter);\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\n\t\t\r\n\t\tAssert.assertEquals(1, tester.getOutput().size());\n\t\tAssert.assertTrue(tester.getOutput().peek() instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) tester.getOutput().poll()).getTimestamp());\n\n\t\t\r\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\t}\n","date":"2017-02-11 00:07:39","endLine":298,"groupId":"28240","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testFileReadingOperatorWithIngestionTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/cc/5cb8e9e4f436c49fddba30d524ed6cbb7ab543.src","preCode":"\tpublic void testFileReadingOperatorWithIngestionTime() throws Exception {\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tTypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);\n\n\t\tfinal long watermarkInterval = 10;\n\n\t\tContinuousFileReaderOperator<String> reader = new ContinuousFileReaderOperator<>(format);\n\t\tfinal OneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester =\n\t\t\tnew OneInputStreamOperatorTestHarness<>(reader);\n\n\t\ttester.getExecutionConfig().setAutoWatermarkInterval(watermarkInterval);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.IngestionTime);\n\t\treader.setOutputType(typeInfo, tester.getExecutionConfig());\n\n\t\ttester.open();\n\n\t\tAssert.assertEquals(TimeCharacteristic.IngestionTime, tester.getTimeCharacteristic());\n\n\t\t\r\n\n\t\tConcurrentLinkedQueue<Object> output = tester.getOutput();\n\n\t\ttester.setProcessingTime(201);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(200, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(301);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(300, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(401);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(400, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(501);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(500, ((Watermark) output.poll()).getTimestamp());\n\n\t\tAssert.assertTrue(output.isEmpty());\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\treader.getRuntimeContext().getNumberOfParallelSubtasks());\n\n\t\t\r\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\n\t\tlong lastSeenWatermark = Long.MIN_VALUE;\n\t\tint lineCounter = 0;\t\r\n\t\tint watermarkCounter = 0;\n\n\t\tfor (FileInputSplit split: splits) {\n\n\t\t\t\r\n\t\t\tlong nextTimestamp = tester.getProcessingTime() + watermarkInterval;\n\t\t\ttester.setProcessingTime(nextTimestamp);\n\n\t\t\t\r\n\t\t\ttester.processElement(new StreamRecord<>(\n\t\t\t\tnew TimestampedFileInputSplit(modTimes.get(split.getPath().getName()),\n\t\t\t\t\tsplit.getSplitNumber(), split.getPath(), split.getStart(),\n\t\t\t\t\tsplit.getLength(), split.getHostnames())));\n\n\t\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\twhile (tester.getOutput().isEmpty() || tester.getOutput().size() != (LINES_PER_FILE + 1)) {\n\t\t\t\tThread.sleep(10);\n\t\t\t}\n\n\t\t\t\r\n\t\t\tfor (Object line: tester.getOutput()) {\n\n\t\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\t\t\t\t\tlineCounter++;\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp, element.getTimestamp());\n\n\t\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\t\tif (content == null) {\n\t\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t\t}\n\t\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t\t} else if (line instanceof Watermark) {\n\t\t\t\t\tlong watermark = ((Watermark) line).getTimestamp();\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp - (nextTimestamp % watermarkInterval), watermark);\n\t\t\t\t\tAssert.assertTrue(watermark > lastSeenWatermark);\n\t\t\t\t\twatermarkCounter++;\n\n\t\t\t\t\tlastSeenWatermark = watermark;\n\t\t\t\t} else {\n\t\t\t\t\tAssert.fail(\"Unknown element in the list.\");\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t\r\n\t\t\ttester.getOutput().clear();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE, lineCounter);\n\n\t\t\r\n\t\tAssert.assertEquals(splits.length, watermarkCounter);\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\n\t\t\r\n\t\tAssert.assertEquals(1, tester.getOutput().size());\n\t\tAssert.assertTrue(tester.getOutput().peek() instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) tester.getOutput().poll()).getTimestamp());\n\n\t\t\r\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":141,"status":"M"},{"authorDate":"2017-02-10 22:14:34","commitOrder":2,"curCode":"\tpublic void testFileReadingOperatorWithEventTime() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\tfilesCreated.add(file.f0);\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tTypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);\n\n\t\tContinuousFileReaderOperator<String> reader = new ContinuousFileReaderOperator<>(format);\n\t\treader.setOutputType(typeInfo, new ExecutionConfig());\n\n\t\tOneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester =\n\t\t\tnew OneInputStreamOperatorTestHarness<>(reader);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.EventTime);\n\t\ttester.open();\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\treader.getRuntimeContext().getNumberOfParallelSubtasks());\n\n\t\t\r\n\t\tfor (FileInputSplit split: splits) {\n\t\t\ttester.processElement(new StreamRecord<>(new TimestampedFileInputSplit(\n\t\t\t\tmodTimes.get(split.getPath().getName()), split.getSplitNumber(), split.getPath(),\n\t\t\t\tsplit.getStart(), split.getLength(), split.getHostnames())));\n\t\t}\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\t\r\n\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE + 1, tester.getOutput().size());\n\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\t\tObject lastElement = null;\n\t\tfor (Object line: tester.getOutput()) {\n\t\t\tlastElement = line;\n\n\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\n\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\tif (content == null) {\n\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t}\n\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t}\n\t\t}\n\n\t\t\r\n\t\tAssert.assertTrue(lastElement instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) lastElement).getTimestamp());\n\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t}\n","date":"2017-02-11 00:07:39","endLine":393,"groupId":"28241","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testFileReadingOperatorWithEventTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/cc/5cb8e9e4f436c49fddba30d524ed6cbb7ab543.src","preCode":"\tpublic void testFileReadingOperatorWithEventTime() throws Exception {\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(hdfsURI, \"file\", i, \"This is test line.\");\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\tfilesCreated.add(file.f0);\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(hdfsURI));\n\t\tTypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);\n\n\t\tContinuousFileReaderOperator<String> reader = new ContinuousFileReaderOperator<>(format);\n\t\treader.setOutputType(typeInfo, new ExecutionConfig());\n\n\t\tOneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester =\n\t\t\tnew OneInputStreamOperatorTestHarness<>(reader);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.EventTime);\n\t\ttester.open();\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\treader.getRuntimeContext().getNumberOfParallelSubtasks());\n\n\t\t\r\n\t\tfor (FileInputSplit split: splits) {\n\t\t\ttester.processElement(new StreamRecord<>(new TimestampedFileInputSplit(\n\t\t\t\tmodTimes.get(split.getPath().getName()), split.getSplitNumber(), split.getPath(),\n\t\t\t\tsplit.getStart(), split.getLength(), split.getHostnames())));\n\t\t}\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\t\r\n\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE + 1, tester.getOutput().size());\n\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\t\tObject lastElement = null;\n\t\tfor (Object line: tester.getOutput()) {\n\t\t\tlastElement = line;\n\n\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\n\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\tif (content == null) {\n\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t}\n\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t}\n\t\t}\n\n\t\t\r\n\t\tAssert.assertTrue(lastElement instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) lastElement).getTimestamp());\n\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":301,"status":"M"}],"commitId":"f6709b4a48a843a0a1818fd59b98d32f82d6184f","commitMessage":"@@@[FLINK-5415] Harden ContinuousFileProcessingTest\n\n- Use TemporaryFolder @ClassRule instead of manually managing HDFS base\ndir.\n- Place files for each test in own sub-directory\n- Harden completeness condition in testFunctionRestore()\n","date":"2017-02-11 00:07:39","modifiedFileCount":"1","status":"M","submitter":"Aljoscha Krettek"},{"authorTime":"2017-02-10 22:14:34","codes":[{"authorDate":"2020-01-30 00:18:39","commitOrder":3,"curCode":"\tpublic void testFileReadingOperatorWithIngestionTime() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tTypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);\n\n\t\tfinal long watermarkInterval = 10;\n\n\t\tContinuousFileReaderOperator<String> reader = new ContinuousFileReaderOperator<>(format);\n\t\tfinal OneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester = new OneInputStreamOperatorTestHarness<>(reader);\n\t\tSteppingMailboxProcessor localMailbox = createLocalMailbox(tester);\n\n\t\ttester.getExecutionConfig().setAutoWatermarkInterval(watermarkInterval);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.IngestionTime);\n\t\treader.setOutputType(typeInfo, tester.getExecutionConfig());\n\n\t\ttester.open();\n\t\tAssert.assertEquals(TimeCharacteristic.IngestionTime, tester.getTimeCharacteristic());\n\n\t\ttester.setProcessingTime(201);\n\n\t\t\r\n\t\tConcurrentLinkedQueue<Object> output = tester.getOutput();\n\t\twhile (output.isEmpty()) {\n\t\t\tlocalMailbox.runMailboxStep();\n\t\t}\n\t\tAssert.assertTrue(output.toString(), output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(200, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(301);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(300, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(401);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(400, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(501);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(500, ((Watermark) output.poll()).getTimestamp());\n\n\t\tAssert.assertTrue(output.isEmpty());\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\treader.getRuntimeContext().getNumberOfParallelSubtasks());\n\n\t\t\r\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\n\t\tlong lastSeenWatermark = Long.MIN_VALUE;\n\t\tint lineCounter = 0;\t\r\n\t\tint watermarkCounter = 0;\n\n\t\tfor (FileInputSplit split: splits) {\n\n\t\t\t\r\n\t\t\tlong nextTimestamp = tester.getProcessingTime() + watermarkInterval;\n\t\t\ttester.setProcessingTime(nextTimestamp);\n\n\t\t\t\r\n\t\t\tRunnableWithException runnableWithException = () -> tester.processElement(new StreamRecord<>(\n\t\t\t\t\tnew TimestampedFileInputSplit(modTimes.get(split.getPath().getName()),\n\t\t\t\t\t\t\tsplit.getSplitNumber(), split.getPath(), split.getStart(),\n\t\t\t\t\t\t\tsplit.getLength(), split.getHostnames())));\n\t\t\trunnableWithException.run();\n\n\t\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\twhile (tester.getOutput().isEmpty() || tester.getOutput().size() != (LINES_PER_FILE + 1)) {\n\t\t\t\tlocalMailbox.runMailboxStep();\n\t\t\t}\n\n\t\t\t\r\n\t\t\tfor (Object line: tester.getOutput()) {\n\n\t\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\t\t\t\t\tlineCounter++;\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp, element.getTimestamp());\n\n\t\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\t\tif (content == null) {\n\t\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t\t}\n\t\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t\t} else if (line instanceof Watermark) {\n\t\t\t\t\tlong watermark = ((Watermark) line).getTimestamp();\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp - (nextTimestamp % watermarkInterval), watermark);\n\t\t\t\t\tAssert.assertTrue(watermark > lastSeenWatermark);\n\t\t\t\t\twatermarkCounter++;\n\n\t\t\t\t\tlastSeenWatermark = watermark;\n\t\t\t\t} else {\n\t\t\t\t\tAssert.fail(\"Unknown element in the list.\");\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t\r\n\t\t\ttester.getOutput().clear();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE, lineCounter);\n\n\t\t\r\n\t\tAssert.assertEquals(splits.length, watermarkCounter);\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\n\t\t\r\n\t\tAssert.assertEquals(1, tester.getOutput().size());\n\t\tAssert.assertTrue(tester.getOutput().peek() instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) tester.getOutput().poll()).getTimestamp());\n\n\t\t\r\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\t}\n","date":"2020-02-10 19:29:59","endLine":313,"groupId":"28240","id":5,"instanceNumber":1,"isCurCommit":0,"methodName":"testFileReadingOperatorWithIngestionTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/7e/2b794f69653ba3e1e44bb988bd13d215ebe91e.src","preCode":"\tpublic void testFileReadingOperatorWithIngestionTime() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tTypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);\n\n\t\tfinal long watermarkInterval = 10;\n\n\t\tContinuousFileReaderOperator<String> reader = new ContinuousFileReaderOperator<>(format);\n\t\tfinal OneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester =\n\t\t\tnew OneInputStreamOperatorTestHarness<>(reader);\n\n\t\ttester.getExecutionConfig().setAutoWatermarkInterval(watermarkInterval);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.IngestionTime);\n\t\treader.setOutputType(typeInfo, tester.getExecutionConfig());\n\n\t\ttester.open();\n\n\t\tAssert.assertEquals(TimeCharacteristic.IngestionTime, tester.getTimeCharacteristic());\n\n\t\t\r\n\n\t\tConcurrentLinkedQueue<Object> output = tester.getOutput();\n\n\t\ttester.setProcessingTime(201);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(200, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(301);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(300, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(401);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(400, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(501);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(500, ((Watermark) output.poll()).getTimestamp());\n\n\t\tAssert.assertTrue(output.isEmpty());\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\treader.getRuntimeContext().getNumberOfParallelSubtasks());\n\n\t\t\r\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\n\t\tlong lastSeenWatermark = Long.MIN_VALUE;\n\t\tint lineCounter = 0;\t\r\n\t\tint watermarkCounter = 0;\n\n\t\tfor (FileInputSplit split: splits) {\n\n\t\t\t\r\n\t\t\tlong nextTimestamp = tester.getProcessingTime() + watermarkInterval;\n\t\t\ttester.setProcessingTime(nextTimestamp);\n\n\t\t\t\r\n\t\t\ttester.processElement(new StreamRecord<>(\n\t\t\t\tnew TimestampedFileInputSplit(modTimes.get(split.getPath().getName()),\n\t\t\t\t\tsplit.getSplitNumber(), split.getPath(), split.getStart(),\n\t\t\t\t\tsplit.getLength(), split.getHostnames())));\n\n\t\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\twhile (tester.getOutput().isEmpty() || tester.getOutput().size() != (LINES_PER_FILE + 1)) {\n\t\t\t\tThread.sleep(10);\n\t\t\t}\n\n\t\t\t\r\n\t\t\tfor (Object line: tester.getOutput()) {\n\n\t\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\t\t\t\t\tlineCounter++;\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp, element.getTimestamp());\n\n\t\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\t\tif (content == null) {\n\t\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t\t}\n\t\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t\t} else if (line instanceof Watermark) {\n\t\t\t\t\tlong watermark = ((Watermark) line).getTimestamp();\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp - (nextTimestamp % watermarkInterval), watermark);\n\t\t\t\t\tAssert.assertTrue(watermark > lastSeenWatermark);\n\t\t\t\t\twatermarkCounter++;\n\n\t\t\t\t\tlastSeenWatermark = watermark;\n\t\t\t\t} else {\n\t\t\t\t\tAssert.fail(\"Unknown element in the list.\");\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t\r\n\t\t\ttester.getOutput().clear();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE, lineCounter);\n\n\t\t\r\n\t\tAssert.assertEquals(splits.length, watermarkCounter);\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\n\t\t\r\n\t\tAssert.assertEquals(1, tester.getOutput().size());\n\t\tAssert.assertTrue(tester.getOutput().peek() instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) tester.getOutput().poll()).getTimestamp());\n\n\t\t\r\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":154,"status":"M"},{"authorDate":"2017-02-10 22:14:34","commitOrder":3,"curCode":"\tpublic void testFileReadingOperatorWithEventTime() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\tfilesCreated.add(file.f0);\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tTypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);\n\n\t\tContinuousFileReaderOperator<String> reader = new ContinuousFileReaderOperator<>(format);\n\t\treader.setOutputType(typeInfo, new ExecutionConfig());\n\n\t\tOneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester =\n\t\t\tnew OneInputStreamOperatorTestHarness<>(reader);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.EventTime);\n\t\ttester.open();\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\treader.getRuntimeContext().getNumberOfParallelSubtasks());\n\n\t\t\r\n\t\tfor (FileInputSplit split: splits) {\n\t\t\ttester.processElement(new StreamRecord<>(new TimestampedFileInputSplit(\n\t\t\t\tmodTimes.get(split.getPath().getName()), split.getSplitNumber(), split.getPath(),\n\t\t\t\tsplit.getStart(), split.getLength(), split.getHostnames())));\n\t\t}\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\t\r\n\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE + 1, tester.getOutput().size());\n\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\t\tObject lastElement = null;\n\t\tfor (Object line: tester.getOutput()) {\n\t\t\tlastElement = line;\n\n\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\n\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\tif (content == null) {\n\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t}\n\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t}\n\t\t}\n\n\t\t\r\n\t\tAssert.assertTrue(lastElement instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) lastElement).getTimestamp());\n\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t}\n","date":"2017-02-11 00:07:39","endLine":393,"groupId":"28241","id":6,"instanceNumber":2,"isCurCommit":0,"methodName":"testFileReadingOperatorWithEventTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/cc/5cb8e9e4f436c49fddba30d524ed6cbb7ab543.src","preCode":"\tpublic void testFileReadingOperatorWithEventTime() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\tfilesCreated.add(file.f0);\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tTypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);\n\n\t\tContinuousFileReaderOperator<String> reader = new ContinuousFileReaderOperator<>(format);\n\t\treader.setOutputType(typeInfo, new ExecutionConfig());\n\n\t\tOneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester =\n\t\t\tnew OneInputStreamOperatorTestHarness<>(reader);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.EventTime);\n\t\ttester.open();\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\treader.getRuntimeContext().getNumberOfParallelSubtasks());\n\n\t\t\r\n\t\tfor (FileInputSplit split: splits) {\n\t\t\ttester.processElement(new StreamRecord<>(new TimestampedFileInputSplit(\n\t\t\t\tmodTimes.get(split.getPath().getName()), split.getSplitNumber(), split.getPath(),\n\t\t\t\tsplit.getStart(), split.getLength(), split.getHostnames())));\n\t\t}\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\t\r\n\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE + 1, tester.getOutput().size());\n\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\t\tObject lastElement = null;\n\t\tfor (Object line: tester.getOutput()) {\n\t\t\tlastElement = line;\n\n\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\n\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\tif (content == null) {\n\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t}\n\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t}\n\t\t}\n\n\t\t\r\n\t\tAssert.assertTrue(lastElement instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) lastElement).getTimestamp());\n\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":301,"status":"N"}],"commitId":"2e96ee7d2b385d8c6e0136cb8aed6bd23d875041","commitMessage":"@@@[FLINK-13955][runtime] use mailbox execution model in ContinuousFileReaderOperator.\n\nMotivation: allow to avoid explicit synchronization and eventually to remove StreamTask.getCheckpointLock().\n\nContinuousFileReaderOperator was changed significantly to avoid\nthe currently necessary thread and extra synchronization between blocking and non-blocking code\n(e.g.  format.reachedEnd and nextRecord)\n\nPerformance is lower by ~15% because of the overhead of enqueueing and processing each mail (instead of just loop).\n","date":"2020-02-10 19:29:59","modifiedFileCount":"10","status":"M","submitter":"Roman Khachatryan"},{"authorTime":"2020-02-14 21:01:04","codes":[{"authorDate":"2020-02-14 21:01:04","commitOrder":4,"curCode":"\tpublic void testFileReadingOperatorWithIngestionTime() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tTypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);\n\n\t\tfinal long watermarkInterval = 10;\n\n\t\tfinal OneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester = createHarness(format);\n\t\tSteppingMailboxProcessor localMailbox = createLocalMailbox(tester);\n\n\t\ttester.getExecutionConfig().setAutoWatermarkInterval(watermarkInterval);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.IngestionTime);\n\n\t\ttester.open();\n\t\tAssert.assertEquals(TimeCharacteristic.IngestionTime, tester.getTimeCharacteristic());\n\n\t\ttester.setProcessingTime(201);\n\n\t\t\r\n\t\tConcurrentLinkedQueue<Object> output = tester.getOutput();\n\t\twhile (output.isEmpty()) {\n\t\t\tlocalMailbox.runMailboxStep();\n\t\t}\n\t\tAssert.assertTrue(output.toString(), output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(200, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(301);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(300, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(401);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(400, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(501);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(500, ((Watermark) output.poll()).getTimestamp());\n\n\t\tAssert.assertTrue(output.isEmpty());\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\ttester.getExecutionConfig().getParallelism());\n\n\t\t\r\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\n\t\tlong lastSeenWatermark = Long.MIN_VALUE;\n\t\tint lineCounter = 0;\t\r\n\t\tint watermarkCounter = 0;\n\n\t\tfor (FileInputSplit split: splits) {\n\n\t\t\t\r\n\t\t\tlong nextTimestamp = tester.getProcessingTime() + watermarkInterval;\n\t\t\ttester.setProcessingTime(nextTimestamp);\n\n\t\t\t\r\n\t\t\tRunnableWithException runnableWithException = () -> tester.processElement(new StreamRecord<>(\n\t\t\t\t\tnew TimestampedFileInputSplit(modTimes.get(split.getPath().getName()),\n\t\t\t\t\t\t\tsplit.getSplitNumber(), split.getPath(), split.getStart(),\n\t\t\t\t\t\t\tsplit.getLength(), split.getHostnames())));\n\t\t\trunnableWithException.run();\n\n\t\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\twhile (tester.getOutput().isEmpty() || tester.getOutput().size() != (LINES_PER_FILE + 1)) {\n\t\t\t\tlocalMailbox.runMailboxStep();\n\t\t\t}\n\n\t\t\t\r\n\t\t\tfor (Object line: tester.getOutput()) {\n\n\t\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\t\t\t\t\tlineCounter++;\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp, element.getTimestamp());\n\n\t\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\t\tif (content == null) {\n\t\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t\t}\n\t\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t\t} else if (line instanceof Watermark) {\n\t\t\t\t\tlong watermark = ((Watermark) line).getTimestamp();\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp - (nextTimestamp % watermarkInterval), watermark);\n\t\t\t\t\tAssert.assertTrue(watermark > lastSeenWatermark);\n\t\t\t\t\twatermarkCounter++;\n\n\t\t\t\t\tlastSeenWatermark = watermark;\n\t\t\t\t} else {\n\t\t\t\t\tAssert.fail(\"Unknown element in the list.\");\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t\r\n\t\t\ttester.getOutput().clear();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE, lineCounter);\n\n\t\t\r\n\t\tAssert.assertEquals(splits.length, watermarkCounter);\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\n\t\t\r\n\t\tAssert.assertEquals(1, tester.getOutput().size());\n\t\tAssert.assertTrue(tester.getOutput().peek() instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) tester.getOutput().poll()).getTimestamp());\n\n\t\t\r\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\t}\n","date":"2020-02-18 22:13:31","endLine":311,"groupId":"28240","id":7,"instanceNumber":1,"isCurCommit":0,"methodName":"testFileReadingOperatorWithIngestionTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/1e/783f1d33db46b2397d490d985fb5d5120bcc35.src","preCode":"\tpublic void testFileReadingOperatorWithIngestionTime() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tTypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);\n\n\t\tfinal long watermarkInterval = 10;\n\n\t\tContinuousFileReaderOperator<String> reader = new ContinuousFileReaderOperator<>(format);\n\t\tfinal OneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester = new OneInputStreamOperatorTestHarness<>(reader);\n\t\tSteppingMailboxProcessor localMailbox = createLocalMailbox(tester);\n\n\t\ttester.getExecutionConfig().setAutoWatermarkInterval(watermarkInterval);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.IngestionTime);\n\t\treader.setOutputType(typeInfo, tester.getExecutionConfig());\n\n\t\ttester.open();\n\t\tAssert.assertEquals(TimeCharacteristic.IngestionTime, tester.getTimeCharacteristic());\n\n\t\ttester.setProcessingTime(201);\n\n\t\t\r\n\t\tConcurrentLinkedQueue<Object> output = tester.getOutput();\n\t\twhile (output.isEmpty()) {\n\t\t\tlocalMailbox.runMailboxStep();\n\t\t}\n\t\tAssert.assertTrue(output.toString(), output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(200, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(301);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(300, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(401);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(400, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(501);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(500, ((Watermark) output.poll()).getTimestamp());\n\n\t\tAssert.assertTrue(output.isEmpty());\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\treader.getRuntimeContext().getNumberOfParallelSubtasks());\n\n\t\t\r\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\n\t\tlong lastSeenWatermark = Long.MIN_VALUE;\n\t\tint lineCounter = 0;\t\r\n\t\tint watermarkCounter = 0;\n\n\t\tfor (FileInputSplit split: splits) {\n\n\t\t\t\r\n\t\t\tlong nextTimestamp = tester.getProcessingTime() + watermarkInterval;\n\t\t\ttester.setProcessingTime(nextTimestamp);\n\n\t\t\t\r\n\t\t\tRunnableWithException runnableWithException = () -> tester.processElement(new StreamRecord<>(\n\t\t\t\t\tnew TimestampedFileInputSplit(modTimes.get(split.getPath().getName()),\n\t\t\t\t\t\t\tsplit.getSplitNumber(), split.getPath(), split.getStart(),\n\t\t\t\t\t\t\tsplit.getLength(), split.getHostnames())));\n\t\t\trunnableWithException.run();\n\n\t\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\twhile (tester.getOutput().isEmpty() || tester.getOutput().size() != (LINES_PER_FILE + 1)) {\n\t\t\t\tlocalMailbox.runMailboxStep();\n\t\t\t}\n\n\t\t\t\r\n\t\t\tfor (Object line: tester.getOutput()) {\n\n\t\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\t\t\t\t\tlineCounter++;\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp, element.getTimestamp());\n\n\t\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\t\tif (content == null) {\n\t\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t\t}\n\t\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t\t} else if (line instanceof Watermark) {\n\t\t\t\t\tlong watermark = ((Watermark) line).getTimestamp();\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp - (nextTimestamp % watermarkInterval), watermark);\n\t\t\t\t\tAssert.assertTrue(watermark > lastSeenWatermark);\n\t\t\t\t\twatermarkCounter++;\n\n\t\t\t\t\tlastSeenWatermark = watermark;\n\t\t\t\t} else {\n\t\t\t\t\tAssert.fail(\"Unknown element in the list.\");\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t\r\n\t\t\ttester.getOutput().clear();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE, lineCounter);\n\n\t\t\r\n\t\tAssert.assertEquals(splits.length, watermarkCounter);\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\n\t\t\r\n\t\tAssert.assertEquals(1, tester.getOutput().size());\n\t\tAssert.assertTrue(tester.getOutput().peek() instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) tester.getOutput().poll()).getTimestamp());\n\n\t\t\r\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":154,"status":"M"},{"authorDate":"2020-02-14 21:01:04","commitOrder":4,"curCode":"\tpublic void testFileReadingOperatorWithEventTime() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\tfilesCreated.add(file.f0);\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tTypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);\n\n\t\tOneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester = createHarness(format);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.EventTime);\n\t\ttester.open();\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\ttester.getExecutionConfig().getParallelism());\n\n\t\t\r\n\t\tfor (FileInputSplit split: splits) {\n\t\t\ttester.processElement(new StreamRecord<>(new TimestampedFileInputSplit(\n\t\t\t\tmodTimes.get(split.getPath().getName()), split.getSplitNumber(), split.getPath(),\n\t\t\t\tsplit.getStart(), split.getLength(), split.getHostnames())));\n\t\t}\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\t\r\n\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE + 1, tester.getOutput().size());\n\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\t\tObject lastElement = null;\n\t\tfor (Object line: tester.getOutput()) {\n\t\t\tlastElement = line;\n\n\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\n\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\tif (content == null) {\n\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t}\n\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t}\n\t\t}\n\n\t\t\r\n\t\tAssert.assertTrue(lastElement instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) lastElement).getTimestamp());\n\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t}\n","date":"2020-02-18 22:13:31","endLine":413,"groupId":"28241","id":8,"instanceNumber":2,"isCurCommit":0,"methodName":"testFileReadingOperatorWithEventTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/1e/783f1d33db46b2397d490d985fb5d5120bcc35.src","preCode":"\tpublic void testFileReadingOperatorWithEventTime() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\tfilesCreated.add(file.f0);\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tTypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);\n\n\t\tContinuousFileReaderOperator<String> reader = new ContinuousFileReaderOperator<>(format);\n\t\treader.setOutputType(typeInfo, new ExecutionConfig());\n\n\t\tOneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester =\n\t\t\tnew OneInputStreamOperatorTestHarness<>(reader);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.EventTime);\n\t\ttester.open();\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\treader.getRuntimeContext().getNumberOfParallelSubtasks());\n\n\t\t\r\n\t\tfor (FileInputSplit split: splits) {\n\t\t\ttester.processElement(new StreamRecord<>(new TimestampedFileInputSplit(\n\t\t\t\tmodTimes.get(split.getPath().getName()), split.getSplitNumber(), split.getPath(),\n\t\t\t\tsplit.getStart(), split.getLength(), split.getHostnames())));\n\t\t}\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\t\r\n\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE + 1, tester.getOutput().size());\n\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\t\tObject lastElement = null;\n\t\tfor (Object line: tester.getOutput()) {\n\t\t\tlastElement = line;\n\n\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\n\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\tif (content == null) {\n\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t}\n\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t}\n\t\t}\n\n\t\t\r\n\t\tAssert.assertTrue(lastElement instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) lastElement).getTimestamp());\n\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":325,"status":"M"}],"commitId":"468f90fd62ff61fefa553db8f819e4542de102a0","commitMessage":"@@@[FLINK-16056][runtime][tests] create ContinuousFileReaderOperator using factory only\nAlso drop test constructor and make the class package-private.\n","date":"2020-02-18 22:13:31","modifiedFileCount":"7","status":"M","submitter":"Roman Khachatryan"},{"authorTime":"2020-02-14 21:01:04","codes":[{"authorDate":"2020-02-11 15:49:00","commitOrder":5,"curCode":"\tpublic void testFileReadingOperatorWithIngestionTime() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\n\t\tfinal long watermarkInterval = 10;\n\n\t\tfinal OneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester = createHarness(format);\n\t\tSteppingMailboxProcessor localMailbox = createLocalMailbox(tester);\n\n\t\ttester.getExecutionConfig().setAutoWatermarkInterval(watermarkInterval);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.IngestionTime);\n\n\t\ttester.open();\n\t\tAssert.assertEquals(TimeCharacteristic.IngestionTime, tester.getTimeCharacteristic());\n\n\t\ttester.setProcessingTime(201);\n\n\t\t\r\n\t\tConcurrentLinkedQueue<Object> output = tester.getOutput();\n\t\twhile (output.isEmpty()) {\n\t\t\tlocalMailbox.runMailboxStep();\n\t\t}\n\t\tAssert.assertTrue(output.toString(), output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(200, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(301);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(300, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(401);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(400, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(501);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(500, ((Watermark) output.poll()).getTimestamp());\n\n\t\tAssert.assertTrue(output.isEmpty());\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\ttester.getExecutionConfig().getParallelism());\n\n\t\t\r\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\n\t\tlong lastSeenWatermark = Long.MIN_VALUE;\n\t\tint lineCounter = 0;\t\r\n\t\tint watermarkCounter = 0;\n\n\t\tfor (FileInputSplit split: splits) {\n\n\t\t\t\r\n\t\t\tlong nextTimestamp = tester.getProcessingTime() + watermarkInterval;\n\t\t\ttester.setProcessingTime(nextTimestamp);\n\n\t\t\t\r\n\t\t\tRunnableWithException runnableWithException = () -> tester.processElement(new StreamRecord<>(\n\t\t\t\t\tnew TimestampedFileInputSplit(modTimes.get(split.getPath().getName()),\n\t\t\t\t\t\t\tsplit.getSplitNumber(), split.getPath(), split.getStart(),\n\t\t\t\t\t\t\tsplit.getLength(), split.getHostnames())));\n\t\t\trunnableWithException.run();\n\n\t\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\twhile (tester.getOutput().isEmpty() || tester.getOutput().size() != (LINES_PER_FILE + 1)) {\n\t\t\t\tlocalMailbox.runMailboxStep();\n\t\t\t}\n\n\t\t\t\r\n\t\t\tfor (Object line: tester.getOutput()) {\n\n\t\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\t\t\t\t\tlineCounter++;\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp, element.getTimestamp());\n\n\t\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\t\tif (content == null) {\n\t\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t\t}\n\t\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t\t} else if (line instanceof Watermark) {\n\t\t\t\t\tlong watermark = ((Watermark) line).getTimestamp();\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp - (nextTimestamp % watermarkInterval), watermark);\n\t\t\t\t\tAssert.assertTrue(watermark > lastSeenWatermark);\n\t\t\t\t\twatermarkCounter++;\n\n\t\t\t\t\tlastSeenWatermark = watermark;\n\t\t\t\t} else {\n\t\t\t\t\tAssert.fail(\"Unknown element in the list.\");\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t\r\n\t\t\ttester.getOutput().clear();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE, lineCounter);\n\n\t\t\r\n\t\tAssert.assertEquals(splits.length, watermarkCounter);\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\n\t\t\r\n\t\tAssert.assertEquals(1, tester.getOutput().size());\n\t\tAssert.assertTrue(tester.getOutput().peek() instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) tester.getOutput().poll()).getTimestamp());\n\n\t\t\r\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\t}\n","date":"2020-02-21 18:59:32","endLine":328,"groupId":"102","id":9,"instanceNumber":1,"isCurCommit":0,"methodName":"testFileReadingOperatorWithIngestionTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/b6/2dcfff5b774a2310d71c3f48df922a7d15607a.src","preCode":"\tpublic void testFileReadingOperatorWithIngestionTime() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tfilesCreated.add(file.f0);\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tTypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);\n\n\t\tfinal long watermarkInterval = 10;\n\n\t\tfinal OneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester = createHarness(format);\n\t\tSteppingMailboxProcessor localMailbox = createLocalMailbox(tester);\n\n\t\ttester.getExecutionConfig().setAutoWatermarkInterval(watermarkInterval);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.IngestionTime);\n\n\t\ttester.open();\n\t\tAssert.assertEquals(TimeCharacteristic.IngestionTime, tester.getTimeCharacteristic());\n\n\t\ttester.setProcessingTime(201);\n\n\t\t\r\n\t\tConcurrentLinkedQueue<Object> output = tester.getOutput();\n\t\twhile (output.isEmpty()) {\n\t\t\tlocalMailbox.runMailboxStep();\n\t\t}\n\t\tAssert.assertTrue(output.toString(), output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(200, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(301);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(300, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(401);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(400, ((Watermark) output.poll()).getTimestamp());\n\n\t\ttester.setProcessingTime(501);\n\t\tAssert.assertTrue(output.peek() instanceof Watermark);\n\t\tAssert.assertEquals(500, ((Watermark) output.poll()).getTimestamp());\n\n\t\tAssert.assertTrue(output.isEmpty());\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\ttester.getExecutionConfig().getParallelism());\n\n\t\t\r\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\n\t\tlong lastSeenWatermark = Long.MIN_VALUE;\n\t\tint lineCounter = 0;\t\r\n\t\tint watermarkCounter = 0;\n\n\t\tfor (FileInputSplit split: splits) {\n\n\t\t\t\r\n\t\t\tlong nextTimestamp = tester.getProcessingTime() + watermarkInterval;\n\t\t\ttester.setProcessingTime(nextTimestamp);\n\n\t\t\t\r\n\t\t\tRunnableWithException runnableWithException = () -> tester.processElement(new StreamRecord<>(\n\t\t\t\t\tnew TimestampedFileInputSplit(modTimes.get(split.getPath().getName()),\n\t\t\t\t\t\t\tsplit.getSplitNumber(), split.getPath(), split.getStart(),\n\t\t\t\t\t\t\tsplit.getLength(), split.getHostnames())));\n\t\t\trunnableWithException.run();\n\n\t\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\twhile (tester.getOutput().isEmpty() || tester.getOutput().size() != (LINES_PER_FILE + 1)) {\n\t\t\t\tlocalMailbox.runMailboxStep();\n\t\t\t}\n\n\t\t\t\r\n\t\t\tfor (Object line: tester.getOutput()) {\n\n\t\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\t\t\t\t\tlineCounter++;\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp, element.getTimestamp());\n\n\t\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\t\tif (content == null) {\n\t\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t\t}\n\t\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t\t} else if (line instanceof Watermark) {\n\t\t\t\t\tlong watermark = ((Watermark) line).getTimestamp();\n\n\t\t\t\t\tAssert.assertEquals(nextTimestamp - (nextTimestamp % watermarkInterval), watermark);\n\t\t\t\t\tAssert.assertTrue(watermark > lastSeenWatermark);\n\t\t\t\t\twatermarkCounter++;\n\n\t\t\t\t\tlastSeenWatermark = watermark;\n\t\t\t\t} else {\n\t\t\t\t\tAssert.fail(\"Unknown element in the list.\");\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t\r\n\t\t\ttester.getOutput().clear();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE, lineCounter);\n\n\t\t\r\n\t\tAssert.assertEquals(splits.length, watermarkCounter);\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\n\t\t\r\n\t\tAssert.assertEquals(1, tester.getOutput().size());\n\t\tAssert.assertTrue(tester.getOutput().peek() instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) tester.getOutput().poll()).getTimestamp());\n\n\t\t\r\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":172,"status":"M"},{"authorDate":"2020-02-14 21:01:04","commitOrder":5,"curCode":"\tpublic void testFileReadingOperatorWithEventTime() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\tfilesCreated.add(file.f0);\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tTypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);\n\n\t\tOneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester = createHarness(format);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.EventTime);\n\t\ttester.open();\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\ttester.getExecutionConfig().getParallelism());\n\n\t\t\r\n\t\tfor (FileInputSplit split: splits) {\n\t\t\ttester.processElement(new StreamRecord<>(new TimestampedFileInputSplit(\n\t\t\t\tmodTimes.get(split.getPath().getName()), split.getSplitNumber(), split.getPath(),\n\t\t\t\tsplit.getStart(), split.getLength(), split.getHostnames())));\n\t\t}\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\t\r\n\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE + 1, tester.getOutput().size());\n\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\t\tObject lastElement = null;\n\t\tfor (Object line: tester.getOutput()) {\n\t\t\tlastElement = line;\n\n\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\n\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\tif (content == null) {\n\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t}\n\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t}\n\t\t}\n\n\t\t\r\n\t\tAssert.assertTrue(lastElement instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) lastElement).getTimestamp());\n\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t}\n","date":"2020-02-18 22:13:31","endLine":413,"groupId":"102","id":10,"instanceNumber":2,"isCurCommit":0,"methodName":"testFileReadingOperatorWithEventTime","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/1e/783f1d33db46b2397d490d985fb5d5120bcc35.src","preCode":"\tpublic void testFileReadingOperatorWithEventTime() throws Exception {\n\t\tString testBasePath = hdfsURI + \"/\" + UUID.randomUUID() + \"/\";\n\n\t\tSet<org.apache.hadoop.fs.Path> filesCreated = new HashSet<>();\n\t\tMap<String, Long> modTimes = new HashMap<>();\n\t\tMap<Integer, String> expectedFileContents = new HashMap<>();\n\t\tfor (int i = 0; i < NO_OF_FILES; i++) {\n\t\t\tTuple2<org.apache.hadoop.fs.Path, String> file = createFileAndFillWithData(testBasePath, \"file\", i, \"This is test line.\");\n\t\t\tmodTimes.put(file.f0.getName(), hdfs.getFileStatus(file.f0).getModificationTime());\n\t\t\tfilesCreated.add(file.f0);\n\t\t\texpectedFileContents.put(i, file.f1);\n\t\t}\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(testBasePath));\n\t\tTypeInformation<String> typeInfo = TypeExtractor.getInputFormatTypes(format);\n\n\t\tOneInputStreamOperatorTestHarness<TimestampedFileInputSplit, String> tester = createHarness(format);\n\t\ttester.setTimeCharacteristic(TimeCharacteristic.EventTime);\n\t\ttester.open();\n\n\t\t\r\n\t\tFileInputSplit[] splits = format.createInputSplits(\n\t\t\ttester.getExecutionConfig().getParallelism());\n\n\t\t\r\n\t\tfor (FileInputSplit split: splits) {\n\t\t\ttester.processElement(new StreamRecord<>(new TimestampedFileInputSplit(\n\t\t\t\tmodTimes.get(split.getPath().getName()), split.getSplitNumber(), split.getPath(),\n\t\t\t\tsplit.getStart(), split.getLength(), split.getHostnames())));\n\t\t}\n\n\t\t\r\n\t\tsynchronized (tester.getCheckpointLock()) {\n\t\t\ttester.close();\n\t\t}\n\n\t\t\r\n\t\t\r\n\t\t\r\n\n\t\tAssert.assertEquals(NO_OF_FILES * LINES_PER_FILE + 1, tester.getOutput().size());\n\n\t\tMap<Integer, List<String>> actualFileContents = new HashMap<>();\n\t\tObject lastElement = null;\n\t\tfor (Object line: tester.getOutput()) {\n\t\t\tlastElement = line;\n\n\t\t\tif (line instanceof StreamRecord) {\n\n\t\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\t\tStreamRecord<String> element = (StreamRecord<String>) line;\n\n\t\t\t\tint fileIdx = Character.getNumericValue(element.getValue().charAt(0));\n\t\t\t\tList<String> content = actualFileContents.get(fileIdx);\n\t\t\t\tif (content == null) {\n\t\t\t\t\tcontent = new ArrayList<>();\n\t\t\t\t\tactualFileContents.put(fileIdx, content);\n\t\t\t\t}\n\t\t\t\tcontent.add(element.getValue() + \"\\n\");\n\t\t\t}\n\t\t}\n\n\t\t\r\n\t\tAssert.assertTrue(lastElement instanceof Watermark);\n\t\tAssert.assertEquals(Long.MAX_VALUE, ((Watermark) lastElement).getTimestamp());\n\n\t\tAssert.assertEquals(expectedFileContents.size(), actualFileContents.size());\n\t\tfor (Integer fileIdx: expectedFileContents.keySet()) {\n\t\t\tAssert.assertTrue(\"file\" + fileIdx + \" not found\", actualFileContents.keySet().contains(fileIdx));\n\n\t\t\tList<String> cntnt = actualFileContents.get(fileIdx);\n\t\t\tCollections.sort(cntnt, new Comparator<String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic int compare(String o1, String o2) {\n\t\t\t\t\treturn getLineNo(o1) - getLineNo(o2);\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tStringBuilder cntntStr = new StringBuilder();\n\t\t\tfor (String line: cntnt) {\n\t\t\t\tcntntStr.append(line);\n\t\t\t}\n\t\t\tAssert.assertEquals(expectedFileContents.get(fileIdx), cntntStr.toString());\n\t\t}\n\n\t\tfor (org.apache.hadoop.fs.Path file: filesCreated) {\n\t\t\thdfs.delete(file, false);\n\t\t}\n\t}\n","realPath":"flink-fs-tests/src/test/java/org/apache/flink/hdfstests/ContinuousFileProcessingTest.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":325,"status":"N"}],"commitId":"ab642cb616fb909893e2c650b0b4c2aa10407e6d","commitMessage":"@@@[FLINK-14231][runtime] Introduce and use ProcessingTimeServiceAware to pass ProcessingTimeService to operator\n","date":"2020-02-21 18:59:32","modifiedFileCount":"27","status":"M","submitter":"sunhaibotb"}]
