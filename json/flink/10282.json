[{"authorTime":"2021-08-16 15:58:23","codes":[{"authorDate":"2021-08-16 15:58:23","commitOrder":1,"curCode":"    public void testAbortTransactionsOfPendingCheckpointsAfterFailure() throws Exception {\n        \r\n        \r\n        final Configuration config = new Configuration();\n        config.setString(StateBackendOptions.STATE_BACKEND, \"filesystem\");\n        final File checkpointDir = temp.newFolder();\n        config.setString(\n                CheckpointingOptions.CHECKPOINTS_DIRECTORY, checkpointDir.toURI().toString());\n        config.set(\n                ExecutionCheckpointingOptions.EXTERNALIZED_CHECKPOINT,\n                CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);\n        config.set(CheckpointingOptions.MAX_RETAINED_CHECKPOINTS, 2);\n        try {\n            executeWithMapper(new FailAsyncCheckpointMapper(1), config, \"firstPrefix\");\n        } catch (Exception e) {\n            assertThat(\n                    e.getCause().getCause().getMessage(),\n                    containsString(\"Exceeded checkpoint tolerable failure\"));\n        }\n        final File completedCheckpoint = TestUtils.getMostRecentCompletedCheckpoint(checkpointDir);\n\n        config.set(SavepointConfigOptions.SAVEPOINT_PATH, completedCheckpoint.toURI().toString());\n\n        \r\n        \r\n        failed.get().set(true);\n        executeWithMapper(\n                new FailingCheckpointMapper(failed, lastCheckpointedRecord), config, \"newPrefix\");\n        final List<ConsumerRecord<byte[], byte[]>> collectedRecords =\n                drainAllRecordsFromTopic(topic);\n        assertEquals(\n                deserializeValues(collectedRecords),\n                LongStream.range(1, lastCheckpointedRecord.get().get() + 1)\n                        .boxed()\n                        .collect(Collectors.toList()));\n    }\n","date":"2021-08-17 06:15:36","endLine":234,"groupId":"35322","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"testAbortTransactionsOfPendingCheckpointsAfterFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/3e/b2c18cc6886dcf8459256954ae7f28bc7417f3.src","preCode":"    public void testAbortTransactionsOfPendingCheckpointsAfterFailure() throws Exception {\n        \r\n        \r\n        final Configuration config = new Configuration();\n        config.setString(StateBackendOptions.STATE_BACKEND, \"filesystem\");\n        final File checkpointDir = temp.newFolder();\n        config.setString(\n                CheckpointingOptions.CHECKPOINTS_DIRECTORY, checkpointDir.toURI().toString());\n        config.set(\n                ExecutionCheckpointingOptions.EXTERNALIZED_CHECKPOINT,\n                CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);\n        config.set(CheckpointingOptions.MAX_RETAINED_CHECKPOINTS, 2);\n        try {\n            executeWithMapper(new FailAsyncCheckpointMapper(1), config, \"firstPrefix\");\n        } catch (Exception e) {\n            assertThat(\n                    e.getCause().getCause().getMessage(),\n                    containsString(\"Exceeded checkpoint tolerable failure\"));\n        }\n        final File completedCheckpoint = TestUtils.getMostRecentCompletedCheckpoint(checkpointDir);\n\n        config.set(SavepointConfigOptions.SAVEPOINT_PATH, completedCheckpoint.toURI().toString());\n\n        \r\n        \r\n        failed.get().set(true);\n        executeWithMapper(\n                new FailingCheckpointMapper(failed, lastCheckpointedRecord), config, \"newPrefix\");\n        final List<ConsumerRecord<byte[], byte[]>> collectedRecords =\n                drainAllRecordsFromTopic(topic);\n        assertEquals(\n                deserializeValues(collectedRecords),\n                LongStream.range(1, lastCheckpointedRecord.get().get() + 1)\n                        .boxed()\n                        .collect(Collectors.toList()));\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaSinkITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":199,"status":"B"},{"authorDate":"2021-08-16 15:58:23","commitOrder":1,"curCode":"    public void testAbortTransactionsAfterScaleInBeforeFirstCheckpoint() throws Exception {\n        \r\n        final Configuration config = new Configuration();\n        config.set(CoreOptions.DEFAULT_PARALLELISM, 5);\n        try {\n            executeWithMapper(new FailAsyncCheckpointMapper(0), config, null);\n        } catch (Exception e) {\n            assertThat(\n                    e.getCause().getCause().getMessage(),\n                    containsString(\"Exceeded checkpoint tolerable failure\"));\n        }\n        assertTrue(deserializeValues(drainAllRecordsFromTopic(topic)).isEmpty());\n\n        \r\n        config.set(CoreOptions.DEFAULT_PARALLELISM, 1);\n        failed.get().set(true);\n        executeWithMapper(\n                new FailingCheckpointMapper(failed, lastCheckpointedRecord), config, null);\n        final List<ConsumerRecord<byte[], byte[]>> collectedRecords =\n                drainAllRecordsFromTopic(topic);\n        assertEquals(\n                deserializeValues(collectedRecords),\n                LongStream.range(1, lastCheckpointedRecord.get().get() + 1)\n                        .boxed()\n                        .collect(Collectors.toList()));\n    }\n","date":"2021-08-17 06:15:36","endLine":262,"groupId":"35325","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"testAbortTransactionsAfterScaleInBeforeFirstCheckpoint","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/3e/b2c18cc6886dcf8459256954ae7f28bc7417f3.src","preCode":"    public void testAbortTransactionsAfterScaleInBeforeFirstCheckpoint() throws Exception {\n        \r\n        final Configuration config = new Configuration();\n        config.set(CoreOptions.DEFAULT_PARALLELISM, 5);\n        try {\n            executeWithMapper(new FailAsyncCheckpointMapper(0), config, null);\n        } catch (Exception e) {\n            assertThat(\n                    e.getCause().getCause().getMessage(),\n                    containsString(\"Exceeded checkpoint tolerable failure\"));\n        }\n        assertTrue(deserializeValues(drainAllRecordsFromTopic(topic)).isEmpty());\n\n        \r\n        config.set(CoreOptions.DEFAULT_PARALLELISM, 1);\n        failed.get().set(true);\n        executeWithMapper(\n                new FailingCheckpointMapper(failed, lastCheckpointedRecord), config, null);\n        final List<ConsumerRecord<byte[], byte[]>> collectedRecords =\n                drainAllRecordsFromTopic(topic);\n        assertEquals(\n                deserializeValues(collectedRecords),\n                LongStream.range(1, lastCheckpointedRecord.get().get() + 1)\n                        .boxed()\n                        .collect(Collectors.toList()));\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaSinkITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":237,"status":"B"}],"commitId":"94da2b587c133621bbdf39d36d55070a64605f56","commitMessage":"@@@[FLINK-23710][connectors/kafka] Move FLIP-143 KafkaSink to org.apache.kafka.connector.kafka.sink\n","date":"2021-08-17 06:15:36","modifiedFileCount":"4","status":"B","submitter":"Fabian Paul"},{"authorTime":"2021-08-31 03:14:52","codes":[{"authorDate":"2021-08-31 03:14:52","commitOrder":2,"curCode":"    public void testAbortTransactionsOfPendingCheckpointsAfterFailure() throws Exception {\n        \r\n        \r\n        final Configuration config = new Configuration();\n        config.setString(StateBackendOptions.STATE_BACKEND, \"filesystem\");\n        final File checkpointDir = temp.newFolder();\n        config.setString(\n                CheckpointingOptions.CHECKPOINTS_DIRECTORY, checkpointDir.toURI().toString());\n        config.set(\n                ExecutionCheckpointingOptions.EXTERNALIZED_CHECKPOINT,\n                CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);\n        config.set(CheckpointingOptions.MAX_RETAINED_CHECKPOINTS, 2);\n        try {\n            executeWithMapper(new FailAsyncCheckpointMapper(1), config, \"firstPrefix\");\n        } catch (Exception e) {\n            assertThat(\n                    e.getCause().getCause().getMessage(),\n                    containsString(\"Exceeded checkpoint tolerable failure\"));\n        }\n        final File completedCheckpoint = TestUtils.getMostRecentCompletedCheckpoint(checkpointDir);\n\n        config.set(SavepointConfigOptions.SAVEPOINT_PATH, completedCheckpoint.toURI().toString());\n\n        \r\n        \r\n        failed.get().set(true);\n        executeWithMapper(\n                new FailingCheckpointMapper(failed, lastCheckpointedRecord), config, \"newPrefix\");\n        final List<ConsumerRecord<byte[], byte[]>> collectedRecords =\n                drainAllRecordsFromTopic(topic);\n        assertThat(\n                deserializeValues(collectedRecords),\n                contains(\n                        LongStream.range(1, lastCheckpointedRecord.get().get() + 1)\n                                .boxed()\n                                .toArray()));\n    }\n","date":"2021-09-01 14:27:59","endLine":245,"groupId":"10282","id":3,"instanceNumber":1,"isCurCommit":0,"methodName":"testAbortTransactionsOfPendingCheckpointsAfterFailure","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/a7/995e296fd6d2d519b80726af3b9c7d8cc827ee.src","preCode":"    public void testAbortTransactionsOfPendingCheckpointsAfterFailure() throws Exception {\n        \r\n        \r\n        final Configuration config = new Configuration();\n        config.setString(StateBackendOptions.STATE_BACKEND, \"filesystem\");\n        final File checkpointDir = temp.newFolder();\n        config.setString(\n                CheckpointingOptions.CHECKPOINTS_DIRECTORY, checkpointDir.toURI().toString());\n        config.set(\n                ExecutionCheckpointingOptions.EXTERNALIZED_CHECKPOINT,\n                CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);\n        config.set(CheckpointingOptions.MAX_RETAINED_CHECKPOINTS, 2);\n        try {\n            executeWithMapper(new FailAsyncCheckpointMapper(1), config, \"firstPrefix\");\n        } catch (Exception e) {\n            assertThat(\n                    e.getCause().getCause().getMessage(),\n                    containsString(\"Exceeded checkpoint tolerable failure\"));\n        }\n        final File completedCheckpoint = TestUtils.getMostRecentCompletedCheckpoint(checkpointDir);\n\n        config.set(SavepointConfigOptions.SAVEPOINT_PATH, completedCheckpoint.toURI().toString());\n\n        \r\n        \r\n        failed.get().set(true);\n        executeWithMapper(\n                new FailingCheckpointMapper(failed, lastCheckpointedRecord), config, \"newPrefix\");\n        final List<ConsumerRecord<byte[], byte[]>> collectedRecords =\n                drainAllRecordsFromTopic(topic);\n        assertEquals(\n                deserializeValues(collectedRecords),\n                LongStream.range(1, lastCheckpointedRecord.get().get() + 1)\n                        .boxed()\n                        .collect(Collectors.toList()));\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaSinkITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":209,"status":"M"},{"authorDate":"2021-08-31 03:14:52","commitOrder":2,"curCode":"    public void testAbortTransactionsAfterScaleInBeforeFirstCheckpoint() throws Exception {\n        \r\n        final Configuration config = new Configuration();\n        config.set(CoreOptions.DEFAULT_PARALLELISM, 5);\n        try {\n            executeWithMapper(new FailAsyncCheckpointMapper(0), config, null);\n        } catch (Exception e) {\n            assertThat(\n                    e.getCause().getCause().getMessage(),\n                    containsString(\"Exceeded checkpoint tolerable failure\"));\n        }\n        assertTrue(deserializeValues(drainAllRecordsFromTopic(topic)).isEmpty());\n\n        \r\n        config.set(CoreOptions.DEFAULT_PARALLELISM, 1);\n        failed.get().set(true);\n        executeWithMapper(\n                new FailingCheckpointMapper(failed, lastCheckpointedRecord), config, null);\n        final List<ConsumerRecord<byte[], byte[]>> collectedRecords =\n                drainAllRecordsFromTopic(topic);\n        assertThat(\n                deserializeValues(collectedRecords),\n                contains(\n                        LongStream.range(1, lastCheckpointedRecord.get().get() + 1)\n                                .boxed()\n                                .toArray()));\n    }\n","date":"2021-09-01 14:27:59","endLine":274,"groupId":"10282","id":4,"instanceNumber":2,"isCurCommit":0,"methodName":"testAbortTransactionsAfterScaleInBeforeFirstCheckpoint","params":"()","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/a7/995e296fd6d2d519b80726af3b9c7d8cc827ee.src","preCode":"    public void testAbortTransactionsAfterScaleInBeforeFirstCheckpoint() throws Exception {\n        \r\n        final Configuration config = new Configuration();\n        config.set(CoreOptions.DEFAULT_PARALLELISM, 5);\n        try {\n            executeWithMapper(new FailAsyncCheckpointMapper(0), config, null);\n        } catch (Exception e) {\n            assertThat(\n                    e.getCause().getCause().getMessage(),\n                    containsString(\"Exceeded checkpoint tolerable failure\"));\n        }\n        assertTrue(deserializeValues(drainAllRecordsFromTopic(topic)).isEmpty());\n\n        \r\n        config.set(CoreOptions.DEFAULT_PARALLELISM, 1);\n        failed.get().set(true);\n        executeWithMapper(\n                new FailingCheckpointMapper(failed, lastCheckpointedRecord), config, null);\n        final List<ConsumerRecord<byte[], byte[]>> collectedRecords =\n                drainAllRecordsFromTopic(topic);\n        assertEquals(\n                deserializeValues(collectedRecords),\n                LongStream.range(1, lastCheckpointedRecord.get().get() + 1)\n                        .boxed()\n                        .collect(Collectors.toList()));\n    }\n","realPath":"flink-connectors/flink-connector-kafka/src/test/java/org/apache/flink/connector/kafka/sink/KafkaSinkITCase.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":248,"status":"M"}],"commitId":"9a51c129f3176ceb7d370058caea2033f6c00c3f","commitMessage":"@@@[FLINK-23678][tests] Re-enable KafkaSinkITCase and optimize it.\n","date":"2021-09-01 14:27:59","modifiedFileCount":"1","status":"M","submitter":"Arvid Heise"}]
