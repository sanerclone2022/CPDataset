[{"authorTime":"2020-09-23 20:37:35","codes":[{"authorDate":"2020-06-05 01:16:27","commitOrder":11,"curCode":"\tprivate <N, S extends State, SV> void migrateStateValues(\n\t\tStateDescriptor<S, SV> stateDesc,\n\t\tTuple2<ColumnFamilyHandle, RegisteredKeyValueStateBackendMetaInfo<N, SV>> stateMetaInfo) throws Exception {\n\n\t\tif (stateDesc.getType() == StateDescriptor.Type.MAP) {\n\t\t\tTypeSerializerSnapshot<SV> previousSerializerSnapshot = stateMetaInfo.f1.getPreviousStateSerializerSnapshot();\n\t\t\tcheckState(previousSerializerSnapshot != null, \"the previous serializer snapshot should exist.\");\n\t\t\tcheckState(previousSerializerSnapshot instanceof MapSerializerSnapshot, \"previous serializer snapshot should be a MapSerializerSnapshot.\");\n\n\t\t\tTypeSerializer<SV> newSerializer = stateMetaInfo.f1.getStateSerializer();\n\t\t\tcheckState(newSerializer instanceof MapSerializer, \"new serializer should be a MapSerializer.\");\n\n\t\t\tMapSerializer<?, ?> mapSerializer = (MapSerializer<?, ?>) newSerializer;\n\t\t\tMapSerializerSnapshot<?, ?> mapSerializerSnapshot = (MapSerializerSnapshot<?, ?>) previousSerializerSnapshot;\n\t\t\tif (!checkMapStateKeySchemaCompatibility(mapSerializerSnapshot, mapSerializer)) {\n\t\t\t\tthrow new StateMigrationException(\n\t\t\t\t\t\"The new serializer for a MapState requires state migration in order for the job to proceed, since the key schema has changed. However, migration for MapState currently only allows value schema evolutions.\");\n\t\t\t}\n\t\t}\n\n\t\tLOG.info(\n\t\t\t\"Performing state migration for state {} because the state serializer's schema, i.e. serialization format, has changed.\",\n\t\t\tstateDesc);\n\n\t\t\r\n\t\t\r\n\t\t\r\n\t\tStateFactory stateFactory = STATE_FACTORIES.get(stateDesc.getClass());\n\t\tif (stateFactory == null) {\n\t\t\tString message = String.format(\"State %s is not supported by %s\",\n\t\t\t\tstateDesc.getClass(), this.getClass());\n\t\t\tthrow new FlinkRuntimeException(message);\n\t\t}\n\t\tState state = stateFactory.createState(\n\t\t\tstateDesc,\n\t\t\tstateMetaInfo,\n\t\t\tRocksDBKeyedStateBackend.this);\n\t\tif (!(state instanceof AbstractRocksDBState)) {\n\t\t\tthrow new FlinkRuntimeException(\n\t\t\t\t\"State should be an AbstractRocksDBState but is \" + state);\n\t\t}\n\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tAbstractRocksDBState<?, ?, SV> rocksDBState = (AbstractRocksDBState<?, ?, SV>) state;\n\n\t\tSnapshot rocksDBSnapshot = db.getSnapshot();\n\t\ttry (\n\t\t\tRocksIteratorWrapper iterator = RocksDBOperationUtils.getRocksIterator(db, stateMetaInfo.f0, readOptions);\n\t\t\tRocksDBWriteBatchWrapper batchWriter = new RocksDBWriteBatchWrapper(db, getWriteOptions(), getWriteBatchSize())\n\t\t) {\n\t\t\titerator.seekToFirst();\n\n\t\t\tDataInputDeserializer serializedValueInput = new DataInputDeserializer();\n\t\t\tDataOutputSerializer migratedSerializedValueOutput = new DataOutputSerializer(512);\n\t\t\twhile (iterator.isValid()) {\n\t\t\t\tserializedValueInput.setBuffer(iterator.value());\n\n\t\t\t\trocksDBState.migrateSerializedValue(\n\t\t\t\t\tserializedValueInput,\n\t\t\t\t\tmigratedSerializedValueOutput,\n\t\t\t\t\tstateMetaInfo.f1.getPreviousStateSerializer(),\n\t\t\t\t\tstateMetaInfo.f1.getStateSerializer());\n\n\t\t\t\tbatchWriter.put(stateMetaInfo.f0, iterator.key(), migratedSerializedValueOutput.getCopyOfBuffer());\n\n\t\t\t\tmigratedSerializedValueOutput.clear();\n\t\t\t\titerator.next();\n\t\t\t}\n\t\t} finally {\n\t\t\tdb.releaseSnapshot(rocksDBSnapshot);\n\t\t\trocksDBSnapshot.close();\n\t\t}\n\t}\n","date":"2020-06-26 22:24:49","endLine":649,"groupId":"8974","id":1,"instanceNumber":1,"isCurCommit":0,"methodName":"migrateStateValues","params":"(StateDescriptor<S@SV>stateDesc@Tuple2<ColumnFamilyHandle@RegisteredKeyValueStateBackendMetaInfo<N@SV>>stateMetaInfo)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/f0/cce0b2ffd7ad2f5ef6fb8fa73a89c8fc0abf0f.src","preCode":"\tprivate <N, S extends State, SV> void migrateStateValues(\n\t\tStateDescriptor<S, SV> stateDesc,\n\t\tTuple2<ColumnFamilyHandle, RegisteredKeyValueStateBackendMetaInfo<N, SV>> stateMetaInfo) throws Exception {\n\n\t\tif (stateDesc.getType() == StateDescriptor.Type.MAP) {\n\t\t\tTypeSerializerSnapshot<SV> previousSerializerSnapshot = stateMetaInfo.f1.getPreviousStateSerializerSnapshot();\n\t\t\tcheckState(previousSerializerSnapshot != null, \"the previous serializer snapshot should exist.\");\n\t\t\tcheckState(previousSerializerSnapshot instanceof MapSerializerSnapshot, \"previous serializer snapshot should be a MapSerializerSnapshot.\");\n\n\t\t\tTypeSerializer<SV> newSerializer = stateMetaInfo.f1.getStateSerializer();\n\t\t\tcheckState(newSerializer instanceof MapSerializer, \"new serializer should be a MapSerializer.\");\n\n\t\t\tMapSerializer<?, ?> mapSerializer = (MapSerializer<?, ?>) newSerializer;\n\t\t\tMapSerializerSnapshot<?, ?> mapSerializerSnapshot = (MapSerializerSnapshot<?, ?>) previousSerializerSnapshot;\n\t\t\tif (!checkMapStateKeySchemaCompatibility(mapSerializerSnapshot, mapSerializer)) {\n\t\t\t\tthrow new StateMigrationException(\n\t\t\t\t\t\"The new serializer for a MapState requires state migration in order for the job to proceed, since the key schema has changed. However, migration for MapState currently only allows value schema evolutions.\");\n\t\t\t}\n\t\t}\n\n\t\tLOG.info(\n\t\t\t\"Performing state migration for state {} because the state serializer's schema, i.e. serialization format, has changed.\",\n\t\t\tstateDesc);\n\n\t\t\r\n\t\t\r\n\t\t\r\n\t\tStateFactory stateFactory = STATE_FACTORIES.get(stateDesc.getClass());\n\t\tif (stateFactory == null) {\n\t\t\tString message = String.format(\"State %s is not supported by %s\",\n\t\t\t\tstateDesc.getClass(), this.getClass());\n\t\t\tthrow new FlinkRuntimeException(message);\n\t\t}\n\t\tState state = stateFactory.createState(\n\t\t\tstateDesc,\n\t\t\tstateMetaInfo,\n\t\t\tRocksDBKeyedStateBackend.this);\n\t\tif (!(state instanceof AbstractRocksDBState)) {\n\t\t\tthrow new FlinkRuntimeException(\n\t\t\t\t\"State should be an AbstractRocksDBState but is \" + state);\n\t\t}\n\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tAbstractRocksDBState<?, ?, SV> rocksDBState = (AbstractRocksDBState<?, ?, SV>) state;\n\n\t\tSnapshot rocksDBSnapshot = db.getSnapshot();\n\t\ttry (\n\t\t\tRocksIteratorWrapper iterator = RocksDBOperationUtils.getRocksIterator(db, stateMetaInfo.f0, readOptions);\n\t\t\tRocksDBWriteBatchWrapper batchWriter = new RocksDBWriteBatchWrapper(db, getWriteOptions(), getWriteBatchSize())\n\t\t) {\n\t\t\titerator.seekToFirst();\n\n\t\t\tDataInputDeserializer serializedValueInput = new DataInputDeserializer();\n\t\t\tDataOutputSerializer migratedSerializedValueOutput = new DataOutputSerializer(512);\n\t\t\twhile (iterator.isValid()) {\n\t\t\t\tserializedValueInput.setBuffer(iterator.value());\n\n\t\t\t\trocksDBState.migrateSerializedValue(\n\t\t\t\t\tserializedValueInput,\n\t\t\t\t\tmigratedSerializedValueOutput,\n\t\t\t\t\tstateMetaInfo.f1.getPreviousStateSerializer(),\n\t\t\t\t\tstateMetaInfo.f1.getStateSerializer());\n\n\t\t\t\tbatchWriter.put(stateMetaInfo.f0, iterator.key(), migratedSerializedValueOutput.getCopyOfBuffer());\n\n\t\t\t\tmigratedSerializedValueOutput.clear();\n\t\t\t\titerator.next();\n\t\t\t}\n\t\t} finally {\n\t\t\tdb.releaseSnapshot(rocksDBSnapshot);\n\t\t\trocksDBSnapshot.close();\n\t\t}\n\t}\n","realPath":"flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBKeyedStateBackend.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":577,"status":"NB"},{"authorDate":"2020-09-23 20:37:35","commitOrder":11,"curCode":"\tprivate <N, SV, S extends State, IS extends S> IS createState(\n\t\t\t\t@Nonnull TypeSerializer<N> namespaceSerializer,\n\t\t\t\t@Nonnull StateDescriptor<S, SV> stateDesc)\n\t\t\tthrows Exception {\n\t\tStateFactory stateFactory = STATE_FACTORIES.get(stateDesc.getClass());\n\t\tif (stateFactory == null) {\n\t\t\tString message = String.format(\"State %s is not supported by %s\",\n\t\t\t\tstateDesc.getClass(), this.getClass());\n\t\t\tthrow new FlinkRuntimeException(message);\n\t\t}\n\t\treturn stateFactory.createState(keySerializer, namespaceSerializer, stateDesc);\n\t}\n","date":"2020-10-12 15:03:28","endLine":214,"groupId":"20603","id":2,"instanceNumber":2,"isCurCommit":0,"methodName":"createState","params":"(@NonnullTypeSerializer<N>namespaceSerializer@@NonnullStateDescriptor<S@SV>stateDesc)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/89/2adc9d7ea19165ed4949fc8e9023acd87c77fd.src","preCode":"\tprivate <N, SV, S extends State, IS extends S> IS createState(\n\t\t\t\t@Nonnull TypeSerializer<N> namespaceSerializer,\n\t\t\t\t@Nonnull StateDescriptor<S, SV> stateDesc)\n\t\t\tthrows Exception {\n\t\tStateFactory stateFactory = STATE_FACTORIES.get(stateDesc.getClass());\n\t\tif (stateFactory == null) {\n\t\t\tString message = String.format(\"State %s is not supported by %s\",\n\t\t\t\tstateDesc.getClass(), this.getClass());\n\t\t\tthrow new FlinkRuntimeException(message);\n\t\t}\n\t\treturn stateFactory.createState(keySerializer, namespaceSerializer, stateDesc);\n\t}\n","realPath":"flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/sorted/state/BatchExecutionKeyedStateBackend.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":203,"status":"B"}],"commitId":"1aea5f059cb1e0aee6168477391e6e17c9d23dc1","commitMessage":"@@@[FLINK-19474] Implement a state backend that holds a single key at a time\n\nThis commit introduces a SingleKeyStateBackend. This state backend is a\nsimplified version of a state backend that can be used in a BATCH\nruntime mode. It requires the input to be sorted.  as it only ever\nremembers the current key. If the key changes.  the current state is\ndiscarded. Moreover this state backend does not support checkpointing.\n","date":"2020-10-12 15:03:28","modifiedFileCount":"2","status":"M","submitter":"Dawid Wysakowicz"},{"authorTime":"2021-06-17 16:28:02","codes":[{"authorDate":"2021-06-17 16:28:02","commitOrder":12,"curCode":"    private <N, S extends State, SV> void migrateStateValues(\n            StateDescriptor<S, SV> stateDesc,\n            Tuple2<ColumnFamilyHandle, RegisteredKeyValueStateBackendMetaInfo<N, SV>> stateMetaInfo)\n            throws Exception {\n\n        if (stateDesc.getType() == StateDescriptor.Type.MAP) {\n            TypeSerializerSnapshot<SV> previousSerializerSnapshot =\n                    stateMetaInfo.f1.getPreviousStateSerializerSnapshot();\n            checkState(\n                    previousSerializerSnapshot != null,\n                    \"the previous serializer snapshot should exist.\");\n            checkState(\n                    previousSerializerSnapshot instanceof MapSerializerSnapshot,\n                    \"previous serializer snapshot should be a MapSerializerSnapshot.\");\n\n            TypeSerializer<SV> newSerializer = stateMetaInfo.f1.getStateSerializer();\n            checkState(\n                    newSerializer instanceof MapSerializer,\n                    \"new serializer should be a MapSerializer.\");\n\n            MapSerializer<?, ?> mapSerializer = (MapSerializer<?, ?>) newSerializer;\n            MapSerializerSnapshot<?, ?> mapSerializerSnapshot =\n                    (MapSerializerSnapshot<?, ?>) previousSerializerSnapshot;\n            if (!checkMapStateKeySchemaCompatibility(mapSerializerSnapshot, mapSerializer)) {\n                throw new StateMigrationException(\n                        \"The new serializer for a MapState requires state migration in order for the job to proceed, since the key schema has changed. However, migration for MapState currently only allows value schema evolutions.\");\n            }\n        }\n\n        LOG.info(\n                \"Performing state migration for state {} because the state serializer's schema, i.e. serialization format, has changed.\",\n                stateDesc);\n\n        \r\n        \r\n        \r\n        StateFactory stateFactory = STATE_FACTORIES.get(stateDesc.getType());\n        if (stateFactory == null) {\n            String message =\n                    String.format(\n                            \"State %s is not supported by %s\",\n                            stateDesc.getClass(), this.getClass());\n            throw new FlinkRuntimeException(message);\n        }\n        State state =\n                stateFactory.createState(stateDesc, stateMetaInfo, RocksDBKeyedStateBackend.this);\n        if (!(state instanceof AbstractRocksDBState)) {\n            throw new FlinkRuntimeException(\n                    \"State should be an AbstractRocksDBState but is \" + state);\n        }\n\n        @SuppressWarnings(\"unchecked\")\n        AbstractRocksDBState<?, ?, SV> rocksDBState = (AbstractRocksDBState<?, ?, SV>) state;\n\n        Snapshot rocksDBSnapshot = db.getSnapshot();\n        try (RocksIteratorWrapper iterator =\n                        RocksDBOperationUtils.getRocksIterator(db, stateMetaInfo.f0, readOptions);\n                RocksDBWriteBatchWrapper batchWriter =\n                        new RocksDBWriteBatchWrapper(db, getWriteOptions(), getWriteBatchSize())) {\n            iterator.seekToFirst();\n\n            DataInputDeserializer serializedValueInput = new DataInputDeserializer();\n            DataOutputSerializer migratedSerializedValueOutput = new DataOutputSerializer(512);\n            while (iterator.isValid()) {\n                serializedValueInput.setBuffer(iterator.value());\n\n                rocksDBState.migrateSerializedValue(\n                        serializedValueInput,\n                        migratedSerializedValueOutput,\n                        stateMetaInfo.f1.getPreviousStateSerializer(),\n                        stateMetaInfo.f1.getStateSerializer());\n\n                batchWriter.put(\n                        stateMetaInfo.f0,\n                        iterator.key(),\n                        migratedSerializedValueOutput.getCopyOfBuffer());\n\n                migratedSerializedValueOutput.clear();\n                iterator.next();\n            }\n        } finally {\n            db.releaseSnapshot(rocksDBSnapshot);\n            rocksDBSnapshot.close();\n        }\n    }\n","date":"2021-06-21 10:50:44","endLine":805,"groupId":"1092","id":3,"instanceNumber":1,"isCurCommit":1,"methodName":"migrateStateValues","params":"(StateDescriptor<S@SV>stateDesc@Tuple2<ColumnFamilyHandle@RegisteredKeyValueStateBackendMetaInfo<N@SV>>stateMetaInfo)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/ea/83250b5dd4dea8c26ee9f3bea8d955cce239d0.src","preCode":"    private <N, S extends State, SV> void migrateStateValues(\n            StateDescriptor<S, SV> stateDesc,\n            Tuple2<ColumnFamilyHandle, RegisteredKeyValueStateBackendMetaInfo<N, SV>> stateMetaInfo)\n            throws Exception {\n\n        if (stateDesc.getType() == StateDescriptor.Type.MAP) {\n            TypeSerializerSnapshot<SV> previousSerializerSnapshot =\n                    stateMetaInfo.f1.getPreviousStateSerializerSnapshot();\n            checkState(\n                    previousSerializerSnapshot != null,\n                    \"the previous serializer snapshot should exist.\");\n            checkState(\n                    previousSerializerSnapshot instanceof MapSerializerSnapshot,\n                    \"previous serializer snapshot should be a MapSerializerSnapshot.\");\n\n            TypeSerializer<SV> newSerializer = stateMetaInfo.f1.getStateSerializer();\n            checkState(\n                    newSerializer instanceof MapSerializer,\n                    \"new serializer should be a MapSerializer.\");\n\n            MapSerializer<?, ?> mapSerializer = (MapSerializer<?, ?>) newSerializer;\n            MapSerializerSnapshot<?, ?> mapSerializerSnapshot =\n                    (MapSerializerSnapshot<?, ?>) previousSerializerSnapshot;\n            if (!checkMapStateKeySchemaCompatibility(mapSerializerSnapshot, mapSerializer)) {\n                throw new StateMigrationException(\n                        \"The new serializer for a MapState requires state migration in order for the job to proceed, since the key schema has changed. However, migration for MapState currently only allows value schema evolutions.\");\n            }\n        }\n\n        LOG.info(\n                \"Performing state migration for state {} because the state serializer's schema, i.e. serialization format, has changed.\",\n                stateDesc);\n\n        \r\n        \r\n        \r\n        StateFactory stateFactory = STATE_FACTORIES.get(stateDesc.getClass());\n        if (stateFactory == null) {\n            String message =\n                    String.format(\n                            \"State %s is not supported by %s\",\n                            stateDesc.getClass(), this.getClass());\n            throw new FlinkRuntimeException(message);\n        }\n        State state =\n                stateFactory.createState(stateDesc, stateMetaInfo, RocksDBKeyedStateBackend.this);\n        if (!(state instanceof AbstractRocksDBState)) {\n            throw new FlinkRuntimeException(\n                    \"State should be an AbstractRocksDBState but is \" + state);\n        }\n\n        @SuppressWarnings(\"unchecked\")\n        AbstractRocksDBState<?, ?, SV> rocksDBState = (AbstractRocksDBState<?, ?, SV>) state;\n\n        Snapshot rocksDBSnapshot = db.getSnapshot();\n        try (RocksIteratorWrapper iterator =\n                        RocksDBOperationUtils.getRocksIterator(db, stateMetaInfo.f0, readOptions);\n                RocksDBWriteBatchWrapper batchWriter =\n                        new RocksDBWriteBatchWrapper(db, getWriteOptions(), getWriteBatchSize())) {\n            iterator.seekToFirst();\n\n            DataInputDeserializer serializedValueInput = new DataInputDeserializer();\n            DataOutputSerializer migratedSerializedValueOutput = new DataOutputSerializer(512);\n            while (iterator.isValid()) {\n                serializedValueInput.setBuffer(iterator.value());\n\n                rocksDBState.migrateSerializedValue(\n                        serializedValueInput,\n                        migratedSerializedValueOutput,\n                        stateMetaInfo.f1.getPreviousStateSerializer(),\n                        stateMetaInfo.f1.getStateSerializer());\n\n                batchWriter.put(\n                        stateMetaInfo.f0,\n                        iterator.key(),\n                        migratedSerializedValueOutput.getCopyOfBuffer());\n\n                migratedSerializedValueOutput.clear();\n                iterator.next();\n            }\n        } finally {\n            db.releaseSnapshot(rocksDBSnapshot);\n            rocksDBSnapshot.close();\n        }\n    }\n","realPath":"flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBKeyedStateBackend.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":721,"status":"M"},{"authorDate":"2021-06-17 16:28:02","commitOrder":12,"curCode":"    private <N, SV, S extends State, IS extends S> IS createState(\n            @Nonnull TypeSerializer<N> namespaceSerializer,\n            @Nonnull StateDescriptor<S, SV> stateDesc)\n            throws Exception {\n        StateFactory stateFactory = STATE_FACTORIES.get(stateDesc.getType());\n        if (stateFactory == null) {\n            String message =\n                    String.format(\n                            \"State %s is not supported by %s\",\n                            stateDesc.getClass(), this.getClass());\n            throw new FlinkRuntimeException(message);\n        }\n        return stateFactory.createState(keySerializer, namespaceSerializer, stateDesc);\n    }\n","date":"2021-06-21 10:50:44","endLine":238,"groupId":"1092","id":4,"instanceNumber":2,"isCurCommit":1,"methodName":"createState","params":"(@NonnullTypeSerializer<N>namespaceSerializer@@NonnullStateDescriptor<S@SV>stateDesc)","path":"/mnt/clonedata/CloneManagementServer/ManagementServer/consistResult/result-flink-10-0.7/blobInfo/CC_OUT/blobs/a4/58d4220af3c04cb40e90c2e26d5ea17098691e.src","preCode":"    private <N, SV, S extends State, IS extends S> IS createState(\n            @Nonnull TypeSerializer<N> namespaceSerializer,\n            @Nonnull StateDescriptor<S, SV> stateDesc)\n            throws Exception {\n        StateFactory stateFactory = STATE_FACTORIES.get(stateDesc.getClass());\n        if (stateFactory == null) {\n            String message =\n                    String.format(\n                            \"State %s is not supported by %s\",\n                            stateDesc.getClass(), this.getClass());\n            throw new FlinkRuntimeException(message);\n        }\n        return stateFactory.createState(keySerializer, namespaceSerializer, stateDesc);\n    }\n","realPath":"flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/sorted/state/BatchExecutionKeyedStateBackend.java","repoName":"flink","snippetEndLine":0,"snippetStartLine":0,"startLine":225,"status":"M"}],"commitId":"73c103b6b117fe3996eedfb9d04e926f00c70996","commitMessage":"@@@[FLINK-23018][state] Enable state factories to handle extended state descriptors\n","date":"2021-06-21 10:50:44","modifiedFileCount":"7","status":"M","submitter":"Yun Tang"}]
